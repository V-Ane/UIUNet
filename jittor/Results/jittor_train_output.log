nohup: ignoring input
[38;5;2m[i 0712 21:55:58.228368 08 compiler.py:944] Jittor(1.3.1.37) src: /root/miniconda3/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0712 21:55:58.231579 08 compiler.py:945] g++ at /usr/bin/g++(7.5.0)[m
[38;5;2m[i 0712 21:55:58.231663 08 compiler.py:946] cache_path: /root/.cache/jittor/jt1.3.1/g++7.5.0/py3.7.9/Linux-5.4.0-12x90/IntelRXeonRPlax40/default[m
[38;5;2m[i 0712 21:55:58.235577 08 __init__.py:372] Found nvcc(11.3.109) at /usr/local/cuda/bin/nvcc.[m
[38;5;2m[i 0712 21:55:58.239243 08 __init__.py:372] Found addr2line(2.30) at /usr/bin/addr2line.[m
[38;5;2m[i 0712 21:55:58.364613 08 compiler.py:997] cuda key:cu11.3.109_sm_75[m
[38;5;2m[i 0712 21:55:58.560661 08 __init__.py:187] Total mem: 375.80GB, using 16 procs for compiling.[m
[38;5;2m[i 0712 21:55:58.655891 08 jit_compiler.cc:27] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0712 21:55:58.823119 08 init.cc:62] Found cuda archs: [75,][m
[38;5;2m[i 0712 21:55:58.838487 08 compile_extern.py:497] mpicc not found, distribution disabled.[m
[38;5;2m[i 0712 21:55:58.884064 08 compile_extern.py:29] found /usr/local/cuda/include/cublas.h[m
[38;5;2m[i 0712 21:55:58.891178 08 compile_extern.py:29] found /usr/local/cuda/lib64/libcublas.so[m
[38;5;2m[i 0712 21:55:58.891290 08 compile_extern.py:29] found /usr/local/cuda/lib64/libcublasLt.so.11[m
[38;5;2m[i 0712 21:55:59.408629 08 compile_extern.py:29] found /usr/include/cudnn.h[m
[38;5;2m[i 0712 21:55:59.440589 08 compile_extern.py:29] found /usr/lib/x86_64-linux-gnu/libcudnn.so.8[m
[38;5;2m[i 0712 21:55:59.440729 08 compile_extern.py:29] found /usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8[m
[38;5;2m[i 0712 21:55:59.443825 08 compile_extern.py:29] found /usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8[m
[38;5;2m[i 0712 21:55:59.444399 08 compile_extern.py:29] found /usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8[m
[38;5;2m[i 0712 21:55:59.483168 08 compile_extern.py:29] found /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8[m
[38;5;2m[i 0712 21:55:59.995779 08 compile_extern.py:29] found /usr/local/cuda/include/curand.h[m
[38;5;2m[i 0712 21:56:00.038810 08 compile_extern.py:29] found /usr/local/cuda/lib64/libcurand.so[m
[38;5;2m[i 0712 21:56:01.253526 08 cuda_flags.cc:32] CUDA enabled.[m
---
train images:  1052
train labels:  1052
---
---
---
test images:  452
test labels:  452
---
---
---define optimizer...
---start training...
l0: 0.678625, l1: 0.951113, l2: 0.676870, l3: 0.575573, l4: 0.670939, l5: 0.855672, l6: 1.050125
[epoch:   1/1000, batch:     4/ 1052, ite: 1] train loss: 5.458918, tar: 0.678625 
l0: 0.358478, l1: 0.705840, l2: 0.490251, l3: 0.296571, l4: 0.277298, l5: 0.221514, l6: 0.232511
[epoch:   1/1000, batch:     8/ 1052, ite: 2] train loss: 4.020690, tar: 0.518551 
l0: 0.158512, l1: 0.469481, l2: 0.348949, l3: 0.158924, l4: 0.072097, l5: 0.075760, l6: 0.025802
[epoch:   1/1000, batch:    12/ 1052, ite: 3] train loss: 3.116969, tar: 0.398538 
l0: 0.078955, l1: 0.334478, l2: 0.224098, l3: 0.071581, l4: 0.019999, l5: 0.014408, l6: 0.010256
[epoch:   1/1000, batch:    16/ 1052, ite: 4] train loss: 2.526170, tar: 0.318643 
l0: 0.045906, l1: 0.214429, l2: 0.153391, l3: 0.033922, l4: 0.007755, l5: 0.007725, l6: 0.005128
[epoch:   1/1000, batch:    20/ 1052, ite: 5] train loss: 2.114588, tar: 0.264095 
l0: 0.031226, l1: 0.154250, l2: 0.106308, l3: 0.023631, l4: 0.011639, l5: 0.012359, l6: 0.011386
[epoch:   1/1000, batch:    24/ 1052, ite: 6] train loss: 1.820623, tar: 0.225284 
l0: 0.021700, l1: 0.101185, l2: 0.069841, l3: 0.016549, l4: 0.012054, l5: 0.010798, l6: 0.010724
[epoch:   1/1000, batch:    28/ 1052, ite: 7] train loss: 1.595227, tar: 0.196200 
l0: 0.010694, l1: 0.068961, l2: 0.051222, l3: 0.008423, l4: 0.004554, l5: 0.003927, l6: 0.003695
[epoch:   1/1000, batch:    32/ 1052, ite: 8] train loss: 1.414758, tar: 0.173012 
l0: 0.007928, l1: 0.056620, l2: 0.037505, l3: 0.006966, l4: 0.005105, l5: 0.004924, l6: 0.005662
[epoch:   1/1000, batch:    36/ 1052, ite: 9] train loss: 1.271420, tar: 0.154669 
l0: 0.008864, l1: 0.042730, l2: 0.027973, l3: 0.006228, l4: 0.006072, l5: 0.007321, l6: 0.008103
[epoch:   1/1000, batch:    40/ 1052, ite: 10] train loss: 1.155007, tar: 0.140089 
l0: 0.005337, l1: 0.038201, l2: 0.021739, l3: 0.004363, l4: 0.005501, l5: 0.003941, l6: 0.004600
[epoch:   1/1000, batch:    44/ 1052, ite: 11] train loss: 1.057614, tar: 0.127839 
l0: 0.005578, l1: 0.024780, l2: 0.017109, l3: 0.004961, l4: 0.004770, l5: 0.005312, l6: 0.005704
[epoch:   1/1000, batch:    48/ 1052, ite: 12] train loss: 0.975164, tar: 0.117650 
l0: 0.006414, l1: 0.023303, l2: 0.016031, l3: 0.005529, l4: 0.009659, l5: 0.007147, l6: 0.009853
[epoch:   1/1000, batch:    52/ 1052, ite: 13] train loss: 0.906146, tar: 0.109094 
l0: 0.009318, l1: 0.020883, l2: 0.015344, l3: 0.008529, l4: 0.010296, l5: 0.012749, l6: 0.015906
[epoch:   1/1000, batch:    56/ 1052, ite: 14] train loss: 0.848066, tar: 0.101967 
l0: 0.008004, l1: 0.015194, l2: 0.013022, l3: 0.008062, l4: 0.009167, l5: 0.010456, l6: 0.013389
[epoch:   1/1000, batch:    60/ 1052, ite: 15] train loss: 0.796681, tar: 0.095703 
l0: 0.005197, l1: 0.012845, l2: 0.009650, l3: 0.005362, l4: 0.006259, l5: 0.007219, l6: 0.010316
[epoch:   1/1000, batch:    64/ 1052, ite: 16] train loss: 0.750442, tar: 0.090046 
l0: 0.005166, l1: 0.011945, l2: 0.010033, l3: 0.004364, l4: 0.006093, l5: 0.007326, l6: 0.008182
[epoch:   1/1000, batch:    68/ 1052, ite: 17] train loss: 0.709422, tar: 0.085053 
l0: 0.010757, l1: 0.013758, l2: 0.012535, l3: 0.010648, l4: 0.014930, l5: 0.019928, l6: 0.023595
[epoch:   1/1000, batch:    72/ 1052, ite: 18] train loss: 0.675907, tar: 0.080926 
l0: 0.005338, l1: 0.009305, l2: 0.007986, l3: 0.005370, l4: 0.006765, l5: 0.006612, l6: 0.009745
[epoch:   1/1000, batch:    76/ 1052, ite: 19] train loss: 0.643024, tar: 0.076947 
l0: 0.003339, l1: 0.007377, l2: 0.006263, l3: 0.003596, l4: 0.003893, l5: 0.004599, l6: 0.005918
[epoch:   1/1000, batch:    80/ 1052, ite: 20] train loss: 0.612622, tar: 0.073267 
l0: 0.011422, l1: 0.007697, l2: 0.007129, l3: 0.009411, l4: 0.016094, l5: 0.022064, l6: 0.023367
[epoch:   1/1000, batch:   160/ 1052, ite: 40] train loss: 0.335312, tar: 0.040096 
l0: 0.003090, l1: 0.003395, l2: 0.002615, l3: 0.003367, l4: 0.004830, l5: 0.004235, l6: 0.006109
[epoch:   1/1000, batch:   240/ 1052, ite: 60] train loss: 0.239028, tar: 0.028488 
l0: 0.003835, l1: 0.003506, l2: 0.003064, l3: 0.003056, l4: 0.004582, l5: 0.006551, l6: 0.008543
[epoch:   1/1000, batch:   320/ 1052, ite: 80] train loss: 0.187284, tar: 0.022275 
l0: 0.002040, l1: 0.001939, l2: 0.002029, l3: 0.001910, l4: 0.002891, l5: 0.004123, l6: 0.004755
[epoch:   1/1000, batch:   400/ 1052, ite: 100] train loss: 0.156596, tar: 0.018581 
l0: 0.002751, l1: 0.002623, l2: 0.002521, l3: 0.002449, l4: 0.004764, l5: 0.006387, l6: 0.008106
[epoch:   1/1000, batch:   480/ 1052, ite: 120] train loss: 0.136395, tar: 0.016121 
l0: 0.002423, l1: 0.002697, l2: 0.002788, l3: 0.002216, l4: 0.003116, l5: 0.005083, l6: 0.007844
[epoch:   1/1000, batch:   560/ 1052, ite: 140] train loss: 0.121312, tar: 0.014270 
l0: 0.002336, l1: 0.002496, l2: 0.002484, l3: 0.002647, l4: 0.002481, l5: 0.002882, l6: 0.003370
[epoch:   1/1000, batch:   640/ 1052, ite: 160] train loss: 0.110928, tar: 0.013015 
l0: 0.003134, l1: 0.003630, l2: 0.003749, l3: 0.003224, l4: 0.003457, l5: 0.005218, l6: 0.005726
[epoch:   1/1000, batch:   720/ 1052, ite: 180] train loss: 0.101233, tar: 0.011834 
l0: 0.001304, l1: 0.001262, l2: 0.001380, l3: 0.001520, l4: 0.001702, l5: 0.003159, l6: 0.004779
[epoch:   1/1000, batch:   800/ 1052, ite: 200] train loss: 0.093402, tar: 0.010872 
l0: 0.003252, l1: 0.002642, l2: 0.003081, l3: 0.003147, l4: 0.004763, l5: 0.006416, l6: 0.008008
[epoch:   1/1000, batch:   880/ 1052, ite: 220] train loss: 0.087976, tar: 0.010202 
l0: 0.003955, l1: 0.003540, l2: 0.003464, l3: 0.002928, l4: 0.005068, l5: 0.008520, l6: 0.011970
[epoch:   1/1000, batch:   960/ 1052, ite: 240] train loss: 0.085852, tar: 0.009944 
l0: 0.002572, l1: 0.002157, l2: 0.002047, l3: 0.002625, l4: 0.005207, l5: 0.007426, l6: 0.009272
[epoch:   1/1000, batch:  1040/ 1052, ite: 260] train loss: 0.081441, tar: 0.009399 
[38;5;3m[w 0712 21:57:21.183373 08 cudnn_conv_Tx_float32__Ty_float32__Tw_float32__XFORMAT_abcd__WFORMAT_oihw__YFORMAT_abcd__J___hash_798cb5ed49dadaa2_op.cc:403] forward_ algorithm cache is full[m
[Epoch 1/1000] Test loss: 0.033, Test target loss: 0.003
l0: 0.003399, l1: 0.002779, l2: 0.002949, l3: 0.003365, l4: 0.004723, l5: 0.007207, l6: 0.009608
[epoch:   2/1000, batch:    68/ 1052, ite: 280] train loss: 0.077599, tar: 0.008925 
l0: 0.001458, l1: 0.001392, l2: 0.001611, l3: 0.001796, l4: 0.001546, l5: 0.003107, l6: 0.003474
[epoch:   2/1000, batch:   148/ 1052, ite: 300] train loss: 0.074113, tar: 0.008506 
l0: 0.017281, l1: 0.011601, l2: 0.014424, l3: 0.014746, l4: 0.015057, l5: 0.014330, l6: 0.034518
[epoch:   2/1000, batch:   228/ 1052, ite: 320] train loss: 0.071450, tar: 0.008195 
l0: 0.000955, l1: 0.000823, l2: 0.000783, l3: 0.000863, l4: 0.001553, l5: 0.002594, l6: 0.004035
[epoch:   2/1000, batch:   308/ 1052, ite: 340] train loss: 0.068699, tar: 0.007863 
l0: 0.001571, l1: 0.001349, l2: 0.001602, l3: 0.001393, l4: 0.002199, l5: 0.002656, l6: 0.003793
[epoch:   2/1000, batch:   388/ 1052, ite: 360] train loss: 0.066551, tar: 0.007612 
l0: 0.002097, l1: 0.001670, l2: 0.002259, l3: 0.001763, l4: 0.001798, l5: 0.003670, l6: 0.005159
[epoch:   2/1000, batch:   468/ 1052, ite: 380] train loss: 0.064152, tar: 0.007334 
l0: 0.002746, l1: 0.002322, l2: 0.002790, l3: 0.002706, l4: 0.004223, l5: 0.006819, l6: 0.006051
[epoch:   2/1000, batch:   548/ 1052, ite: 400] train loss: 0.061776, tar: 0.007053 
l0: 0.001891, l1: 0.001747, l2: 0.001887, l3: 0.002149, l4: 0.002328, l5: 0.003967, l6: 0.004562
[epoch:   2/1000, batch:   628/ 1052, ite: 420] train loss: 0.059897, tar: 0.006825 
l0: 0.016633, l1: 0.017557, l2: 0.016569, l3: 0.019324, l4: 0.016701, l5: 0.016868, l6: 0.015536
[epoch:   2/1000, batch:   708/ 1052, ite: 440] train loss: 0.058513, tar: 0.006673 
l0: 0.001016, l1: 0.001318, l2: 0.001128, l3: 0.000941, l4: 0.001233, l5: 0.001949, l6: 0.002212
[epoch:   2/1000, batch:   788/ 1052, ite: 460] train loss: 0.056788, tar: 0.006470 
l0: 0.000842, l1: 0.000827, l2: 0.001096, l3: 0.001080, l4: 0.001193, l5: 0.001944, l6: 0.002089
[epoch:   2/1000, batch:   868/ 1052, ite: 480] train loss: 0.055123, tar: 0.006277 
l0: 0.000882, l1: 0.000806, l2: 0.001343, l3: 0.001153, l4: 0.001242, l5: 0.002376, l6: 0.002725
[epoch:   2/1000, batch:   948/ 1052, ite: 500] train loss: 0.053691, tar: 0.006108 
l0: 0.001247, l1: 0.001356, l2: 0.001240, l3: 0.001553, l4: 0.001784, l5: 0.002168, l6: 0.002728
[epoch:   2/1000, batch:  1028/ 1052, ite: 520] train loss: 0.052345, tar: 0.005952 
[Epoch 2/1000] Test loss: 0.017, Test target loss: 0.002
l0: 0.001196, l1: 0.001086, l2: 0.001139, l3: 0.001179, l4: 0.001603, l5: 0.002504, l6: 0.003791
[epoch:   3/1000, batch:    56/ 1052, ite: 540] train loss: 0.050949, tar: 0.005789 
l0: 0.000966, l1: 0.000882, l2: 0.000836, l3: 0.000871, l4: 0.001223, l5: 0.001799, l6: 0.003037
[epoch:   3/1000, batch:   136/ 1052, ite: 560] train loss: 0.049653, tar: 0.005635 
l0: 0.001212, l1: 0.000920, l2: 0.001166, l3: 0.001416, l4: 0.001808, l5: 0.002645, l6: 0.003450
[epoch:   3/1000, batch:   216/ 1052, ite: 580] train loss: 0.048506, tar: 0.005499 
l0: 0.003388, l1: 0.002699, l2: 0.003092, l3: 0.003050, l4: 0.003699, l5: 0.004148, l6: 0.005636
[epoch:   3/1000, batch:   296/ 1052, ite: 600] train loss: 0.047359, tar: 0.005367 
l0: 0.002097, l1: 0.001760, l2: 0.002041, l3: 0.002503, l4: 0.002286, l5: 0.003026, l6: 0.004690
[epoch:   3/1000, batch:   376/ 1052, ite: 620] train loss: 0.046332, tar: 0.005248 
l0: 0.007495, l1: 0.004991, l2: 0.007638, l3: 0.008678, l4: 0.009144, l5: 0.011783, l6: 0.009856
[epoch:   3/1000, batch:   456/ 1052, ite: 640] train loss: 0.045357, tar: 0.005135 
l0: 0.001707, l1: 0.001591, l2: 0.001822, l3: 0.001849, l4: 0.001765, l5: 0.002854, l6: 0.002838
[epoch:   3/1000, batch:   536/ 1052, ite: 660] train loss: 0.044490, tar: 0.005030 
l0: 0.001213, l1: 0.001188, l2: 0.001241, l3: 0.001740, l4: 0.001467, l5: 0.002636, l6: 0.002469
[epoch:   3/1000, batch:   616/ 1052, ite: 680] train loss: 0.043631, tar: 0.004930 
l0: 0.001107, l1: 0.001102, l2: 0.000875, l3: 0.001093, l4: 0.001689, l5: 0.004481, l6: 0.003648
[epoch:   3/1000, batch:   696/ 1052, ite: 700] train loss: 0.042742, tar: 0.004826 
l0: 0.016353, l1: 0.012516, l2: 0.017099, l3: 0.019631, l4: 0.014905, l5: 0.009789, l6: 0.010297
[epoch:   3/1000, batch:   776/ 1052, ite: 720] train loss: 0.042237, tar: 0.004778 
l0: 0.000822, l1: 0.001898, l2: 0.001165, l3: 0.000932, l4: 0.001315, l5: 0.001593, l6: 0.002616
[epoch:   3/1000, batch:   856/ 1052, ite: 740] train loss: 0.041559, tar: 0.004701 
l0: 0.000708, l1: 0.000832, l2: 0.000792, l3: 0.001032, l4: 0.001038, l5: 0.001728, l6: 0.001786
[epoch:   3/1000, batch:   936/ 1052, ite: 760] train loss: 0.041038, tar: 0.004636 
l0: 0.000925, l1: 0.000842, l2: 0.000954, l3: 0.001403, l4: 0.001597, l5: 0.002822, l6: 0.001827
[epoch:   3/1000, batch:  1016/ 1052, ite: 780] train loss: 0.040499, tar: 0.004573 
[Epoch 3/1000] Test loss: 0.026, Test target loss: 0.004
l0: 0.001937, l1: 0.002577, l2: 0.001877, l3: 0.002392, l4: 0.002301, l5: 0.003159, l6: 0.004330
[epoch:   4/1000, batch:    44/ 1052, ite: 800] train loss: 0.039935, tar: 0.004509 
l0: 0.001118, l1: 0.001464, l2: 0.001146, l3: 0.001294, l4: 0.001924, l5: 0.002241, l6: 0.003657
[epoch:   4/1000, batch:   124/ 1052, ite: 820] train loss: 0.039441, tar: 0.004450 
l0: 0.001357, l1: 0.001627, l2: 0.001380, l3: 0.001567, l4: 0.001966, l5: 0.002250, l6: 0.002392
[epoch:   4/1000, batch:   204/ 1052, ite: 840] train loss: 0.038876, tar: 0.004382 
l0: 0.001646, l1: 0.001383, l2: 0.001698, l3: 0.001661, l4: 0.002835, l5: 0.004447, l6: 0.005145
[epoch:   4/1000, batch:   284/ 1052, ite: 860] train loss: 0.038380, tar: 0.004324 
l0: 0.001389, l1: 0.001031, l2: 0.001240, l3: 0.001701, l4: 0.002282, l5: 0.003260, l6: 0.003720
[epoch:   4/1000, batch:   364/ 1052, ite: 880] train loss: 0.037791, tar: 0.004253 
l0: 0.000973, l1: 0.000895, l2: 0.001524, l3: 0.001230, l4: 0.001256, l5: 0.001620, l6: 0.002469
[epoch:   4/1000, batch:   444/ 1052, ite: 900] train loss: 0.037259, tar: 0.004192 
l0: 0.001032, l1: 0.001020, l2: 0.001152, l3: 0.001199, l4: 0.001528, l5: 0.002222, l6: 0.002476
[epoch:   4/1000, batch:   524/ 1052, ite: 920] train loss: 0.036916, tar: 0.004154 
l0: 0.004153, l1: 0.003920, l2: 0.004100, l3: 0.004169, l4: 0.003061, l5: 0.005161, l6: 0.005838
[epoch:   4/1000, batch:   604/ 1052, ite: 940] train loss: 0.036488, tar: 0.004105 
l0: 0.001181, l1: 0.001147, l2: 0.001497, l3: 0.001780, l4: 0.002208, l5: 0.002213, l6: 0.002642
[epoch:   4/1000, batch:   684/ 1052, ite: 960] train loss: 0.036001, tar: 0.004048 
l0: 0.001234, l1: 0.001270, l2: 0.001305, l3: 0.001548, l4: 0.002210, l5: 0.002341, l6: 0.003572
[epoch:   4/1000, batch:   764/ 1052, ite: 980] train loss: 0.035521, tar: 0.003991 
l0: 0.000605, l1: 0.000469, l2: 0.000638, l3: 0.000724, l4: 0.001067, l5: 0.001612, l6: 0.002162
[epoch:   4/1000, batch:   844/ 1052, ite: 1000] train loss: 0.035008, tar: 0.003930 
l0: 0.001841, l1: 0.001309, l2: 0.001761, l3: 0.002032, l4: 0.002514, l5: 0.002809, l6: 0.003926
[epoch:   4/1000, batch:   924/ 1052, ite: 1020] train loss: 0.034659, tar: 0.003893 
l0: 0.000549, l1: 0.000506, l2: 0.000559, l3: 0.000663, l4: 0.001188, l5: 0.001613, l6: 0.002481
[epoch:   4/1000, batch:  1004/ 1052, ite: 1040] train loss: 0.034256, tar: 0.003845 
[Epoch 4/1000] Test loss: 0.013, Test target loss: 0.001
l0: 0.001246, l1: 0.001097, l2: 0.001350, l3: 0.001751, l4: 0.002500, l5: 0.002690, l6: 0.004092
[epoch:   5/1000, batch:    32/ 1052, ite: 1060] train loss: 0.033859, tar: 0.003799 
l0: 0.001806, l1: 0.002337, l2: 0.001789, l3: 0.002565, l4: 0.002270, l5: 0.003560, l6: 0.002980
[epoch:   5/1000, batch:   112/ 1052, ite: 1080] train loss: 0.033508, tar: 0.003758 
l0: 0.000915, l1: 0.001490, l2: 0.001098, l3: 0.001762, l4: 0.002278, l5: 0.002377, l6: 0.002789
[epoch:   5/1000, batch:   192/ 1052, ite: 1100] train loss: 0.033271, tar: 0.003726 
l0: 0.001065, l1: 0.000964, l2: 0.001164, l3: 0.001179, l4: 0.001531, l5: 0.002870, l6: 0.003182
[epoch:   5/1000, batch:   272/ 1052, ite: 1120] train loss: 0.032950, tar: 0.003688 
l0: 0.001452, l1: 0.001169, l2: 0.001562, l3: 0.001795, l4: 0.002542, l5: 0.003501, l6: 0.004513
[epoch:   5/1000, batch:   352/ 1052, ite: 1140] train loss: 0.032603, tar: 0.003647 
l0: 0.000603, l1: 0.000582, l2: 0.000598, l3: 0.000743, l4: 0.001326, l5: 0.001877, l6: 0.002383
[epoch:   5/1000, batch:   432/ 1052, ite: 1160] train loss: 0.032260, tar: 0.003607 
l0: 0.000637, l1: 0.001031, l2: 0.001167, l3: 0.000878, l4: 0.000983, l5: 0.001208, l6: 0.001733
[epoch:   5/1000, batch:   512/ 1052, ite: 1180] train loss: 0.031929, tar: 0.003568 
l0: 0.001100, l1: 0.000822, l2: 0.000978, l3: 0.001357, l4: 0.001853, l5: 0.002832, l6: 0.004459
[epoch:   5/1000, batch:   592/ 1052, ite: 1200] train loss: 0.031567, tar: 0.003525 
l0: 0.004280, l1: 0.005508, l2: 0.003928, l3: 0.003651, l4: 0.004735, l5: 0.006828, l6: 0.007057
[epoch:   5/1000, batch:   672/ 1052, ite: 1220] train loss: 0.031250, tar: 0.003488 
l0: 0.001122, l1: 0.000902, l2: 0.000845, l3: 0.001193, l4: 0.001308, l5: 0.002115, l6: 0.003067
[epoch:   5/1000, batch:   752/ 1052, ite: 1240] train loss: 0.030926, tar: 0.003449 
l0: 0.001198, l1: 0.001064, l2: 0.001289, l3: 0.001477, l4: 0.001735, l5: 0.003194, l6: 0.003935
[epoch:   5/1000, batch:   832/ 1052, ite: 1260] train loss: 0.030777, tar: 0.003437 
l0: 0.001063, l1: 0.001016, l2: 0.000962, l3: 0.001159, l4: 0.001298, l5: 0.001656, l6: 0.002386
[epoch:   5/1000, batch:   912/ 1052, ite: 1280] train loss: 0.030560, tar: 0.003410 
l0: 0.000906, l1: 0.000963, l2: 0.000955, l3: 0.001013, l4: 0.001216, l5: 0.001725, l6: 0.002386
[epoch:   5/1000, batch:   992/ 1052, ite: 1300] train loss: 0.030270, tar: 0.003375 
[Epoch 5/1000] Test loss: 0.016, Test target loss: 0.001
l0: 0.002619, l1: 0.002468, l2: 0.002245, l3: 0.002296, l4: 0.004016, l5: 0.006101, l6: 0.006822
[epoch:   6/1000, batch:    20/ 1052, ite: 1320] train loss: 0.030059, tar: 0.003351 
l0: 0.001027, l1: 0.000952, l2: 0.000971, l3: 0.001085, l4: 0.001410, l5: 0.002087, l6: 0.002930
[epoch:   6/1000, batch:   100/ 1052, ite: 1340] train loss: 0.029828, tar: 0.003323 
l0: 0.002104, l1: 0.001992, l2: 0.002007, l3: 0.002165, l4: 0.002748, l5: 0.003721, l6: 0.005174
[epoch:   6/1000, batch:   180/ 1052, ite: 1360] train loss: 0.029558, tar: 0.003291 
l0: 0.000900, l1: 0.000847, l2: 0.000931, l3: 0.001164, l4: 0.001562, l5: 0.001644, l6: 0.001818
[epoch:   6/1000, batch:   260/ 1052, ite: 1380] train loss: 0.029261, tar: 0.003256 
l0: 0.001287, l1: 0.001419, l2: 0.001201, l3: 0.001573, l4: 0.001993, l5: 0.002476, l6: 0.003519
[epoch:   6/1000, batch:   340/ 1052, ite: 1400] train loss: 0.028996, tar: 0.003226 
l0: 0.000689, l1: 0.000604, l2: 0.000666, l3: 0.000718, l4: 0.001002, l5: 0.001861, l6: 0.002578
[epoch:   6/1000, batch:   420/ 1052, ite: 1420] train loss: 0.028743, tar: 0.003195 
l0: 0.000413, l1: 0.000410, l2: 0.000429, l3: 0.000517, l4: 0.000753, l5: 0.001503, l6: 0.001496
[epoch:   6/1000, batch:   500/ 1052, ite: 1440] train loss: 0.028480, tar: 0.003164 
l0: 0.002263, l1: 0.002332, l2: 0.002153, l3: 0.002369, l4: 0.002548, l5: 0.003722, l6: 0.005098
[epoch:   6/1000, batch:   580/ 1052, ite: 1460] train loss: 0.028274, tar: 0.003142 
l0: 0.001804, l1: 0.001773, l2: 0.002087, l3: 0.001421, l4: 0.002670, l5: 0.003877, l6: 0.005792
[epoch:   6/1000, batch:   660/ 1052, ite: 1480] train loss: 0.028186, tar: 0.003134 
l0: 0.001010, l1: 0.000759, l2: 0.000946, l3: 0.001298, l4: 0.001647, l5: 0.001855, l6: 0.002193
[epoch:   6/1000, batch:   740/ 1052, ite: 1500] train loss: 0.028079, tar: 0.003122 
l0: 0.012928, l1: 0.013031, l2: 0.013114, l3: 0.012313, l4: 0.010646, l5: 0.010596, l6: 0.012474
[epoch:   6/1000, batch:   820/ 1052, ite: 1520] train loss: 0.027940, tar: 0.003107 
l0: 0.003225, l1: 0.004177, l2: 0.002138, l3: 0.003760, l4: 0.002450, l5: 0.004232, l6: 0.004434
[epoch:   6/1000, batch:   900/ 1052, ite: 1540] train loss: 0.027810, tar: 0.003091 
l0: 0.001644, l1: 0.001515, l2: 0.001925, l3: 0.001963, l4: 0.002403, l5: 0.002854, l6: 0.003259
[epoch:   6/1000, batch:   980/ 1052, ite: 1560] train loss: 0.027697, tar: 0.003078 
[Epoch 6/1000] Test loss: 0.017, Test target loss: 0.002
l0: 0.000798, l1: 0.000730, l2: 0.000850, l3: 0.000935, l4: 0.001215, l5: 0.001942, l6: 0.003095
[epoch:   7/1000, batch:     8/ 1052, ite: 1580] train loss: 0.027556, tar: 0.003064 
l0: 0.001229, l1: 0.001238, l2: 0.001356, l3: 0.001145, l4: 0.001289, l5: 0.001987, l6: 0.003353
[epoch:   7/1000, batch:    88/ 1052, ite: 1600] train loss: 0.027401, tar: 0.003045 
l0: 0.000632, l1: 0.000508, l2: 0.000785, l3: 0.000963, l4: 0.001058, l5: 0.002021, l6: 0.002948
[epoch:   7/1000, batch:   168/ 1052, ite: 1620] train loss: 0.027273, tar: 0.003030 
l0: 0.001412, l1: 0.001133, l2: 0.001445, l3: 0.001689, l4: 0.002472, l5: 0.002897, l6: 0.003822
[epoch:   7/1000, batch:   248/ 1052, ite: 1640] train loss: 0.027092, tar: 0.003007 
l0: 0.000854, l1: 0.000877, l2: 0.000904, l3: 0.000973, l4: 0.001495, l5: 0.002477, l6: 0.004131
[epoch:   7/1000, batch:   328/ 1052, ite: 1660] train loss: 0.026923, tar: 0.002986 
l0: 0.002048, l1: 0.001694, l2: 0.001805, l3: 0.002585, l4: 0.003005, l5: 0.004205, l6: 0.005247
[epoch:   7/1000, batch:   408/ 1052, ite: 1680] train loss: 0.026768, tar: 0.002967 
l0: 0.000844, l1: 0.000888, l2: 0.001128, l3: 0.001409, l4: 0.001195, l5: 0.001839, l6: 0.001988
[epoch:   7/1000, batch:   488/ 1052, ite: 1700] train loss: 0.026620, tar: 0.002951 
l0: 0.002195, l1: 0.001653, l2: 0.002373, l3: 0.002055, l4: 0.002195, l5: 0.002749, l6: 0.003704
[epoch:   7/1000, batch:   568/ 1052, ite: 1720] train loss: 0.026451, tar: 0.002931 
l0: 0.001331, l1: 0.001080, l2: 0.001541, l3: 0.001304, l4: 0.001590, l5: 0.002368, l6: 0.003333
[epoch:   7/1000, batch:   648/ 1052, ite: 1740] train loss: 0.026277, tar: 0.002910 
l0: 0.000563, l1: 0.000447, l2: 0.000547, l3: 0.000615, l4: 0.000942, l5: 0.001564, l6: 0.002977
[epoch:   7/1000, batch:   728/ 1052, ite: 1760] train loss: 0.026124, tar: 0.002892 
l0: 0.000495, l1: 0.000661, l2: 0.000921, l3: 0.000747, l4: 0.000839, l5: 0.000910, l6: 0.001268
[epoch:   7/1000, batch:   808/ 1052, ite: 1780] train loss: 0.025994, tar: 0.002879 
l0: 0.001100, l1: 0.001021, l2: 0.001412, l3: 0.001433, l4: 0.001354, l5: 0.001664, l6: 0.002680
[epoch:   7/1000, batch:   888/ 1052, ite: 1800] train loss: 0.025913, tar: 0.002871 
l0: 0.001826, l1: 0.002833, l2: 0.002861, l3: 0.001891, l4: 0.001730, l5: 0.002092, l6: 0.002415
[epoch:   7/1000, batch:   968/ 1052, ite: 1820] train loss: 0.025791, tar: 0.002858 
l0: 0.000895, l1: 0.000727, l2: 0.001099, l3: 0.000954, l4: 0.001296, l5: 0.001951, l6: 0.002749
[epoch:   7/1000, batch:  1048/ 1052, ite: 1840] train loss: 0.025635, tar: 0.002840 
[Epoch 7/1000] Test loss: 0.012, Test target loss: 0.001
l0: 0.000527, l1: 0.000615, l2: 0.000616, l3: 0.000744, l4: 0.001030, l5: 0.001639, l6: 0.001678
[epoch:   8/1000, batch:    76/ 1052, ite: 1860] train loss: 0.025473, tar: 0.002819 
l0: 0.001446, l1: 0.001511, l2: 0.001524, l3: 0.001607, l4: 0.002020, l5: 0.002734, l6: 0.003121
[epoch:   8/1000, batch:   156/ 1052, ite: 1880] train loss: 0.025381, tar: 0.002809 
l0: 0.000940, l1: 0.001209, l2: 0.000897, l3: 0.000948, l4: 0.001292, l5: 0.002374, l6: 0.002752
[epoch:   8/1000, batch:   236/ 1052, ite: 1900] train loss: 0.025227, tar: 0.002791 
l0: 0.000892, l1: 0.000887, l2: 0.000865, l3: 0.000908, l4: 0.001319, l5: 0.001733, l6: 0.003179
[epoch:   8/1000, batch:   316/ 1052, ite: 1920] train loss: 0.025092, tar: 0.002775 
l0: 0.000851, l1: 0.000712, l2: 0.000805, l3: 0.001038, l4: 0.001420, l5: 0.002407, l6: 0.002906
[epoch:   8/1000, batch:   396/ 1052, ite: 1940] train loss: 0.024944, tar: 0.002757 
l0: 0.002373, l1: 0.002424, l2: 0.002087, l3: 0.002562, l4: 0.003322, l5: 0.003704, l6: 0.005017
[epoch:   8/1000, batch:   476/ 1052, ite: 1960] train loss: 0.024787, tar: 0.002739 
l0: 0.000777, l1: 0.000639, l2: 0.000766, l3: 0.001023, l4: 0.001314, l5: 0.001975, l6: 0.002907
[epoch:   8/1000, batch:   556/ 1052, ite: 1980] train loss: 0.024650, tar: 0.002721 
l0: 0.002856, l1: 0.002459, l2: 0.003034, l3: 0.003340, l4: 0.003902, l5: 0.004559, l6: 0.004694
[epoch:   8/1000, batch:   636/ 1052, ite: 2000] train loss: 0.024517, tar: 0.002705 
l0: 0.000436, l1: 0.000870, l2: 0.000483, l3: 0.000550, l4: 0.000714, l5: 0.001082, l6: 0.001611
[epoch:   8/1000, batch:   716/ 1052, ite: 2020] train loss: 0.024382, tar: 0.002689 
l0: 0.001565, l1: 0.001487, l2: 0.001453, l3: 0.001825, l4: 0.001614, l5: 0.002951, l6: 0.002913
[epoch:   8/1000, batch:   796/ 1052, ite: 2040] train loss: 0.024330, tar: 0.002681 
l0: 0.003130, l1: 0.003066, l2: 0.002625, l3: 0.003348, l4: 0.002698, l5: 0.003119, l6: 0.004669
[epoch:   8/1000, batch:   876/ 1052, ite: 2060] train loss: 0.024186, tar: 0.002664 
l0: 0.001107, l1: 0.001120, l2: 0.001189, l3: 0.001448, l4: 0.001816, l5: 0.002694, l6: 0.003062
[epoch:   8/1000, batch:   956/ 1052, ite: 2080] train loss: 0.024074, tar: 0.002651 
l0: 0.000971, l1: 0.000995, l2: 0.001014, l3: 0.001431, l4: 0.001119, l5: 0.001352, l6: 0.002711
[epoch:   8/1000, batch:  1036/ 1052, ite: 2100] train loss: 0.023977, tar: 0.002640 
[Epoch 8/1000] Test loss: 0.012, Test target loss: 0.001
l0: 0.000716, l1: 0.000676, l2: 0.000634, l3: 0.000691, l4: 0.000832, l5: 0.001643, l6: 0.002512
[epoch:   9/1000, batch:    64/ 1052, ite: 2120] train loss: 0.023847, tar: 0.002624 
l0: 0.000684, l1: 0.000760, l2: 0.000726, l3: 0.000819, l4: 0.000985, l5: 0.001402, l6: 0.002592
[epoch:   9/1000, batch:   144/ 1052, ite: 2140] train loss: 0.023729, tar: 0.002610 
l0: 0.001308, l1: 0.001253, l2: 0.001300, l3: 0.001272, l4: 0.001155, l5: 0.001906, l6: 0.002588
[epoch:   9/1000, batch:   224/ 1052, ite: 2160] train loss: 0.023608, tar: 0.002595 
l0: 0.000979, l1: 0.000791, l2: 0.001369, l3: 0.001321, l4: 0.001078, l5: 0.001899, l6: 0.002440
[epoch:   9/1000, batch:   304/ 1052, ite: 2180] train loss: 0.023509, tar: 0.002584 
l0: 0.006528, l1: 0.004342, l2: 0.004754, l3: 0.006393, l4: 0.006557, l5: 0.006975, l6: 0.014261
[epoch:   9/1000, batch:   384/ 1052, ite: 2200] train loss: 0.023423, tar: 0.002574 
l0: 0.000588, l1: 0.000504, l2: 0.000796, l3: 0.000857, l4: 0.001310, l5: 0.001154, l6: 0.002561
[epoch:   9/1000, batch:   464/ 1052, ite: 2220] train loss: 0.023330, tar: 0.002562 
l0: 0.001755, l1: 0.001874, l2: 0.001553, l3: 0.001403, l4: 0.001264, l5: 0.002295, l6: 0.004061
[epoch:   9/1000, batch:   544/ 1052, ite: 2240] train loss: 0.023258, tar: 0.002553 
l0: 0.000493, l1: 0.000675, l2: 0.000801, l3: 0.000769, l4: 0.000948, l5: 0.001690, l6: 0.001592
[epoch:   9/1000, batch:   624/ 1052, ite: 2260] train loss: 0.023157, tar: 0.002541 
l0: 0.000604, l1: 0.000544, l2: 0.000626, l3: 0.000820, l4: 0.000924, l5: 0.001415, l6: 0.002337
[epoch:   9/1000, batch:   704/ 1052, ite: 2280] train loss: 0.023062, tar: 0.002529 
l0: 0.007083, l1: 0.006599, l2: 0.007633, l3: 0.007473, l4: 0.008700, l5: 0.008621, l6: 0.008779
[epoch:   9/1000, batch:   784/ 1052, ite: 2300] train loss: 0.023052, tar: 0.002529 
l0: 0.000756, l1: 0.000737, l2: 0.000694, l3: 0.000687, l4: 0.000971, l5: 0.001448, l6: 0.002331
[epoch:   9/1000, batch:   864/ 1052, ite: 2320] train loss: 0.022969, tar: 0.002517 
l0: 0.000966, l1: 0.001285, l2: 0.000942, l3: 0.001243, l4: 0.001273, l5: 0.001610, l6: 0.002475
[epoch:   9/1000, batch:   944/ 1052, ite: 2340] train loss: 0.022890, tar: 0.002508 
l0: 0.001132, l1: 0.001216, l2: 0.001124, l3: 0.001309, l4: 0.002360, l5: 0.002876, l6: 0.003424
[epoch:   9/1000, batch:  1024/ 1052, ite: 2360] train loss: 0.022833, tar: 0.002499 
[Epoch 9/1000] Test loss: 0.012, Test target loss: 0.001
l0: 0.000473, l1: 0.000413, l2: 0.000515, l3: 0.000731, l4: 0.001210, l5: 0.001421, l6: 0.002009
[epoch:  10/1000, batch:    52/ 1052, ite: 2380] train loss: 0.022773, tar: 0.002491 
l0: 0.003449, l1: 0.005160, l2: 0.003348, l3: 0.002483, l4: 0.002837, l5: 0.004964, l6: 0.007012
[epoch:  10/1000, batch:   132/ 1052, ite: 2400] train loss: 0.022681, tar: 0.002480 
l0: 0.000547, l1: 0.000504, l2: 0.000600, l3: 0.000816, l4: 0.000907, l5: 0.001360, l6: 0.001801
[epoch:  10/1000, batch:   212/ 1052, ite: 2420] train loss: 0.022587, tar: 0.002468 
l0: 0.001684, l1: 0.002012, l2: 0.002591, l3: 0.001664, l4: 0.002498, l5: 0.003508, l6: 0.004466
[epoch:  10/1000, batch:   292/ 1052, ite: 2440] train loss: 0.022567, tar: 0.002466 
l0: 0.000922, l1: 0.000886, l2: 0.001016, l3: 0.001393, l4: 0.001421, l5: 0.001883, l6: 0.002247
[epoch:  10/1000, batch:   372/ 1052, ite: 2460] train loss: 0.022496, tar: 0.002459 
l0: 0.000578, l1: 0.000560, l2: 0.000638, l3: 0.000767, l4: 0.000954, l5: 0.001436, l6: 0.002336
[epoch:  10/1000, batch:   452/ 1052, ite: 2480] train loss: 0.022418, tar: 0.002450 
l0: 0.002205, l1: 0.001793, l2: 0.002561, l3: 0.002297, l4: 0.004389, l5: 0.006170, l6: 0.006395
[epoch:  10/1000, batch:   532/ 1052, ite: 2500] train loss: 0.022354, tar: 0.002441 
l0: 0.000385, l1: 0.000855, l2: 0.000469, l3: 0.000508, l4: 0.000682, l5: 0.000952, l6: 0.001310
[epoch:  10/1000, batch:   612/ 1052, ite: 2520] train loss: 0.022313, tar: 0.002436 
l0: 0.001860, l1: 0.001846, l2: 0.001922, l3: 0.002041, l4: 0.002222, l5: 0.002850, l6: 0.003661
[epoch:  10/1000, batch:   692/ 1052, ite: 2540] train loss: 0.022230, tar: 0.002425 
l0: 0.000668, l1: 0.000514, l2: 0.000636, l3: 0.000711, l4: 0.001112, l5: 0.001568, l6: 0.002578
[epoch:  10/1000, batch:   772/ 1052, ite: 2560] train loss: 0.022129, tar: 0.002413 
l0: 0.001082, l1: 0.000842, l2: 0.001722, l3: 0.001430, l4: 0.001762, l5: 0.002273, l6: 0.002827
[epoch:  10/1000, batch:   852/ 1052, ite: 2580] train loss: 0.022049, tar: 0.002404 
l0: 0.003252, l1: 0.003449, l2: 0.002970, l3: 0.003405, l4: 0.002620, l5: 0.003101, l6: 0.003803
[epoch:  10/1000, batch:   932/ 1052, ite: 2600] train loss: 0.021968, tar: 0.002395 
l0: 0.002006, l1: 0.002048, l2: 0.002143, l3: 0.002400, l4: 0.002716, l5: 0.003670, l6: 0.003351
[epoch:  10/1000, batch:  1012/ 1052, ite: 2620] train loss: 0.021877, tar: 0.002384 
[Epoch 10/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.002397, l1: 0.002126, l2: 0.002019, l3: 0.002963, l4: 0.004145, l5: 0.003349, l6: 0.006068
[epoch:  11/1000, batch:    40/ 1052, ite: 2640] train loss: 0.021778, tar: 0.002372 
l0: 0.004026, l1: 0.003641, l2: 0.005080, l3: 0.003072, l4: 0.003935, l5: 0.005650, l6: 0.006449
[epoch:  11/1000, batch:   120/ 1052, ite: 2660] train loss: 0.021707, tar: 0.002364 
l0: 0.000554, l1: 0.000542, l2: 0.000568, l3: 0.000768, l4: 0.001109, l5: 0.001895, l6: 0.002917
[epoch:  11/1000, batch:   200/ 1052, ite: 2680] train loss: 0.021640, tar: 0.002356 
l0: 0.001142, l1: 0.001161, l2: 0.001299, l3: 0.001603, l4: 0.002352, l5: 0.003263, l6: 0.004208
[epoch:  11/1000, batch:   280/ 1052, ite: 2700] train loss: 0.021576, tar: 0.002348 
l0: 0.001400, l1: 0.000981, l2: 0.001828, l3: 0.001329, l4: 0.001175, l5: 0.001663, l6: 0.002564
[epoch:  11/1000, batch:   360/ 1052, ite: 2720] train loss: 0.021482, tar: 0.002336 
l0: 0.001410, l1: 0.002275, l2: 0.001208, l3: 0.001625, l4: 0.001797, l5: 0.001881, l6: 0.003262
[epoch:  11/1000, batch:   440/ 1052, ite: 2740] train loss: 0.021434, tar: 0.002332 
l0: 0.000852, l1: 0.000733, l2: 0.000844, l3: 0.000994, l4: 0.001165, l5: 0.001215, l6: 0.001995
[epoch:  11/1000, batch:   520/ 1052, ite: 2760] train loss: 0.021348, tar: 0.002322 
l0: 0.000613, l1: 0.000548, l2: 0.000726, l3: 0.001004, l4: 0.001360, l5: 0.001312, l6: 0.002458
[epoch:  11/1000, batch:   600/ 1052, ite: 2780] train loss: 0.021272, tar: 0.002313 
l0: 0.000598, l1: 0.000521, l2: 0.000614, l3: 0.000964, l4: 0.001408, l5: 0.001697, l6: 0.002680
[epoch:  11/1000, batch:   680/ 1052, ite: 2800] train loss: 0.021204, tar: 0.002304 
l0: 0.001038, l1: 0.000686, l2: 0.000993, l3: 0.001274, l4: 0.001613, l5: 0.001845, l6: 0.003091
[epoch:  11/1000, batch:   760/ 1052, ite: 2820] train loss: 0.021124, tar: 0.002294 
l0: 0.006153, l1: 0.006307, l2: 0.006381, l3: 0.006500, l4: 0.006930, l5: 0.005794, l6: 0.007209
[epoch:  11/1000, batch:   840/ 1052, ite: 2840] train loss: 0.021108, tar: 0.002294 
l0: 0.002603, l1: 0.002423, l2: 0.002173, l3: 0.004002, l4: 0.004399, l5: 0.003437, l6: 0.004193
[epoch:  11/1000, batch:   920/ 1052, ite: 2860] train loss: 0.021048, tar: 0.002286 
l0: 0.000358, l1: 0.000323, l2: 0.000367, l3: 0.000500, l4: 0.000669, l5: 0.001087, l6: 0.001362
[epoch:  11/1000, batch:  1000/ 1052, ite: 2880] train loss: 0.020989, tar: 0.002279 
[Epoch 11/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000477, l1: 0.000458, l2: 0.000425, l3: 0.000708, l4: 0.000771, l5: 0.001134, l6: 0.001474
[epoch:  12/1000, batch:    28/ 1052, ite: 2900] train loss: 0.020906, tar: 0.002270 
l0: 0.000914, l1: 0.000968, l2: 0.001034, l3: 0.001083, l4: 0.001090, l5: 0.002003, l6: 0.002916
[epoch:  12/1000, batch:   108/ 1052, ite: 2920] train loss: 0.020845, tar: 0.002262 
l0: 0.001247, l1: 0.001219, l2: 0.001204, l3: 0.001427, l4: 0.001711, l5: 0.001679, l6: 0.003081
[epoch:  12/1000, batch:   188/ 1052, ite: 2940] train loss: 0.020763, tar: 0.002252 
l0: 0.001354, l1: 0.001349, l2: 0.001504, l3: 0.001735, l4: 0.001911, l5: 0.002209, l6: 0.002791
[epoch:  12/1000, batch:   268/ 1052, ite: 2960] train loss: 0.020694, tar: 0.002244 
l0: 0.000533, l1: 0.000609, l2: 0.000502, l3: 0.000566, l4: 0.000771, l5: 0.001382, l6: 0.001851
[epoch:  12/1000, batch:   348/ 1052, ite: 2980] train loss: 0.020616, tar: 0.002235 
l0: 0.000430, l1: 0.000546, l2: 0.000587, l3: 0.000704, l4: 0.000783, l5: 0.001062, l6: 0.001791
[epoch:  12/1000, batch:   428/ 1052, ite: 3000] train loss: 0.020542, tar: 0.002226 
l0: 0.001552, l1: 0.001195, l2: 0.002135, l3: 0.002492, l4: 0.002248, l5: 0.002966, l6: 0.003546
[epoch:  12/1000, batch:   508/ 1052, ite: 3020] train loss: 0.020521, tar: 0.002223 
l0: 0.000518, l1: 0.000498, l2: 0.000550, l3: 0.000789, l4: 0.000833, l5: 0.001468, l6: 0.002497
[epoch:  12/1000, batch:   588/ 1052, ite: 3040] train loss: 0.020459, tar: 0.002216 
l0: 0.000618, l1: 0.000625, l2: 0.000895, l3: 0.001178, l4: 0.001155, l5: 0.001467, l6: 0.001669
[epoch:  12/1000, batch:   668/ 1052, ite: 3060] train loss: 0.020400, tar: 0.002209 
l0: 0.000375, l1: 0.000363, l2: 0.000420, l3: 0.000576, l4: 0.000694, l5: 0.000942, l6: 0.001616
[epoch:  12/1000, batch:   748/ 1052, ite: 3080] train loss: 0.020339, tar: 0.002202 
l0: 0.001310, l1: 0.002072, l2: 0.001193, l3: 0.001555, l4: 0.001766, l5: 0.001915, l6: 0.002251
[epoch:  12/1000, batch:   828/ 1052, ite: 3100] train loss: 0.020268, tar: 0.002193 
l0: 0.004706, l1: 0.004640, l2: 0.005242, l3: 0.004915, l4: 0.003977, l5: 0.003754, l6: 0.004822
[epoch:  12/1000, batch:   908/ 1052, ite: 3120] train loss: 0.020230, tar: 0.002189 
l0: 0.001805, l1: 0.001763, l2: 0.001905, l3: 0.001908, l4: 0.002464, l5: 0.002592, l6: 0.003247
[epoch:  12/1000, batch:   988/ 1052, ite: 3140] train loss: 0.020170, tar: 0.002182 
[Epoch 12/1000] Test loss: 0.018, Test target loss: 0.002
l0: 0.000880, l1: 0.001105, l2: 0.000922, l3: 0.001085, l4: 0.001420, l5: 0.002302, l6: 0.002880
[epoch:  13/1000, batch:    16/ 1052, ite: 3160] train loss: 0.020171, tar: 0.002184 
l0: 0.000687, l1: 0.000712, l2: 0.000732, l3: 0.001003, l4: 0.001139, l5: 0.001964, l6: 0.002247
[epoch:  13/1000, batch:    96/ 1052, ite: 3180] train loss: 0.020132, tar: 0.002179 
l0: 0.002651, l1: 0.002738, l2: 0.002621, l3: 0.002635, l4: 0.003054, l5: 0.003604, l6: 0.004317
[epoch:  13/1000, batch:   176/ 1052, ite: 3200] train loss: 0.020073, tar: 0.002172 
l0: 0.000680, l1: 0.000719, l2: 0.000871, l3: 0.000853, l4: 0.000731, l5: 0.001515, l6: 0.001910
[epoch:  13/1000, batch:   256/ 1052, ite: 3220] train loss: 0.020018, tar: 0.002165 
l0: 0.001298, l1: 0.001661, l2: 0.001408, l3: 0.001285, l4: 0.001754, l5: 0.002537, l6: 0.002931
[epoch:  13/1000, batch:   336/ 1052, ite: 3240] train loss: 0.019988, tar: 0.002163 
l0: 0.000861, l1: 0.000782, l2: 0.000842, l3: 0.000962, l4: 0.001330, l5: 0.001752, l6: 0.003221
[epoch:  13/1000, batch:   416/ 1052, ite: 3260] train loss: 0.019930, tar: 0.002156 
l0: 0.000523, l1: 0.000644, l2: 0.000570, l3: 0.000567, l4: 0.000667, l5: 0.001467, l6: 0.002180
[epoch:  13/1000, batch:   496/ 1052, ite: 3280] train loss: 0.019863, tar: 0.002147 
l0: 0.001126, l1: 0.000813, l2: 0.001468, l3: 0.001161, l4: 0.001383, l5: 0.001832, l6: 0.002436
[epoch:  13/1000, batch:   576/ 1052, ite: 3300] train loss: 0.019803, tar: 0.002140 
l0: 0.001035, l1: 0.000891, l2: 0.000981, l3: 0.001223, l4: 0.001550, l5: 0.002895, l6: 0.004612
[epoch:  13/1000, batch:   656/ 1052, ite: 3320] train loss: 0.019743, tar: 0.002133 
l0: 0.000693, l1: 0.000715, l2: 0.000723, l3: 0.000744, l4: 0.001103, l5: 0.001815, l6: 0.002417
[epoch:  13/1000, batch:   736/ 1052, ite: 3340] train loss: 0.019677, tar: 0.002125 
l0: 0.001961, l1: 0.001904, l2: 0.001922, l3: 0.002431, l4: 0.002411, l5: 0.003169, l6: 0.004652
[epoch:  13/1000, batch:   816/ 1052, ite: 3360] train loss: 0.019627, tar: 0.002119 
l0: 0.001218, l1: 0.001820, l2: 0.001371, l3: 0.001394, l4: 0.001596, l5: 0.001537, l6: 0.002197
[epoch:  13/1000, batch:   896/ 1052, ite: 3380] train loss: 0.019588, tar: 0.002115 
l0: 0.000214, l1: 0.000194, l2: 0.000229, l3: 0.000289, l4: 0.000525, l5: 0.000724, l6: 0.001181
[epoch:  13/1000, batch:   976/ 1052, ite: 3400] train loss: 0.019532, tar: 0.002108 
[Epoch 13/1000] Test loss: 0.013, Test target loss: 0.001
l0: 0.000462, l1: 0.000553, l2: 0.000484, l3: 0.000568, l4: 0.000579, l5: 0.000994, l6: 0.001670
[epoch:  14/1000, batch:     4/ 1052, ite: 3420] train loss: 0.019489, tar: 0.002103 
l0: 0.000613, l1: 0.000634, l2: 0.000661, l3: 0.000855, l4: 0.000865, l5: 0.001543, l6: 0.002125
[epoch:  14/1000, batch:    84/ 1052, ite: 3440] train loss: 0.019427, tar: 0.002095 
l0: 0.003252, l1: 0.003656, l2: 0.002705, l3: 0.003014, l4: 0.003098, l5: 0.003183, l6: 0.004998
[epoch:  14/1000, batch:   164/ 1052, ite: 3460] train loss: 0.019392, tar: 0.002091 
l0: 0.001232, l1: 0.001247, l2: 0.001207, l3: 0.001411, l4: 0.001904, l5: 0.002271, l6: 0.004056
[epoch:  14/1000, batch:   244/ 1052, ite: 3480] train loss: 0.019354, tar: 0.002086 
l0: 0.000601, l1: 0.000547, l2: 0.000583, l3: 0.001010, l4: 0.001328, l5: 0.001650, l6: 0.001849
[epoch:  14/1000, batch:   324/ 1052, ite: 3500] train loss: 0.019301, tar: 0.002079 
l0: 0.000435, l1: 0.000420, l2: 0.000481, l3: 0.000553, l4: 0.000662, l5: 0.001216, l6: 0.001503
[epoch:  14/1000, batch:   404/ 1052, ite: 3520] train loss: 0.019245, tar: 0.002072 
l0: 0.000545, l1: 0.000542, l2: 0.000555, l3: 0.000591, l4: 0.000803, l5: 0.001384, l6: 0.001932
[epoch:  14/1000, batch:   484/ 1052, ite: 3540] train loss: 0.019209, tar: 0.002067 
l0: 0.001218, l1: 0.001648, l2: 0.001494, l3: 0.001818, l4: 0.001886, l5: 0.002052, l6: 0.002519
[epoch:  14/1000, batch:   564/ 1052, ite: 3560] train loss: 0.019162, tar: 0.002061 
l0: 0.022136, l1: 0.020414, l2: 0.021638, l3: 0.023689, l4: 0.022935, l5: 0.024772, l6: 0.028293
[epoch:  14/1000, batch:   644/ 1052, ite: 3580] train loss: 0.019186, tar: 0.002064 
l0: 0.000437, l1: 0.000446, l2: 0.000403, l3: 0.000588, l4: 0.000737, l5: 0.001118, l6: 0.001625
[epoch:  14/1000, batch:   724/ 1052, ite: 3600] train loss: 0.019134, tar: 0.002057 
l0: 0.000790, l1: 0.000984, l2: 0.001343, l3: 0.000940, l4: 0.000982, l5: 0.001180, l6: 0.001752
[epoch:  14/1000, batch:   804/ 1052, ite: 3620] train loss: 0.019097, tar: 0.002053 
l0: 0.001088, l1: 0.001263, l2: 0.000724, l3: 0.001097, l4: 0.001204, l5: 0.001713, l6: 0.003361
[epoch:  14/1000, batch:   884/ 1052, ite: 3640] train loss: 0.019053, tar: 0.002048 
l0: 0.000351, l1: 0.000429, l2: 0.000463, l3: 0.000400, l4: 0.000515, l5: 0.000956, l6: 0.001131
[epoch:  14/1000, batch:   964/ 1052, ite: 3660] train loss: 0.018999, tar: 0.002041 
l0: 0.000818, l1: 0.000769, l2: 0.001054, l3: 0.000885, l4: 0.001119, l5: 0.001644, l6: 0.002014
[epoch:  14/1000, batch:  1044/ 1052, ite: 3680] train loss: 0.018963, tar: 0.002037 
[Epoch 14/1000] Test loss: 0.014, Test target loss: 0.001
l0: 0.002193, l1: 0.001841, l2: 0.002135, l3: 0.002337, l4: 0.002164, l5: 0.002615, l6: 0.004463
[epoch:  15/1000, batch:    72/ 1052, ite: 3700] train loss: 0.018912, tar: 0.002031 
l0: 0.004793, l1: 0.005253, l2: 0.004683, l3: 0.005124, l4: 0.005518, l5: 0.005151, l6: 0.005072
[epoch:  15/1000, batch:   152/ 1052, ite: 3720] train loss: 0.018866, tar: 0.002026 
l0: 0.001281, l1: 0.000961, l2: 0.001311, l3: 0.002024, l4: 0.001767, l5: 0.002538, l6: 0.002721
[epoch:  15/1000, batch:   232/ 1052, ite: 3740] train loss: 0.018820, tar: 0.002021 
l0: 0.002000, l1: 0.001499, l2: 0.002113, l3: 0.002452, l4: 0.002795, l5: 0.002969, l6: 0.004251
[epoch:  15/1000, batch:   312/ 1052, ite: 3760] train loss: 0.018773, tar: 0.002015 
l0: 0.001077, l1: 0.000897, l2: 0.001515, l3: 0.001099, l4: 0.001320, l5: 0.001561, l6: 0.002539
[epoch:  15/1000, batch:   392/ 1052, ite: 3780] train loss: 0.018744, tar: 0.002012 
l0: 0.000846, l1: 0.000658, l2: 0.000860, l3: 0.000933, l4: 0.001020, l5: 0.001614, l6: 0.002561
[epoch:  15/1000, batch:   472/ 1052, ite: 3800] train loss: 0.018693, tar: 0.002006 
l0: 0.000789, l1: 0.000723, l2: 0.000799, l3: 0.001074, l4: 0.001183, l5: 0.001867, l6: 0.003328
[epoch:  15/1000, batch:   552/ 1052, ite: 3820] train loss: 0.018657, tar: 0.002001 
l0: 0.000410, l1: 0.000640, l2: 0.000518, l3: 0.000438, l4: 0.000583, l5: 0.000774, l6: 0.001299
[epoch:  15/1000, batch:   632/ 1052, ite: 3840] train loss: 0.018644, tar: 0.002001 
l0: 0.002485, l1: 0.003306, l2: 0.003104, l3: 0.002708, l4: 0.002375, l5: 0.001786, l6: 0.001899
[epoch:  15/1000, batch:   712/ 1052, ite: 3860] train loss: 0.018614, tar: 0.001998 
l0: 0.000562, l1: 0.000465, l2: 0.000601, l3: 0.000791, l4: 0.000893, l5: 0.001561, l6: 0.001989
[epoch:  15/1000, batch:   792/ 1052, ite: 3880] train loss: 0.018567, tar: 0.001992 
l0: 0.000480, l1: 0.000403, l2: 0.000437, l3: 0.000509, l4: 0.000904, l5: 0.001357, l6: 0.002228
[epoch:  15/1000, batch:   872/ 1052, ite: 3900] train loss: 0.018532, tar: 0.001988 
l0: 0.000656, l1: 0.000649, l2: 0.000660, l3: 0.000817, l4: 0.001178, l5: 0.002137, l6: 0.002936
[epoch:  15/1000, batch:   952/ 1052, ite: 3920] train loss: 0.018489, tar: 0.001982 
l0: 0.001728, l1: 0.001230, l2: 0.002183, l3: 0.001807, l4: 0.001558, l5: 0.002499, l6: 0.002762
[epoch:  15/1000, batch:  1032/ 1052, ite: 3940] train loss: 0.018455, tar: 0.001978 
[Epoch 15/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.002618, l1: 0.002066, l2: 0.002911, l3: 0.002855, l4: 0.003965, l5: 0.003682, l6: 0.004207
[epoch:  16/1000, batch:    60/ 1052, ite: 3960] train loss: 0.018405, tar: 0.001972 
l0: 0.000840, l1: 0.000977, l2: 0.000892, l3: 0.001522, l4: 0.001567, l5: 0.001321, l6: 0.002062
[epoch:  16/1000, batch:   140/ 1052, ite: 3980] train loss: 0.018384, tar: 0.001969 
l0: 0.001152, l1: 0.000944, l2: 0.001316, l3: 0.001851, l4: 0.001696, l5: 0.002883, l6: 0.003798
[epoch:  16/1000, batch:   220/ 1052, ite: 4000] train loss: 0.018337, tar: 0.001963 
l0: 0.001249, l1: 0.001232, l2: 0.001439, l3: 0.001605, l4: 0.001796, l5: 0.002017, l6: 0.002977
[epoch:  16/1000, batch:   300/ 1052, ite: 4020] train loss: 0.018296, tar: 0.001958 
l0: 0.000636, l1: 0.000581, l2: 0.000753, l3: 0.000883, l4: 0.001190, l5: 0.001682, l6: 0.002615
[epoch:  16/1000, batch:   380/ 1052, ite: 4040] train loss: 0.018253, tar: 0.001953 
l0: 0.000765, l1: 0.000643, l2: 0.000860, l3: 0.001515, l4: 0.001510, l5: 0.001218, l6: 0.002180
[epoch:  16/1000, batch:   460/ 1052, ite: 4060] train loss: 0.018215, tar: 0.001948 
l0: 0.000576, l1: 0.000620, l2: 0.000580, l3: 0.000794, l4: 0.001513, l5: 0.001899, l6: 0.002723
[epoch:  16/1000, batch:   540/ 1052, ite: 4080] train loss: 0.018179, tar: 0.001943 
l0: 0.000867, l1: 0.001056, l2: 0.000983, l3: 0.001245, l4: 0.001829, l5: 0.002057, l6: 0.003027
[epoch:  16/1000, batch:   620/ 1052, ite: 4100] train loss: 0.018144, tar: 0.001939 
l0: 0.019104, l1: 0.019847, l2: 0.016651, l3: 0.020136, l4: 0.016688, l5: 0.020640, l6: 0.023969
[epoch:  16/1000, batch:   700/ 1052, ite: 4120] train loss: 0.018145, tar: 0.001939 
l0: 0.000655, l1: 0.000951, l2: 0.000685, l3: 0.001024, l4: 0.001082, l5: 0.001781, l6: 0.002320
[epoch:  16/1000, batch:   780/ 1052, ite: 4140] train loss: 0.018119, tar: 0.001936 
l0: 0.001590, l1: 0.002134, l2: 0.001602, l3: 0.002237, l4: 0.002121, l5: 0.002224, l6: 0.003548
[epoch:  16/1000, batch:   860/ 1052, ite: 4160] train loss: 0.018088, tar: 0.001931 
l0: 0.001819, l1: 0.001409, l2: 0.002319, l3: 0.002044, l4: 0.002166, l5: 0.002648, l6: 0.004200
[epoch:  16/1000, batch:   940/ 1052, ite: 4180] train loss: 0.018059, tar: 0.001927 
l0: 0.002518, l1: 0.002483, l2: 0.002327, l3: 0.002596, l4: 0.002647, l5: 0.004040, l6: 0.006185
[epoch:  16/1000, batch:  1020/ 1052, ite: 4200] train loss: 0.018030, tar: 0.001923 
[Epoch 16/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000659, l1: 0.000589, l2: 0.000750, l3: 0.000853, l4: 0.000976, l5: 0.001465, l6: 0.001938
[epoch:  17/1000, batch:    48/ 1052, ite: 4220] train loss: 0.017994, tar: 0.001919 
l0: 0.000894, l1: 0.001017, l2: 0.001025, l3: 0.001092, l4: 0.001427, l5: 0.001990, l6: 0.002993
[epoch:  17/1000, batch:   128/ 1052, ite: 4240] train loss: 0.017966, tar: 0.001915 
l0: 0.000508, l1: 0.000435, l2: 0.000507, l3: 0.000672, l4: 0.000837, l5: 0.001103, l6: 0.002091
[epoch:  17/1000, batch:   208/ 1052, ite: 4260] train loss: 0.017931, tar: 0.001911 
l0: 0.001056, l1: 0.001110, l2: 0.001046, l3: 0.001415, l4: 0.002000, l5: 0.002431, l6: 0.003369
[epoch:  17/1000, batch:   288/ 1052, ite: 4280] train loss: 0.017891, tar: 0.001906 
l0: 0.000658, l1: 0.000706, l2: 0.000742, l3: 0.000950, l4: 0.001296, l5: 0.002054, l6: 0.002953
[epoch:  17/1000, batch:   368/ 1052, ite: 4300] train loss: 0.017865, tar: 0.001902 
l0: 0.000495, l1: 0.000516, l2: 0.000471, l3: 0.000569, l4: 0.001066, l5: 0.001283, l6: 0.002234
[epoch:  17/1000, batch:   448/ 1052, ite: 4320] train loss: 0.017825, tar: 0.001897 
l0: 0.001216, l1: 0.001323, l2: 0.001253, l3: 0.001580, l4: 0.002158, l5: 0.002936, l6: 0.005238
[epoch:  17/1000, batch:   528/ 1052, ite: 4340] train loss: 0.017814, tar: 0.001896 
l0: 0.000272, l1: 0.000300, l2: 0.000311, l3: 0.000306, l4: 0.000456, l5: 0.000790, l6: 0.001012
[epoch:  17/1000, batch:   608/ 1052, ite: 4360] train loss: 0.017777, tar: 0.001892 
l0: 0.000432, l1: 0.000396, l2: 0.000434, l3: 0.000531, l4: 0.000601, l5: 0.001581, l6: 0.003010
[epoch:  17/1000, batch:   688/ 1052, ite: 4380] train loss: 0.017740, tar: 0.001887 
l0: 0.000819, l1: 0.000692, l2: 0.000737, l3: 0.001018, l4: 0.001561, l5: 0.002431, l6: 0.003463
[epoch:  17/1000, batch:   768/ 1052, ite: 4400] train loss: 0.017707, tar: 0.001882 
l0: 0.003421, l1: 0.003468, l2: 0.002946, l3: 0.004085, l4: 0.003310, l5: 0.003005, l6: 0.004605
[epoch:  17/1000, batch:   848/ 1052, ite: 4420] train loss: 0.017707, tar: 0.001883 
l0: 0.001033, l1: 0.000884, l2: 0.001112, l3: 0.001277, l4: 0.001331, l5: 0.002679, l6: 0.003718
[epoch:  17/1000, batch:   928/ 1052, ite: 4440] train loss: 0.017688, tar: 0.001881 
l0: 0.000861, l1: 0.000802, l2: 0.000893, l3: 0.001090, l4: 0.001249, l5: 0.001839, l6: 0.002667
[epoch:  17/1000, batch:  1008/ 1052, ite: 4460] train loss: 0.017650, tar: 0.001877 
[Epoch 17/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000618, l1: 0.000558, l2: 0.000571, l3: 0.000789, l4: 0.000941, l5: 0.001353, l6: 0.001966
[epoch:  18/1000, batch:    36/ 1052, ite: 4480] train loss: 0.017617, tar: 0.001873 
l0: 0.000660, l1: 0.000967, l2: 0.000834, l3: 0.000633, l4: 0.000751, l5: 0.001311, l6: 0.001313
[epoch:  18/1000, batch:   116/ 1052, ite: 4500] train loss: 0.017587, tar: 0.001869 
l0: 0.000629, l1: 0.000683, l2: 0.000626, l3: 0.000661, l4: 0.000932, l5: 0.001621, l6: 0.002298
[epoch:  18/1000, batch:   196/ 1052, ite: 4520] train loss: 0.017556, tar: 0.001865 
l0: 0.000851, l1: 0.000893, l2: 0.000814, l3: 0.000844, l4: 0.000882, l5: 0.001150, l6: 0.002390
[epoch:  18/1000, batch:   276/ 1052, ite: 4540] train loss: 0.017519, tar: 0.001861 
l0: 0.000457, l1: 0.000454, l2: 0.000509, l3: 0.000571, l4: 0.000799, l5: 0.001311, l6: 0.002033
[epoch:  18/1000, batch:   356/ 1052, ite: 4560] train loss: 0.017482, tar: 0.001856 
l0: 0.000617, l1: 0.000699, l2: 0.000633, l3: 0.000854, l4: 0.000821, l5: 0.001468, l6: 0.001739
[epoch:  18/1000, batch:   436/ 1052, ite: 4580] train loss: 0.017455, tar: 0.001853 
l0: 0.001521, l1: 0.001832, l2: 0.001191, l3: 0.001929, l4: 0.002331, l5: 0.004232, l6: 0.005778
[epoch:  18/1000, batch:   516/ 1052, ite: 4600] train loss: 0.017444, tar: 0.001852 
l0: 0.000395, l1: 0.000464, l2: 0.000383, l3: 0.000369, l4: 0.000464, l5: 0.000770, l6: 0.001571
[epoch:  18/1000, batch:   596/ 1052, ite: 4620] train loss: 0.017413, tar: 0.001848 
l0: 0.000576, l1: 0.000674, l2: 0.000557, l3: 0.000661, l4: 0.000830, l5: 0.001292, l6: 0.002673
[epoch:  18/1000, batch:   676/ 1052, ite: 4640] train loss: 0.017384, tar: 0.001845 
l0: 0.001404, l1: 0.001616, l2: 0.001524, l3: 0.001538, l4: 0.002138, l5: 0.002970, l6: 0.004773
[epoch:  18/1000, batch:   756/ 1052, ite: 4660] train loss: 0.017356, tar: 0.001841 
l0: 0.000200, l1: 0.000236, l2: 0.000218, l3: 0.000402, l4: 0.000348, l5: 0.000821, l6: 0.001033
[epoch:  18/1000, batch:   836/ 1052, ite: 4680] train loss: 0.017320, tar: 0.001837 
l0: 0.002353, l1: 0.001879, l2: 0.001929, l3: 0.004092, l4: 0.003233, l5: 0.002630, l6: 0.004552
[epoch:  18/1000, batch:   916/ 1052, ite: 4700] train loss: 0.017321, tar: 0.001837 
l0: 0.000617, l1: 0.000497, l2: 0.000769, l3: 0.001324, l4: 0.001285, l5: 0.001193, l6: 0.001966
[epoch:  18/1000, batch:   996/ 1052, ite: 4720] train loss: 0.017288, tar: 0.001833 
[Epoch 18/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.001083, l1: 0.001044, l2: 0.001163, l3: 0.001444, l4: 0.001490, l5: 0.002323, l6: 0.002535
[epoch:  19/1000, batch:    24/ 1052, ite: 4740] train loss: 0.017261, tar: 0.001829 
l0: 0.000356, l1: 0.000378, l2: 0.000448, l3: 0.000393, l4: 0.000418, l5: 0.000849, l6: 0.001451
[epoch:  19/1000, batch:   104/ 1052, ite: 4760] train loss: 0.017222, tar: 0.001824 
l0: 0.001082, l1: 0.001015, l2: 0.001068, l3: 0.001279, l4: 0.001379, l5: 0.002091, l6: 0.003896
[epoch:  19/1000, batch:   184/ 1052, ite: 4780] train loss: 0.017194, tar: 0.001821 
l0: 0.000877, l1: 0.000805, l2: 0.000876, l3: 0.001053, l4: 0.001292, l5: 0.002087, l6: 0.003074
[epoch:  19/1000, batch:   264/ 1052, ite: 4800] train loss: 0.017160, tar: 0.001817 
l0: 0.000848, l1: 0.000736, l2: 0.000892, l3: 0.001415, l4: 0.001748, l5: 0.002828, l6: 0.004400
[epoch:  19/1000, batch:   344/ 1052, ite: 4820] train loss: 0.017129, tar: 0.001813 
l0: 0.001951, l1: 0.002098, l2: 0.002866, l3: 0.003018, l4: 0.002153, l5: 0.002473, l6: 0.002836
[epoch:  19/1000, batch:   424/ 1052, ite: 4840] train loss: 0.017097, tar: 0.001809 
l0: 0.001296, l1: 0.001210, l2: 0.001274, l3: 0.001478, l4: 0.002005, l5: 0.003593, l6: 0.005744
[epoch:  19/1000, batch:   504/ 1052, ite: 4860] train loss: 0.017079, tar: 0.001807 
l0: 0.002551, l1: 0.002146, l2: 0.002794, l3: 0.003633, l4: 0.004750, l5: 0.006424, l6: 0.006609
[epoch:  19/1000, batch:   584/ 1052, ite: 4880] train loss: 0.017058, tar: 0.001804 
l0: 0.000928, l1: 0.001063, l2: 0.000928, l3: 0.001020, l4: 0.001143, l5: 0.001789, l6: 0.002478
[epoch:  19/1000, batch:   664/ 1052, ite: 4900] train loss: 0.017031, tar: 0.001801 
l0: 0.000730, l1: 0.000768, l2: 0.000743, l3: 0.000787, l4: 0.001091, l5: 0.001747, l6: 0.002766
[epoch:  19/1000, batch:   744/ 1052, ite: 4920] train loss: 0.017006, tar: 0.001798 
l0: 0.000679, l1: 0.000651, l2: 0.000702, l3: 0.000717, l4: 0.000992, l5: 0.001610, l6: 0.002851
[epoch:  19/1000, batch:   824/ 1052, ite: 4940] train loss: 0.016975, tar: 0.001794 
l0: 0.000939, l1: 0.000997, l2: 0.000934, l3: 0.000905, l4: 0.000910, l5: 0.001426, l6: 0.002036
[epoch:  19/1000, batch:   904/ 1052, ite: 4960] train loss: 0.016944, tar: 0.001791 
l0: 0.000701, l1: 0.000705, l2: 0.000899, l3: 0.000861, l4: 0.000843, l5: 0.001267, l6: 0.001635
[epoch:  19/1000, batch:   984/ 1052, ite: 4980] train loss: 0.016921, tar: 0.001788 
[Epoch 19/1000] Test loss: 0.018, Test target loss: 0.001
l0: 0.000662, l1: 0.000655, l2: 0.000893, l3: 0.001094, l4: 0.001339, l5: 0.001609, l6: 0.002514
[epoch:  20/1000, batch:    12/ 1052, ite: 5000] train loss: 0.016929, tar: 0.001790 
l0: 0.000800, l1: 0.000824, l2: 0.001039, l3: 0.001691, l4: 0.001316, l5: 0.001408, l6: 0.001934
[epoch:  20/1000, batch:    92/ 1052, ite: 5020] train loss: 0.016914, tar: 0.001788 
l0: 0.001149, l1: 0.001395, l2: 0.001157, l3: 0.001154, l4: 0.001417, l5: 0.002576, l6: 0.003568
[epoch:  20/1000, batch:   172/ 1052, ite: 5040] train loss: 0.016893, tar: 0.001785 
l0: 0.000999, l1: 0.000966, l2: 0.000995, l3: 0.001170, l4: 0.001182, l5: 0.001564, l6: 0.002997
[epoch:  20/1000, batch:   252/ 1052, ite: 5060] train loss: 0.016869, tar: 0.001782 
l0: 0.000785, l1: 0.000766, l2: 0.000836, l3: 0.000888, l4: 0.001126, l5: 0.001526, l6: 0.003296
[epoch:  20/1000, batch:   332/ 1052, ite: 5080] train loss: 0.016872, tar: 0.001782 
l0: 0.000516, l1: 0.000492, l2: 0.000554, l3: 0.000747, l4: 0.001508, l5: 0.002519, l6: 0.003650
[epoch:  20/1000, batch:   412/ 1052, ite: 5100] train loss: 0.016850, tar: 0.001779 
l0: 0.000378, l1: 0.000369, l2: 0.000430, l3: 0.000444, l4: 0.000719, l5: 0.001175, l6: 0.002196
[epoch:  20/1000, batch:   492/ 1052, ite: 5120] train loss: 0.016821, tar: 0.001775 
l0: 0.000905, l1: 0.000755, l2: 0.000918, l3: 0.001315, l4: 0.001723, l5: 0.002009, l6: 0.004836
[epoch:  20/1000, batch:   572/ 1052, ite: 5140] train loss: 0.016796, tar: 0.001772 
l0: 0.002023, l1: 0.002575, l2: 0.001806, l3: 0.001884, l4: 0.001543, l5: 0.002635, l6: 0.004476
[epoch:  20/1000, batch:   652/ 1052, ite: 5160] train loss: 0.016772, tar: 0.001769 
l0: 0.001037, l1: 0.000905, l2: 0.001056, l3: 0.001358, l4: 0.001371, l5: 0.001955, l6: 0.002638
[epoch:  20/1000, batch:   732/ 1052, ite: 5180] train loss: 0.016749, tar: 0.001766 
l0: 0.000710, l1: 0.000604, l2: 0.000753, l3: 0.001077, l4: 0.001140, l5: 0.001832, l6: 0.002213
[epoch:  20/1000, batch:   812/ 1052, ite: 5200] train loss: 0.016725, tar: 0.001763 
l0: 0.001172, l1: 0.001265, l2: 0.001068, l3: 0.001421, l4: 0.001599, l5: 0.002170, l6: 0.002455
[epoch:  20/1000, batch:   892/ 1052, ite: 5220] train loss: 0.016696, tar: 0.001759 
l0: 0.000980, l1: 0.001250, l2: 0.000990, l3: 0.001554, l4: 0.001325, l5: 0.002254, l6: 0.002662
[epoch:  20/1000, batch:   972/ 1052, ite: 5240] train loss: 0.016670, tar: 0.001756 
l0: 0.001091, l1: 0.000819, l2: 0.001441, l3: 0.001618, l4: 0.001671, l5: 0.002267, l6: 0.003000
[epoch:  20/1000, batch:  1052/ 1052, ite: 5260] train loss: 0.016644, tar: 0.001753 
[Epoch 20/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000326, l1: 0.000357, l2: 0.000353, l3: 0.000348, l4: 0.000682, l5: 0.001153, l6: 0.001238
[epoch:  21/1000, batch:    80/ 1052, ite: 5280] train loss: 0.016611, tar: 0.001749 
l0: 0.000305, l1: 0.000250, l2: 0.000393, l3: 0.000509, l4: 0.000653, l5: 0.001211, l6: 0.001404
[epoch:  21/1000, batch:   160/ 1052, ite: 5300] train loss: 0.016581, tar: 0.001745 
l0: 0.000925, l1: 0.000984, l2: 0.000972, l3: 0.000980, l4: 0.001388, l5: 0.002112, l6: 0.004044
[epoch:  21/1000, batch:   240/ 1052, ite: 5320] train loss: 0.016580, tar: 0.001746 
l0: 0.000657, l1: 0.000678, l2: 0.000683, l3: 0.000784, l4: 0.000939, l5: 0.001361, l6: 0.001739
[epoch:  21/1000, batch:   320/ 1052, ite: 5340] train loss: 0.016555, tar: 0.001743 
l0: 0.003246, l1: 0.004145, l2: 0.003477, l3: 0.002899, l4: 0.002454, l5: 0.002588, l6: 0.002182
[epoch:  21/1000, batch:   400/ 1052, ite: 5360] train loss: 0.016536, tar: 0.001740 
l0: 0.000529, l1: 0.000560, l2: 0.000584, l3: 0.000569, l4: 0.000717, l5: 0.001637, l6: 0.002084
[epoch:  21/1000, batch:   480/ 1052, ite: 5380] train loss: 0.016506, tar: 0.001737 
l0: 0.000648, l1: 0.000783, l2: 0.000664, l3: 0.000748, l4: 0.000783, l5: 0.001186, l6: 0.002321
[epoch:  21/1000, batch:   560/ 1052, ite: 5400] train loss: 0.016483, tar: 0.001734 
l0: 0.000564, l1: 0.000671, l2: 0.000491, l3: 0.000529, l4: 0.000611, l5: 0.000800, l6: 0.001024
[epoch:  21/1000, batch:   640/ 1052, ite: 5420] train loss: 0.016464, tar: 0.001732 
l0: 0.000842, l1: 0.000995, l2: 0.000986, l3: 0.001000, l4: 0.001315, l5: 0.001695, l6: 0.002181
[epoch:  21/1000, batch:   720/ 1052, ite: 5440] train loss: 0.016443, tar: 0.001729 
l0: 0.001488, l1: 0.001214, l2: 0.001673, l3: 0.001785, l4: 0.002163, l5: 0.003043, l6: 0.003265
[epoch:  21/1000, batch:   800/ 1052, ite: 5460] train loss: 0.016419, tar: 0.001726 
l0: 0.001619, l1: 0.001623, l2: 0.001517, l3: 0.001731, l4: 0.002138, l5: 0.002872, l6: 0.005131
[epoch:  21/1000, batch:   880/ 1052, ite: 5480] train loss: 0.016391, tar: 0.001722 
l0: 0.000571, l1: 0.000495, l2: 0.001033, l3: 0.000698, l4: 0.000931, l5: 0.001492, l6: 0.002410
[epoch:  21/1000, batch:   960/ 1052, ite: 5500] train loss: 0.016378, tar: 0.001721 
l0: 0.000235, l1: 0.000220, l2: 0.000276, l3: 0.000306, l4: 0.000780, l5: 0.000829, l6: 0.001651
[epoch:  21/1000, batch:  1040/ 1052, ite: 5520] train loss: 0.016362, tar: 0.001718 
[Epoch 21/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.001652, l1: 0.001936, l2: 0.001743, l3: 0.001658, l4: 0.002100, l5: 0.003140, l6: 0.004425
[epoch:  22/1000, batch:    68/ 1052, ite: 5540] train loss: 0.016341, tar: 0.001715 
l0: 0.000599, l1: 0.000606, l2: 0.000600, l3: 0.000676, l4: 0.001017, l5: 0.001354, l6: 0.002308
[epoch:  22/1000, batch:   148/ 1052, ite: 5560] train loss: 0.016318, tar: 0.001713 
l0: 0.000492, l1: 0.000505, l2: 0.000557, l3: 0.000701, l4: 0.000866, l5: 0.001790, l6: 0.001762
[epoch:  22/1000, batch:   228/ 1052, ite: 5580] train loss: 0.016318, tar: 0.001713 
l0: 0.000824, l1: 0.000780, l2: 0.000870, l3: 0.000897, l4: 0.000891, l5: 0.001582, l6: 0.002761
[epoch:  22/1000, batch:   308/ 1052, ite: 5600] train loss: 0.016299, tar: 0.001711 
l0: 0.000528, l1: 0.000562, l2: 0.000598, l3: 0.000704, l4: 0.000976, l5: 0.001215, l6: 0.001623
[epoch:  22/1000, batch:   388/ 1052, ite: 5620] train loss: 0.016272, tar: 0.001707 
l0: 0.000675, l1: 0.000907, l2: 0.000695, l3: 0.001371, l4: 0.001515, l5: 0.001936, l6: 0.002064
[epoch:  22/1000, batch:   468/ 1052, ite: 5640] train loss: 0.016253, tar: 0.001705 
l0: 0.002414, l1: 0.002405, l2: 0.002543, l3: 0.002344, l4: 0.002329, l5: 0.003500, l6: 0.005294
[epoch:  22/1000, batch:   548/ 1052, ite: 5660] train loss: 0.016233, tar: 0.001703 
l0: 0.001528, l1: 0.001403, l2: 0.001505, l3: 0.001863, l4: 0.001983, l5: 0.002670, l6: 0.004587
[epoch:  22/1000, batch:   628/ 1052, ite: 5680] train loss: 0.016212, tar: 0.001700 
l0: 0.000628, l1: 0.000586, l2: 0.000642, l3: 0.000808, l4: 0.001414, l5: 0.001903, l6: 0.003576
[epoch:  22/1000, batch:   708/ 1052, ite: 5700] train loss: 0.016196, tar: 0.001699 
l0: 0.000885, l1: 0.000818, l2: 0.001150, l3: 0.001175, l4: 0.001803, l5: 0.002317, l6: 0.003250
[epoch:  22/1000, batch:   788/ 1052, ite: 5720] train loss: 0.016183, tar: 0.001697 
l0: 0.000674, l1: 0.000820, l2: 0.000667, l3: 0.000629, l4: 0.000902, l5: 0.001246, l6: 0.002519
[epoch:  22/1000, batch:   868/ 1052, ite: 5740] train loss: 0.016162, tar: 0.001694 
l0: 0.000359, l1: 0.000373, l2: 0.000461, l3: 0.000643, l4: 0.000690, l5: 0.000593, l6: 0.000904
[epoch:  22/1000, batch:   948/ 1052, ite: 5760] train loss: 0.016140, tar: 0.001692 
l0: 0.000635, l1: 0.000633, l2: 0.000610, l3: 0.000820, l4: 0.000781, l5: 0.001277, l6: 0.002033
[epoch:  22/1000, batch:  1028/ 1052, ite: 5780] train loss: 0.016123, tar: 0.001690 
[Epoch 22/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000508, l1: 0.000536, l2: 0.000475, l3: 0.000593, l4: 0.000716, l5: 0.001176, l6: 0.001636
[epoch:  23/1000, batch:    56/ 1052, ite: 5800] train loss: 0.016126, tar: 0.001692 
l0: 0.000489, l1: 0.000485, l2: 0.000535, l3: 0.000578, l4: 0.000980, l5: 0.001248, l6: 0.002396
[epoch:  23/1000, batch:   136/ 1052, ite: 5820] train loss: 0.016108, tar: 0.001690 
l0: 0.000931, l1: 0.000934, l2: 0.001029, l3: 0.000963, l4: 0.001111, l5: 0.002217, l6: 0.003178
[epoch:  23/1000, batch:   216/ 1052, ite: 5840] train loss: 0.016091, tar: 0.001687 
l0: 0.000556, l1: 0.000633, l2: 0.000519, l3: 0.000727, l4: 0.001085, l5: 0.001892, l6: 0.003543
[epoch:  23/1000, batch:   296/ 1052, ite: 5860] train loss: 0.016071, tar: 0.001685 
l0: 0.000717, l1: 0.000809, l2: 0.000771, l3: 0.000760, l4: 0.001069, l5: 0.001481, l6: 0.002345
[epoch:  23/1000, batch:   376/ 1052, ite: 5880] train loss: 0.016047, tar: 0.001682 
l0: 0.000783, l1: 0.000770, l2: 0.000872, l3: 0.000906, l4: 0.000976, l5: 0.001049, l6: 0.001900
[epoch:  23/1000, batch:   456/ 1052, ite: 5900] train loss: 0.016030, tar: 0.001680 
l0: 0.000963, l1: 0.000974, l2: 0.001214, l3: 0.001021, l4: 0.001397, l5: 0.001635, l6: 0.002807
[epoch:  23/1000, batch:   536/ 1052, ite: 5920] train loss: 0.016012, tar: 0.001678 
l0: 0.000581, l1: 0.000508, l2: 0.000646, l3: 0.000765, l4: 0.000870, l5: 0.001624, l6: 0.002284
[epoch:  23/1000, batch:   616/ 1052, ite: 5940] train loss: 0.015990, tar: 0.001675 
l0: 0.000932, l1: 0.000974, l2: 0.000910, l3: 0.001122, l4: 0.001158, l5: 0.001619, l6: 0.002518
[epoch:  23/1000, batch:   696/ 1052, ite: 5960] train loss: 0.015964, tar: 0.001672 
l0: 0.000839, l1: 0.000756, l2: 0.000919, l3: 0.001399, l4: 0.001693, l5: 0.002241, l6: 0.003342
[epoch:  23/1000, batch:   776/ 1052, ite: 5980] train loss: 0.015955, tar: 0.001671 
l0: 0.001555, l1: 0.001405, l2: 0.001858, l3: 0.001723, l4: 0.001602, l5: 0.002205, l6: 0.002953
[epoch:  23/1000, batch:   856/ 1052, ite: 6000] train loss: 0.015941, tar: 0.001669 
l0: 0.001027, l1: 0.001310, l2: 0.001185, l3: 0.001093, l4: 0.001634, l5: 0.002228, l6: 0.003401
[epoch:  23/1000, batch:   936/ 1052, ite: 6020] train loss: 0.015934, tar: 0.001668 
l0: 0.000679, l1: 0.000676, l2: 0.000690, l3: 0.000860, l4: 0.000928, l5: 0.001765, l6: 0.003118
[epoch:  23/1000, batch:  1016/ 1052, ite: 6040] train loss: 0.015923, tar: 0.001667 
[Epoch 23/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.001315, l1: 0.001270, l2: 0.001461, l3: 0.001466, l4: 0.001690, l5: 0.002494, l6: 0.004689
[epoch:  24/1000, batch:    44/ 1052, ite: 6060] train loss: 0.015901, tar: 0.001664 
l0: 0.000928, l1: 0.000832, l2: 0.000994, l3: 0.001370, l4: 0.001512, l5: 0.001922, l6: 0.002577
[epoch:  24/1000, batch:   124/ 1052, ite: 6080] train loss: 0.015877, tar: 0.001661 
l0: 0.000586, l1: 0.000633, l2: 0.000571, l3: 0.000872, l4: 0.001094, l5: 0.001353, l6: 0.001629
[epoch:  24/1000, batch:   204/ 1052, ite: 6100] train loss: 0.015856, tar: 0.001658 
l0: 0.000500, l1: 0.000501, l2: 0.000520, l3: 0.000632, l4: 0.000708, l5: 0.001337, l6: 0.002085
[epoch:  24/1000, batch:   284/ 1052, ite: 6120] train loss: 0.015838, tar: 0.001656 
l0: 0.004475, l1: 0.004417, l2: 0.004233, l3: 0.005045, l4: 0.003568, l5: 0.004012, l6: 0.006728
[epoch:  24/1000, batch:   364/ 1052, ite: 6140] train loss: 0.015816, tar: 0.001653 
l0: 0.000484, l1: 0.000470, l2: 0.000513, l3: 0.000637, l4: 0.000726, l5: 0.001339, l6: 0.001548
[epoch:  24/1000, batch:   444/ 1052, ite: 6160] train loss: 0.015795, tar: 0.001651 
l0: 0.000888, l1: 0.001343, l2: 0.001159, l3: 0.000774, l4: 0.000739, l5: 0.001021, l6: 0.001288
[epoch:  24/1000, batch:   524/ 1052, ite: 6180] train loss: 0.015784, tar: 0.001649 
l0: 0.001408, l1: 0.001265, l2: 0.001748, l3: 0.001557, l4: 0.001718, l5: 0.002232, l6: 0.002980
[epoch:  24/1000, batch:   604/ 1052, ite: 6200] train loss: 0.015780, tar: 0.001649 
l0: 0.000678, l1: 0.000602, l2: 0.000756, l3: 0.000905, l4: 0.001113, l5: 0.001814, l6: 0.002758
[epoch:  24/1000, batch:   684/ 1052, ite: 6220] train loss: 0.015755, tar: 0.001645 
l0: 0.000650, l1: 0.000725, l2: 0.000766, l3: 0.000702, l4: 0.001024, l5: 0.001722, l6: 0.001780
[epoch:  24/1000, batch:   764/ 1052, ite: 6240] train loss: 0.015735, tar: 0.001643 
l0: 0.000833, l1: 0.000780, l2: 0.001170, l3: 0.001022, l4: 0.001317, l5: 0.002097, l6: 0.002767
[epoch:  24/1000, batch:   844/ 1052, ite: 6260] train loss: 0.015717, tar: 0.001641 
l0: 0.000914, l1: 0.000936, l2: 0.001212, l3: 0.001118, l4: 0.001621, l5: 0.002462, l6: 0.002872
[epoch:  24/1000, batch:   924/ 1052, ite: 6280] train loss: 0.015724, tar: 0.001642 
l0: 0.000691, l1: 0.000904, l2: 0.000758, l3: 0.000968, l4: 0.000962, l5: 0.001529, l6: 0.001468
[epoch:  24/1000, batch:  1004/ 1052, ite: 6300] train loss: 0.015729, tar: 0.001643 
[Epoch 24/1000] Test loss: 0.012, Test target loss: 0.001
l0: 0.000743, l1: 0.000848, l2: 0.000808, l3: 0.000841, l4: 0.001129, l5: 0.001989, l6: 0.003113
[epoch:  25/1000, batch:    32/ 1052, ite: 6320] train loss: 0.015718, tar: 0.001641 
l0: 0.001985, l1: 0.001612, l2: 0.002262, l3: 0.002905, l4: 0.002828, l5: 0.003346, l6: 0.007718
[epoch:  25/1000, batch:   112/ 1052, ite: 6340] train loss: 0.015710, tar: 0.001640 
l0: 0.001310, l1: 0.001687, l2: 0.001199, l3: 0.001131, l4: 0.002090, l5: 0.003895, l6: 0.003708
[epoch:  25/1000, batch:   192/ 1052, ite: 6360] train loss: 0.015706, tar: 0.001640 
l0: 0.000761, l1: 0.000773, l2: 0.000801, l3: 0.000924, l4: 0.001101, l5: 0.001927, l6: 0.002386
[epoch:  25/1000, batch:   272/ 1052, ite: 6380] train loss: 0.015688, tar: 0.001637 
l0: 0.000473, l1: 0.000509, l2: 0.000612, l3: 0.000473, l4: 0.000476, l5: 0.001093, l6: 0.001279
[epoch:  25/1000, batch:   352/ 1052, ite: 6400] train loss: 0.015667, tar: 0.001635 
l0: 0.000762, l1: 0.000775, l2: 0.000902, l3: 0.001167, l4: 0.001288, l5: 0.001513, l6: 0.001861
[epoch:  25/1000, batch:   432/ 1052, ite: 6420] train loss: 0.015650, tar: 0.001633 
l0: 0.000865, l1: 0.000906, l2: 0.000902, l3: 0.001090, l4: 0.001367, l5: 0.001959, l6: 0.002946
[epoch:  25/1000, batch:   512/ 1052, ite: 6440] train loss: 0.015632, tar: 0.001631 
l0: 0.000474, l1: 0.000450, l2: 0.000527, l3: 0.000646, l4: 0.000976, l5: 0.001225, l6: 0.002031
[epoch:  25/1000, batch:   592/ 1052, ite: 6460] train loss: 0.015611, tar: 0.001628 
l0: 0.000883, l1: 0.000928, l2: 0.000952, l3: 0.000952, l4: 0.001103, l5: 0.001761, l6: 0.002733
[epoch:  25/1000, batch:   672/ 1052, ite: 6480] train loss: 0.015593, tar: 0.001626 
l0: 0.000475, l1: 0.000510, l2: 0.000487, l3: 0.000568, l4: 0.000729, l5: 0.001345, l6: 0.002020
[epoch:  25/1000, batch:   752/ 1052, ite: 6500] train loss: 0.015577, tar: 0.001624 
l0: 0.000444, l1: 0.000480, l2: 0.000441, l3: 0.000567, l4: 0.000776, l5: 0.001573, l6: 0.001793
[epoch:  25/1000, batch:   832/ 1052, ite: 6520] train loss: 0.015573, tar: 0.001624 
l0: 0.001159, l1: 0.001100, l2: 0.001165, l3: 0.001417, l4: 0.002002, l5: 0.003291, l6: 0.004904
[epoch:  25/1000, batch:   912/ 1052, ite: 6540] train loss: 0.015560, tar: 0.001622 
l0: 0.000704, l1: 0.000950, l2: 0.000782, l3: 0.000780, l4: 0.000951, l5: 0.001453, l6: 0.001454
[epoch:  25/1000, batch:   992/ 1052, ite: 6560] train loss: 0.015548, tar: 0.001621 
[Epoch 25/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000996, l1: 0.001027, l2: 0.000990, l3: 0.001324, l4: 0.001384, l5: 0.002322, l6: 0.002276
[epoch:  26/1000, batch:    20/ 1052, ite: 6580] train loss: 0.015536, tar: 0.001619 
l0: 0.000858, l1: 0.000840, l2: 0.000871, l3: 0.001121, l4: 0.001710, l5: 0.002624, l6: 0.003442
[epoch:  26/1000, batch:   100/ 1052, ite: 6600] train loss: 0.015525, tar: 0.001617 
l0: 0.000622, l1: 0.001161, l2: 0.000539, l3: 0.000947, l4: 0.001405, l5: 0.001131, l6: 0.001536
[epoch:  26/1000, batch:   180/ 1052, ite: 6620] train loss: 0.015507, tar: 0.001615 
l0: 0.000581, l1: 0.000548, l2: 0.000608, l3: 0.000883, l4: 0.001086, l5: 0.001568, l6: 0.002431
[epoch:  26/1000, batch:   260/ 1052, ite: 6640] train loss: 0.015488, tar: 0.001613 
l0: 0.000874, l1: 0.001001, l2: 0.000910, l3: 0.000865, l4: 0.001056, l5: 0.001981, l6: 0.002368
[epoch:  26/1000, batch:   340/ 1052, ite: 6660] train loss: 0.015471, tar: 0.001611 
l0: 0.000469, l1: 0.000436, l2: 0.000512, l3: 0.000605, l4: 0.000778, l5: 0.001182, l6: 0.002352
[epoch:  26/1000, batch:   420/ 1052, ite: 6680] train loss: 0.015453, tar: 0.001608 
l0: 0.000385, l1: 0.000368, l2: 0.000401, l3: 0.000533, l4: 0.000556, l5: 0.001075, l6: 0.001707
[epoch:  26/1000, batch:   500/ 1052, ite: 6700] train loss: 0.015433, tar: 0.001606 
l0: 0.000532, l1: 0.000539, l2: 0.000503, l3: 0.000761, l4: 0.001074, l5: 0.001876, l6: 0.002399
[epoch:  26/1000, batch:   580/ 1052, ite: 6720] train loss: 0.015418, tar: 0.001604 
l0: 0.000511, l1: 0.000457, l2: 0.000523, l3: 0.000787, l4: 0.000996, l5: 0.001439, l6: 0.002244
[epoch:  26/1000, batch:   660/ 1052, ite: 6740] train loss: 0.015415, tar: 0.001605 
l0: 0.000853, l1: 0.000878, l2: 0.001006, l3: 0.000856, l4: 0.000953, l5: 0.001785, l6: 0.001631
[epoch:  26/1000, batch:   740/ 1052, ite: 6760] train loss: 0.015401, tar: 0.001603 
l0: 0.002141, l1: 0.002415, l2: 0.002361, l3: 0.001882, l4: 0.001889, l5: 0.002291, l6: 0.003106
[epoch:  26/1000, batch:   820/ 1052, ite: 6780] train loss: 0.015388, tar: 0.001602 
l0: 0.000730, l1: 0.000777, l2: 0.000773, l3: 0.000694, l4: 0.000913, l5: 0.001385, l6: 0.001985
[epoch:  26/1000, batch:   900/ 1052, ite: 6800] train loss: 0.015368, tar: 0.001599 
l0: 0.000649, l1: 0.000611, l2: 0.000683, l3: 0.000821, l4: 0.001134, l5: 0.001238, l6: 0.001724
[epoch:  26/1000, batch:   980/ 1052, ite: 6820] train loss: 0.015354, tar: 0.001598 
[Epoch 26/1000] Test loss: 0.012, Test target loss: 0.001
l0: 0.000473, l1: 0.000434, l2: 0.000448, l3: 0.000729, l4: 0.000794, l5: 0.001551, l6: 0.001780
[epoch:  27/1000, batch:     8/ 1052, ite: 6840] train loss: 0.015340, tar: 0.001596 
l0: 0.001367, l1: 0.001481, l2: 0.001553, l3: 0.001347, l4: 0.001424, l5: 0.002505, l6: 0.002644
[epoch:  27/1000, batch:    88/ 1052, ite: 6860] train loss: 0.015323, tar: 0.001594 
l0: 0.000645, l1: 0.000639, l2: 0.000602, l3: 0.000840, l4: 0.001123, l5: 0.001751, l6: 0.002451
[epoch:  27/1000, batch:   168/ 1052, ite: 6880] train loss: 0.015304, tar: 0.001591 
l0: 0.000561, l1: 0.000611, l2: 0.000561, l3: 0.000660, l4: 0.000852, l5: 0.001181, l6: 0.002155
[epoch:  27/1000, batch:   248/ 1052, ite: 6900] train loss: 0.015288, tar: 0.001589 
l0: 0.001248, l1: 0.001296, l2: 0.001320, l3: 0.001277, l4: 0.001316, l5: 0.001674, l6: 0.002650
[epoch:  27/1000, batch:   328/ 1052, ite: 6920] train loss: 0.015269, tar: 0.001587 
l0: 0.000752, l1: 0.000780, l2: 0.000777, l3: 0.000939, l4: 0.001216, l5: 0.001441, l6: 0.002514
[epoch:  27/1000, batch:   408/ 1052, ite: 6940] train loss: 0.015255, tar: 0.001585 
l0: 0.000559, l1: 0.000520, l2: 0.000669, l3: 0.000661, l4: 0.000978, l5: 0.001513, l6: 0.002703
[epoch:  27/1000, batch:   488/ 1052, ite: 6960] train loss: 0.015239, tar: 0.001583 
l0: 0.001097, l1: 0.001154, l2: 0.001098, l3: 0.001192, l4: 0.001220, l5: 0.001934, l6: 0.003138
[epoch:  27/1000, batch:   568/ 1052, ite: 6980] train loss: 0.015224, tar: 0.001581 
l0: 0.000766, l1: 0.000690, l2: 0.000892, l3: 0.000914, l4: 0.001066, l5: 0.002043, l6: 0.002663
[epoch:  27/1000, batch:   648/ 1052, ite: 7000] train loss: 0.015209, tar: 0.001579 
l0: 0.001814, l1: 0.002099, l2: 0.001883, l3: 0.002386, l4: 0.001954, l5: 0.002212, l6: 0.002622
[epoch:  27/1000, batch:   728/ 1052, ite: 7020] train loss: 0.015216, tar: 0.001581 
l0: 0.001236, l1: 0.001263, l2: 0.001306, l3: 0.001607, l4: 0.002015, l5: 0.002453, l6: 0.003372
[epoch:  27/1000, batch:   808/ 1052, ite: 7040] train loss: 0.015210, tar: 0.001580 
l0: 0.001716, l1: 0.001689, l2: 0.002062, l3: 0.001775, l4: 0.001766, l5: 0.002106, l6: 0.003282
[epoch:  27/1000, batch:   888/ 1052, ite: 7060] train loss: 0.015194, tar: 0.001578 
l0: 0.000571, l1: 0.000619, l2: 0.000556, l3: 0.000645, l4: 0.000832, l5: 0.001657, l6: 0.003029
[epoch:  27/1000, batch:   968/ 1052, ite: 7080] train loss: 0.015175, tar: 0.001576 
l0: 0.000453, l1: 0.000560, l2: 0.000454, l3: 0.000544, l4: 0.000644, l5: 0.001092, l6: 0.001918
[epoch:  27/1000, batch:  1048/ 1052, ite: 7100] train loss: 0.015161, tar: 0.001574 
[Epoch 27/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000704, l1: 0.000659, l2: 0.000802, l3: 0.000868, l4: 0.001079, l5: 0.001613, l6: 0.002989
[epoch:  28/1000, batch:    76/ 1052, ite: 7120] train loss: 0.015145, tar: 0.001572 
l0: 0.000566, l1: 0.000552, l2: 0.000602, l3: 0.000716, l4: 0.000934, l5: 0.001271, l6: 0.002028
[epoch:  28/1000, batch:   156/ 1052, ite: 7140] train loss: 0.015127, tar: 0.001569 
l0: 0.000541, l1: 0.000548, l2: 0.000542, l3: 0.000618, l4: 0.001239, l5: 0.002703, l6: 0.003210
[epoch:  28/1000, batch:   236/ 1052, ite: 7160] train loss: 0.015118, tar: 0.001568 
l0: 0.000899, l1: 0.000772, l2: 0.001030, l3: 0.001278, l4: 0.001520, l5: 0.002075, l6: 0.003232
[epoch:  28/1000, batch:   316/ 1052, ite: 7180] train loss: 0.015097, tar: 0.001566 
l0: 0.000817, l1: 0.000866, l2: 0.000760, l3: 0.001086, l4: 0.001189, l5: 0.001573, l6: 0.002664
[epoch:  28/1000, batch:   396/ 1052, ite: 7200] train loss: 0.015081, tar: 0.001563 
l0: 0.000345, l1: 0.000380, l2: 0.000348, l3: 0.000357, l4: 0.000540, l5: 0.000775, l6: 0.001223
[epoch:  28/1000, batch:   476/ 1052, ite: 7220] train loss: 0.015066, tar: 0.001562 
l0: 0.000634, l1: 0.000618, l2: 0.000705, l3: 0.000858, l4: 0.000832, l5: 0.001358, l6: 0.001763
[epoch:  28/1000, batch:   556/ 1052, ite: 7240] train loss: 0.015054, tar: 0.001560 
l0: 0.001065, l1: 0.001576, l2: 0.001067, l3: 0.000713, l4: 0.000724, l5: 0.001356, l6: 0.001830
[epoch:  28/1000, batch:   636/ 1052, ite: 7260] train loss: 0.015045, tar: 0.001559 
l0: 0.000534, l1: 0.000477, l2: 0.000639, l3: 0.000772, l4: 0.000942, l5: 0.001365, l6: 0.002383
[epoch:  28/1000, batch:   716/ 1052, ite: 7280] train loss: 0.015029, tar: 0.001557 
l0: 0.000379, l1: 0.000363, l2: 0.000423, l3: 0.000463, l4: 0.000520, l5: 0.000926, l6: 0.002017
[epoch:  28/1000, batch:   796/ 1052, ite: 7300] train loss: 0.015014, tar: 0.001555 
l0: 0.000355, l1: 0.000386, l2: 0.000443, l3: 0.000410, l4: 0.000524, l5: 0.000950, l6: 0.000697
[epoch:  28/1000, batch:   876/ 1052, ite: 7320] train loss: 0.014998, tar: 0.001553 
l0: 0.000404, l1: 0.000399, l2: 0.000421, l3: 0.000543, l4: 0.000698, l5: 0.001189, l6: 0.001667
[epoch:  28/1000, batch:   956/ 1052, ite: 7340] train loss: 0.014982, tar: 0.001551 
l0: 0.003161, l1: 0.003219, l2: 0.003749, l3: 0.004025, l4: 0.004200, l5: 0.006156, l6: 0.006908
[epoch:  28/1000, batch:  1036/ 1052, ite: 7360] train loss: 0.014989, tar: 0.001552 
[Epoch 28/1000] Test loss: 0.015, Test target loss: 0.001
l0: 0.000744, l1: 0.000918, l2: 0.000780, l3: 0.000922, l4: 0.001204, l5: 0.001152, l6: 0.001861
[epoch:  29/1000, batch:    64/ 1052, ite: 7380] train loss: 0.014970, tar: 0.001550 
l0: 0.000629, l1: 0.000652, l2: 0.000640, l3: 0.000663, l4: 0.001000, l5: 0.001565, l6: 0.001632
[epoch:  29/1000, batch:   144/ 1052, ite: 7400] train loss: 0.014957, tar: 0.001548 
l0: 0.003222, l1: 0.003360, l2: 0.003467, l3: 0.003553, l4: 0.004193, l5: 0.004960, l6: 0.005856
[epoch:  29/1000, batch:   224/ 1052, ite: 7420] train loss: 0.014945, tar: 0.001547 
l0: 0.001144, l1: 0.001161, l2: 0.001284, l3: 0.001676, l4: 0.001755, l5: 0.002182, l6: 0.002875
[epoch:  29/1000, batch:   304/ 1052, ite: 7440] train loss: 0.014940, tar: 0.001546 
l0: 0.000834, l1: 0.000847, l2: 0.000848, l3: 0.000931, l4: 0.001330, l5: 0.001397, l6: 0.002544
[epoch:  29/1000, batch:   384/ 1052, ite: 7460] train loss: 0.014921, tar: 0.001544 
l0: 0.001090, l1: 0.001303, l2: 0.001048, l3: 0.001217, l4: 0.001135, l5: 0.001593, l6: 0.001958
[epoch:  29/1000, batch:   464/ 1052, ite: 7480] train loss: 0.014921, tar: 0.001543 
l0: 0.000750, l1: 0.000767, l2: 0.000752, l3: 0.001131, l4: 0.001244, l5: 0.001469, l6: 0.002156
[epoch:  29/1000, batch:   544/ 1052, ite: 7500] train loss: 0.014911, tar: 0.001542 
l0: 0.000666, l1: 0.000684, l2: 0.000719, l3: 0.000775, l4: 0.000886, l5: 0.001480, l6: 0.001891
[epoch:  29/1000, batch:   624/ 1052, ite: 7520] train loss: 0.014899, tar: 0.001541 
l0: 0.001410, l1: 0.001277, l2: 0.001502, l3: 0.002246, l4: 0.002286, l5: 0.002861, l6: 0.004258
[epoch:  29/1000, batch:   704/ 1052, ite: 7540] train loss: 0.014888, tar: 0.001539 
l0: 0.000656, l1: 0.000658, l2: 0.000677, l3: 0.000850, l4: 0.001439, l5: 0.001602, l6: 0.002324
[epoch:  29/1000, batch:   784/ 1052, ite: 7560] train loss: 0.014887, tar: 0.001539 
l0: 0.000595, l1: 0.000559, l2: 0.000845, l3: 0.001095, l4: 0.001135, l5: 0.001652, l6: 0.001893
[epoch:  29/1000, batch:   864/ 1052, ite: 7580] train loss: 0.014884, tar: 0.001538 
l0: 0.000456, l1: 0.000473, l2: 0.000572, l3: 0.000665, l4: 0.000764, l5: 0.001411, l6: 0.002198
[epoch:  29/1000, batch:   944/ 1052, ite: 7600] train loss: 0.014871, tar: 0.001536 
l0: 0.001096, l1: 0.001594, l2: 0.001807, l3: 0.000955, l4: 0.000923, l5: 0.001562, l6: 0.002544
[epoch:  29/1000, batch:  1024/ 1052, ite: 7620] train loss: 0.014855, tar: 0.001534 
[Epoch 29/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000276, l1: 0.000302, l2: 0.000305, l3: 0.000290, l4: 0.000476, l5: 0.000948, l6: 0.001494
[epoch:  30/1000, batch:    52/ 1052, ite: 7640] train loss: 0.014838, tar: 0.001532 
l0: 0.001946, l1: 0.002123, l2: 0.001961, l3: 0.002154, l4: 0.002438, l5: 0.002657, l6: 0.003529
[epoch:  30/1000, batch:   132/ 1052, ite: 7660] train loss: 0.014824, tar: 0.001530 
l0: 0.000594, l1: 0.000778, l2: 0.000542, l3: 0.000750, l4: 0.000674, l5: 0.001112, l6: 0.001674
[epoch:  30/1000, batch:   212/ 1052, ite: 7680] train loss: 0.014812, tar: 0.001528 
l0: 0.000391, l1: 0.000412, l2: 0.000408, l3: 0.000552, l4: 0.000681, l5: 0.001129, l6: 0.001330
[epoch:  30/1000, batch:   292/ 1052, ite: 7700] train loss: 0.014802, tar: 0.001527 
l0: 0.000712, l1: 0.000754, l2: 0.000665, l3: 0.000860, l4: 0.001168, l5: 0.002188, l6: 0.002794
[epoch:  30/1000, batch:   372/ 1052, ite: 7720] train loss: 0.014791, tar: 0.001526 
l0: 0.000429, l1: 0.000427, l2: 0.000419, l3: 0.000556, l4: 0.000673, l5: 0.001062, l6: 0.002022
[epoch:  30/1000, batch:   452/ 1052, ite: 7740] train loss: 0.014777, tar: 0.001524 
l0: 0.001282, l1: 0.001327, l2: 0.001342, l3: 0.001671, l4: 0.002066, l5: 0.002448, l6: 0.003926
[epoch:  30/1000, batch:   532/ 1052, ite: 7760] train loss: 0.014766, tar: 0.001522 
l0: 0.000439, l1: 0.000518, l2: 0.000415, l3: 0.000541, l4: 0.000624, l5: 0.001386, l6: 0.001828
[epoch:  30/1000, batch:   612/ 1052, ite: 7780] train loss: 0.014751, tar: 0.001520 
l0: 0.000591, l1: 0.000532, l2: 0.000582, l3: 0.000914, l4: 0.000945, l5: 0.001739, l6: 0.003600
[epoch:  30/1000, batch:   692/ 1052, ite: 7800] train loss: 0.014739, tar: 0.001519 
l0: 0.000537, l1: 0.000490, l2: 0.000577, l3: 0.000748, l4: 0.001118, l5: 0.001555, l6: 0.002531
[epoch:  30/1000, batch:   772/ 1052, ite: 7820] train loss: 0.014722, tar: 0.001517 
l0: 0.000505, l1: 0.000508, l2: 0.000498, l3: 0.000618, l4: 0.000796, l5: 0.001555, l6: 0.002354
[epoch:  30/1000, batch:   852/ 1052, ite: 7840] train loss: 0.014711, tar: 0.001515 
l0: 0.002192, l1: 0.001810, l2: 0.002457, l3: 0.002862, l4: 0.002513, l5: 0.002802, l6: 0.004906
[epoch:  30/1000, batch:   932/ 1052, ite: 7860] train loss: 0.014697, tar: 0.001514 
l0: 0.000732, l1: 0.000669, l2: 0.000948, l3: 0.000860, l4: 0.001240, l5: 0.001555, l6: 0.002539
[epoch:  30/1000, batch:  1012/ 1052, ite: 7880] train loss: 0.014704, tar: 0.001515 
[Epoch 30/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000326, l1: 0.000294, l2: 0.000404, l3: 0.000494, l4: 0.000712, l5: 0.000763, l6: 0.001542
[epoch:  31/1000, batch:    40/ 1052, ite: 7900] train loss: 0.014685, tar: 0.001513 
l0: 0.003992, l1: 0.004527, l2: 0.003998, l3: 0.004756, l4: 0.004782, l5: 0.004556, l6: 0.004761
[epoch:  31/1000, batch:   120/ 1052, ite: 7920] train loss: 0.014691, tar: 0.001514 
l0: 0.001113, l1: 0.001300, l2: 0.001228, l3: 0.001454, l4: 0.001486, l5: 0.001986, l6: 0.002438
[epoch:  31/1000, batch:   200/ 1052, ite: 7940] train loss: 0.014683, tar: 0.001513 
l0: 0.001423, l1: 0.001993, l2: 0.001181, l3: 0.001170, l4: 0.001363, l5: 0.001749, l6: 0.002838
[epoch:  31/1000, batch:   280/ 1052, ite: 7960] train loss: 0.014671, tar: 0.001511 
l0: 0.000598, l1: 0.000853, l2: 0.000764, l3: 0.000747, l4: 0.000867, l5: 0.001508, l6: 0.001732
[epoch:  31/1000, batch:   360/ 1052, ite: 7980] train loss: 0.014658, tar: 0.001510 
l0: 0.000336, l1: 0.000319, l2: 0.000317, l3: 0.000448, l4: 0.000584, l5: 0.000972, l6: 0.001665
[epoch:  31/1000, batch:   440/ 1052, ite: 8000] train loss: 0.014643, tar: 0.001508 
l0: 0.000548, l1: 0.000520, l2: 0.000557, l3: 0.000839, l4: 0.000771, l5: 0.000991, l6: 0.001760
[epoch:  31/1000, batch:   520/ 1052, ite: 8020] train loss: 0.014635, tar: 0.001507 
l0: 0.001049, l1: 0.001151, l2: 0.001080, l3: 0.001073, l4: 0.001439, l5: 0.001952, l6: 0.004364
[epoch:  31/1000, batch:   600/ 1052, ite: 8040] train loss: 0.014620, tar: 0.001505 
l0: 0.000493, l1: 0.000484, l2: 0.000555, l3: 0.000538, l4: 0.000764, l5: 0.001414, l6: 0.002006
[epoch:  31/1000, batch:   680/ 1052, ite: 8060] train loss: 0.014608, tar: 0.001503 
l0: 0.000641, l1: 0.000672, l2: 0.000659, l3: 0.000676, l4: 0.000775, l5: 0.001730, l6: 0.002723
[epoch:  31/1000, batch:   760/ 1052, ite: 8080] train loss: 0.014598, tar: 0.001502 
l0: 0.000622, l1: 0.000591, l2: 0.000743, l3: 0.000825, l4: 0.001359, l5: 0.002408, l6: 0.003219
[epoch:  31/1000, batch:   840/ 1052, ite: 8100] train loss: 0.014583, tar: 0.001500 
l0: 0.000990, l1: 0.000984, l2: 0.001054, l3: 0.001077, l4: 0.001678, l5: 0.002357, l6: 0.002809
[epoch:  31/1000, batch:   920/ 1052, ite: 8120] train loss: 0.014568, tar: 0.001498 
l0: 0.000449, l1: 0.000485, l2: 0.000523, l3: 0.000431, l4: 0.000619, l5: 0.000867, l6: 0.001593
[epoch:  31/1000, batch:  1000/ 1052, ite: 8140] train loss: 0.014553, tar: 0.001496 
[Epoch 31/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000396, l1: 0.000382, l2: 0.000440, l3: 0.000512, l4: 0.000681, l5: 0.001308, l6: 0.001949
[epoch:  32/1000, batch:    28/ 1052, ite: 8160] train loss: 0.014537, tar: 0.001494 
l0: 0.000833, l1: 0.000777, l2: 0.000908, l3: 0.000939, l4: 0.001155, l5: 0.001627, l6: 0.003393
[epoch:  32/1000, batch:   108/ 1052, ite: 8180] train loss: 0.014521, tar: 0.001492 
l0: 0.000364, l1: 0.000391, l2: 0.000365, l3: 0.000429, l4: 0.000671, l5: 0.001247, l6: 0.001617
[epoch:  32/1000, batch:   188/ 1052, ite: 8200] train loss: 0.014504, tar: 0.001490 
l0: 0.000332, l1: 0.000311, l2: 0.000453, l3: 0.000424, l4: 0.000476, l5: 0.000800, l6: 0.001220
[epoch:  32/1000, batch:   268/ 1052, ite: 8220] train loss: 0.014512, tar: 0.001491 
l0: 0.000505, l1: 0.000550, l2: 0.000497, l3: 0.000686, l4: 0.000898, l5: 0.001672, l6: 0.002406
[epoch:  32/1000, batch:   348/ 1052, ite: 8240] train loss: 0.014497, tar: 0.001489 
l0: 0.000291, l1: 0.000288, l2: 0.000300, l3: 0.000393, l4: 0.000651, l5: 0.001158, l6: 0.001558
[epoch:  32/1000, batch:   428/ 1052, ite: 8260] train loss: 0.014486, tar: 0.001488 
l0: 0.000656, l1: 0.000761, l2: 0.000654, l3: 0.000723, l4: 0.000625, l5: 0.000815, l6: 0.001202
[epoch:  32/1000, batch:   508/ 1052, ite: 8280] train loss: 0.014471, tar: 0.001486 
l0: 0.002813, l1: 0.002876, l2: 0.003039, l3: 0.003578, l4: 0.003096, l5: 0.003150, l6: 0.003475
[epoch:  32/1000, batch:   588/ 1052, ite: 8300] train loss: 0.014463, tar: 0.001485 
l0: 0.002051, l1: 0.002344, l2: 0.002294, l3: 0.001938, l4: 0.002005, l5: 0.002495, l6: 0.004712
[epoch:  32/1000, batch:   668/ 1052, ite: 8320] train loss: 0.014451, tar: 0.001484 
l0: 0.000397, l1: 0.000376, l2: 0.000439, l3: 0.000608, l4: 0.000706, l5: 0.000937, l6: 0.001555
[epoch:  32/1000, batch:   748/ 1052, ite: 8340] train loss: 0.014436, tar: 0.001482 
l0: 0.000533, l1: 0.000572, l2: 0.000517, l3: 0.000620, l4: 0.000755, l5: 0.001687, l6: 0.002558
[epoch:  32/1000, batch:   828/ 1052, ite: 8360] train loss: 0.014422, tar: 0.001480 
l0: 0.000221, l1: 0.000210, l2: 0.000268, l3: 0.000301, l4: 0.000500, l5: 0.000874, l6: 0.000917
[epoch:  32/1000, batch:   908/ 1052, ite: 8380] train loss: 0.014407, tar: 0.001478 
l0: 0.001051, l1: 0.001167, l2: 0.001169, l3: 0.001038, l4: 0.001744, l5: 0.002402, l6: 0.002816
[epoch:  32/1000, batch:   988/ 1052, ite: 8400] train loss: 0.014397, tar: 0.001477 
[Epoch 32/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000726, l1: 0.000759, l2: 0.000722, l3: 0.000707, l4: 0.001279, l5: 0.002023, l6: 0.002953
[epoch:  33/1000, batch:    16/ 1052, ite: 8420] train loss: 0.014388, tar: 0.001476 
l0: 0.000610, l1: 0.000560, l2: 0.000634, l3: 0.000702, l4: 0.000753, l5: 0.001395, l6: 0.002637
[epoch:  33/1000, batch:    96/ 1052, ite: 8440] train loss: 0.014375, tar: 0.001474 
l0: 0.000289, l1: 0.000414, l2: 0.000233, l3: 0.000278, l4: 0.000414, l5: 0.000859, l6: 0.001397
[epoch:  33/1000, batch:   176/ 1052, ite: 8460] train loss: 0.014363, tar: 0.001473 
l0: 0.000499, l1: 0.000480, l2: 0.000506, l3: 0.001061, l4: 0.001329, l5: 0.001806, l6: 0.002491
[epoch:  33/1000, batch:   256/ 1052, ite: 8480] train loss: 0.014348, tar: 0.001471 
l0: 0.000763, l1: 0.000904, l2: 0.000866, l3: 0.000821, l4: 0.001168, l5: 0.002246, l6: 0.001954
[epoch:  33/1000, batch:   336/ 1052, ite: 8500] train loss: 0.014352, tar: 0.001471 
l0: 0.001092, l1: 0.001244, l2: 0.001232, l3: 0.001561, l4: 0.001493, l5: 0.001739, l6: 0.002776
[epoch:  33/1000, batch:   416/ 1052, ite: 8520] train loss: 0.014341, tar: 0.001470 
l0: 0.000566, l1: 0.000494, l2: 0.000619, l3: 0.000754, l4: 0.000933, l5: 0.001432, l6: 0.002869
[epoch:  33/1000, batch:   496/ 1052, ite: 8540] train loss: 0.014335, tar: 0.001469 
l0: 0.001525, l1: 0.001699, l2: 0.001546, l3: 0.001772, l4: 0.002586, l5: 0.003471, l6: 0.004444
[epoch:  33/1000, batch:   576/ 1052, ite: 8560] train loss: 0.014324, tar: 0.001468 
l0: 0.000770, l1: 0.000745, l2: 0.000833, l3: 0.001040, l4: 0.001561, l5: 0.002150, l6: 0.004004
[epoch:  33/1000, batch:   656/ 1052, ite: 8580] train loss: 0.014319, tar: 0.001467 
l0: 0.001436, l1: 0.001553, l2: 0.001398, l3: 0.001586, l4: 0.001871, l5: 0.002182, l6: 0.002499
[epoch:  33/1000, batch:   736/ 1052, ite: 8600] train loss: 0.014305, tar: 0.001465 
l0: 0.000581, l1: 0.000616, l2: 0.000572, l3: 0.000605, l4: 0.000824, l5: 0.001203, l6: 0.001529
[epoch:  33/1000, batch:   816/ 1052, ite: 8620] train loss: 0.014294, tar: 0.001464 
l0: 0.000466, l1: 0.000464, l2: 0.000505, l3: 0.000537, l4: 0.000743, l5: 0.001084, l6: 0.001927
[epoch:  33/1000, batch:   896/ 1052, ite: 8640] train loss: 0.014283, tar: 0.001462 
l0: 0.000714, l1: 0.000649, l2: 0.000646, l3: 0.000873, l4: 0.001394, l5: 0.002332, l6: 0.003175
[epoch:  33/1000, batch:   976/ 1052, ite: 8660] train loss: 0.014269, tar: 0.001461 
[Epoch 33/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.001319, l1: 0.001242, l2: 0.001277, l3: 0.002323, l4: 0.002692, l5: 0.002912, l6: 0.004289
[epoch:  34/1000, batch:     4/ 1052, ite: 8680] train loss: 0.014256, tar: 0.001459 
l0: 0.001015, l1: 0.000993, l2: 0.001078, l3: 0.001197, l4: 0.001674, l5: 0.002383, l6: 0.003838
[epoch:  34/1000, batch:    84/ 1052, ite: 8700] train loss: 0.014247, tar: 0.001458 
l0: 0.000412, l1: 0.000477, l2: 0.000431, l3: 0.000424, l4: 0.000610, l5: 0.001055, l6: 0.001550
[epoch:  34/1000, batch:   164/ 1052, ite: 8720] train loss: 0.014233, tar: 0.001456 
l0: 0.000922, l1: 0.000940, l2: 0.000952, l3: 0.000969, l4: 0.001188, l5: 0.001780, l6: 0.002570
[epoch:  34/1000, batch:   244/ 1052, ite: 8740] train loss: 0.014219, tar: 0.001454 
l0: 0.000651, l1: 0.000722, l2: 0.000626, l3: 0.000624, l4: 0.000911, l5: 0.001588, l6: 0.002643
[epoch:  34/1000, batch:   324/ 1052, ite: 8760] train loss: 0.014204, tar: 0.001453 
l0: 0.000354, l1: 0.000371, l2: 0.000374, l3: 0.000440, l4: 0.000527, l5: 0.000834, l6: 0.001517
[epoch:  34/1000, batch:   404/ 1052, ite: 8780] train loss: 0.014191, tar: 0.001451 
l0: 0.001155, l1: 0.001087, l2: 0.001055, l3: 0.002408, l4: 0.002330, l5: 0.003300, l6: 0.003685
[epoch:  34/1000, batch:   484/ 1052, ite: 8800] train loss: 0.014190, tar: 0.001451 
l0: 0.002606, l1: 0.003151, l2: 0.002977, l3: 0.001939, l4: 0.002772, l5: 0.003899, l6: 0.004655
[epoch:  34/1000, batch:   564/ 1052, ite: 8820] train loss: 0.014186, tar: 0.001450 
l0: 0.000521, l1: 0.000433, l2: 0.001137, l3: 0.001302, l4: 0.001555, l5: 0.001289, l6: 0.001350
[epoch:  34/1000, batch:   644/ 1052, ite: 8840] train loss: 0.014183, tar: 0.001449 
l0: 0.000839, l1: 0.000833, l2: 0.000913, l3: 0.001068, l4: 0.001653, l5: 0.002280, l6: 0.004339
[epoch:  34/1000, batch:   724/ 1052, ite: 8860] train loss: 0.014172, tar: 0.001448 
l0: 0.000650, l1: 0.000666, l2: 0.000753, l3: 0.000715, l4: 0.001084, l5: 0.001953, l6: 0.003052
[epoch:  34/1000, batch:   804/ 1052, ite: 8880] train loss: 0.014163, tar: 0.001446 
l0: 0.000713, l1: 0.000845, l2: 0.000745, l3: 0.000716, l4: 0.000814, l5: 0.001239, l6: 0.001577
[epoch:  34/1000, batch:   884/ 1052, ite: 8900] train loss: 0.014166, tar: 0.001447 
l0: 0.000375, l1: 0.000357, l2: 0.000430, l3: 0.000512, l4: 0.000753, l5: 0.000845, l6: 0.001634
[epoch:  34/1000, batch:   964/ 1052, ite: 8920] train loss: 0.014157, tar: 0.001446 
l0: 0.000691, l1: 0.000776, l2: 0.000689, l3: 0.000935, l4: 0.000843, l5: 0.001085, l6: 0.001952
[epoch:  34/1000, batch:  1044/ 1052, ite: 8940] train loss: 0.014144, tar: 0.001445 
[Epoch 34/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000352, l1: 0.000327, l2: 0.000360, l3: 0.000505, l4: 0.000810, l5: 0.001597, l6: 0.002527
[epoch:  35/1000, batch:    72/ 1052, ite: 8960] train loss: 0.014131, tar: 0.001443 
l0: 0.000730, l1: 0.000748, l2: 0.000756, l3: 0.001042, l4: 0.001117, l5: 0.001078, l6: 0.001432
[epoch:  35/1000, batch:   152/ 1052, ite: 8980] train loss: 0.014117, tar: 0.001441 
l0: 0.000933, l1: 0.000935, l2: 0.001084, l3: 0.001255, l4: 0.001643, l5: 0.002026, l6: 0.003168
[epoch:  35/1000, batch:   232/ 1052, ite: 9000] train loss: 0.014104, tar: 0.001440 
l0: 0.000636, l1: 0.000624, l2: 0.000657, l3: 0.000796, l4: 0.001111, l5: 0.001659, l6: 0.002785
[epoch:  35/1000, batch:   312/ 1052, ite: 9020] train loss: 0.014096, tar: 0.001438 
l0: 0.000635, l1: 0.000639, l2: 0.000684, l3: 0.000805, l4: 0.001061, l5: 0.001695, l6: 0.002406
[epoch:  35/1000, batch:   392/ 1052, ite: 9040] train loss: 0.014081, tar: 0.001437 
l0: 0.000672, l1: 0.000644, l2: 0.000670, l3: 0.000814, l4: 0.001219, l5: 0.001597, l6: 0.002254
[epoch:  35/1000, batch:   472/ 1052, ite: 9060] train loss: 0.014072, tar: 0.001435 
l0: 0.000621, l1: 0.000587, l2: 0.000669, l3: 0.000857, l4: 0.001051, l5: 0.001575, l6: 0.002714
[epoch:  35/1000, batch:   552/ 1052, ite: 9080] train loss: 0.014061, tar: 0.001434 
l0: 0.000625, l1: 0.000559, l2: 0.000691, l3: 0.001145, l4: 0.001156, l5: 0.002005, l6: 0.002775
[epoch:  35/1000, batch:   632/ 1052, ite: 9100] train loss: 0.014066, tar: 0.001435 
l0: 0.001668, l1: 0.001739, l2: 0.001783, l3: 0.001902, l4: 0.001506, l5: 0.002411, l6: 0.004405
[epoch:  35/1000, batch:   712/ 1052, ite: 9120] train loss: 0.014061, tar: 0.001434 
l0: 0.000511, l1: 0.000470, l2: 0.000507, l3: 0.000673, l4: 0.000809, l5: 0.001590, l6: 0.002094
[epoch:  35/1000, batch:   792/ 1052, ite: 9140] train loss: 0.014054, tar: 0.001433 
l0: 0.000453, l1: 0.000417, l2: 0.000523, l3: 0.000639, l4: 0.000935, l5: 0.001765, l6: 0.001744
[epoch:  35/1000, batch:   872/ 1052, ite: 9160] train loss: 0.014043, tar: 0.001432 
l0: 0.000401, l1: 0.000370, l2: 0.000428, l3: 0.000510, l4: 0.000643, l5: 0.001249, l6: 0.002274
[epoch:  35/1000, batch:   952/ 1052, ite: 9180] train loss: 0.014030, tar: 0.001430 
l0: 0.000556, l1: 0.000599, l2: 0.000588, l3: 0.000661, l4: 0.001006, l5: 0.001100, l6: 0.002104
[epoch:  35/1000, batch:  1032/ 1052, ite: 9200] train loss: 0.014021, tar: 0.001429 
[Epoch 35/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000682, l1: 0.000773, l2: 0.000665, l3: 0.000777, l4: 0.000883, l5: 0.001265, l6: 0.001865
[epoch:  36/1000, batch:    60/ 1052, ite: 9220] train loss: 0.014014, tar: 0.001428 
l0: 0.000931, l1: 0.000933, l2: 0.001238, l3: 0.001028, l4: 0.001247, l5: 0.001723, l6: 0.003041
[epoch:  36/1000, batch:   140/ 1052, ite: 9240] train loss: 0.014005, tar: 0.001427 
l0: 0.000622, l1: 0.000659, l2: 0.000672, l3: 0.000705, l4: 0.001178, l5: 0.001236, l6: 0.002264
[epoch:  36/1000, batch:   220/ 1052, ite: 9260] train loss: 0.013997, tar: 0.001426 
l0: 0.001700, l1: 0.001728, l2: 0.001782, l3: 0.001864, l4: 0.002063, l5: 0.002483, l6: 0.003590
[epoch:  36/1000, batch:   300/ 1052, ite: 9280] train loss: 0.013988, tar: 0.001425 
l0: 0.000546, l1: 0.000596, l2: 0.000527, l3: 0.000710, l4: 0.000787, l5: 0.001127, l6: 0.001908
[epoch:  36/1000, batch:   380/ 1052, ite: 9300] train loss: 0.013976, tar: 0.001424 
l0: 0.000419, l1: 0.000504, l2: 0.000392, l3: 0.000560, l4: 0.000775, l5: 0.001081, l6: 0.001606
[epoch:  36/1000, batch:   460/ 1052, ite: 9320] train loss: 0.013969, tar: 0.001423 
l0: 0.000678, l1: 0.001044, l2: 0.000478, l3: 0.001627, l4: 0.001804, l5: 0.002157, l6: 0.001182
[epoch:  36/1000, batch:   540/ 1052, ite: 9340] train loss: 0.013971, tar: 0.001423 
l0: 0.002372, l1: 0.002098, l2: 0.001988, l3: 0.006677, l4: 0.008017, l5: 0.005766, l6: 0.004537
[epoch:  36/1000, batch:   620/ 1052, ite: 9360] train loss: 0.013964, tar: 0.001422 
l0: 0.000947, l1: 0.001137, l2: 0.000850, l3: 0.001023, l4: 0.000972, l5: 0.001515, l6: 0.001659
[epoch:  36/1000, batch:   700/ 1052, ite: 9380] train loss: 0.013954, tar: 0.001421 
l0: 0.002326, l1: 0.002426, l2: 0.002798, l3: 0.002824, l4: 0.002719, l5: 0.003560, l6: 0.003704
[epoch:  36/1000, batch:   780/ 1052, ite: 9400] train loss: 0.013947, tar: 0.001420 
l0: 0.001560, l1: 0.001702, l2: 0.001711, l3: 0.002150, l4: 0.001817, l5: 0.002267, l6: 0.003530
[epoch:  36/1000, batch:   860/ 1052, ite: 9420] train loss: 0.013937, tar: 0.001419 
l0: 0.000562, l1: 0.000629, l2: 0.000553, l3: 0.000534, l4: 0.000678, l5: 0.001390, l6: 0.002353
[epoch:  36/1000, batch:   940/ 1052, ite: 9440] train loss: 0.013923, tar: 0.001417 
l0: 0.001022, l1: 0.001186, l2: 0.001335, l3: 0.000968, l4: 0.001123, l5: 0.001713, l6: 0.003342
[epoch:  36/1000, batch:  1020/ 1052, ite: 9460] train loss: 0.013912, tar: 0.001416 
[Epoch 36/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000659, l1: 0.000672, l2: 0.000843, l3: 0.000798, l4: 0.001126, l5: 0.001259, l6: 0.001910
[epoch:  37/1000, batch:    48/ 1052, ite: 9480] train loss: 0.013911, tar: 0.001415 
l0: 0.003711, l1: 0.004203, l2: 0.003561, l3: 0.005236, l4: 0.004465, l5: 0.005465, l6: 0.006332
[epoch:  37/1000, batch:   128/ 1052, ite: 9500] train loss: 0.013905, tar: 0.001415 
l0: 0.000446, l1: 0.000501, l2: 0.000446, l3: 0.000554, l4: 0.000609, l5: 0.001182, l6: 0.001870
[epoch:  37/1000, batch:   208/ 1052, ite: 9520] train loss: 0.013893, tar: 0.001413 
l0: 0.000752, l1: 0.000719, l2: 0.000821, l3: 0.001164, l4: 0.001304, l5: 0.002334, l6: 0.002303
[epoch:  37/1000, batch:   288/ 1052, ite: 9540] train loss: 0.013889, tar: 0.001413 
l0: 0.001082, l1: 0.001068, l2: 0.001063, l3: 0.001375, l4: 0.001608, l5: 0.002269, l6: 0.003721
[epoch:  37/1000, batch:   368/ 1052, ite: 9560] train loss: 0.013876, tar: 0.001411 
l0: 0.000291, l1: 0.000322, l2: 0.000319, l3: 0.000338, l4: 0.000504, l5: 0.000836, l6: 0.001107
[epoch:  37/1000, batch:   448/ 1052, ite: 9580] train loss: 0.013864, tar: 0.001410 
l0: 0.001123, l1: 0.001404, l2: 0.000993, l3: 0.001244, l4: 0.001495, l5: 0.001955, l6: 0.003026
[epoch:  37/1000, batch:   528/ 1052, ite: 9600] train loss: 0.013857, tar: 0.001409 
l0: 0.001022, l1: 0.001002, l2: 0.001018, l3: 0.001163, l4: 0.001411, l5: 0.003134, l6: 0.003835
[epoch:  37/1000, batch:   608/ 1052, ite: 9620] train loss: 0.013847, tar: 0.001408 
l0: 0.000656, l1: 0.000623, l2: 0.000749, l3: 0.000860, l4: 0.001701, l5: 0.001620, l6: 0.002165
[epoch:  37/1000, batch:   688/ 1052, ite: 9640] train loss: 0.013836, tar: 0.001406 
l0: 0.000765, l1: 0.000916, l2: 0.000900, l3: 0.000843, l4: 0.001009, l5: 0.001285, l6: 0.002716
[epoch:  37/1000, batch:   768/ 1052, ite: 9660] train loss: 0.013827, tar: 0.001405 
l0: 0.000545, l1: 0.000554, l2: 0.000637, l3: 0.000704, l4: 0.000987, l5: 0.001759, l6: 0.002915
[epoch:  37/1000, batch:   848/ 1052, ite: 9680] train loss: 0.013820, tar: 0.001404 
l0: 0.000530, l1: 0.000507, l2: 0.000658, l3: 0.000707, l4: 0.000899, l5: 0.001500, l6: 0.002535
[epoch:  37/1000, batch:   928/ 1052, ite: 9700] train loss: 0.013814, tar: 0.001403 
l0: 0.000654, l1: 0.000714, l2: 0.001280, l3: 0.000684, l4: 0.000699, l5: 0.001311, l6: 0.001320
[epoch:  37/1000, batch:  1008/ 1052, ite: 9720] train loss: 0.013806, tar: 0.001402 
[Epoch 37/1000] Test loss: 0.012, Test target loss: 0.001
l0: 0.000795, l1: 0.000824, l2: 0.000876, l3: 0.001284, l4: 0.001374, l5: 0.001844, l6: 0.003368
[epoch:  38/1000, batch:    36/ 1052, ite: 9740] train loss: 0.013795, tar: 0.001401 
l0: 0.000443, l1: 0.000418, l2: 0.000439, l3: 0.000589, l4: 0.000752, l5: 0.001301, l6: 0.002490
[epoch:  38/1000, batch:   116/ 1052, ite: 9760] train loss: 0.013784, tar: 0.001399 
l0: 0.000445, l1: 0.000479, l2: 0.000462, l3: 0.000588, l4: 0.000927, l5: 0.001736, l6: 0.002038
[epoch:  38/1000, batch:   196/ 1052, ite: 9780] train loss: 0.013778, tar: 0.001399 
l0: 0.003226, l1: 0.003267, l2: 0.003809, l3: 0.005258, l4: 0.004075, l5: 0.005276, l6: 0.005987
[epoch:  38/1000, batch:   276/ 1052, ite: 9800] train loss: 0.013770, tar: 0.001398 
l0: 0.000343, l1: 0.000646, l2: 0.000339, l3: 0.000500, l4: 0.000404, l5: 0.000969, l6: 0.000852
[epoch:  38/1000, batch:   356/ 1052, ite: 9820] train loss: 0.013764, tar: 0.001397 
l0: 0.001188, l1: 0.001152, l2: 0.001017, l3: 0.001404, l4: 0.001745, l5: 0.002656, l6: 0.003007
[epoch:  38/1000, batch:   436/ 1052, ite: 9840] train loss: 0.013755, tar: 0.001396 
l0: 0.000486, l1: 0.000540, l2: 0.000497, l3: 0.000667, l4: 0.000834, l5: 0.001144, l6: 0.001746
[epoch:  38/1000, batch:   516/ 1052, ite: 9860] train loss: 0.013745, tar: 0.001395 
l0: 0.000887, l1: 0.001005, l2: 0.001022, l3: 0.001227, l4: 0.001417, l5: 0.001919, l6: 0.003448
[epoch:  38/1000, batch:   596/ 1052, ite: 9880] train loss: 0.013753, tar: 0.001396 
l0: 0.000698, l1: 0.000641, l2: 0.000729, l3: 0.001100, l4: 0.001305, l5: 0.001604, l6: 0.002511
[epoch:  38/1000, batch:   676/ 1052, ite: 9900] train loss: 0.013754, tar: 0.001396 
l0: 0.000733, l1: 0.000731, l2: 0.000737, l3: 0.000986, l4: 0.001377, l5: 0.002075, l6: 0.002860
[epoch:  38/1000, batch:   756/ 1052, ite: 9920] train loss: 0.013752, tar: 0.001395 
l0: 0.000306, l1: 0.000309, l2: 0.000328, l3: 0.000399, l4: 0.000564, l5: 0.000781, l6: 0.001685
[epoch:  38/1000, batch:   836/ 1052, ite: 9940] train loss: 0.013740, tar: 0.001394 
l0: 0.000937, l1: 0.000924, l2: 0.001007, l3: 0.001184, l4: 0.001246, l5: 0.002135, l6: 0.002864
[epoch:  38/1000, batch:   916/ 1052, ite: 9960] train loss: 0.013736, tar: 0.001393 
l0: 0.000537, l1: 0.000568, l2: 0.000673, l3: 0.000479, l4: 0.000605, l5: 0.001232, l6: 0.002369
[epoch:  38/1000, batch:   996/ 1052, ite: 9980] train loss: 0.013730, tar: 0.001393 
[Epoch 38/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000521, l1: 0.000481, l2: 0.000526, l3: 0.000604, l4: 0.000959, l5: 0.001355, l6: 0.001547
[epoch:  39/1000, batch:    24/ 1052, ite: 10000] train loss: 0.013726, tar: 0.001392 
l0: 0.000800, l1: 0.000769, l2: 0.000845, l3: 0.000834, l4: 0.001224, l5: 0.001880, l6: 0.002440
[epoch:  39/1000, batch:   104/ 1052, ite: 10020] train loss: 0.013714, tar: 0.001391 
l0: 0.000420, l1: 0.000407, l2: 0.000475, l3: 0.000563, l4: 0.000668, l5: 0.001321, l6: 0.001747
[epoch:  39/1000, batch:   184/ 1052, ite: 10040] train loss: 0.013704, tar: 0.001389 
l0: 0.000650, l1: 0.000590, l2: 0.000593, l3: 0.000809, l4: 0.001016, l5: 0.001522, l6: 0.002936
[epoch:  39/1000, batch:   264/ 1052, ite: 10060] train loss: 0.013698, tar: 0.001389 
l0: 0.001489, l1: 0.001570, l2: 0.001607, l3: 0.002003, l4: 0.001855, l5: 0.002478, l6: 0.003878
[epoch:  39/1000, batch:   344/ 1052, ite: 10080] train loss: 0.013708, tar: 0.001390 
l0: 0.000245, l1: 0.000238, l2: 0.000243, l3: 0.000538, l4: 0.000732, l5: 0.000941, l6: 0.001155
[epoch:  39/1000, batch:   424/ 1052, ite: 10100] train loss: 0.013699, tar: 0.001389 
l0: 0.000613, l1: 0.000573, l2: 0.000689, l3: 0.000827, l4: 0.001225, l5: 0.002166, l6: 0.002718
[epoch:  39/1000, batch:   504/ 1052, ite: 10120] train loss: 0.013692, tar: 0.001388 
l0: 0.000553, l1: 0.000502, l2: 0.000720, l3: 0.000614, l4: 0.000855, l5: 0.001251, l6: 0.002282
[epoch:  39/1000, batch:   584/ 1052, ite: 10140] train loss: 0.013686, tar: 0.001387 
l0: 0.000342, l1: 0.000317, l2: 0.000343, l3: 0.000534, l4: 0.000606, l5: 0.000917, l6: 0.001720
[epoch:  39/1000, batch:   664/ 1052, ite: 10160] train loss: 0.013674, tar: 0.001386 
l0: 0.000836, l1: 0.000789, l2: 0.000932, l3: 0.000901, l4: 0.001047, l5: 0.001889, l6: 0.003146
[epoch:  39/1000, batch:   744/ 1052, ite: 10180] train loss: 0.013667, tar: 0.001385 
l0: 0.001063, l1: 0.000981, l2: 0.001314, l3: 0.001443, l4: 0.001292, l5: 0.001780, l6: 0.003221
[epoch:  39/1000, batch:   824/ 1052, ite: 10200] train loss: 0.013660, tar: 0.001384 
l0: 0.000538, l1: 0.000497, l2: 0.000540, l3: 0.000705, l4: 0.001122, l5: 0.001323, l6: 0.002022
[epoch:  39/1000, batch:   904/ 1052, ite: 10220] train loss: 0.013651, tar: 0.001383 
l0: 0.002055, l1: 0.002081, l2: 0.002230, l3: 0.002143, l4: 0.002851, l5: 0.004105, l6: 0.005703
[epoch:  39/1000, batch:   984/ 1052, ite: 10240] train loss: 0.013641, tar: 0.001381 
[Epoch 39/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000464, l1: 0.000437, l2: 0.000575, l3: 0.000675, l4: 0.001030, l5: 0.001465, l6: 0.002397
[epoch:  40/1000, batch:    12/ 1052, ite: 10260] train loss: 0.013631, tar: 0.001380 
l0: 0.000365, l1: 0.000392, l2: 0.000384, l3: 0.000478, l4: 0.000574, l5: 0.001209, l6: 0.001751
[epoch:  40/1000, batch:    92/ 1052, ite: 10280] train loss: 0.013622, tar: 0.001379 
l0: 0.000710, l1: 0.000817, l2: 0.000643, l3: 0.000748, l4: 0.000817, l5: 0.001546, l6: 0.002391
[epoch:  40/1000, batch:   172/ 1052, ite: 10300] train loss: 0.013612, tar: 0.001377 
l0: 0.000570, l1: 0.000520, l2: 0.000566, l3: 0.000679, l4: 0.000674, l5: 0.001277, l6: 0.002511
[epoch:  40/1000, batch:   252/ 1052, ite: 10320] train loss: 0.013613, tar: 0.001378 
l0: 0.000533, l1: 0.000512, l2: 0.000629, l3: 0.000639, l4: 0.000988, l5: 0.001276, l6: 0.002049
[epoch:  40/1000, batch:   332/ 1052, ite: 10340] train loss: 0.013606, tar: 0.001377 
l0: 0.000893, l1: 0.000828, l2: 0.001085, l3: 0.001313, l4: 0.001365, l5: 0.002075, l6: 0.003750
[epoch:  40/1000, batch:   412/ 1052, ite: 10360] train loss: 0.013597, tar: 0.001376 
l0: 0.000657, l1: 0.000567, l2: 0.000753, l3: 0.001115, l4: 0.001468, l5: 0.001857, l6: 0.002916
[epoch:  40/1000, batch:   492/ 1052, ite: 10380] train loss: 0.013590, tar: 0.001375 
l0: 0.001201, l1: 0.001298, l2: 0.001337, l3: 0.001465, l4: 0.001664, l5: 0.002365, l6: 0.002995
[epoch:  40/1000, batch:   572/ 1052, ite: 10400] train loss: 0.013580, tar: 0.001374 
l0: 0.000673, l1: 0.000643, l2: 0.000585, l3: 0.000752, l4: 0.001180, l5: 0.001754, l6: 0.002196
[epoch:  40/1000, batch:   652/ 1052, ite: 10420] train loss: 0.013573, tar: 0.001373 
l0: 0.000190, l1: 0.000190, l2: 0.000233, l3: 0.000269, l4: 0.000472, l5: 0.000716, l6: 0.000628
[epoch:  40/1000, batch:   732/ 1052, ite: 10440] train loss: 0.013564, tar: 0.001372 
l0: 0.000672, l1: 0.000678, l2: 0.000700, l3: 0.000868, l4: 0.001013, l5: 0.001486, l6: 0.003207
[epoch:  40/1000, batch:   812/ 1052, ite: 10460] train loss: 0.013554, tar: 0.001371 
l0: 0.000702, l1: 0.000687, l2: 0.000722, l3: 0.001033, l4: 0.001300, l5: 0.002177, l6: 0.004102
[epoch:  40/1000, batch:   892/ 1052, ite: 10480] train loss: 0.013544, tar: 0.001369 
l0: 0.001505, l1: 0.001435, l2: 0.001656, l3: 0.001819, l4: 0.002131, l5: 0.002681, l6: 0.003395
[epoch:  40/1000, batch:   972/ 1052, ite: 10500] train loss: 0.013535, tar: 0.001368 
l0: 0.001378, l1: 0.001419, l2: 0.001324, l3: 0.001785, l4: 0.001710, l5: 0.002220, l6: 0.002301
[epoch:  40/1000, batch:  1052/ 1052, ite: 10520] train loss: 0.013526, tar: 0.001367 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_10520.pkl
[Epoch 40/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000890, l1: 0.000815, l2: 0.001503, l3: 0.001439, l4: 0.001284, l5: 0.001801, l6: 0.003168
[epoch:  41/1000, batch:    80/ 1052, ite: 10540] train loss: 0.007782, tar: 0.000660 
l0: 0.000498, l1: 0.000493, l2: 0.000485, l3: 0.000545, l4: 0.000663, l5: 0.001189, l6: 0.002126
[epoch:  41/1000, batch:   160/ 1052, ite: 10560] train loss: 0.008913, tar: 0.000785 
l0: 0.000679, l1: 0.000603, l2: 0.000695, l3: 0.001153, l4: 0.001274, l5: 0.001640, l6: 0.003332
[epoch:  41/1000, batch:   240/ 1052, ite: 10580] train loss: 0.009629, tar: 0.000858 
l0: 0.000913, l1: 0.000900, l2: 0.001318, l3: 0.001208, l4: 0.001182, l5: 0.001813, l6: 0.002619
[epoch:  41/1000, batch:   320/ 1052, ite: 10600] train loss: 0.009474, tar: 0.000834 
l0: 0.000791, l1: 0.000809, l2: 0.000896, l3: 0.000893, l4: 0.001039, l5: 0.001835, l6: 0.002734
[epoch:  41/1000, batch:   400/ 1052, ite: 10620] train loss: 0.010796, tar: 0.001003 
l0: 0.000845, l1: 0.000924, l2: 0.000842, l3: 0.000936, l4: 0.001377, l5: 0.002150, l6: 0.003346
[epoch:  41/1000, batch:   480/ 1052, ite: 10640] train loss: 0.010586, tar: 0.000969 
l0: 0.000666, l1: 0.000633, l2: 0.000688, l3: 0.001117, l4: 0.001221, l5: 0.001700, l6: 0.002558
[epoch:  41/1000, batch:   560/ 1052, ite: 10660] train loss: 0.010438, tar: 0.000948 
l0: 0.001155, l1: 0.001185, l2: 0.001361, l3: 0.001298, l4: 0.001594, l5: 0.002229, l6: 0.002987
[epoch:  41/1000, batch:   640/ 1052, ite: 10680] train loss: 0.010347, tar: 0.000937 
l0: 0.000770, l1: 0.000726, l2: 0.000795, l3: 0.000995, l4: 0.001387, l5: 0.002164, l6: 0.003311
[epoch:  41/1000, batch:   720/ 1052, ite: 10700] train loss: 0.010291, tar: 0.000927 
l0: 0.000504, l1: 0.000525, l2: 0.000519, l3: 0.000641, l4: 0.000922, l5: 0.001135, l6: 0.001331
[epoch:  41/1000, batch:   800/ 1052, ite: 10720] train loss: 0.010107, tar: 0.000911 
l0: 0.000555, l1: 0.000507, l2: 0.000586, l3: 0.000556, l4: 0.000706, l5: 0.001293, l6: 0.002449
[epoch:  41/1000, batch:   880/ 1052, ite: 10740] train loss: 0.009991, tar: 0.000898 
l0: 0.001133, l1: 0.001065, l2: 0.001177, l3: 0.001403, l4: 0.001653, l5: 0.002119, l6: 0.003169
[epoch:  41/1000, batch:   960/ 1052, ite: 10760] train loss: 0.009791, tar: 0.000876 
l0: 0.000978, l1: 0.000957, l2: 0.000919, l3: 0.001268, l4: 0.001442, l5: 0.002035, l6: 0.003296
[epoch:  41/1000, batch:  1040/ 1052, ite: 10780] train loss: 0.009684, tar: 0.000870 
[Epoch 41/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000597, l1: 0.000608, l2: 0.000580, l3: 0.000662, l4: 0.000846, l5: 0.001154, l6: 0.001981
[epoch:  42/1000, batch:    68/ 1052, ite: 10800] train loss: 0.009636, tar: 0.000868 
l0: 0.000672, l1: 0.000731, l2: 0.000778, l3: 0.000843, l4: 0.001013, l5: 0.001114, l6: 0.002344
[epoch:  42/1000, batch:   148/ 1052, ite: 10820] train loss: 0.009474, tar: 0.000848 
l0: 0.000656, l1: 0.000700, l2: 0.000637, l3: 0.000943, l4: 0.000986, l5: 0.001415, l6: 0.001735
[epoch:  42/1000, batch:   228/ 1052, ite: 10840] train loss: 0.009416, tar: 0.000838 
l0: 0.001744, l1: 0.001934, l2: 0.001766, l3: 0.002310, l4: 0.003420, l5: 0.003061, l6: 0.006222
[epoch:  42/1000, batch:   308/ 1052, ite: 10860] train loss: 0.009476, tar: 0.000847 
l0: 0.000650, l1: 0.000672, l2: 0.000684, l3: 0.000615, l4: 0.000711, l5: 0.000888, l6: 0.001770
[epoch:  42/1000, batch:   388/ 1052, ite: 10880] train loss: 0.009379, tar: 0.000839 
l0: 0.000690, l1: 0.000663, l2: 0.000768, l3: 0.000896, l4: 0.001075, l5: 0.001561, l6: 0.002375
[epoch:  42/1000, batch:   468/ 1052, ite: 10900] train loss: 0.009353, tar: 0.000837 
l0: 0.001480, l1: 0.001503, l2: 0.001474, l3: 0.001558, l4: 0.001759, l5: 0.002117, l6: 0.002501
[epoch:  42/1000, batch:   548/ 1052, ite: 10920] train loss: 0.009278, tar: 0.000830 
l0: 0.000820, l1: 0.000751, l2: 0.000754, l3: 0.001085, l4: 0.001317, l5: 0.001799, l6: 0.003175
[epoch:  42/1000, batch:   628/ 1052, ite: 10940] train loss: 0.009250, tar: 0.000828 
l0: 0.000594, l1: 0.000696, l2: 0.000669, l3: 0.000678, l4: 0.000766, l5: 0.001050, l6: 0.002146
[epoch:  42/1000, batch:   708/ 1052, ite: 10960] train loss: 0.009239, tar: 0.000827 
l0: 0.000695, l1: 0.000760, l2: 0.000685, l3: 0.000613, l4: 0.000793, l5: 0.001054, l6: 0.001322
[epoch:  42/1000, batch:   788/ 1052, ite: 10980] train loss: 0.009233, tar: 0.000827 
l0: 0.000614, l1: 0.000670, l2: 0.001115, l3: 0.001099, l4: 0.000701, l5: 0.000902, l6: 0.002184
[epoch:  42/1000, batch:   868/ 1052, ite: 11000] train loss: 0.009214, tar: 0.000823 
l0: 0.001234, l1: 0.001073, l2: 0.001592, l3: 0.001673, l4: 0.001519, l5: 0.003103, l6: 0.003863
[epoch:  42/1000, batch:   948/ 1052, ite: 11020] train loss: 0.009229, tar: 0.000825 
l0: 0.001145, l1: 0.001223, l2: 0.001375, l3: 0.002044, l4: 0.001822, l5: 0.001997, l6: 0.003492
[epoch:  42/1000, batch:  1028/ 1052, ite: 11040] train loss: 0.009736, tar: 0.000882 
[Epoch 42/1000] Test loss: 0.014, Test target loss: 0.001
l0: 0.000523, l1: 0.000503, l2: 0.000568, l3: 0.000697, l4: 0.000841, l5: 0.001459, l6: 0.002259
[epoch:  43/1000, batch:    56/ 1052, ite: 11060] train loss: 0.009855, tar: 0.000894 
l0: 0.000712, l1: 0.000639, l2: 0.000845, l3: 0.001047, l4: 0.001354, l5: 0.002221, l6: 0.003672
[epoch:  43/1000, batch:   136/ 1052, ite: 11080] train loss: 0.009838, tar: 0.000892 
l0: 0.000647, l1: 0.000649, l2: 0.000631, l3: 0.000753, l4: 0.000726, l5: 0.001538, l6: 0.002187
[epoch:  43/1000, batch:   216/ 1052, ite: 11100] train loss: 0.009775, tar: 0.000884 
l0: 0.000487, l1: 0.000505, l2: 0.000440, l3: 0.000639, l4: 0.001049, l5: 0.001210, l6: 0.001677
[epoch:  43/1000, batch:   296/ 1052, ite: 11120] train loss: 0.009804, tar: 0.000887 
l0: 0.001091, l1: 0.001036, l2: 0.001096, l3: 0.001367, l4: 0.001682, l5: 0.002347, l6: 0.003524
[epoch:  43/1000, batch:   376/ 1052, ite: 11140] train loss: 0.009785, tar: 0.000882 
l0: 0.000457, l1: 0.000442, l2: 0.000536, l3: 0.000586, l4: 0.000844, l5: 0.001578, l6: 0.001917
[epoch:  43/1000, batch:   456/ 1052, ite: 11160] train loss: 0.009783, tar: 0.000883 
l0: 0.001038, l1: 0.001008, l2: 0.001299, l3: 0.001251, l4: 0.001231, l5: 0.001738, l6: 0.003068
[epoch:  43/1000, batch:   536/ 1052, ite: 11180] train loss: 0.009758, tar: 0.000880 
l0: 0.000507, l1: 0.000522, l2: 0.000491, l3: 0.000487, l4: 0.000785, l5: 0.001348, l6: 0.002803
[epoch:  43/1000, batch:   616/ 1052, ite: 11200] train loss: 0.009740, tar: 0.000878 
l0: 0.000669, l1: 0.000686, l2: 0.000622, l3: 0.000712, l4: 0.000937, l5: 0.001626, l6: 0.002026
[epoch:  43/1000, batch:   696/ 1052, ite: 11220] train loss: 0.009730, tar: 0.000880 
l0: 0.002981, l1: 0.003252, l2: 0.002870, l3: 0.003917, l4: 0.003623, l5: 0.003994, l6: 0.004979
[epoch:  43/1000, batch:   776/ 1052, ite: 11240] train loss: 0.009715, tar: 0.000878 
l0: 0.000833, l1: 0.000890, l2: 0.001320, l3: 0.000992, l4: 0.001172, l5: 0.001077, l6: 0.002278
[epoch:  43/1000, batch:   856/ 1052, ite: 11260] train loss: 0.009895, tar: 0.000900 
l0: 0.000887, l1: 0.000829, l2: 0.000985, l3: 0.001350, l4: 0.001249, l5: 0.001707, l6: 0.002772
[epoch:  43/1000, batch:   936/ 1052, ite: 11280] train loss: 0.009926, tar: 0.000905 
l0: 0.001490, l1: 0.001567, l2: 0.001373, l3: 0.001607, l4: 0.001962, l5: 0.003055, l6: 0.004536
[epoch:  43/1000, batch:  1016/ 1052, ite: 11300] train loss: 0.009874, tar: 0.000900 
[Epoch 43/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000575, l1: 0.000537, l2: 0.000623, l3: 0.000758, l4: 0.000989, l5: 0.001057, l6: 0.002189
[epoch:  44/1000, batch:    44/ 1052, ite: 11320] train loss: 0.009841, tar: 0.000896 
l0: 0.000374, l1: 0.000518, l2: 0.000297, l3: 0.000482, l4: 0.000473, l5: 0.001073, l6: 0.000803
[epoch:  44/1000, batch:   124/ 1052, ite: 11340] train loss: 0.009796, tar: 0.000890 
l0: 0.001188, l1: 0.001108, l2: 0.001514, l3: 0.001269, l4: 0.001316, l5: 0.002219, l6: 0.002996
[epoch:  44/1000, batch:   204/ 1052, ite: 11360] train loss: 0.009738, tar: 0.000885 
l0: 0.001686, l1: 0.001911, l2: 0.001443, l3: 0.003535, l4: 0.003745, l5: 0.003113, l6: 0.004372
[epoch:  44/1000, batch:   284/ 1052, ite: 11380] train loss: 0.009857, tar: 0.000902 
l0: 0.000753, l1: 0.000778, l2: 0.000749, l3: 0.000982, l4: 0.001095, l5: 0.001535, l6: 0.001980
[epoch:  44/1000, batch:   364/ 1052, ite: 11400] train loss: 0.009867, tar: 0.000902 
l0: 0.000567, l1: 0.000589, l2: 0.000586, l3: 0.000613, l4: 0.000781, l5: 0.001567, l6: 0.003320
[epoch:  44/1000, batch:   444/ 1052, ite: 11420] train loss: 0.009850, tar: 0.000902 
l0: 0.000289, l1: 0.000267, l2: 0.000343, l3: 0.000422, l4: 0.000591, l5: 0.001026, l6: 0.001707
[epoch:  44/1000, batch:   524/ 1052, ite: 11440] train loss: 0.009851, tar: 0.000902 
l0: 0.000837, l1: 0.000895, l2: 0.000881, l3: 0.001046, l4: 0.001091, l5: 0.001818, l6: 0.002758
[epoch:  44/1000, batch:   604/ 1052, ite: 11460] train loss: 0.009840, tar: 0.000901 
l0: 0.002430, l1: 0.002347, l2: 0.002808, l3: 0.003331, l4: 0.004289, l5: 0.004354, l6: 0.006149
[epoch:  44/1000, batch:   684/ 1052, ite: 11480] train loss: 0.009786, tar: 0.000895 
l0: 0.001450, l1: 0.001533, l2: 0.001467, l3: 0.001540, l4: 0.001692, l5: 0.001600, l6: 0.002293
[epoch:  44/1000, batch:   764/ 1052, ite: 11500] train loss: 0.009829, tar: 0.000899 
l0: 0.000336, l1: 0.000340, l2: 0.000389, l3: 0.000556, l4: 0.000810, l5: 0.000882, l6: 0.000891
[epoch:  44/1000, batch:   844/ 1052, ite: 11520] train loss: 0.009818, tar: 0.000898 
l0: 0.001294, l1: 0.001762, l2: 0.001384, l3: 0.001230, l4: 0.001129, l5: 0.001892, l6: 0.002010
[epoch:  44/1000, batch:   924/ 1052, ite: 11540] train loss: 0.009911, tar: 0.000914 
l0: 0.000527, l1: 0.000548, l2: 0.000644, l3: 0.000616, l4: 0.000904, l5: 0.001465, l6: 0.002281
[epoch:  44/1000, batch:  1004/ 1052, ite: 11560] train loss: 0.009900, tar: 0.000913 
[Epoch 44/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000349, l1: 0.000343, l2: 0.000347, l3: 0.000455, l4: 0.000583, l5: 0.001168, l6: 0.001844
[epoch:  45/1000, batch:    32/ 1052, ite: 11580] train loss: 0.009887, tar: 0.000911 
l0: 0.000791, l1: 0.000777, l2: 0.001187, l3: 0.000694, l4: 0.000844, l5: 0.000839, l6: 0.001104
[epoch:  45/1000, batch:   112/ 1052, ite: 11600] train loss: 0.009826, tar: 0.000904 
l0: 0.000507, l1: 0.000498, l2: 0.000523, l3: 0.000708, l4: 0.000921, l5: 0.001674, l6: 0.003001
[epoch:  45/1000, batch:   192/ 1052, ite: 11620] train loss: 0.009790, tar: 0.000900 
l0: 0.000347, l1: 0.000352, l2: 0.000427, l3: 0.000508, l4: 0.000764, l5: 0.001158, l6: 0.002030
[epoch:  45/1000, batch:   272/ 1052, ite: 11640] train loss: 0.009782, tar: 0.000898 
l0: 0.000740, l1: 0.000691, l2: 0.000757, l3: 0.001009, l4: 0.001057, l5: 0.001462, l6: 0.002158
[epoch:  45/1000, batch:   352/ 1052, ite: 11660] train loss: 0.009762, tar: 0.000896 
l0: 0.000552, l1: 0.000692, l2: 0.000435, l3: 0.001373, l4: 0.000721, l5: 0.001293, l6: 0.001954
[epoch:  45/1000, batch:   432/ 1052, ite: 11680] train loss: 0.009735, tar: 0.000892 
l0: 0.001068, l1: 0.001187, l2: 0.001107, l3: 0.001151, l4: 0.001169, l5: 0.001476, l6: 0.003280
[epoch:  45/1000, batch:   512/ 1052, ite: 11700] train loss: 0.009692, tar: 0.000888 
l0: 0.000670, l1: 0.000630, l2: 0.000658, l3: 0.000935, l4: 0.001232, l5: 0.001942, l6: 0.003198
[epoch:  45/1000, batch:   592/ 1052, ite: 11720] train loss: 0.009670, tar: 0.000884 
l0: 0.000316, l1: 0.000311, l2: 0.000359, l3: 0.000445, l4: 0.000568, l5: 0.001046, l6: 0.001152
[epoch:  45/1000, batch:   672/ 1052, ite: 11740] train loss: 0.009667, tar: 0.000884 
l0: 0.000466, l1: 0.000462, l2: 0.000517, l3: 0.000544, l4: 0.000683, l5: 0.001015, l6: 0.001872
[epoch:  45/1000, batch:   752/ 1052, ite: 11760] train loss: 0.009740, tar: 0.000896 
l0: 0.001695, l1: 0.002121, l2: 0.001264, l3: 0.001243, l4: 0.001826, l5: 0.002760, l6: 0.004700
[epoch:  45/1000, batch:   832/ 1052, ite: 11780] train loss: 0.009746, tar: 0.000897 
l0: 0.000829, l1: 0.000885, l2: 0.000811, l3: 0.000795, l4: 0.001012, l5: 0.001478, l6: 0.001952
[epoch:  45/1000, batch:   912/ 1052, ite: 11800] train loss: 0.009806, tar: 0.000904 
l0: 0.000812, l1: 0.000935, l2: 0.000762, l3: 0.000717, l4: 0.000786, l5: 0.001050, l6: 0.001994
[epoch:  45/1000, batch:   992/ 1052, ite: 11820] train loss: 0.009807, tar: 0.000904 
[Epoch 45/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000894, l1: 0.000953, l2: 0.001007, l3: 0.001102, l4: 0.000998, l5: 0.001763, l6: 0.002496
[epoch:  46/1000, batch:    20/ 1052, ite: 11840] train loss: 0.009787, tar: 0.000902 
l0: 0.000809, l1: 0.000808, l2: 0.000828, l3: 0.000891, l4: 0.001098, l5: 0.001884, l6: 0.002997
[epoch:  46/1000, batch:   100/ 1052, ite: 11860] train loss: 0.009834, tar: 0.000910 
l0: 0.001929, l1: 0.002213, l2: 0.001753, l3: 0.002381, l4: 0.002812, l5: 0.004515, l6: 0.007133
[epoch:  46/1000, batch:   180/ 1052, ite: 11880] train loss: 0.009869, tar: 0.000913 
l0: 0.001483, l1: 0.001844, l2: 0.001992, l3: 0.001279, l4: 0.001843, l5: 0.001998, l6: 0.002334
[epoch:  46/1000, batch:   260/ 1052, ite: 11900] train loss: 0.009879, tar: 0.000914 
l0: 0.001027, l1: 0.000997, l2: 0.001094, l3: 0.001226, l4: 0.001485, l5: 0.002292, l6: 0.003646
[epoch:  46/1000, batch:   340/ 1052, ite: 11920] train loss: 0.009896, tar: 0.000916 
l0: 0.000529, l1: 0.000571, l2: 0.000609, l3: 0.000685, l4: 0.000934, l5: 0.001117, l6: 0.001845
[epoch:  46/1000, batch:   420/ 1052, ite: 11940] train loss: 0.009876, tar: 0.000914 
l0: 0.000379, l1: 0.000383, l2: 0.000476, l3: 0.000485, l4: 0.000460, l5: 0.001048, l6: 0.001408
[epoch:  46/1000, batch:   500/ 1052, ite: 11960] train loss: 0.009854, tar: 0.000912 
l0: 0.000680, l1: 0.000665, l2: 0.000736, l3: 0.000798, l4: 0.001239, l5: 0.001990, l6: 0.003377
[epoch:  46/1000, batch:   580/ 1052, ite: 11980] train loss: 0.009845, tar: 0.000910 
l0: 0.001186, l1: 0.001386, l2: 0.001233, l3: 0.001143, l4: 0.001491, l5: 0.002041, l6: 0.003206
[epoch:  46/1000, batch:   660/ 1052, ite: 12000] train loss: 0.009847, tar: 0.000910 
l0: 0.000813, l1: 0.000843, l2: 0.000763, l3: 0.001039, l4: 0.001183, l5: 0.001673, l6: 0.003495
[epoch:  46/1000, batch:   740/ 1052, ite: 12020] train loss: 0.009811, tar: 0.000906 
l0: 0.000832, l1: 0.000793, l2: 0.000877, l3: 0.000974, l4: 0.001457, l5: 0.001869, l6: 0.002719
[epoch:  46/1000, batch:   820/ 1052, ite: 12040] train loss: 0.009806, tar: 0.000906 
l0: 0.000805, l1: 0.000889, l2: 0.000931, l3: 0.001003, l4: 0.001207, l5: 0.001523, l6: 0.002658
[epoch:  46/1000, batch:   900/ 1052, ite: 12060] train loss: 0.009790, tar: 0.000903 
l0: 0.000530, l1: 0.000595, l2: 0.000549, l3: 0.000550, l4: 0.000730, l5: 0.001541, l6: 0.001792
[epoch:  46/1000, batch:   980/ 1052, ite: 12080] train loss: 0.009763, tar: 0.000900 
[Epoch 46/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000428, l1: 0.000478, l2: 0.000430, l3: 0.000821, l4: 0.001211, l5: 0.001745, l6: 0.002546
[epoch:  47/1000, batch:     8/ 1052, ite: 12100] train loss: 0.009745, tar: 0.000898 
l0: 0.000545, l1: 0.000540, l2: 0.000617, l3: 0.000740, l4: 0.000808, l5: 0.001106, l6: 0.002462
[epoch:  47/1000, batch:    88/ 1052, ite: 12120] train loss: 0.009731, tar: 0.000897 
l0: 0.000634, l1: 0.000659, l2: 0.000581, l3: 0.000774, l4: 0.001230, l5: 0.001918, l6: 0.002882
[epoch:  47/1000, batch:   168/ 1052, ite: 12140] train loss: 0.009710, tar: 0.000895 
l0: 0.001393, l1: 0.001344, l2: 0.001429, l3: 0.001337, l4: 0.001447, l5: 0.002488, l6: 0.003285
[epoch:  47/1000, batch:   248/ 1052, ite: 12160] train loss: 0.009686, tar: 0.000892 
l0: 0.000703, l1: 0.000638, l2: 0.000679, l3: 0.000857, l4: 0.001130, l5: 0.001871, l6: 0.002357
[epoch:  47/1000, batch:   328/ 1052, ite: 12180] train loss: 0.009660, tar: 0.000889 
l0: 0.000292, l1: 0.000290, l2: 0.000304, l3: 0.000368, l4: 0.000621, l5: 0.000865, l6: 0.001783
[epoch:  47/1000, batch:   408/ 1052, ite: 12200] train loss: 0.009632, tar: 0.000886 
l0: 0.000816, l1: 0.000850, l2: 0.000794, l3: 0.000744, l4: 0.000979, l5: 0.001662, l6: 0.001972
[epoch:  47/1000, batch:   488/ 1052, ite: 12220] train loss: 0.009631, tar: 0.000885 
l0: 0.000391, l1: 0.000418, l2: 0.000384, l3: 0.000468, l4: 0.000628, l5: 0.000910, l6: 0.001427
[epoch:  47/1000, batch:   568/ 1052, ite: 12240] train loss: 0.009613, tar: 0.000883 
l0: 0.000278, l1: 0.000287, l2: 0.000278, l3: 0.000365, l4: 0.000490, l5: 0.000935, l6: 0.000974
[epoch:  47/1000, batch:   648/ 1052, ite: 12260] train loss: 0.009606, tar: 0.000882 
l0: 0.000725, l1: 0.000703, l2: 0.000655, l3: 0.001015, l4: 0.001368, l5: 0.001857, l6: 0.003936
[epoch:  47/1000, batch:   728/ 1052, ite: 12280] train loss: 0.009600, tar: 0.000881 
l0: 0.000374, l1: 0.000354, l2: 0.000377, l3: 0.000505, l4: 0.000751, l5: 0.001014, l6: 0.003077
[epoch:  47/1000, batch:   808/ 1052, ite: 12300] train loss: 0.009580, tar: 0.000879 
l0: 0.000658, l1: 0.000692, l2: 0.001008, l3: 0.000944, l4: 0.001191, l5: 0.001389, l6: 0.002438
[epoch:  47/1000, batch:   888/ 1052, ite: 12320] train loss: 0.009617, tar: 0.000884 
l0: 0.000469, l1: 0.000549, l2: 0.000450, l3: 0.000667, l4: 0.000799, l5: 0.001083, l6: 0.001179
[epoch:  47/1000, batch:   968/ 1052, ite: 12340] train loss: 0.009622, tar: 0.000884 
l0: 0.000749, l1: 0.000723, l2: 0.000697, l3: 0.000851, l4: 0.001494, l5: 0.002308, l6: 0.003166
[epoch:  47/1000, batch:  1048/ 1052, ite: 12360] train loss: 0.009617, tar: 0.000884 
[Epoch 47/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000579, l1: 0.000559, l2: 0.000695, l3: 0.000769, l4: 0.001027, l5: 0.001401, l6: 0.001987
[epoch:  48/1000, batch:    76/ 1052, ite: 12380] train loss: 0.009602, tar: 0.000882 
l0: 0.000794, l1: 0.000800, l2: 0.000770, l3: 0.000953, l4: 0.001425, l5: 0.002460, l6: 0.003928
[epoch:  48/1000, batch:   156/ 1052, ite: 12400] train loss: 0.009589, tar: 0.000881 
l0: 0.000571, l1: 0.000632, l2: 0.000659, l3: 0.001097, l4: 0.000728, l5: 0.000887, l6: 0.001838
[epoch:  48/1000, batch:   236/ 1052, ite: 12420] train loss: 0.009630, tar: 0.000888 
l0: 0.000717, l1: 0.000748, l2: 0.000712, l3: 0.001272, l4: 0.001407, l5: 0.001223, l6: 0.001784
[epoch:  48/1000, batch:   316/ 1052, ite: 12440] train loss: 0.009629, tar: 0.000887 
l0: 0.000634, l1: 0.000744, l2: 0.000564, l3: 0.000892, l4: 0.000973, l5: 0.001575, l6: 0.001532
[epoch:  48/1000, batch:   396/ 1052, ite: 12460] train loss: 0.009609, tar: 0.000885 
l0: 0.000633, l1: 0.000651, l2: 0.000729, l3: 0.000835, l4: 0.001185, l5: 0.001716, l6: 0.002723
[epoch:  48/1000, batch:   476/ 1052, ite: 12480] train loss: 0.009603, tar: 0.000884 
l0: 0.000514, l1: 0.000530, l2: 0.000531, l3: 0.000770, l4: 0.000903, l5: 0.001275, l6: 0.002627
[epoch:  48/1000, batch:   556/ 1052, ite: 12500] train loss: 0.009595, tar: 0.000883 
l0: 0.000841, l1: 0.000994, l2: 0.000827, l3: 0.001264, l4: 0.001256, l5: 0.001896, l6: 0.002624
[epoch:  48/1000, batch:   636/ 1052, ite: 12520] train loss: 0.009599, tar: 0.000883 
l0: 0.002302, l1: 0.002537, l2: 0.002641, l3: 0.001917, l4: 0.002781, l5: 0.003124, l6: 0.005554
[epoch:  48/1000, batch:   716/ 1052, ite: 12540] train loss: 0.009586, tar: 0.000882 
l0: 0.000426, l1: 0.000427, l2: 0.000463, l3: 0.000513, l4: 0.000745, l5: 0.001108, l6: 0.002471
[epoch:  48/1000, batch:   796/ 1052, ite: 12560] train loss: 0.009560, tar: 0.000879 
l0: 0.000557, l1: 0.000532, l2: 0.000604, l3: 0.000727, l4: 0.000865, l5: 0.001213, l6: 0.002655
[epoch:  48/1000, batch:   876/ 1052, ite: 12580] train loss: 0.009552, tar: 0.000877 
l0: 0.000750, l1: 0.000733, l2: 0.000798, l3: 0.001098, l4: 0.001676, l5: 0.001168, l6: 0.002495
[epoch:  48/1000, batch:   956/ 1052, ite: 12600] train loss: 0.009540, tar: 0.000875 
l0: 0.000791, l1: 0.000810, l2: 0.000840, l3: 0.000924, l4: 0.001367, l5: 0.001742, l6: 0.002456
[epoch:  48/1000, batch:  1036/ 1052, ite: 12620] train loss: 0.009519, tar: 0.000873 
[Epoch 48/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000415, l1: 0.000462, l2: 0.000454, l3: 0.000507, l4: 0.000707, l5: 0.001236, l6: 0.002209
[epoch:  49/1000, batch:    64/ 1052, ite: 12640] train loss: 0.009505, tar: 0.000872 
l0: 0.000434, l1: 0.000436, l2: 0.000475, l3: 0.000515, l4: 0.000626, l5: 0.000998, l6: 0.001844
[epoch:  49/1000, batch:   144/ 1052, ite: 12660] train loss: 0.009502, tar: 0.000871 
l0: 0.000476, l1: 0.000500, l2: 0.000507, l3: 0.000593, l4: 0.000690, l5: 0.001006, l6: 0.001736
[epoch:  49/1000, batch:   224/ 1052, ite: 12680] train loss: 0.009491, tar: 0.000870 
l0: 0.000718, l1: 0.000739, l2: 0.000749, l3: 0.000873, l4: 0.001067, l5: 0.001689, l6: 0.002613
[epoch:  49/1000, batch:   304/ 1052, ite: 12700] train loss: 0.009488, tar: 0.000870 
l0: 0.000447, l1: 0.000457, l2: 0.000466, l3: 0.000520, l4: 0.000576, l5: 0.001064, l6: 0.001657
[epoch:  49/1000, batch:   384/ 1052, ite: 12720] train loss: 0.009497, tar: 0.000870 
l0: 0.001123, l1: 0.001196, l2: 0.001342, l3: 0.001472, l4: 0.001765, l5: 0.002428, l6: 0.002932
[epoch:  49/1000, batch:   464/ 1052, ite: 12740] train loss: 0.009502, tar: 0.000870 
l0: 0.000599, l1: 0.000608, l2: 0.000658, l3: 0.000764, l4: 0.000971, l5: 0.001260, l6: 0.002842
[epoch:  49/1000, batch:   544/ 1052, ite: 12760] train loss: 0.009483, tar: 0.000867 
l0: 0.000485, l1: 0.000529, l2: 0.000547, l3: 0.000598, l4: 0.000736, l5: 0.001054, l6: 0.002152
[epoch:  49/1000, batch:   624/ 1052, ite: 12780] train loss: 0.009476, tar: 0.000867 
l0: 0.000561, l1: 0.000539, l2: 0.000616, l3: 0.000662, l4: 0.000941, l5: 0.001510, l6: 0.002755
[epoch:  49/1000, batch:   704/ 1052, ite: 12800] train loss: 0.009466, tar: 0.000866 
l0: 0.000646, l1: 0.000715, l2: 0.000623, l3: 0.000633, l4: 0.000678, l5: 0.001440, l6: 0.002372
[epoch:  49/1000, batch:   784/ 1052, ite: 12820] train loss: 0.009447, tar: 0.000864 
l0: 0.001181, l1: 0.001181, l2: 0.001142, l3: 0.001499, l4: 0.001666, l5: 0.002829, l6: 0.003425
[epoch:  49/1000, batch:   864/ 1052, ite: 12840] train loss: 0.009454, tar: 0.000865 
l0: 0.001443, l1: 0.001285, l2: 0.002020, l3: 0.002134, l4: 0.001988, l5: 0.002922, l6: 0.004848
[epoch:  49/1000, batch:   944/ 1052, ite: 12860] train loss: 0.009455, tar: 0.000866 
l0: 0.000779, l1: 0.000884, l2: 0.000828, l3: 0.000800, l4: 0.000875, l5: 0.001397, l6: 0.001977
[epoch:  49/1000, batch:  1024/ 1052, ite: 12880] train loss: 0.009484, tar: 0.000871 
[Epoch 49/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000532, l1: 0.000518, l2: 0.000635, l3: 0.000727, l4: 0.001108, l5: 0.001236, l6: 0.002802
[epoch:  50/1000, batch:    52/ 1052, ite: 12900] train loss: 0.009476, tar: 0.000870 
l0: 0.000714, l1: 0.000728, l2: 0.000754, l3: 0.000996, l4: 0.001117, l5: 0.001869, l6: 0.003079
[epoch:  50/1000, batch:   132/ 1052, ite: 12920] train loss: 0.009457, tar: 0.000868 
l0: 0.000465, l1: 0.000477, l2: 0.000412, l3: 0.000604, l4: 0.000688, l5: 0.001216, l6: 0.002269
[epoch:  50/1000, batch:   212/ 1052, ite: 12940] train loss: 0.009448, tar: 0.000867 
l0: 0.000554, l1: 0.000550, l2: 0.000712, l3: 0.000734, l4: 0.000885, l5: 0.001216, l6: 0.001525
[epoch:  50/1000, batch:   292/ 1052, ite: 12960] train loss: 0.009457, tar: 0.000867 
l0: 0.001042, l1: 0.001086, l2: 0.001154, l3: 0.001675, l4: 0.001852, l5: 0.002239, l6: 0.003086
[epoch:  50/1000, batch:   372/ 1052, ite: 12980] train loss: 0.009461, tar: 0.000867 
l0: 0.000368, l1: 0.000357, l2: 0.000362, l3: 0.000371, l4: 0.000494, l5: 0.001152, l6: 0.001344
[epoch:  50/1000, batch:   452/ 1052, ite: 13000] train loss: 0.009446, tar: 0.000866 
l0: 0.001177, l1: 0.001105, l2: 0.001161, l3: 0.001467, l4: 0.001906, l5: 0.002878, l6: 0.004429
[epoch:  50/1000, batch:   532/ 1052, ite: 13020] train loss: 0.009436, tar: 0.000864 
l0: 0.000413, l1: 0.000455, l2: 0.000427, l3: 0.000540, l4: 0.000583, l5: 0.001069, l6: 0.001416
[epoch:  50/1000, batch:   612/ 1052, ite: 13040] train loss: 0.009435, tar: 0.000864 
l0: 0.000541, l1: 0.000541, l2: 0.000539, l3: 0.000606, l4: 0.000844, l5: 0.001473, l6: 0.002729
[epoch:  50/1000, batch:   692/ 1052, ite: 13060] train loss: 0.009428, tar: 0.000864 
l0: 0.001198, l1: 0.001923, l2: 0.000896, l3: 0.001338, l4: 0.001631, l5: 0.002106, l6: 0.003699
[epoch:  50/1000, batch:   772/ 1052, ite: 13080] train loss: 0.009411, tar: 0.000862 
l0: 0.000355, l1: 0.000315, l2: 0.000425, l3: 0.000472, l4: 0.000472, l5: 0.000833, l6: 0.000893
[epoch:  50/1000, batch:   852/ 1052, ite: 13100] train loss: 0.009393, tar: 0.000860 
l0: 0.001005, l1: 0.001128, l2: 0.001115, l3: 0.001056, l4: 0.000923, l5: 0.001296, l6: 0.001641
[epoch:  50/1000, batch:   932/ 1052, ite: 13120] train loss: 0.009424, tar: 0.000864 
l0: 0.000419, l1: 0.000384, l2: 0.000454, l3: 0.000506, l4: 0.000716, l5: 0.001197, l6: 0.002377
[epoch:  50/1000, batch:  1012/ 1052, ite: 13140] train loss: 0.009417, tar: 0.000863 
[Epoch 50/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000911, l1: 0.000967, l2: 0.000999, l3: 0.001036, l4: 0.001510, l5: 0.002240, l6: 0.004214
[epoch:  51/1000, batch:    40/ 1052, ite: 13160] train loss: 0.009409, tar: 0.000861 
l0: 0.000559, l1: 0.000563, l2: 0.000565, l3: 0.000625, l4: 0.000726, l5: 0.001009, l6: 0.001700
[epoch:  51/1000, batch:   120/ 1052, ite: 13180] train loss: 0.009394, tar: 0.000859 
l0: 0.000362, l1: 0.000324, l2: 0.000346, l3: 0.000422, l4: 0.000603, l5: 0.000967, l6: 0.001927
[epoch:  51/1000, batch:   200/ 1052, ite: 13200] train loss: 0.009377, tar: 0.000858 
l0: 0.000556, l1: 0.000569, l2: 0.000556, l3: 0.000630, l4: 0.000847, l5: 0.001073, l6: 0.001587
[epoch:  51/1000, batch:   280/ 1052, ite: 13220] train loss: 0.009366, tar: 0.000856 
l0: 0.000659, l1: 0.000634, l2: 0.000651, l3: 0.000686, l4: 0.000940, l5: 0.001451, l6: 0.002400
[epoch:  51/1000, batch:   360/ 1052, ite: 13240] train loss: 0.009346, tar: 0.000854 
l0: 0.000277, l1: 0.000296, l2: 0.000264, l3: 0.000335, l4: 0.000697, l5: 0.000725, l6: 0.002112
[epoch:  51/1000, batch:   440/ 1052, ite: 13260] train loss: 0.009337, tar: 0.000854 
l0: 0.000440, l1: 0.000444, l2: 0.000471, l3: 0.000642, l4: 0.000776, l5: 0.001657, l6: 0.002289
[epoch:  51/1000, batch:   520/ 1052, ite: 13280] train loss: 0.009322, tar: 0.000852 
l0: 0.001446, l1: 0.001547, l2: 0.001354, l3: 0.001479, l4: 0.001766, l5: 0.002577, l6: 0.003159
[epoch:  51/1000, batch:   600/ 1052, ite: 13300] train loss: 0.009316, tar: 0.000852 
l0: 0.000326, l1: 0.000331, l2: 0.000365, l3: 0.000389, l4: 0.000585, l5: 0.000875, l6: 0.001577
[epoch:  51/1000, batch:   680/ 1052, ite: 13320] train loss: 0.009302, tar: 0.000850 
l0: 0.001061, l1: 0.001188, l2: 0.001126, l3: 0.001145, l4: 0.001168, l5: 0.001579, l6: 0.002509
[epoch:  51/1000, batch:   760/ 1052, ite: 13340] train loss: 0.009296, tar: 0.000849 
l0: 0.001097, l1: 0.001080, l2: 0.001124, l3: 0.001304, l4: 0.001525, l5: 0.003118, l6: 0.003827
[epoch:  51/1000, batch:   840/ 1052, ite: 13360] train loss: 0.009303, tar: 0.000849 
l0: 0.000666, l1: 0.000663, l2: 0.000740, l3: 0.000871, l4: 0.000943, l5: 0.001401, l6: 0.002122
[epoch:  51/1000, batch:   920/ 1052, ite: 13380] train loss: 0.009315, tar: 0.000852 
l0: 0.000458, l1: 0.000446, l2: 0.000507, l3: 0.000415, l4: 0.000636, l5: 0.001100, l6: 0.002197
[epoch:  51/1000, batch:  1000/ 1052, ite: 13400] train loss: 0.009325, tar: 0.000853 
[Epoch 51/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000538, l1: 0.000515, l2: 0.000500, l3: 0.000509, l4: 0.000759, l5: 0.001269, l6: 0.002066
[epoch:  52/1000, batch:    28/ 1052, ite: 13420] train loss: 0.009337, tar: 0.000854 
l0: 0.000284, l1: 0.000282, l2: 0.000319, l3: 0.000397, l4: 0.000506, l5: 0.000698, l6: 0.001121
[epoch:  52/1000, batch:   108/ 1052, ite: 13440] train loss: 0.009332, tar: 0.000854 
l0: 0.000642, l1: 0.000607, l2: 0.000691, l3: 0.001053, l4: 0.001296, l5: 0.001999, l6: 0.002254
[epoch:  52/1000, batch:   188/ 1052, ite: 13460] train loss: 0.009327, tar: 0.000853 
l0: 0.000526, l1: 0.000546, l2: 0.000521, l3: 0.000615, l4: 0.000821, l5: 0.001521, l6: 0.002593
[epoch:  52/1000, batch:   268/ 1052, ite: 13480] train loss: 0.009356, tar: 0.000857 
l0: 0.000436, l1: 0.000464, l2: 0.000475, l3: 0.000452, l4: 0.000567, l5: 0.001106, l6: 0.002169
[epoch:  52/1000, batch:   348/ 1052, ite: 13500] train loss: 0.009359, tar: 0.000857 
l0: 0.000573, l1: 0.000569, l2: 0.000608, l3: 0.000672, l4: 0.000899, l5: 0.001623, l6: 0.001906
[epoch:  52/1000, batch:   428/ 1052, ite: 13520] train loss: 0.009346, tar: 0.000856 
l0: 0.000624, l1: 0.000561, l2: 0.000798, l3: 0.001041, l4: 0.001385, l5: 0.002015, l6: 0.002648
[epoch:  52/1000, batch:   508/ 1052, ite: 13540] train loss: 0.009347, tar: 0.000855 
l0: 0.000653, l1: 0.000640, l2: 0.000627, l3: 0.000877, l4: 0.001008, l5: 0.001317, l6: 0.001448
[epoch:  52/1000, batch:   588/ 1052, ite: 13560] train loss: 0.009349, tar: 0.000855 
l0: 0.000324, l1: 0.000369, l2: 0.000310, l3: 0.000306, l4: 0.000506, l5: 0.000992, l6: 0.001227
[epoch:  52/1000, batch:   668/ 1052, ite: 13580] train loss: 0.009336, tar: 0.000854 
l0: 0.000643, l1: 0.000656, l2: 0.000686, l3: 0.000826, l4: 0.001151, l5: 0.001598, l6: 0.002823
[epoch:  52/1000, batch:   748/ 1052, ite: 13600] train loss: 0.009326, tar: 0.000852 
l0: 0.000514, l1: 0.000510, l2: 0.000636, l3: 0.000819, l4: 0.000946, l5: 0.001810, l6: 0.002072
[epoch:  52/1000, batch:   828/ 1052, ite: 13620] train loss: 0.009318, tar: 0.000851 
l0: 0.000991, l1: 0.001241, l2: 0.000693, l3: 0.000888, l4: 0.001107, l5: 0.001607, l6: 0.002232
[epoch:  52/1000, batch:   908/ 1052, ite: 13640] train loss: 0.009312, tar: 0.000850 
l0: 0.000705, l1: 0.000739, l2: 0.000651, l3: 0.000769, l4: 0.001136, l5: 0.001484, l6: 0.002187
[epoch:  52/1000, batch:   988/ 1052, ite: 13660] train loss: 0.009320, tar: 0.000851 
[Epoch 52/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000494, l1: 0.000517, l2: 0.000496, l3: 0.000531, l4: 0.000793, l5: 0.001301, l6: 0.002047
[epoch:  53/1000, batch:    16/ 1052, ite: 13680] train loss: 0.009313, tar: 0.000850 
l0: 0.000815, l1: 0.000885, l2: 0.000861, l3: 0.000829, l4: 0.001035, l5: 0.001529, l6: 0.002205
[epoch:  53/1000, batch:    96/ 1052, ite: 13700] train loss: 0.009342, tar: 0.000854 
l0: 0.000533, l1: 0.000577, l2: 0.000655, l3: 0.000586, l4: 0.000761, l5: 0.001154, l6: 0.002102
[epoch:  53/1000, batch:   176/ 1052, ite: 13720] train loss: 0.009326, tar: 0.000852 
l0: 0.000406, l1: 0.000446, l2: 0.000397, l3: 0.000494, l4: 0.000588, l5: 0.001379, l6: 0.001752
[epoch:  53/1000, batch:   256/ 1052, ite: 13740] train loss: 0.009320, tar: 0.000852 
l0: 0.001202, l1: 0.001111, l2: 0.001273, l3: 0.001644, l4: 0.002120, l5: 0.003561, l6: 0.006234
[epoch:  53/1000, batch:   336/ 1052, ite: 13760] train loss: 0.009316, tar: 0.000851 
l0: 0.000416, l1: 0.000445, l2: 0.000431, l3: 0.000486, l4: 0.000601, l5: 0.000854, l6: 0.001164
[epoch:  53/1000, batch:   416/ 1052, ite: 13780] train loss: 0.009308, tar: 0.000850 
l0: 0.000818, l1: 0.000891, l2: 0.000827, l3: 0.001015, l4: 0.001179, l5: 0.001869, l6: 0.002511
[epoch:  53/1000, batch:   496/ 1052, ite: 13800] train loss: 0.009299, tar: 0.000848 
l0: 0.000531, l1: 0.000580, l2: 0.000506, l3: 0.000882, l4: 0.000899, l5: 0.001460, l6: 0.001761
[epoch:  53/1000, batch:   576/ 1052, ite: 13820] train loss: 0.009287, tar: 0.000847 
l0: 0.000421, l1: 0.000408, l2: 0.000468, l3: 0.000470, l4: 0.000607, l5: 0.001117, l6: 0.002102
[epoch:  53/1000, batch:   656/ 1052, ite: 13840] train loss: 0.009285, tar: 0.000847 
l0: 0.000530, l1: 0.000538, l2: 0.000504, l3: 0.000585, l4: 0.000636, l5: 0.001164, l6: 0.001095
[epoch:  53/1000, batch:   736/ 1052, ite: 13860] train loss: 0.009281, tar: 0.000846 
l0: 0.000866, l1: 0.000829, l2: 0.000923, l3: 0.001278, l4: 0.001506, l5: 0.002058, l6: 0.002703
[epoch:  53/1000, batch:   816/ 1052, ite: 13880] train loss: 0.009279, tar: 0.000846 
l0: 0.000725, l1: 0.000690, l2: 0.000767, l3: 0.000931, l4: 0.001391, l5: 0.002067, l6: 0.002448
[epoch:  53/1000, batch:   896/ 1052, ite: 13900] train loss: 0.009274, tar: 0.000846 
l0: 0.000578, l1: 0.000616, l2: 0.000596, l3: 0.000954, l4: 0.000827, l5: 0.001464, l6: 0.002713
[epoch:  53/1000, batch:   976/ 1052, ite: 13920] train loss: 0.009273, tar: 0.000846 
[Epoch 53/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000552, l1: 0.000531, l2: 0.000613, l3: 0.000771, l4: 0.001207, l5: 0.001541, l6: 0.002302
[epoch:  54/1000, batch:     4/ 1052, ite: 13940] train loss: 0.009274, tar: 0.000846 
l0: 0.000572, l1: 0.000602, l2: 0.000607, l3: 0.000633, l4: 0.000700, l5: 0.001461, l6: 0.001942
[epoch:  54/1000, batch:    84/ 1052, ite: 13960] train loss: 0.009305, tar: 0.000850 
l0: 0.001130, l1: 0.001287, l2: 0.001021, l3: 0.000945, l4: 0.001352, l5: 0.001898, l6: 0.002683
[epoch:  54/1000, batch:   164/ 1052, ite: 13980] train loss: 0.009315, tar: 0.000851 
l0: 0.000627, l1: 0.000650, l2: 0.000707, l3: 0.000761, l4: 0.000888, l5: 0.001678, l6: 0.002946
[epoch:  54/1000, batch:   244/ 1052, ite: 14000] train loss: 0.009307, tar: 0.000850 
l0: 0.001189, l1: 0.001125, l2: 0.001164, l3: 0.001455, l4: 0.001858, l5: 0.003356, l6: 0.004802
[epoch:  54/1000, batch:   324/ 1052, ite: 14020] train loss: 0.009300, tar: 0.000849 
l0: 0.000457, l1: 0.000482, l2: 0.000469, l3: 0.000486, l4: 0.000680, l5: 0.001286, l6: 0.002375
[epoch:  54/1000, batch:   404/ 1052, ite: 14040] train loss: 0.009294, tar: 0.000848 
l0: 0.000243, l1: 0.000249, l2: 0.000271, l3: 0.000312, l4: 0.000486, l5: 0.000947, l6: 0.001189
[epoch:  54/1000, batch:   484/ 1052, ite: 14060] train loss: 0.009292, tar: 0.000848 
l0: 0.000516, l1: 0.000499, l2: 0.000610, l3: 0.000776, l4: 0.000912, l5: 0.001174, l6: 0.002595
[epoch:  54/1000, batch:   564/ 1052, ite: 14080] train loss: 0.009285, tar: 0.000847 
l0: 0.000437, l1: 0.000391, l2: 0.000483, l3: 0.000520, l4: 0.000692, l5: 0.001114, l6: 0.001342
[epoch:  54/1000, batch:   644/ 1052, ite: 14100] train loss: 0.009279, tar: 0.000846 
l0: 0.000668, l1: 0.000670, l2: 0.000656, l3: 0.000790, l4: 0.000878, l5: 0.001369, l6: 0.002118
[epoch:  54/1000, batch:   724/ 1052, ite: 14120] train loss: 0.009274, tar: 0.000845 
l0: 0.000446, l1: 0.000479, l2: 0.000420, l3: 0.000499, l4: 0.000663, l5: 0.001048, l6: 0.001391
[epoch:  54/1000, batch:   804/ 1052, ite: 14140] train loss: 0.009267, tar: 0.000845 
l0: 0.000625, l1: 0.000638, l2: 0.000800, l3: 0.000956, l4: 0.000964, l5: 0.001339, l6: 0.001521
[epoch:  54/1000, batch:   884/ 1052, ite: 14160] train loss: 0.009260, tar: 0.000844 
l0: 0.000421, l1: 0.000372, l2: 0.000446, l3: 0.000545, l4: 0.000848, l5: 0.001405, l6: 0.002196
[epoch:  54/1000, batch:   964/ 1052, ite: 14180] train loss: 0.009251, tar: 0.000843 
l0: 0.000401, l1: 0.000417, l2: 0.000365, l3: 0.000388, l4: 0.000469, l5: 0.001062, l6: 0.001813
[epoch:  54/1000, batch:  1044/ 1052, ite: 14200] train loss: 0.009257, tar: 0.000843 
[Epoch 54/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000307, l1: 0.000342, l2: 0.000346, l3: 0.000464, l4: 0.000469, l5: 0.000750, l6: 0.000995
[epoch:  55/1000, batch:    72/ 1052, ite: 14220] train loss: 0.009255, tar: 0.000843 
l0: 0.001087, l1: 0.001297, l2: 0.001035, l3: 0.001128, l4: 0.001772, l5: 0.002220, l6: 0.003671
[epoch:  55/1000, batch:   152/ 1052, ite: 14240] train loss: 0.009247, tar: 0.000842 
l0: 0.001684, l1: 0.001973, l2: 0.001460, l3: 0.002577, l4: 0.001976, l5: 0.002217, l6: 0.003291
[epoch:  55/1000, batch:   232/ 1052, ite: 14260] train loss: 0.009248, tar: 0.000842 
l0: 0.000512, l1: 0.000513, l2: 0.000593, l3: 0.000722, l4: 0.000887, l5: 0.001021, l6: 0.002654
[epoch:  55/1000, batch:   312/ 1052, ite: 14280] train loss: 0.009236, tar: 0.000841 
l0: 0.000751, l1: 0.000785, l2: 0.000784, l3: 0.000875, l4: 0.001050, l5: 0.001449, l6: 0.002644
[epoch:  55/1000, batch:   392/ 1052, ite: 14300] train loss: 0.009235, tar: 0.000841 
l0: 0.001018, l1: 0.001038, l2: 0.001041, l3: 0.001311, l4: 0.001435, l5: 0.003180, l6: 0.003827
[epoch:  55/1000, batch:   472/ 1052, ite: 14320] train loss: 0.009228, tar: 0.000840 
l0: 0.000588, l1: 0.000571, l2: 0.000687, l3: 0.000864, l4: 0.001008, l5: 0.001319, l6: 0.002399
[epoch:  55/1000, batch:   552/ 1052, ite: 14340] train loss: 0.009220, tar: 0.000840 
l0: 0.000603, l1: 0.000564, l2: 0.000667, l3: 0.000763, l4: 0.001004, l5: 0.001344, l6: 0.002030
[epoch:  55/1000, batch:   632/ 1052, ite: 14360] train loss: 0.009213, tar: 0.000839 
l0: 0.000854, l1: 0.000898, l2: 0.000832, l3: 0.000920, l4: 0.001132, l5: 0.001316, l6: 0.002355
[epoch:  55/1000, batch:   712/ 1052, ite: 14380] train loss: 0.009207, tar: 0.000838 
l0: 0.000437, l1: 0.000386, l2: 0.000460, l3: 0.000529, l4: 0.000845, l5: 0.001560, l6: 0.001497
[epoch:  55/1000, batch:   792/ 1052, ite: 14400] train loss: 0.009198, tar: 0.000837 
l0: 0.000588, l1: 0.000872, l2: 0.000606, l3: 0.000569, l4: 0.000783, l5: 0.000896, l6: 0.001799
[epoch:  55/1000, batch:   872/ 1052, ite: 14420] train loss: 0.009226, tar: 0.000842 
l0: 0.001014, l1: 0.001069, l2: 0.001276, l3: 0.002697, l4: 0.002591, l5: 0.001814, l6: 0.002262
[epoch:  55/1000, batch:   952/ 1052, ite: 14440] train loss: 0.009232, tar: 0.000842 
l0: 0.000961, l1: 0.001112, l2: 0.000906, l3: 0.000729, l4: 0.000994, l5: 0.001295, l6: 0.001632
[epoch:  55/1000, batch:  1032/ 1052, ite: 14460] train loss: 0.009237, tar: 0.000842 
[Epoch 55/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000424, l1: 0.000476, l2: 0.000391, l3: 0.000400, l4: 0.000683, l5: 0.000931, l6: 0.001516
[epoch:  56/1000, batch:    60/ 1052, ite: 14480] train loss: 0.009237, tar: 0.000842 
l0: 0.000378, l1: 0.000361, l2: 0.000369, l3: 0.000391, l4: 0.000677, l5: 0.001166, l6: 0.002279
[epoch:  56/1000, batch:   140/ 1052, ite: 14500] train loss: 0.009227, tar: 0.000841 
l0: 0.000343, l1: 0.000327, l2: 0.000363, l3: 0.000402, l4: 0.000626, l5: 0.001066, l6: 0.001799
[epoch:  56/1000, batch:   220/ 1052, ite: 14520] train loss: 0.009219, tar: 0.000840 
l0: 0.000626, l1: 0.000621, l2: 0.000610, l3: 0.000756, l4: 0.000999, l5: 0.001513, l6: 0.002320
[epoch:  56/1000, batch:   300/ 1052, ite: 14540] train loss: 0.009220, tar: 0.000840 
l0: 0.000744, l1: 0.000719, l2: 0.000875, l3: 0.000871, l4: 0.001292, l5: 0.001673, l6: 0.002537
[epoch:  56/1000, batch:   380/ 1052, ite: 14560] train loss: 0.009240, tar: 0.000843 
l0: 0.000670, l1: 0.000759, l2: 0.000824, l3: 0.001035, l4: 0.001788, l5: 0.002331, l6: 0.002804
[epoch:  56/1000, batch:   460/ 1052, ite: 14580] train loss: 0.009244, tar: 0.000843 
l0: 0.000594, l1: 0.000558, l2: 0.000659, l3: 0.000778, l4: 0.000863, l5: 0.001747, l6: 0.002760
[epoch:  56/1000, batch:   540/ 1052, ite: 14600] train loss: 0.009245, tar: 0.000843 
l0: 0.000324, l1: 0.000327, l2: 0.000326, l3: 0.000520, l4: 0.000755, l5: 0.000855, l6: 0.001683
[epoch:  56/1000, batch:   620/ 1052, ite: 14620] train loss: 0.009242, tar: 0.000843 
l0: 0.000405, l1: 0.000390, l2: 0.000423, l3: 0.000519, l4: 0.000626, l5: 0.001309, l6: 0.001518
[epoch:  56/1000, batch:   700/ 1052, ite: 14640] train loss: 0.009233, tar: 0.000842 
l0: 0.000494, l1: 0.000519, l2: 0.000535, l3: 0.000677, l4: 0.000829, l5: 0.001363, l6: 0.002057
[epoch:  56/1000, batch:   780/ 1052, ite: 14660] train loss: 0.009228, tar: 0.000841 
l0: 0.000841, l1: 0.000920, l2: 0.000870, l3: 0.000978, l4: 0.001079, l5: 0.001602, l6: 0.002299
[epoch:  56/1000, batch:   860/ 1052, ite: 14680] train loss: 0.009229, tar: 0.000841 
l0: 0.000406, l1: 0.000425, l2: 0.000410, l3: 0.000471, l4: 0.000576, l5: 0.001094, l6: 0.001471
[epoch:  56/1000, batch:   940/ 1052, ite: 14700] train loss: 0.009218, tar: 0.000840 
l0: 0.001065, l1: 0.001178, l2: 0.000924, l3: 0.001090, l4: 0.001190, l5: 0.002717, l6: 0.003066
[epoch:  56/1000, batch:  1020/ 1052, ite: 14720] train loss: 0.009221, tar: 0.000841 
[Epoch 56/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000269, l1: 0.000320, l2: 0.000304, l3: 0.000382, l4: 0.000681, l5: 0.000971, l6: 0.001761
[epoch:  57/1000, batch:    48/ 1052, ite: 14740] train loss: 0.009213, tar: 0.000840 
l0: 0.001009, l1: 0.000950, l2: 0.001063, l3: 0.001234, l4: 0.001483, l5: 0.002011, l6: 0.003343
[epoch:  57/1000, batch:   128/ 1052, ite: 14760] train loss: 0.009208, tar: 0.000839 
l0: 0.000278, l1: 0.000344, l2: 0.000294, l3: 0.000374, l4: 0.000472, l5: 0.000598, l6: 0.001031
[epoch:  57/1000, batch:   208/ 1052, ite: 14780] train loss: 0.009206, tar: 0.000839 
l0: 0.000975, l1: 0.000926, l2: 0.000994, l3: 0.000979, l4: 0.001101, l5: 0.002249, l6: 0.002168
[epoch:  57/1000, batch:   288/ 1052, ite: 14800] train loss: 0.009196, tar: 0.000838 
l0: 0.000743, l1: 0.000763, l2: 0.000764, l3: 0.000828, l4: 0.001113, l5: 0.001169, l6: 0.002399
[epoch:  57/1000, batch:   368/ 1052, ite: 14820] train loss: 0.009189, tar: 0.000837 
l0: 0.000592, l1: 0.000593, l2: 0.000712, l3: 0.000763, l4: 0.001007, l5: 0.001245, l6: 0.001904
[epoch:  57/1000, batch:   448/ 1052, ite: 14840] train loss: 0.009178, tar: 0.000836 
l0: 0.000767, l1: 0.000664, l2: 0.000890, l3: 0.001023, l4: 0.001302, l5: 0.001560, l6: 0.002616
[epoch:  57/1000, batch:   528/ 1052, ite: 14860] train loss: 0.009175, tar: 0.000835 
l0: 0.000994, l1: 0.001047, l2: 0.001068, l3: 0.001054, l4: 0.001220, l5: 0.001845, l6: 0.002464
[epoch:  57/1000, batch:   608/ 1052, ite: 14880] train loss: 0.009166, tar: 0.000834 
l0: 0.001246, l1: 0.001377, l2: 0.001426, l3: 0.001462, l4: 0.001905, l5: 0.002492, l6: 0.003446
[epoch:  57/1000, batch:   688/ 1052, ite: 14900] train loss: 0.009182, tar: 0.000837 
l0: 0.000551, l1: 0.000545, l2: 0.000702, l3: 0.000728, l4: 0.000776, l5: 0.001305, l6: 0.001488
[epoch:  57/1000, batch:   768/ 1052, ite: 14920] train loss: 0.009175, tar: 0.000836 
l0: 0.000616, l1: 0.000719, l2: 0.000544, l3: 0.000651, l4: 0.000764, l5: 0.001385, l6: 0.002034
[epoch:  57/1000, batch:   848/ 1052, ite: 14940] train loss: 0.009168, tar: 0.000835 
l0: 0.000261, l1: 0.000258, l2: 0.000254, l3: 0.000304, l4: 0.000500, l5: 0.001121, l6: 0.001763
[epoch:  57/1000, batch:   928/ 1052, ite: 14960] train loss: 0.009160, tar: 0.000834 
l0: 0.000596, l1: 0.000604, l2: 0.000638, l3: 0.000672, l4: 0.001029, l5: 0.001351, l6: 0.001684
[epoch:  57/1000, batch:  1008/ 1052, ite: 14980] train loss: 0.009161, tar: 0.000834 
[Epoch 57/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000500, l1: 0.000492, l2: 0.000531, l3: 0.000594, l4: 0.000894, l5: 0.001211, l6: 0.002586
[epoch:  58/1000, batch:    36/ 1052, ite: 15000] train loss: 0.009160, tar: 0.000834 
l0: 0.000925, l1: 0.000993, l2: 0.000905, l3: 0.000898, l4: 0.001109, l5: 0.001296, l6: 0.002656
[epoch:  58/1000, batch:   116/ 1052, ite: 15020] train loss: 0.009159, tar: 0.000834 
l0: 0.000653, l1: 0.000647, l2: 0.000726, l3: 0.000863, l4: 0.000974, l5: 0.001350, l6: 0.001833
[epoch:  58/1000, batch:   196/ 1052, ite: 15040] train loss: 0.009149, tar: 0.000833 
l0: 0.000848, l1: 0.000815, l2: 0.000885, l3: 0.000932, l4: 0.001131, l5: 0.001562, l6: 0.002171
[epoch:  58/1000, batch:   276/ 1052, ite: 15060] train loss: 0.009142, tar: 0.000832 
l0: 0.003446, l1: 0.002867, l2: 0.004606, l3: 0.004503, l4: 0.004086, l5: 0.005097, l6: 0.005124
[epoch:  58/1000, batch:   356/ 1052, ite: 15080] train loss: 0.009144, tar: 0.000832 
l0: 0.000533, l1: 0.000511, l2: 0.000568, l3: 0.000758, l4: 0.000928, l5: 0.001285, l6: 0.001641
[epoch:  58/1000, batch:   436/ 1052, ite: 15100] train loss: 0.009158, tar: 0.000834 
l0: 0.000707, l1: 0.000753, l2: 0.000747, l3: 0.000874, l4: 0.001198, l5: 0.001966, l6: 0.002639
[epoch:  58/1000, batch:   516/ 1052, ite: 15120] train loss: 0.009152, tar: 0.000833 
l0: 0.000550, l1: 0.000624, l2: 0.000598, l3: 0.000746, l4: 0.000737, l5: 0.000953, l6: 0.001580
[epoch:  58/1000, batch:   596/ 1052, ite: 15140] train loss: 0.009152, tar: 0.000833 
l0: 0.000599, l1: 0.000588, l2: 0.000612, l3: 0.000892, l4: 0.001179, l5: 0.001897, l6: 0.002723
[epoch:  58/1000, batch:   676/ 1052, ite: 15160] train loss: 0.009143, tar: 0.000831 
l0: 0.001190, l1: 0.001079, l2: 0.001093, l3: 0.001328, l4: 0.001835, l5: 0.002552, l6: 0.002966
[epoch:  58/1000, batch:   756/ 1052, ite: 15180] train loss: 0.009146, tar: 0.000832 
l0: 0.001210, l1: 0.001124, l2: 0.001502, l3: 0.001691, l4: 0.001617, l5: 0.002161, l6: 0.003100
[epoch:  58/1000, batch:   836/ 1052, ite: 15200] train loss: 0.009139, tar: 0.000831 
l0: 0.000328, l1: 0.000322, l2: 0.000363, l3: 0.000386, l4: 0.000544, l5: 0.000903, l6: 0.001654
[epoch:  58/1000, batch:   916/ 1052, ite: 15220] train loss: 0.009135, tar: 0.000831 
l0: 0.000529, l1: 0.000577, l2: 0.000503, l3: 0.000562, l4: 0.000551, l5: 0.001097, l6: 0.001666
[epoch:  58/1000, batch:   996/ 1052, ite: 15240] train loss: 0.009127, tar: 0.000830 
[Epoch 58/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000769, l1: 0.000716, l2: 0.000712, l3: 0.000977, l4: 0.001057, l5: 0.001485, l6: 0.002362
[epoch:  59/1000, batch:    24/ 1052, ite: 15260] train loss: 0.009126, tar: 0.000830 
l0: 0.000586, l1: 0.000591, l2: 0.000635, l3: 0.000751, l4: 0.000846, l5: 0.001476, l6: 0.001485
[epoch:  59/1000, batch:   104/ 1052, ite: 15280] train loss: 0.009118, tar: 0.000828 
l0: 0.000844, l1: 0.000780, l2: 0.000849, l3: 0.001002, l4: 0.001227, l5: 0.002158, l6: 0.003847
[epoch:  59/1000, batch:   184/ 1052, ite: 15300] train loss: 0.009115, tar: 0.000828 
l0: 0.000522, l1: 0.000490, l2: 0.000460, l3: 0.000658, l4: 0.000876, l5: 0.001573, l6: 0.001788
[epoch:  59/1000, batch:   264/ 1052, ite: 15320] train loss: 0.009108, tar: 0.000827 
l0: 0.000450, l1: 0.000424, l2: 0.000492, l3: 0.000625, l4: 0.000911, l5: 0.001268, l6: 0.002749
[epoch:  59/1000, batch:   344/ 1052, ite: 15340] train loss: 0.009108, tar: 0.000827 
l0: 0.000478, l1: 0.000482, l2: 0.000511, l3: 0.000611, l4: 0.000736, l5: 0.001440, l6: 0.001968
[epoch:  59/1000, batch:   424/ 1052, ite: 15360] train loss: 0.009106, tar: 0.000827 
l0: 0.000625, l1: 0.000592, l2: 0.000787, l3: 0.000804, l4: 0.000947, l5: 0.001368, l6: 0.002136
[epoch:  59/1000, batch:   504/ 1052, ite: 15380] train loss: 0.009104, tar: 0.000827 
l0: 0.000304, l1: 0.000345, l2: 0.000346, l3: 0.000317, l4: 0.000567, l5: 0.000606, l6: 0.001503
[epoch:  59/1000, batch:   584/ 1052, ite: 15400] train loss: 0.009100, tar: 0.000826 
l0: 0.000563, l1: 0.000596, l2: 0.000623, l3: 0.000497, l4: 0.000591, l5: 0.000888, l6: 0.001951
[epoch:  59/1000, batch:   664/ 1052, ite: 15420] train loss: 0.009094, tar: 0.000825 
l0: 0.000474, l1: 0.000449, l2: 0.000484, l3: 0.000583, l4: 0.000903, l5: 0.001553, l6: 0.003365
[epoch:  59/1000, batch:   744/ 1052, ite: 15440] train loss: 0.009087, tar: 0.000825 
l0: 0.000533, l1: 0.000536, l2: 0.000599, l3: 0.000637, l4: 0.001085, l5: 0.001105, l6: 0.001808
[epoch:  59/1000, batch:   824/ 1052, ite: 15460] train loss: 0.009100, tar: 0.000826 
l0: 0.000684, l1: 0.000750, l2: 0.000760, l3: 0.000914, l4: 0.001065, l5: 0.001558, l6: 0.002152
[epoch:  59/1000, batch:   904/ 1052, ite: 15480] train loss: 0.009095, tar: 0.000826 
l0: 0.000827, l1: 0.000770, l2: 0.000909, l3: 0.000978, l4: 0.001356, l5: 0.002469, l6: 0.003654
[epoch:  59/1000, batch:   984/ 1052, ite: 15500] train loss: 0.009108, tar: 0.000827 
[Epoch 59/1000] Test loss: 0.012, Test target loss: 0.001
l0: 0.000680, l1: 0.000749, l2: 0.000519, l3: 0.000639, l4: 0.000678, l5: 0.001125, l6: 0.001384
[epoch:  60/1000, batch:    12/ 1052, ite: 15520] train loss: 0.009117, tar: 0.000828 
l0: 0.000683, l1: 0.000630, l2: 0.000860, l3: 0.001014, l4: 0.001152, l5: 0.001806, l6: 0.001834
[epoch:  60/1000, batch:    92/ 1052, ite: 15540] train loss: 0.009116, tar: 0.000828 
l0: 0.001834, l1: 0.002055, l2: 0.002003, l3: 0.002059, l4: 0.002276, l5: 0.004512, l6: 0.006041
[epoch:  60/1000, batch:   172/ 1052, ite: 15560] train loss: 0.009118, tar: 0.000828 
l0: 0.000657, l1: 0.000621, l2: 0.000780, l3: 0.001083, l4: 0.001289, l5: 0.001758, l6: 0.002856
[epoch:  60/1000, batch:   252/ 1052, ite: 15580] train loss: 0.009113, tar: 0.000828 
l0: 0.000456, l1: 0.000451, l2: 0.000488, l3: 0.000537, l4: 0.000681, l5: 0.001139, l6: 0.002122
[epoch:  60/1000, batch:   332/ 1052, ite: 15600] train loss: 0.009109, tar: 0.000827 
l0: 0.000533, l1: 0.000677, l2: 0.000481, l3: 0.000860, l4: 0.000696, l5: 0.001117, l6: 0.001216
[epoch:  60/1000, batch:   412/ 1052, ite: 15620] train loss: 0.009110, tar: 0.000827 
l0: 0.000630, l1: 0.000672, l2: 0.000632, l3: 0.000717, l4: 0.000916, l5: 0.001602, l6: 0.001671
[epoch:  60/1000, batch:   492/ 1052, ite: 15640] train loss: 0.009108, tar: 0.000827 
l0: 0.000821, l1: 0.000905, l2: 0.000786, l3: 0.001013, l4: 0.001332, l5: 0.001858, l6: 0.002115
[epoch:  60/1000, batch:   572/ 1052, ite: 15660] train loss: 0.009103, tar: 0.000826 
l0: 0.000559, l1: 0.000553, l2: 0.000553, l3: 0.000659, l4: 0.000801, l5: 0.001172, l6: 0.002238
[epoch:  60/1000, batch:   652/ 1052, ite: 15680] train loss: 0.009100, tar: 0.000826 
l0: 0.009823, l1: 0.007549, l2: 0.015064, l3: 0.019862, l4: 0.014618, l5: 0.008774, l6: 0.010992
[epoch:  60/1000, batch:   732/ 1052, ite: 15700] train loss: 0.009111, tar: 0.000827 
l0: 0.000523, l1: 0.000499, l2: 0.000586, l3: 0.000737, l4: 0.000973, l5: 0.001364, l6: 0.001809
[epoch:  60/1000, batch:   812/ 1052, ite: 15720] train loss: 0.009105, tar: 0.000826 
l0: 0.000655, l1: 0.000707, l2: 0.000702, l3: 0.000912, l4: 0.001108, l5: 0.001496, l6: 0.002396
[epoch:  60/1000, batch:   892/ 1052, ite: 15740] train loss: 0.009103, tar: 0.000826 
l0: 0.000420, l1: 0.000374, l2: 0.000435, l3: 0.000534, l4: 0.000895, l5: 0.001576, l6: 0.002372
[epoch:  60/1000, batch:   972/ 1052, ite: 15760] train loss: 0.009096, tar: 0.000825 
l0: 0.000268, l1: 0.000268, l2: 0.000343, l3: 0.000343, l4: 0.000478, l5: 0.001007, l6: 0.001436
[epoch:  60/1000, batch:  1052/ 1052, ite: 15780] train loss: 0.009092, tar: 0.000825 
[Epoch 60/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000415, l1: 0.000423, l2: 0.000468, l3: 0.000585, l4: 0.000717, l5: 0.001286, l6: 0.002197
[epoch:  61/1000, batch:    80/ 1052, ite: 15800] train loss: 0.009085, tar: 0.000824 
l0: 0.000685, l1: 0.000687, l2: 0.000777, l3: 0.001029, l4: 0.001283, l5: 0.001819, l6: 0.002710
[epoch:  61/1000, batch:   160/ 1052, ite: 15820] train loss: 0.009078, tar: 0.000823 
l0: 0.000845, l1: 0.000834, l2: 0.000806, l3: 0.001109, l4: 0.001067, l5: 0.001824, l6: 0.002792
[epoch:  61/1000, batch:   240/ 1052, ite: 15840] train loss: 0.009073, tar: 0.000823 
l0: 0.000412, l1: 0.000416, l2: 0.000439, l3: 0.000513, l4: 0.000637, l5: 0.001313, l6: 0.002557
[epoch:  61/1000, batch:   320/ 1052, ite: 15860] train loss: 0.009065, tar: 0.000822 
l0: 0.000382, l1: 0.000353, l2: 0.000415, l3: 0.000628, l4: 0.000810, l5: 0.001394, l6: 0.001822
[epoch:  61/1000, batch:   400/ 1052, ite: 15880] train loss: 0.009059, tar: 0.000821 
l0: 0.001261, l1: 0.001341, l2: 0.001246, l3: 0.001640, l4: 0.001715, l5: 0.001926, l6: 0.002523
[epoch:  61/1000, batch:   480/ 1052, ite: 15900] train loss: 0.009064, tar: 0.000822 
l0: 0.000610, l1: 0.000749, l2: 0.000548, l3: 0.000694, l4: 0.000565, l5: 0.001191, l6: 0.002017
[epoch:  61/1000, batch:   560/ 1052, ite: 15920] train loss: 0.009087, tar: 0.000825 
l0: 0.000779, l1: 0.000785, l2: 0.000874, l3: 0.001230, l4: 0.001755, l5: 0.002415, l6: 0.003969
[epoch:  61/1000, batch:   640/ 1052, ite: 15940] train loss: 0.009099, tar: 0.000826 
l0: 0.000726, l1: 0.000727, l2: 0.000726, l3: 0.000986, l4: 0.001511, l5: 0.001858, l6: 0.004072
[epoch:  61/1000, batch:   720/ 1052, ite: 15960] train loss: 0.009099, tar: 0.000826 
l0: 0.000969, l1: 0.000967, l2: 0.001084, l3: 0.001092, l4: 0.001697, l5: 0.003210, l6: 0.003884
[epoch:  61/1000, batch:   800/ 1052, ite: 15980] train loss: 0.009124, tar: 0.000828 
l0: 0.004074, l1: 0.003980, l2: 0.005837, l3: 0.008073, l4: 0.008177, l5: 0.006094, l6: 0.009111
[epoch:  61/1000, batch:   880/ 1052, ite: 16000] train loss: 0.009126, tar: 0.000828 
l0: 0.000974, l1: 0.001049, l2: 0.001213, l3: 0.001331, l4: 0.001499, l5: 0.001402, l6: 0.002073
[epoch:  61/1000, batch:   960/ 1052, ite: 16020] train loss: 0.009126, tar: 0.000828 
l0: 0.001198, l1: 0.001204, l2: 0.001079, l3: 0.001537, l4: 0.002789, l5: 0.002431, l6: 0.002875
[epoch:  61/1000, batch:  1040/ 1052, ite: 16040] train loss: 0.009126, tar: 0.000828 
[Epoch 61/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.001521, l1: 0.001502, l2: 0.001409, l3: 0.001774, l4: 0.002018, l5: 0.002172, l6: 0.004778
[epoch:  62/1000, batch:    68/ 1052, ite: 16060] train loss: 0.009120, tar: 0.000828 
l0: 0.000723, l1: 0.000701, l2: 0.000719, l3: 0.000787, l4: 0.001146, l5: 0.001837, l6: 0.002938
[epoch:  62/1000, batch:   148/ 1052, ite: 16080] train loss: 0.009116, tar: 0.000827 
l0: 0.000651, l1: 0.000682, l2: 0.000661, l3: 0.000766, l4: 0.000997, l5: 0.001293, l6: 0.001599
[epoch:  62/1000, batch:   228/ 1052, ite: 16100] train loss: 0.009116, tar: 0.000827 
l0: 0.000445, l1: 0.000437, l2: 0.000466, l3: 0.000505, l4: 0.000549, l5: 0.001006, l6: 0.001414
[epoch:  62/1000, batch:   308/ 1052, ite: 16120] train loss: 0.009109, tar: 0.000826 
l0: 0.000742, l1: 0.000726, l2: 0.000711, l3: 0.000764, l4: 0.000974, l5: 0.001366, l6: 0.002590
[epoch:  62/1000, batch:   388/ 1052, ite: 16140] train loss: 0.009106, tar: 0.000826 
l0: 0.000411, l1: 0.000395, l2: 0.000397, l3: 0.000587, l4: 0.000707, l5: 0.001608, l6: 0.002061
[epoch:  62/1000, batch:   468/ 1052, ite: 16160] train loss: 0.009126, tar: 0.000828 
l0: 0.000444, l1: 0.000437, l2: 0.000527, l3: 0.000520, l4: 0.000725, l5: 0.001126, l6: 0.001266
[epoch:  62/1000, batch:   548/ 1052, ite: 16180] train loss: 0.009128, tar: 0.000829 
l0: 0.001119, l1: 0.001042, l2: 0.001612, l3: 0.001576, l4: 0.001165, l5: 0.002236, l6: 0.002764
[epoch:  62/1000, batch:   628/ 1052, ite: 16200] train loss: 0.009121, tar: 0.000828 
l0: 0.000973, l1: 0.001016, l2: 0.001512, l3: 0.001076, l4: 0.001339, l5: 0.002727, l6: 0.002713
[epoch:  62/1000, batch:   708/ 1052, ite: 16220] train loss: 0.009127, tar: 0.000829 
l0: 0.001055, l1: 0.001121, l2: 0.001298, l3: 0.001314, l4: 0.001647, l5: 0.001499, l6: 0.002626
[epoch:  62/1000, batch:   788/ 1052, ite: 16240] train loss: 0.009129, tar: 0.000829 
l0: 0.000796, l1: 0.000796, l2: 0.000825, l3: 0.001374, l4: 0.001147, l5: 0.001718, l6: 0.002755
[epoch:  62/1000, batch:   868/ 1052, ite: 16260] train loss: 0.009133, tar: 0.000830 
l0: 0.000631, l1: 0.000671, l2: 0.000610, l3: 0.000767, l4: 0.000927, l5: 0.001310, l6: 0.002181
[epoch:  62/1000, batch:   948/ 1052, ite: 16280] train loss: 0.009132, tar: 0.000829 
l0: 0.000548, l1: 0.000599, l2: 0.000512, l3: 0.000599, l4: 0.000941, l5: 0.001144, l6: 0.001811
[epoch:  62/1000, batch:  1028/ 1052, ite: 16300] train loss: 0.009131, tar: 0.000829 
[Epoch 62/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000614, l1: 0.000589, l2: 0.000622, l3: 0.000760, l4: 0.000844, l5: 0.001463, l6: 0.002442
[epoch:  63/1000, batch:    56/ 1052, ite: 16320] train loss: 0.009146, tar: 0.000831 
l0: 0.000472, l1: 0.000433, l2: 0.000566, l3: 0.000674, l4: 0.000744, l5: 0.001072, l6: 0.002241
[epoch:  63/1000, batch:   136/ 1052, ite: 16340] train loss: 0.009142, tar: 0.000831 
l0: 0.000735, l1: 0.000793, l2: 0.000691, l3: 0.000734, l4: 0.000923, l5: 0.001284, l6: 0.001964
[epoch:  63/1000, batch:   216/ 1052, ite: 16360] train loss: 0.009138, tar: 0.000830 
l0: 0.000488, l1: 0.000494, l2: 0.000527, l3: 0.000652, l4: 0.000970, l5: 0.001116, l6: 0.001882
[epoch:  63/1000, batch:   296/ 1052, ite: 16380] train loss: 0.009136, tar: 0.000830 
l0: 0.000411, l1: 0.000376, l2: 0.000429, l3: 0.000626, l4: 0.000864, l5: 0.001380, l6: 0.002033
[epoch:  63/1000, batch:   376/ 1052, ite: 16400] train loss: 0.009127, tar: 0.000829 
l0: 0.000543, l1: 0.000585, l2: 0.000577, l3: 0.000808, l4: 0.001192, l5: 0.002039, l6: 0.002769
[epoch:  63/1000, batch:   456/ 1052, ite: 16420] train loss: 0.009120, tar: 0.000828 
l0: 0.001181, l1: 0.001183, l2: 0.001238, l3: 0.001376, l4: 0.001497, l5: 0.001484, l6: 0.002749
[epoch:  63/1000, batch:   536/ 1052, ite: 16440] train loss: 0.009118, tar: 0.000828 
l0: 0.000858, l1: 0.000808, l2: 0.000803, l3: 0.001094, l4: 0.001501, l5: 0.002085, l6: 0.003376
[epoch:  63/1000, batch:   616/ 1052, ite: 16460] train loss: 0.009120, tar: 0.000828 
l0: 0.000437, l1: 0.000439, l2: 0.000450, l3: 0.000432, l4: 0.000657, l5: 0.001025, l6: 0.001444
[epoch:  63/1000, batch:   696/ 1052, ite: 16480] train loss: 0.009122, tar: 0.000828 
l0: 0.001475, l1: 0.001569, l2: 0.001458, l3: 0.001539, l4: 0.001834, l5: 0.002951, l6: 0.004850
[epoch:  63/1000, batch:   776/ 1052, ite: 16500] train loss: 0.009121, tar: 0.000828 
l0: 0.000837, l1: 0.000819, l2: 0.000836, l3: 0.000953, l4: 0.001265, l5: 0.002212, l6: 0.003883
[epoch:  63/1000, batch:   856/ 1052, ite: 16520] train loss: 0.009117, tar: 0.000828 
l0: 0.000673, l1: 0.000651, l2: 0.000755, l3: 0.000835, l4: 0.001159, l5: 0.001625, l6: 0.002825
[epoch:  63/1000, batch:   936/ 1052, ite: 16540] train loss: 0.009117, tar: 0.000828 
l0: 0.000589, l1: 0.000564, l2: 0.000602, l3: 0.000754, l4: 0.000943, l5: 0.001580, l6: 0.002487
[epoch:  63/1000, batch:  1016/ 1052, ite: 16560] train loss: 0.009113, tar: 0.000827 
[Epoch 63/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000207, l1: 0.000220, l2: 0.000226, l3: 0.000281, l4: 0.000469, l5: 0.000856, l6: 0.000969
[epoch:  64/1000, batch:    44/ 1052, ite: 16580] train loss: 0.009111, tar: 0.000827 
l0: 0.000798, l1: 0.000722, l2: 0.000867, l3: 0.001067, l4: 0.001302, l5: 0.002109, l6: 0.002278
[epoch:  64/1000, batch:   124/ 1052, ite: 16600] train loss: 0.009105, tar: 0.000826 
l0: 0.000304, l1: 0.000296, l2: 0.000337, l3: 0.000367, l4: 0.000555, l5: 0.001199, l6: 0.001369
[epoch:  64/1000, batch:   204/ 1052, ite: 16620] train loss: 0.009096, tar: 0.000826 
l0: 0.000832, l1: 0.000882, l2: 0.000964, l3: 0.000954, l4: 0.000940, l5: 0.001440, l6: 0.001572
[epoch:  64/1000, batch:   284/ 1052, ite: 16640] train loss: 0.009105, tar: 0.000827 
l0: 0.000430, l1: 0.000399, l2: 0.000413, l3: 0.000499, l4: 0.000788, l5: 0.001536, l6: 0.002729
[epoch:  64/1000, batch:   364/ 1052, ite: 16660] train loss: 0.009104, tar: 0.000826 
l0: 0.000616, l1: 0.000554, l2: 0.000604, l3: 0.000758, l4: 0.001035, l5: 0.002413, l6: 0.003434
[epoch:  64/1000, batch:   444/ 1052, ite: 16680] train loss: 0.009103, tar: 0.000826 
l0: 0.000923, l1: 0.000947, l2: 0.000939, l3: 0.001252, l4: 0.001259, l5: 0.001895, l6: 0.002994
[epoch:  64/1000, batch:   524/ 1052, ite: 16700] train loss: 0.009100, tar: 0.000826 
l0: 0.000395, l1: 0.000407, l2: 0.000417, l3: 0.000448, l4: 0.000657, l5: 0.001080, l6: 0.001382
[epoch:  64/1000, batch:   604/ 1052, ite: 16720] train loss: 0.009097, tar: 0.000825 
l0: 0.000301, l1: 0.000310, l2: 0.000290, l3: 0.000337, l4: 0.000482, l5: 0.000944, l6: 0.001791
[epoch:  64/1000, batch:   684/ 1052, ite: 16740] train loss: 0.009091, tar: 0.000825 
l0: 0.000787, l1: 0.000821, l2: 0.000853, l3: 0.000888, l4: 0.001038, l5: 0.001663, l6: 0.002864
[epoch:  64/1000, batch:   764/ 1052, ite: 16760] train loss: 0.009091, tar: 0.000825 
l0: 0.000709, l1: 0.000686, l2: 0.000708, l3: 0.000934, l4: 0.001449, l5: 0.002082, l6: 0.002387
[epoch:  64/1000, batch:   844/ 1052, ite: 16780] train loss: 0.009094, tar: 0.000825 
l0: 0.000443, l1: 0.000455, l2: 0.000506, l3: 0.000639, l4: 0.000744, l5: 0.001231, l6: 0.001631
[epoch:  64/1000, batch:   924/ 1052, ite: 16800] train loss: 0.009090, tar: 0.000825 
l0: 0.000258, l1: 0.000245, l2: 0.000256, l3: 0.000296, l4: 0.000493, l5: 0.000730, l6: 0.001744
[epoch:  64/1000, batch:  1004/ 1052, ite: 16820] train loss: 0.009087, tar: 0.000824 
[Epoch 64/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000347, l1: 0.000338, l2: 0.000373, l3: 0.000427, l4: 0.000620, l5: 0.001232, l6: 0.002323
[epoch:  65/1000, batch:    32/ 1052, ite: 16840] train loss: 0.009084, tar: 0.000824 
l0: 0.000371, l1: 0.000368, l2: 0.000399, l3: 0.000435, l4: 0.000626, l5: 0.001195, l6: 0.002126
[epoch:  65/1000, batch:   112/ 1052, ite: 16860] train loss: 0.009079, tar: 0.000823 
l0: 0.000937, l1: 0.001236, l2: 0.001731, l3: 0.000498, l4: 0.000568, l5: 0.000794, l6: 0.001518
[epoch:  65/1000, batch:   192/ 1052, ite: 16880] train loss: 0.009078, tar: 0.000823 
l0: 0.000493, l1: 0.000463, l2: 0.000510, l3: 0.000661, l4: 0.000740, l5: 0.000974, l6: 0.001570
[epoch:  65/1000, batch:   272/ 1052, ite: 16900] train loss: 0.009069, tar: 0.000822 
l0: 0.000522, l1: 0.000510, l2: 0.000564, l3: 0.000624, l4: 0.000858, l5: 0.001331, l6: 0.002085
[epoch:  65/1000, batch:   352/ 1052, ite: 16920] train loss: 0.009066, tar: 0.000821 
l0: 0.000370, l1: 0.000402, l2: 0.000373, l3: 0.000462, l4: 0.000627, l5: 0.001301, l6: 0.002025
[epoch:  65/1000, batch:   432/ 1052, ite: 16940] train loss: 0.009059, tar: 0.000821 
l0: 0.000315, l1: 0.000295, l2: 0.000337, l3: 0.000389, l4: 0.000566, l5: 0.000938, l6: 0.001598
[epoch:  65/1000, batch:   512/ 1052, ite: 16960] train loss: 0.009053, tar: 0.000820 
l0: 0.003298, l1: 0.003610, l2: 0.004430, l3: 0.003442, l4: 0.003508, l5: 0.005247, l6: 0.007271
[epoch:  65/1000, batch:   592/ 1052, ite: 16980] train loss: 0.009071, tar: 0.000824 
l0: 0.000412, l1: 0.000409, l2: 0.000443, l3: 0.000541, l4: 0.000615, l5: 0.000909, l6: 0.002014
[epoch:  65/1000, batch:   672/ 1052, ite: 17000] train loss: 0.009065, tar: 0.000823 
l0: 0.000664, l1: 0.000605, l2: 0.000713, l3: 0.000841, l4: 0.001309, l5: 0.002022, l6: 0.003282
[epoch:  65/1000, batch:   752/ 1052, ite: 17020] train loss: 0.009061, tar: 0.000823 
l0: 0.000176, l1: 0.000184, l2: 0.000186, l3: 0.000212, l4: 0.000294, l5: 0.000547, l6: 0.000871
[epoch:  65/1000, batch:   832/ 1052, ite: 17040] train loss: 0.009061, tar: 0.000822 
l0: 0.000656, l1: 0.000656, l2: 0.000701, l3: 0.000954, l4: 0.001218, l5: 0.001472, l6: 0.002280
[epoch:  65/1000, batch:   912/ 1052, ite: 17060] train loss: 0.009057, tar: 0.000822 
l0: 0.000447, l1: 0.000428, l2: 0.000479, l3: 0.000464, l4: 0.000576, l5: 0.001219, l6: 0.001720
[epoch:  65/1000, batch:   992/ 1052, ite: 17080] train loss: 0.009053, tar: 0.000822 
[Epoch 65/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000544, l1: 0.000637, l2: 0.000543, l3: 0.000592, l4: 0.000896, l5: 0.001613, l6: 0.001856
[epoch:  66/1000, batch:    20/ 1052, ite: 17100] train loss: 0.009049, tar: 0.000821 
l0: 0.000881, l1: 0.000913, l2: 0.000936, l3: 0.000957, l4: 0.001070, l5: 0.001514, l6: 0.002458
[epoch:  66/1000, batch:   100/ 1052, ite: 17120] train loss: 0.009043, tar: 0.000821 
l0: 0.000460, l1: 0.000471, l2: 0.000545, l3: 0.000670, l4: 0.000777, l5: 0.001169, l6: 0.002262
[epoch:  66/1000, batch:   180/ 1052, ite: 17140] train loss: 0.009040, tar: 0.000820 
l0: 0.000459, l1: 0.000492, l2: 0.000474, l3: 0.000467, l4: 0.000593, l5: 0.000982, l6: 0.001803
[epoch:  66/1000, batch:   260/ 1052, ite: 17160] train loss: 0.009037, tar: 0.000820 
l0: 0.000999, l1: 0.001090, l2: 0.001140, l3: 0.001265, l4: 0.001187, l5: 0.002286, l6: 0.003670
[epoch:  66/1000, batch:   340/ 1052, ite: 17180] train loss: 0.009037, tar: 0.000820 
l0: 0.000885, l1: 0.000928, l2: 0.000886, l3: 0.000820, l4: 0.001233, l5: 0.002160, l6: 0.004020
[epoch:  66/1000, batch:   420/ 1052, ite: 17200] train loss: 0.009039, tar: 0.000820 
l0: 0.000620, l1: 0.000583, l2: 0.000633, l3: 0.000768, l4: 0.001225, l5: 0.001817, l6: 0.002344
[epoch:  66/1000, batch:   500/ 1052, ite: 17220] train loss: 0.009040, tar: 0.000820 
l0: 0.001199, l1: 0.001224, l2: 0.001057, l3: 0.001261, l4: 0.001641, l5: 0.002549, l6: 0.003535
[epoch:  66/1000, batch:   580/ 1052, ite: 17240] train loss: 0.009036, tar: 0.000820 
l0: 0.000329, l1: 0.000363, l2: 0.000377, l3: 0.000496, l4: 0.000747, l5: 0.001037, l6: 0.001631
[epoch:  66/1000, batch:   660/ 1052, ite: 17260] train loss: 0.009033, tar: 0.000819 
l0: 0.000352, l1: 0.000338, l2: 0.000362, l3: 0.000435, l4: 0.000594, l5: 0.000721, l6: 0.001669
[epoch:  66/1000, batch:   740/ 1052, ite: 17280] train loss: 0.009039, tar: 0.000821 
l0: 0.000444, l1: 0.000512, l2: 0.000530, l3: 0.000522, l4: 0.000704, l5: 0.001075, l6: 0.002033
[epoch:  66/1000, batch:   820/ 1052, ite: 17300] train loss: 0.009040, tar: 0.000821 
l0: 0.000731, l1: 0.000820, l2: 0.000801, l3: 0.000846, l4: 0.000965, l5: 0.001477, l6: 0.002898
[epoch:  66/1000, batch:   900/ 1052, ite: 17320] train loss: 0.009038, tar: 0.000821 
l0: 0.000346, l1: 0.000329, l2: 0.000416, l3: 0.000417, l4: 0.000759, l5: 0.001057, l6: 0.001707
[epoch:  66/1000, batch:   980/ 1052, ite: 17340] train loss: 0.009033, tar: 0.000820 
[Epoch 66/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.001189, l1: 0.001150, l2: 0.001537, l3: 0.001357, l4: 0.001187, l5: 0.001040, l6: 0.001910
[epoch:  67/1000, batch:     8/ 1052, ite: 17360] train loss: 0.009026, tar: 0.000819 
l0: 0.000455, l1: 0.000460, l2: 0.000466, l3: 0.000592, l4: 0.000801, l5: 0.001232, l6: 0.001658
[epoch:  67/1000, batch:    88/ 1052, ite: 17380] train loss: 0.009021, tar: 0.000819 
l0: 0.000593, l1: 0.000530, l2: 0.000630, l3: 0.000668, l4: 0.001254, l5: 0.001801, l6: 0.003717
[epoch:  67/1000, batch:   168/ 1052, ite: 17400] train loss: 0.009017, tar: 0.000818 
l0: 0.000968, l1: 0.001095, l2: 0.001078, l3: 0.001723, l4: 0.002596, l5: 0.003231, l6: 0.003621
[epoch:  67/1000, batch:   248/ 1052, ite: 17420] train loss: 0.009028, tar: 0.000819 
l0: 0.000488, l1: 0.000431, l2: 0.000613, l3: 0.000719, l4: 0.000858, l5: 0.001216, l6: 0.001936
[epoch:  67/1000, batch:   328/ 1052, ite: 17440] train loss: 0.009025, tar: 0.000819 
l0: 0.001221, l1: 0.001416, l2: 0.001280, l3: 0.001196, l4: 0.001646, l5: 0.002160, l6: 0.003434
[epoch:  67/1000, batch:   408/ 1052, ite: 17460] train loss: 0.009028, tar: 0.000819 
l0: 0.000394, l1: 0.000412, l2: 0.000412, l3: 0.000474, l4: 0.000659, l5: 0.001025, l6: 0.001490
[epoch:  67/1000, batch:   488/ 1052, ite: 17480] train loss: 0.009025, tar: 0.000819 
l0: 0.000706, l1: 0.000775, l2: 0.000852, l3: 0.001002, l4: 0.000946, l5: 0.001473, l6: 0.001419
[epoch:  67/1000, batch:   568/ 1052, ite: 17500] train loss: 0.009021, tar: 0.000819 
l0: 0.000688, l1: 0.000721, l2: 0.000766, l3: 0.000804, l4: 0.000975, l5: 0.001463, l6: 0.002689
[epoch:  67/1000, batch:   648/ 1052, ite: 17520] train loss: 0.009015, tar: 0.000818 
l0: 0.000547, l1: 0.000540, l2: 0.000631, l3: 0.000685, l4: 0.000981, l5: 0.001823, l6: 0.002167
[epoch:  67/1000, batch:   728/ 1052, ite: 17540] train loss: 0.009018, tar: 0.000818 
l0: 0.000417, l1: 0.000477, l2: 0.000427, l3: 0.000420, l4: 0.000563, l5: 0.000972, l6: 0.002088
[epoch:  67/1000, batch:   808/ 1052, ite: 17560] train loss: 0.009014, tar: 0.000818 
l0: 0.001991, l1: 0.002268, l2: 0.002334, l3: 0.002292, l4: 0.002478, l5: 0.003775, l6: 0.005124
[epoch:  67/1000, batch:   888/ 1052, ite: 17580] train loss: 0.009011, tar: 0.000817 
l0: 0.001116, l1: 0.001233, l2: 0.001336, l3: 0.001377, l4: 0.001450, l5: 0.002166, l6: 0.001693
[epoch:  67/1000, batch:   968/ 1052, ite: 17600] train loss: 0.009009, tar: 0.000817 
l0: 0.000390, l1: 0.000394, l2: 0.000488, l3: 0.000528, l4: 0.000689, l5: 0.001065, l6: 0.001870
[epoch:  67/1000, batch:  1048/ 1052, ite: 17620] train loss: 0.009015, tar: 0.000818 
[Epoch 67/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000503, l1: 0.000549, l2: 0.000523, l3: 0.000516, l4: 0.000736, l5: 0.001327, l6: 0.001455
[epoch:  68/1000, batch:    76/ 1052, ite: 17640] train loss: 0.009011, tar: 0.000817 
l0: 0.001041, l1: 0.001097, l2: 0.001187, l3: 0.001050, l4: 0.001173, l5: 0.001494, l6: 0.002052
[epoch:  68/1000, batch:   156/ 1052, ite: 17660] train loss: 0.009007, tar: 0.000817 
l0: 0.000530, l1: 0.000524, l2: 0.000515, l3: 0.000556, l4: 0.000805, l5: 0.001519, l6: 0.002580
[epoch:  68/1000, batch:   236/ 1052, ite: 17680] train loss: 0.008999, tar: 0.000816 
l0: 0.000696, l1: 0.000654, l2: 0.000769, l3: 0.001018, l4: 0.000950, l5: 0.001508, l6: 0.002444
[epoch:  68/1000, batch:   316/ 1052, ite: 17700] train loss: 0.009005, tar: 0.000816 
l0: 0.000571, l1: 0.000581, l2: 0.000580, l3: 0.000645, l4: 0.000827, l5: 0.001195, l6: 0.002559
[epoch:  68/1000, batch:   396/ 1052, ite: 17720] train loss: 0.009004, tar: 0.000816 
l0: 0.001617, l1: 0.001370, l2: 0.001853, l3: 0.003774, l4: 0.003946, l5: 0.003042, l6: 0.004110
[epoch:  68/1000, batch:   476/ 1052, ite: 17740] train loss: 0.009005, tar: 0.000816 
l0: 0.000669, l1: 0.000631, l2: 0.000838, l3: 0.000857, l4: 0.001041, l5: 0.001606, l6: 0.003210
[epoch:  68/1000, batch:   556/ 1052, ite: 17760] train loss: 0.009004, tar: 0.000816 
l0: 0.000314, l1: 0.000339, l2: 0.000304, l3: 0.000347, l4: 0.000521, l5: 0.000685, l6: 0.001239
[epoch:  68/1000, batch:   636/ 1052, ite: 17780] train loss: 0.009001, tar: 0.000815 
l0: 0.001259, l1: 0.001309, l2: 0.001377, l3: 0.001335, l4: 0.002062, l5: 0.003630, l6: 0.004458
[epoch:  68/1000, batch:   716/ 1052, ite: 17800] train loss: 0.008999, tar: 0.000815 
l0: 0.000340, l1: 0.000332, l2: 0.000354, l3: 0.000480, l4: 0.000668, l5: 0.001172, l6: 0.001757
[epoch:  68/1000, batch:   796/ 1052, ite: 17820] train loss: 0.008994, tar: 0.000814 
l0: 0.001025, l1: 0.001069, l2: 0.001033, l3: 0.001071, l4: 0.001133, l5: 0.001875, l6: 0.002775
[epoch:  68/1000, batch:   876/ 1052, ite: 17840] train loss: 0.008986, tar: 0.000814 
l0: 0.000339, l1: 0.000348, l2: 0.000307, l3: 0.000588, l4: 0.000831, l5: 0.000862, l6: 0.001054
[epoch:  68/1000, batch:   956/ 1052, ite: 17860] train loss: 0.008984, tar: 0.000813 
l0: 0.000813, l1: 0.000884, l2: 0.000951, l3: 0.000788, l4: 0.000892, l5: 0.000938, l6: 0.001619
[epoch:  68/1000, batch:  1036/ 1052, ite: 17880] train loss: 0.008980, tar: 0.000813 
[Epoch 68/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.001112, l1: 0.001049, l2: 0.001108, l3: 0.001456, l4: 0.001845, l5: 0.002403, l6: 0.004478
[epoch:  69/1000, batch:    64/ 1052, ite: 17900] train loss: 0.008979, tar: 0.000813 
l0: 0.000349, l1: 0.000329, l2: 0.000370, l3: 0.000451, l4: 0.000643, l5: 0.001065, l6: 0.001914
[epoch:  69/1000, batch:   144/ 1052, ite: 17920] train loss: 0.008973, tar: 0.000812 
l0: 0.000310, l1: 0.000305, l2: 0.000334, l3: 0.000438, l4: 0.000535, l5: 0.000807, l6: 0.001121
[epoch:  69/1000, batch:   224/ 1052, ite: 17940] train loss: 0.008968, tar: 0.000812 
l0: 0.000375, l1: 0.000367, l2: 0.000406, l3: 0.000443, l4: 0.000700, l5: 0.000778, l6: 0.001907
[epoch:  69/1000, batch:   304/ 1052, ite: 17960] train loss: 0.008963, tar: 0.000811 
l0: 0.000523, l1: 0.000494, l2: 0.000542, l3: 0.000624, l4: 0.001087, l5: 0.002230, l6: 0.003050
[epoch:  69/1000, batch:   384/ 1052, ite: 17980] train loss: 0.008959, tar: 0.000811 
l0: 0.000240, l1: 0.000247, l2: 0.000251, l3: 0.000358, l4: 0.000425, l5: 0.000770, l6: 0.000938
[epoch:  69/1000, batch:   464/ 1052, ite: 18000] train loss: 0.008957, tar: 0.000811 
l0: 0.000715, l1: 0.000652, l2: 0.000809, l3: 0.000923, l4: 0.001269, l5: 0.002148, l6: 0.003320
[epoch:  69/1000, batch:   544/ 1052, ite: 18020] train loss: 0.008952, tar: 0.000810 
l0: 0.000803, l1: 0.000804, l2: 0.000854, l3: 0.000998, l4: 0.001264, l5: 0.002110, l6: 0.003447
[epoch:  69/1000, batch:   624/ 1052, ite: 18040] train loss: 0.008950, tar: 0.000810 
l0: 0.005584, l1: 0.005783, l2: 0.006231, l3: 0.008318, l4: 0.006990, l5: 0.004028, l6: 0.005371
[epoch:  69/1000, batch:   704/ 1052, ite: 18060] train loss: 0.008949, tar: 0.000810 
l0: 0.000338, l1: 0.000323, l2: 0.000384, l3: 0.000449, l4: 0.000686, l5: 0.001148, l6: 0.002122
[epoch:  69/1000, batch:   784/ 1052, ite: 18080] train loss: 0.008944, tar: 0.000809 
l0: 0.000539, l1: 0.000590, l2: 0.000558, l3: 0.000669, l4: 0.000880, l5: 0.001441, l6: 0.001941
[epoch:  69/1000, batch:   864/ 1052, ite: 18100] train loss: 0.008944, tar: 0.000809 
l0: 0.000503, l1: 0.000524, l2: 0.000489, l3: 0.000524, l4: 0.000680, l5: 0.001142, l6: 0.001781
[epoch:  69/1000, batch:   944/ 1052, ite: 18120] train loss: 0.008940, tar: 0.000809 
l0: 0.000403, l1: 0.000378, l2: 0.000483, l3: 0.000503, l4: 0.000545, l5: 0.000879, l6: 0.001851
[epoch:  69/1000, batch:  1024/ 1052, ite: 18140] train loss: 0.008934, tar: 0.000808 
[Epoch 69/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000832, l1: 0.000838, l2: 0.001238, l3: 0.000819, l4: 0.000988, l5: 0.001372, l6: 0.002553
[epoch:  70/1000, batch:    52/ 1052, ite: 18160] train loss: 0.008931, tar: 0.000808 
l0: 0.000778, l1: 0.000761, l2: 0.000837, l3: 0.000937, l4: 0.001340, l5: 0.002781, l6: 0.003201
[epoch:  70/1000, batch:   132/ 1052, ite: 18180] train loss: 0.008928, tar: 0.000807 
l0: 0.000920, l1: 0.000966, l2: 0.000918, l3: 0.001017, l4: 0.001152, l5: 0.001664, l6: 0.002805
[epoch:  70/1000, batch:   212/ 1052, ite: 18200] train loss: 0.008924, tar: 0.000807 
l0: 0.000489, l1: 0.000540, l2: 0.000493, l3: 0.000459, l4: 0.000735, l5: 0.001081, l6: 0.002199
[epoch:  70/1000, batch:   292/ 1052, ite: 18220] train loss: 0.008920, tar: 0.000806 
l0: 0.000511, l1: 0.000492, l2: 0.000514, l3: 0.000593, l4: 0.000747, l5: 0.001694, l6: 0.001938
[epoch:  70/1000, batch:   372/ 1052, ite: 18240] train loss: 0.008916, tar: 0.000806 
l0: 0.000816, l1: 0.000863, l2: 0.000758, l3: 0.000890, l4: 0.000899, l5: 0.001655, l6: 0.002294
[epoch:  70/1000, batch:   452/ 1052, ite: 18260] train loss: 0.008913, tar: 0.000806 
l0: 0.000378, l1: 0.000370, l2: 0.000424, l3: 0.000441, l4: 0.000609, l5: 0.001119, l6: 0.001788
[epoch:  70/1000, batch:   532/ 1052, ite: 18280] train loss: 0.008910, tar: 0.000805 
l0: 0.006757, l1: 0.006888, l2: 0.008408, l3: 0.007911, l4: 0.008210, l5: 0.005498, l6: 0.005115
[epoch:  70/1000, batch:   612/ 1052, ite: 18300] train loss: 0.008916, tar: 0.000806 
l0: 0.001590, l1: 0.001738, l2: 0.001740, l3: 0.001863, l4: 0.002027, l5: 0.002308, l6: 0.003699
[epoch:  70/1000, batch:   692/ 1052, ite: 18320] train loss: 0.008922, tar: 0.000807 
l0: 0.000527, l1: 0.000510, l2: 0.000517, l3: 0.000737, l4: 0.001475, l5: 0.001761, l6: 0.002502
[epoch:  70/1000, batch:   772/ 1052, ite: 18340] train loss: 0.008920, tar: 0.000806 
l0: 0.000427, l1: 0.000431, l2: 0.000510, l3: 0.000600, l4: 0.000680, l5: 0.000833, l6: 0.001029
[epoch:  70/1000, batch:   852/ 1052, ite: 18360] train loss: 0.008917, tar: 0.000806 
l0: 0.000416, l1: 0.000422, l2: 0.000464, l3: 0.000470, l4: 0.000677, l5: 0.000984, l6: 0.001451
[epoch:  70/1000, batch:   932/ 1052, ite: 18380] train loss: 0.008918, tar: 0.000806 
l0: 0.000451, l1: 0.000404, l2: 0.000351, l3: 0.000817, l4: 0.001084, l5: 0.000656, l6: 0.001140
[epoch:  70/1000, batch:  1012/ 1052, ite: 18400] train loss: 0.008914, tar: 0.000806 
[Epoch 70/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.001267, l1: 0.001313, l2: 0.001338, l3: 0.001195, l4: 0.001585, l5: 0.002403, l6: 0.002634
[epoch:  71/1000, batch:    40/ 1052, ite: 18420] train loss: 0.008911, tar: 0.000805 
l0: 0.000566, l1: 0.000604, l2: 0.000558, l3: 0.000687, l4: 0.000906, l5: 0.001710, l6: 0.002973
[epoch:  71/1000, batch:   120/ 1052, ite: 18440] train loss: 0.008907, tar: 0.000805 
l0: 0.000659, l1: 0.000683, l2: 0.000756, l3: 0.000785, l4: 0.001021, l5: 0.001358, l6: 0.002413
[epoch:  71/1000, batch:   200/ 1052, ite: 18460] train loss: 0.008903, tar: 0.000804 
l0: 0.000865, l1: 0.000914, l2: 0.000964, l3: 0.000995, l4: 0.001056, l5: 0.001160, l6: 0.002234
[epoch:  71/1000, batch:   280/ 1052, ite: 18480] train loss: 0.008901, tar: 0.000804 
l0: 0.000479, l1: 0.000504, l2: 0.000519, l3: 0.000585, l4: 0.000685, l5: 0.001076, l6: 0.002010
[epoch:  71/1000, batch:   360/ 1052, ite: 18500] train loss: 0.008897, tar: 0.000804 
l0: 0.000272, l1: 0.000262, l2: 0.000275, l3: 0.000356, l4: 0.000425, l5: 0.000723, l6: 0.001176
[epoch:  71/1000, batch:   440/ 1052, ite: 18520] train loss: 0.008891, tar: 0.000803 
l0: 0.000877, l1: 0.000797, l2: 0.000982, l3: 0.001198, l4: 0.001471, l5: 0.002115, l6: 0.003376
[epoch:  71/1000, batch:   520/ 1052, ite: 18540] train loss: 0.008891, tar: 0.000803 
l0: 0.000414, l1: 0.000418, l2: 0.000466, l3: 0.000604, l4: 0.000886, l5: 0.001086, l6: 0.001872
[epoch:  71/1000, batch:   600/ 1052, ite: 18560] train loss: 0.008891, tar: 0.000803 
l0: 0.000380, l1: 0.000385, l2: 0.000439, l3: 0.000546, l4: 0.000571, l5: 0.000993, l6: 0.001549
[epoch:  71/1000, batch:   680/ 1052, ite: 18580] train loss: 0.008891, tar: 0.000804 
l0: 0.000801, l1: 0.000818, l2: 0.000962, l3: 0.000576, l4: 0.000816, l5: 0.001065, l6: 0.001637
[epoch:  71/1000, batch:   760/ 1052, ite: 18600] train loss: 0.008890, tar: 0.000803 
l0: 0.001803, l1: 0.001920, l2: 0.001908, l3: 0.001912, l4: 0.001781, l5: 0.003432, l6: 0.005992
[epoch:  71/1000, batch:   840/ 1052, ite: 18620] train loss: 0.008887, tar: 0.000803 
l0: 0.000519, l1: 0.000590, l2: 0.000646, l3: 0.000704, l4: 0.000979, l5: 0.001229, l6: 0.002457
[epoch:  71/1000, batch:   920/ 1052, ite: 18640] train loss: 0.008900, tar: 0.000805 
l0: 0.000520, l1: 0.000591, l2: 0.000487, l3: 0.000641, l4: 0.000750, l5: 0.001425, l6: 0.002663
[epoch:  71/1000, batch:  1000/ 1052, ite: 18660] train loss: 0.008899, tar: 0.000805 
[Epoch 71/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000558, l1: 0.000572, l2: 0.000584, l3: 0.000612, l4: 0.000935, l5: 0.001562, l6: 0.001829
[epoch:  72/1000, batch:    28/ 1052, ite: 18680] train loss: 0.008898, tar: 0.000805 
l0: 0.000478, l1: 0.000485, l2: 0.000486, l3: 0.000564, l4: 0.000645, l5: 0.001056, l6: 0.001280
[epoch:  72/1000, batch:   108/ 1052, ite: 18700] train loss: 0.008897, tar: 0.000804 
l0: 0.001415, l1: 0.001607, l2: 0.001413, l3: 0.001392, l4: 0.001589, l5: 0.001839, l6: 0.002814
[epoch:  72/1000, batch:   188/ 1052, ite: 18720] train loss: 0.008891, tar: 0.000804 
l0: 0.000664, l1: 0.000618, l2: 0.000642, l3: 0.000674, l4: 0.001054, l5: 0.001904, l6: 0.002409
[epoch:  72/1000, batch:   268/ 1052, ite: 18740] train loss: 0.008887, tar: 0.000803 
l0: 0.001579, l1: 0.001652, l2: 0.001941, l3: 0.001730, l4: 0.002091, l5: 0.002228, l6: 0.003389
[epoch:  72/1000, batch:   348/ 1052, ite: 18760] train loss: 0.008889, tar: 0.000804 
l0: 0.000848, l1: 0.000874, l2: 0.000867, l3: 0.000988, l4: 0.001444, l5: 0.002229, l6: 0.002666
[epoch:  72/1000, batch:   428/ 1052, ite: 18780] train loss: 0.008891, tar: 0.000804 
l0: 0.000360, l1: 0.000341, l2: 0.000355, l3: 0.000513, l4: 0.000707, l5: 0.001155, l6: 0.001376
[epoch:  72/1000, batch:   508/ 1052, ite: 18800] train loss: 0.008889, tar: 0.000803 
l0: 0.000393, l1: 0.000430, l2: 0.000407, l3: 0.000455, l4: 0.000616, l5: 0.001208, l6: 0.001186
[epoch:  72/1000, batch:   588/ 1052, ite: 18820] train loss: 0.008888, tar: 0.000803 
l0: 0.000593, l1: 0.000575, l2: 0.000612, l3: 0.000750, l4: 0.001096, l5: 0.001939, l6: 0.002563
[epoch:  72/1000, batch:   668/ 1052, ite: 18840] train loss: 0.008885, tar: 0.000803 
l0: 0.000703, l1: 0.000673, l2: 0.000720, l3: 0.000794, l4: 0.001114, l5: 0.002011, l6: 0.003342
[epoch:  72/1000, batch:   748/ 1052, ite: 18860] train loss: 0.008878, tar: 0.000802 
l0: 0.000330, l1: 0.000314, l2: 0.000347, l3: 0.000380, l4: 0.000598, l5: 0.000829, l6: 0.001461
[epoch:  72/1000, batch:   828/ 1052, ite: 18880] train loss: 0.008873, tar: 0.000802 
l0: 0.000190, l1: 0.000232, l2: 0.000224, l3: 0.000276, l4: 0.000393, l5: 0.000414, l6: 0.000797
[epoch:  72/1000, batch:   908/ 1052, ite: 18900] train loss: 0.008871, tar: 0.000801 
l0: 0.001103, l1: 0.001057, l2: 0.001302, l3: 0.001180, l4: 0.001787, l5: 0.002597, l6: 0.003677
[epoch:  72/1000, batch:   988/ 1052, ite: 18920] train loss: 0.008867, tar: 0.000801 
[Epoch 72/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000923, l1: 0.000803, l2: 0.000927, l3: 0.001264, l4: 0.001791, l5: 0.002153, l6: 0.003116
[epoch:  73/1000, batch:    16/ 1052, ite: 18940] train loss: 0.008870, tar: 0.000801 
l0: 0.000575, l1: 0.000616, l2: 0.000672, l3: 0.000646, l4: 0.000798, l5: 0.000969, l6: 0.001711
[epoch:  73/1000, batch:    96/ 1052, ite: 18960] train loss: 0.008871, tar: 0.000801 
l0: 0.000509, l1: 0.000511, l2: 0.000568, l3: 0.000615, l4: 0.000862, l5: 0.001596, l6: 0.001541
[epoch:  73/1000, batch:   176/ 1052, ite: 18980] train loss: 0.008872, tar: 0.000801 
l0: 0.000521, l1: 0.000514, l2: 0.000656, l3: 0.000598, l4: 0.000731, l5: 0.001341, l6: 0.002240
[epoch:  73/1000, batch:   256/ 1052, ite: 19000] train loss: 0.008872, tar: 0.000802 
l0: 0.000579, l1: 0.000571, l2: 0.000542, l3: 0.000638, l4: 0.000863, l5: 0.001382, l6: 0.002377
[epoch:  73/1000, batch:   336/ 1052, ite: 19020] train loss: 0.008865, tar: 0.000801 
l0: 0.000906, l1: 0.000850, l2: 0.001049, l3: 0.001057, l4: 0.001031, l5: 0.002060, l6: 0.003107
[epoch:  73/1000, batch:   416/ 1052, ite: 19040] train loss: 0.008863, tar: 0.000801 
l0: 0.001443, l1: 0.001441, l2: 0.001561, l3: 0.001631, l4: 0.001964, l5: 0.002611, l6: 0.004483
[epoch:  73/1000, batch:   496/ 1052, ite: 19060] train loss: 0.008862, tar: 0.000800 
l0: 0.000752, l1: 0.000743, l2: 0.000734, l3: 0.000865, l4: 0.001286, l5: 0.001870, l6: 0.002915
[epoch:  73/1000, batch:   576/ 1052, ite: 19080] train loss: 0.008861, tar: 0.000800 
l0: 0.000418, l1: 0.000412, l2: 0.000413, l3: 0.000454, l4: 0.000573, l5: 0.001249, l6: 0.001840
[epoch:  73/1000, batch:   656/ 1052, ite: 19100] train loss: 0.008863, tar: 0.000800 
l0: 0.000394, l1: 0.000367, l2: 0.000480, l3: 0.000388, l4: 0.000523, l5: 0.000863, l6: 0.001241
[epoch:  73/1000, batch:   736/ 1052, ite: 19120] train loss: 0.008861, tar: 0.000800 
l0: 0.000427, l1: 0.000408, l2: 0.000416, l3: 0.000459, l4: 0.000631, l5: 0.001096, l6: 0.002619
[epoch:  73/1000, batch:   816/ 1052, ite: 19140] train loss: 0.008859, tar: 0.000800 
l0: 0.000504, l1: 0.000491, l2: 0.000520, l3: 0.000704, l4: 0.000772, l5: 0.001051, l6: 0.001969
[epoch:  73/1000, batch:   896/ 1052, ite: 19160] train loss: 0.008858, tar: 0.000800 
l0: 0.001405, l1: 0.001623, l2: 0.001485, l3: 0.001639, l4: 0.001975, l5: 0.003529, l6: 0.004782
[epoch:  73/1000, batch:   976/ 1052, ite: 19180] train loss: 0.008857, tar: 0.000799 
[Epoch 73/1000] Test loss: 0.007, Test target loss: 0.001
l0: 0.000711, l1: 0.000688, l2: 0.000716, l3: 0.000791, l4: 0.001182, l5: 0.001855, l6: 0.003701
[epoch:  74/1000, batch:     4/ 1052, ite: 19200] train loss: 0.008854, tar: 0.000799 
l0: 0.000476, l1: 0.000454, l2: 0.000541, l3: 0.000640, l4: 0.000833, l5: 0.001359, l6: 0.002614
[epoch:  74/1000, batch:    84/ 1052, ite: 19220] train loss: 0.008850, tar: 0.000799 
l0: 0.000422, l1: 0.000435, l2: 0.000425, l3: 0.000457, l4: 0.000620, l5: 0.000943, l6: 0.002025
[epoch:  74/1000, batch:   164/ 1052, ite: 19240] train loss: 0.008846, tar: 0.000798 
l0: 0.000579, l1: 0.000601, l2: 0.000629, l3: 0.000653, l4: 0.001019, l5: 0.001720, l6: 0.002284
[epoch:  74/1000, batch:   244/ 1052, ite: 19260] train loss: 0.008841, tar: 0.000798 
l0: 0.000549, l1: 0.000563, l2: 0.000551, l3: 0.000593, l4: 0.000909, l5: 0.001241, l6: 0.001941
[epoch:  74/1000, batch:   324/ 1052, ite: 19280] train loss: 0.008838, tar: 0.000797 
l0: 0.000450, l1: 0.000455, l2: 0.000447, l3: 0.000630, l4: 0.000689, l5: 0.001176, l6: 0.001433
[epoch:  74/1000, batch:   404/ 1052, ite: 19300] train loss: 0.008834, tar: 0.000797 
l0: 0.000855, l1: 0.000831, l2: 0.000803, l3: 0.000759, l4: 0.000906, l5: 0.001711, l6: 0.002234
[epoch:  74/1000, batch:   484/ 1052, ite: 19320] train loss: 0.008835, tar: 0.000797 
l0: 0.000548, l1: 0.000576, l2: 0.000597, l3: 0.000704, l4: 0.000683, l5: 0.001521, l6: 0.001945
[epoch:  74/1000, batch:   564/ 1052, ite: 19340] train loss: 0.008833, tar: 0.000797 
l0: 0.000462, l1: 0.000454, l2: 0.000511, l3: 0.000646, l4: 0.000925, l5: 0.001599, l6: 0.002289
[epoch:  74/1000, batch:   644/ 1052, ite: 19360] train loss: 0.008830, tar: 0.000796 
l0: 0.000613, l1: 0.000612, l2: 0.000681, l3: 0.000682, l4: 0.000944, l5: 0.001402, l6: 0.002514
[epoch:  74/1000, batch:   724/ 1052, ite: 19380] train loss: 0.008829, tar: 0.000796 
l0: 0.000571, l1: 0.000565, l2: 0.000577, l3: 0.000668, l4: 0.001060, l5: 0.001920, l6: 0.001910
[epoch:  74/1000, batch:   804/ 1052, ite: 19400] train loss: 0.008827, tar: 0.000796 
l0: 0.001037, l1: 0.001055, l2: 0.000967, l3: 0.001206, l4: 0.001389, l5: 0.002420, l6: 0.003891
[epoch:  74/1000, batch:   884/ 1052, ite: 19420] train loss: 0.008826, tar: 0.000796 
l0: 0.000471, l1: 0.000467, l2: 0.000531, l3: 0.000688, l4: 0.000879, l5: 0.001193, l6: 0.002371
[epoch:  74/1000, batch:   964/ 1052, ite: 19440] train loss: 0.008822, tar: 0.000795 
l0: 0.000312, l1: 0.000291, l2: 0.000330, l3: 0.000535, l4: 0.000548, l5: 0.001075, l6: 0.001430
[epoch:  74/1000, batch:  1044/ 1052, ite: 19460] train loss: 0.008822, tar: 0.000795 
[Epoch 74/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000341, l1: 0.000321, l2: 0.000345, l3: 0.000436, l4: 0.000596, l5: 0.001158, l6: 0.001855
[epoch:  75/1000, batch:    72/ 1052, ite: 19480] train loss: 0.008817, tar: 0.000795 
l0: 0.000229, l1: 0.000220, l2: 0.000249, l3: 0.000462, l4: 0.000627, l5: 0.001066, l6: 0.001329
[epoch:  75/1000, batch:   152/ 1052, ite: 19500] train loss: 0.008816, tar: 0.000795 
l0: 0.000772, l1: 0.000774, l2: 0.000804, l3: 0.000880, l4: 0.000929, l5: 0.001627, l6: 0.002985
[epoch:  75/1000, batch:   232/ 1052, ite: 19520] train loss: 0.008812, tar: 0.000794 
l0: 0.000615, l1: 0.000691, l2: 0.000736, l3: 0.000611, l4: 0.000947, l5: 0.001211, l6: 0.001915
[epoch:  75/1000, batch:   312/ 1052, ite: 19540] train loss: 0.008811, tar: 0.000794 
l0: 0.000853, l1: 0.000818, l2: 0.000770, l3: 0.000875, l4: 0.001122, l5: 0.001583, l6: 0.003233
[epoch:  75/1000, batch:   392/ 1052, ite: 19560] train loss: 0.008808, tar: 0.000794 
l0: 0.000567, l1: 0.000619, l2: 0.000575, l3: 0.000695, l4: 0.000875, l5: 0.001361, l6: 0.001945
[epoch:  75/1000, batch:   472/ 1052, ite: 19580] train loss: 0.008807, tar: 0.000793 
l0: 0.001022, l1: 0.001016, l2: 0.001052, l3: 0.001109, l4: 0.001400, l5: 0.001831, l6: 0.004047
[epoch:  75/1000, batch:   552/ 1052, ite: 19600] train loss: 0.008804, tar: 0.000793 
l0: 0.000615, l1: 0.000635, l2: 0.000591, l3: 0.000719, l4: 0.001004, l5: 0.001529, l6: 0.002859
[epoch:  75/1000, batch:   632/ 1052, ite: 19620] train loss: 0.008800, tar: 0.000793 
l0: 0.000424, l1: 0.000439, l2: 0.000446, l3: 0.000444, l4: 0.000588, l5: 0.001123, l6: 0.001513
[epoch:  75/1000, batch:   712/ 1052, ite: 19640] train loss: 0.008798, tar: 0.000793 
l0: 0.000706, l1: 0.000670, l2: 0.000716, l3: 0.000913, l4: 0.001377, l5: 0.002053, l6: 0.003646
[epoch:  75/1000, batch:   792/ 1052, ite: 19660] train loss: 0.008794, tar: 0.000792 
l0: 0.000545, l1: 0.000512, l2: 0.000539, l3: 0.000693, l4: 0.000859, l5: 0.001299, l6: 0.002614
[epoch:  75/1000, batch:   872/ 1052, ite: 19680] train loss: 0.008795, tar: 0.000792 
l0: 0.000745, l1: 0.000750, l2: 0.000793, l3: 0.000839, l4: 0.001056, l5: 0.001892, l6: 0.002969
[epoch:  75/1000, batch:   952/ 1052, ite: 19700] train loss: 0.008791, tar: 0.000792 
l0: 0.000540, l1: 0.000621, l2: 0.000505, l3: 0.000705, l4: 0.000544, l5: 0.000497, l6: 0.000912
[epoch:  75/1000, batch:  1032/ 1052, ite: 19720] train loss: 0.008787, tar: 0.000791 
[Epoch 75/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000563, l1: 0.000555, l2: 0.000580, l3: 0.000597, l4: 0.000813, l5: 0.001384, l6: 0.001948
[epoch:  76/1000, batch:    60/ 1052, ite: 19740] train loss: 0.008785, tar: 0.000791 
l0: 0.001031, l1: 0.001082, l2: 0.001114, l3: 0.001143, l4: 0.001254, l5: 0.002053, l6: 0.003373
[epoch:  76/1000, batch:   140/ 1052, ite: 19760] train loss: 0.008782, tar: 0.000791 
l0: 0.000362, l1: 0.000349, l2: 0.000381, l3: 0.000461, l4: 0.000664, l5: 0.001125, l6: 0.001629
[epoch:  76/1000, batch:   220/ 1052, ite: 19780] train loss: 0.008779, tar: 0.000790 
l0: 0.000287, l1: 0.000293, l2: 0.000287, l3: 0.000322, l4: 0.000492, l5: 0.000821, l6: 0.001361
[epoch:  76/1000, batch:   300/ 1052, ite: 19800] train loss: 0.008776, tar: 0.000790 
l0: 0.000764, l1: 0.000735, l2: 0.000781, l3: 0.000989, l4: 0.001122, l5: 0.001547, l6: 0.002749
[epoch:  76/1000, batch:   380/ 1052, ite: 19820] train loss: 0.008773, tar: 0.000790 
l0: 0.001862, l1: 0.001973, l2: 0.002016, l3: 0.001980, l4: 0.002760, l5: 0.003703, l6: 0.005071
[epoch:  76/1000, batch:   460/ 1052, ite: 19840] train loss: 0.008773, tar: 0.000790 
l0: 0.000332, l1: 0.000378, l2: 0.000255, l3: 0.000654, l4: 0.000699, l5: 0.000681, l6: 0.000783
[epoch:  76/1000, batch:   540/ 1052, ite: 19860] train loss: 0.008778, tar: 0.000790 
l0: 0.000308, l1: 0.000352, l2: 0.000383, l3: 0.000391, l4: 0.000539, l5: 0.001009, l6: 0.001481
[epoch:  76/1000, batch:   620/ 1052, ite: 19880] train loss: 0.008775, tar: 0.000790 
l0: 0.000573, l1: 0.000585, l2: 0.000635, l3: 0.000726, l4: 0.000804, l5: 0.001616, l6: 0.003023
[epoch:  76/1000, batch:   700/ 1052, ite: 19900] train loss: 0.008772, tar: 0.000790 
l0: 0.000344, l1: 0.000332, l2: 0.000336, l3: 0.000390, l4: 0.000612, l5: 0.000986, l6: 0.001728
[epoch:  76/1000, batch:   780/ 1052, ite: 19920] train loss: 0.008769, tar: 0.000789 
l0: 0.000354, l1: 0.000361, l2: 0.000370, l3: 0.000438, l4: 0.000539, l5: 0.000988, l6: 0.002127
[epoch:  76/1000, batch:   860/ 1052, ite: 19940] train loss: 0.008764, tar: 0.000789 
l0: 0.000583, l1: 0.000601, l2: 0.000709, l3: 0.000627, l4: 0.000872, l5: 0.001445, l6: 0.002061
[epoch:  76/1000, batch:   940/ 1052, ite: 19960] train loss: 0.008762, tar: 0.000789 
l0: 0.000245, l1: 0.000242, l2: 0.000223, l3: 0.000351, l4: 0.000463, l5: 0.000667, l6: 0.001122
[epoch:  76/1000, batch:  1020/ 1052, ite: 19980] train loss: 0.008759, tar: 0.000788 
[Epoch 76/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000267, l1: 0.000256, l2: 0.000279, l3: 0.000322, l4: 0.000449, l5: 0.000901, l6: 0.001315
[epoch:  77/1000, batch:    48/ 1052, ite: 20000] train loss: 0.008755, tar: 0.000788 
l0: 0.000356, l1: 0.000342, l2: 0.000580, l3: 0.000384, l4: 0.000585, l5: 0.000834, l6: 0.001613
[epoch:  77/1000, batch:   128/ 1052, ite: 20020] train loss: 0.008758, tar: 0.000788 
l0: 0.000482, l1: 0.000488, l2: 0.000507, l3: 0.000596, l4: 0.000921, l5: 0.001511, l6: 0.002258
[epoch:  77/1000, batch:   208/ 1052, ite: 20040] train loss: 0.008756, tar: 0.000788 
l0: 0.000734, l1: 0.000844, l2: 0.000923, l3: 0.000798, l4: 0.000805, l5: 0.000832, l6: 0.001162
[epoch:  77/1000, batch:   288/ 1052, ite: 20060] train loss: 0.008754, tar: 0.000788 
l0: 0.000475, l1: 0.000490, l2: 0.000520, l3: 0.000534, l4: 0.000882, l5: 0.001110, l6: 0.001470
[epoch:  77/1000, batch:   368/ 1052, ite: 20080] train loss: 0.008753, tar: 0.000788 
l0: 0.001478, l1: 0.001565, l2: 0.001679, l3: 0.001244, l4: 0.001684, l5: 0.003145, l6: 0.005488
[epoch:  77/1000, batch:   448/ 1052, ite: 20100] train loss: 0.008751, tar: 0.000787 
l0: 0.000804, l1: 0.000796, l2: 0.000928, l3: 0.000879, l4: 0.001068, l5: 0.002005, l6: 0.002641
[epoch:  77/1000, batch:   528/ 1052, ite: 20120] train loss: 0.008746, tar: 0.000787 
l0: 0.000524, l1: 0.000600, l2: 0.000537, l3: 0.000599, l4: 0.000695, l5: 0.001036, l6: 0.001600
[epoch:  77/1000, batch:   608/ 1052, ite: 20140] train loss: 0.008746, tar: 0.000787 
l0: 0.000222, l1: 0.000226, l2: 0.000237, l3: 0.000345, l4: 0.000370, l5: 0.000589, l6: 0.001451
[epoch:  77/1000, batch:   688/ 1052, ite: 20160] train loss: 0.008742, tar: 0.000786 
l0: 0.000745, l1: 0.000795, l2: 0.000785, l3: 0.000594, l4: 0.000868, l5: 0.001360, l6: 0.001571
[epoch:  77/1000, batch:   768/ 1052, ite: 20180] train loss: 0.008742, tar: 0.000787 
l0: 0.000237, l1: 0.000242, l2: 0.000246, l3: 0.000335, l4: 0.000454, l5: 0.000927, l6: 0.002941
[epoch:  77/1000, batch:   848/ 1052, ite: 20200] train loss: 0.008741, tar: 0.000786 
l0: 0.000503, l1: 0.000475, l2: 0.000579, l3: 0.000608, l4: 0.000760, l5: 0.001156, l6: 0.001996
[epoch:  77/1000, batch:   928/ 1052, ite: 20220] train loss: 0.008738, tar: 0.000786 
l0: 0.001330, l1: 0.001325, l2: 0.001386, l3: 0.001600, l4: 0.002285, l5: 0.003023, l6: 0.004127
[epoch:  77/1000, batch:  1008/ 1052, ite: 20240] train loss: 0.008736, tar: 0.000786 
[Epoch 77/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000553, l1: 0.000575, l2: 0.000588, l3: 0.000711, l4: 0.000954, l5: 0.001594, l6: 0.002326
[epoch:  78/1000, batch:    36/ 1052, ite: 20260] train loss: 0.008735, tar: 0.000785 
l0: 0.000600, l1: 0.000630, l2: 0.000543, l3: 0.000578, l4: 0.000757, l5: 0.001263, l6: 0.002072
[epoch:  78/1000, batch:   116/ 1052, ite: 20280] train loss: 0.008731, tar: 0.000785 
l0: 0.000420, l1: 0.000400, l2: 0.000446, l3: 0.000491, l4: 0.000758, l5: 0.001540, l6: 0.002001
[epoch:  78/1000, batch:   196/ 1052, ite: 20300] train loss: 0.008728, tar: 0.000785 
l0: 0.000351, l1: 0.000347, l2: 0.000358, l3: 0.000441, l4: 0.000691, l5: 0.000831, l6: 0.001547
[epoch:  78/1000, batch:   276/ 1052, ite: 20320] train loss: 0.008726, tar: 0.000785 
l0: 0.000404, l1: 0.000418, l2: 0.000491, l3: 0.000422, l4: 0.000549, l5: 0.000817, l6: 0.001194
[epoch:  78/1000, batch:   356/ 1052, ite: 20340] train loss: 0.008724, tar: 0.000784 
l0: 0.000785, l1: 0.000788, l2: 0.000834, l3: 0.001016, l4: 0.001308, l5: 0.002109, l6: 0.003047
[epoch:  78/1000, batch:   436/ 1052, ite: 20360] train loss: 0.008724, tar: 0.000784 
l0: 0.000609, l1: 0.000579, l2: 0.000608, l3: 0.000625, l4: 0.000794, l5: 0.001605, l6: 0.002705
[epoch:  78/1000, batch:   516/ 1052, ite: 20380] train loss: 0.008722, tar: 0.000784 
l0: 0.000518, l1: 0.000514, l2: 0.000513, l3: 0.000554, l4: 0.000791, l5: 0.001553, l6: 0.002870
[epoch:  78/1000, batch:   596/ 1052, ite: 20400] train loss: 0.008718, tar: 0.000784 
l0: 0.000334, l1: 0.000326, l2: 0.000385, l3: 0.000397, l4: 0.000577, l5: 0.000753, l6: 0.001426
[epoch:  78/1000, batch:   676/ 1052, ite: 20420] train loss: 0.008715, tar: 0.000783 
l0: 0.000470, l1: 0.000472, l2: 0.000497, l3: 0.000491, l4: 0.000666, l5: 0.001408, l6: 0.002741
[epoch:  78/1000, batch:   756/ 1052, ite: 20440] train loss: 0.008710, tar: 0.000783 
l0: 0.000474, l1: 0.000478, l2: 0.000475, l3: 0.000550, l4: 0.000719, l5: 0.001127, l6: 0.001800
[epoch:  78/1000, batch:   836/ 1052, ite: 20460] train loss: 0.008707, tar: 0.000782 
l0: 0.000790, l1: 0.000798, l2: 0.000818, l3: 0.000846, l4: 0.001127, l5: 0.001786, l6: 0.003030
[epoch:  78/1000, batch:   916/ 1052, ite: 20480] train loss: 0.008703, tar: 0.000782 
l0: 0.000623, l1: 0.000613, l2: 0.000665, l3: 0.000631, l4: 0.000845, l5: 0.001106, l6: 0.001959
[epoch:  78/1000, batch:   996/ 1052, ite: 20500] train loss: 0.008700, tar: 0.000781 
[Epoch 78/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000467, l1: 0.000438, l2: 0.000476, l3: 0.000668, l4: 0.000783, l5: 0.001623, l6: 0.002598
[epoch:  79/1000, batch:    24/ 1052, ite: 20520] train loss: 0.008705, tar: 0.000782 
l0: 0.000341, l1: 0.000317, l2: 0.000335, l3: 0.000448, l4: 0.000657, l5: 0.000797, l6: 0.001519
[epoch:  79/1000, batch:   104/ 1052, ite: 20540] train loss: 0.008705, tar: 0.000782 
l0: 0.000675, l1: 0.000592, l2: 0.000683, l3: 0.000850, l4: 0.001177, l5: 0.001756, l6: 0.003127
[epoch:  79/1000, batch:   184/ 1052, ite: 20560] train loss: 0.008701, tar: 0.000782 
l0: 0.000579, l1: 0.000592, l2: 0.000568, l3: 0.000875, l4: 0.000909, l5: 0.001751, l6: 0.002289
[epoch:  79/1000, batch:   264/ 1052, ite: 20580] train loss: 0.008698, tar: 0.000781 
l0: 0.000602, l1: 0.000585, l2: 0.000669, l3: 0.000694, l4: 0.000809, l5: 0.001200, l6: 0.001905
[epoch:  79/1000, batch:   344/ 1052, ite: 20600] train loss: 0.008696, tar: 0.000781 
l0: 0.000781, l1: 0.001039, l2: 0.000819, l3: 0.000330, l4: 0.000625, l5: 0.001187, l6: 0.000903
[epoch:  79/1000, batch:   424/ 1052, ite: 20620] train loss: 0.008692, tar: 0.000781 
l0: 0.000727, l1: 0.000762, l2: 0.000695, l3: 0.000789, l4: 0.001534, l5: 0.002344, l6: 0.003004
[epoch:  79/1000, batch:   504/ 1052, ite: 20640] train loss: 0.008691, tar: 0.000781 
l0: 0.000307, l1: 0.000347, l2: 0.000339, l3: 0.000382, l4: 0.000524, l5: 0.000801, l6: 0.001823
[epoch:  79/1000, batch:   584/ 1052, ite: 20660] train loss: 0.008687, tar: 0.000780 
l0: 0.000343, l1: 0.000351, l2: 0.000356, l3: 0.000362, l4: 0.000634, l5: 0.000954, l6: 0.001564
[epoch:  79/1000, batch:   664/ 1052, ite: 20680] train loss: 0.008682, tar: 0.000780 
l0: 0.000620, l1: 0.000589, l2: 0.000673, l3: 0.000614, l4: 0.000825, l5: 0.001608, l6: 0.003131
[epoch:  79/1000, batch:   744/ 1052, ite: 20700] train loss: 0.008679, tar: 0.000779 
l0: 0.000435, l1: 0.000441, l2: 0.000435, l3: 0.000472, l4: 0.000711, l5: 0.001292, l6: 0.002722
[epoch:  79/1000, batch:   824/ 1052, ite: 20720] train loss: 0.008676, tar: 0.000779 
l0: 0.000684, l1: 0.000741, l2: 0.000672, l3: 0.000883, l4: 0.001074, l5: 0.001471, l6: 0.002937
[epoch:  79/1000, batch:   904/ 1052, ite: 20740] train loss: 0.008674, tar: 0.000779 
l0: 0.000854, l1: 0.000906, l2: 0.000868, l3: 0.000936, l4: 0.001185, l5: 0.002158, l6: 0.003955
[epoch:  79/1000, batch:   984/ 1052, ite: 20760] train loss: 0.008672, tar: 0.000778 
[Epoch 79/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000568, l1: 0.000510, l2: 0.000693, l3: 0.000902, l4: 0.000970, l5: 0.001465, l6: 0.002580
[epoch:  80/1000, batch:    12/ 1052, ite: 20780] train loss: 0.008671, tar: 0.000778 
l0: 0.000413, l1: 0.000416, l2: 0.000443, l3: 0.000593, l4: 0.000704, l5: 0.001179, l6: 0.001879
[epoch:  80/1000, batch:    92/ 1052, ite: 20800] train loss: 0.008667, tar: 0.000778 
l0: 0.000482, l1: 0.000458, l2: 0.000458, l3: 0.000570, l4: 0.000697, l5: 0.001354, l6: 0.001716
[epoch:  80/1000, batch:   172/ 1052, ite: 20820] train loss: 0.008663, tar: 0.000777 
l0: 0.000259, l1: 0.000234, l2: 0.000230, l3: 0.000286, l4: 0.000537, l5: 0.000798, l6: 0.001209
[epoch:  80/1000, batch:   252/ 1052, ite: 20840] train loss: 0.008659, tar: 0.000777 
l0: 0.000213, l1: 0.000207, l2: 0.000223, l3: 0.000261, l4: 0.000375, l5: 0.000723, l6: 0.001223
[epoch:  80/1000, batch:   332/ 1052, ite: 20860] train loss: 0.008657, tar: 0.000777 
l0: 0.000926, l1: 0.000852, l2: 0.000961, l3: 0.001067, l4: 0.001509, l5: 0.001871, l6: 0.002860
[epoch:  80/1000, batch:   412/ 1052, ite: 20880] train loss: 0.008654, tar: 0.000776 
l0: 0.000791, l1: 0.000796, l2: 0.000806, l3: 0.001043, l4: 0.001434, l5: 0.001783, l6: 0.003937
[epoch:  80/1000, batch:   492/ 1052, ite: 20900] train loss: 0.008652, tar: 0.000776 
l0: 0.000515, l1: 0.000536, l2: 0.000464, l3: 0.000639, l4: 0.000898, l5: 0.001421, l6: 0.001929
[epoch:  80/1000, batch:   572/ 1052, ite: 20920] train loss: 0.008650, tar: 0.000776 
l0: 0.000531, l1: 0.000552, l2: 0.000550, l3: 0.000671, l4: 0.000906, l5: 0.001636, l6: 0.002549
[epoch:  80/1000, batch:   652/ 1052, ite: 20940] train loss: 0.008648, tar: 0.000775 
l0: 0.000374, l1: 0.000376, l2: 0.000392, l3: 0.000475, l4: 0.000644, l5: 0.000794, l6: 0.001878
[epoch:  80/1000, batch:   732/ 1052, ite: 20960] train loss: 0.008647, tar: 0.000775 
l0: 0.000366, l1: 0.000373, l2: 0.000384, l3: 0.000419, l4: 0.000646, l5: 0.001213, l6: 0.001897
[epoch:  80/1000, batch:   812/ 1052, ite: 20980] train loss: 0.008648, tar: 0.000775 
l0: 0.000610, l1: 0.000599, l2: 0.000683, l3: 0.000765, l4: 0.001044, l5: 0.001648, l6: 0.002496
[epoch:  80/1000, batch:   892/ 1052, ite: 21000] train loss: 0.008643, tar: 0.000775 
l0: 0.000502, l1: 0.000516, l2: 0.000521, l3: 0.000553, l4: 0.000707, l5: 0.001344, l6: 0.002437
[epoch:  80/1000, batch:   972/ 1052, ite: 21020] train loss: 0.008638, tar: 0.000774 
l0: 0.000298, l1: 0.000303, l2: 0.000343, l3: 0.000411, l4: 0.000549, l5: 0.001111, l6: 0.002142
[epoch:  80/1000, batch:  1052/ 1052, ite: 21040] train loss: 0.008636, tar: 0.000774 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_21040.pkl
[Epoch 80/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000298, l1: 0.000282, l2: 0.000376, l3: 0.001011, l4: 0.000533, l5: 0.001058, l6: 0.001401
[epoch:  81/1000, batch:    80/ 1052, ite: 21060] train loss: 0.007243, tar: 0.000644 
l0: 0.000399, l1: 0.000407, l2: 0.000456, l3: 0.000481, l4: 0.000489, l5: 0.001108, l6: 0.001310
[epoch:  81/1000, batch:   160/ 1052, ite: 21080] train loss: 0.006931, tar: 0.000607 
l0: 0.000603, l1: 0.000585, l2: 0.000636, l3: 0.000798, l4: 0.000983, l5: 0.001209, l6: 0.003381
[epoch:  81/1000, batch:   240/ 1052, ite: 21100] train loss: 0.007255, tar: 0.000629 
l0: 0.001487, l1: 0.001676, l2: 0.001491, l3: 0.001449, l4: 0.001604, l5: 0.002062, l6: 0.003506
[epoch:  81/1000, batch:   320/ 1052, ite: 21120] train loss: 0.007470, tar: 0.000650 
l0: 0.000308, l1: 0.000305, l2: 0.000328, l3: 0.000352, l4: 0.000482, l5: 0.001074, l6: 0.002490
[epoch:  81/1000, batch:   400/ 1052, ite: 21140] train loss: 0.007427, tar: 0.000636 
l0: 0.000355, l1: 0.000356, l2: 0.000395, l3: 0.000388, l4: 0.000492, l5: 0.000764, l6: 0.001294
[epoch:  81/1000, batch:   480/ 1052, ite: 21160] train loss: 0.007515, tar: 0.000644 
l0: 0.000297, l1: 0.000313, l2: 0.000342, l3: 0.000346, l4: 0.000524, l5: 0.000833, l6: 0.001534
[epoch:  81/1000, batch:   560/ 1052, ite: 21180] train loss: 0.007470, tar: 0.000647 
l0: 0.000905, l1: 0.001060, l2: 0.000794, l3: 0.000950, l4: 0.001155, l5: 0.001690, l6: 0.002998
[epoch:  81/1000, batch:   640/ 1052, ite: 21200] train loss: 0.007603, tar: 0.000660 
l0: 0.000869, l1: 0.000795, l2: 0.000903, l3: 0.001071, l4: 0.001410, l5: 0.002602, l6: 0.003159
[epoch:  81/1000, batch:   720/ 1052, ite: 21220] train loss: 0.007704, tar: 0.000667 
l0: 0.000702, l1: 0.000685, l2: 0.000763, l3: 0.000868, l4: 0.000906, l5: 0.001458, l6: 0.002669
[epoch:  81/1000, batch:   800/ 1052, ite: 21240] train loss: 0.007617, tar: 0.000658 
l0: 0.000832, l1: 0.000797, l2: 0.000833, l3: 0.000969, l4: 0.001183, l5: 0.001890, l6: 0.002967
[epoch:  81/1000, batch:   880/ 1052, ite: 21260] train loss: 0.007650, tar: 0.000662 
l0: 0.000795, l1: 0.000788, l2: 0.000855, l3: 0.000873, l4: 0.001217, l5: 0.001520, l6: 0.002300
[epoch:  81/1000, batch:   960/ 1052, ite: 21280] train loss: 0.007581, tar: 0.000651 
l0: 0.000152, l1: 0.000162, l2: 0.000163, l3: 0.000208, l4: 0.000276, l5: 0.000590, l6: 0.000863
[epoch:  81/1000, batch:  1040/ 1052, ite: 21300] train loss: 0.007583, tar: 0.000653 
[Epoch 81/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000747, l1: 0.000607, l2: 0.000791, l3: 0.000878, l4: 0.000973, l5: 0.002232, l6: 0.002114
[epoch:  82/1000, batch:    68/ 1052, ite: 21320] train loss: 0.007500, tar: 0.000643 
l0: 0.003045, l1: 0.003993, l2: 0.002622, l3: 0.001932, l4: 0.003414, l5: 0.003505, l6: 0.005521
[epoch:  82/1000, batch:   148/ 1052, ite: 21340] train loss: 0.007610, tar: 0.000658 
l0: 0.000537, l1: 0.000524, l2: 0.000584, l3: 0.000656, l4: 0.000713, l5: 0.001065, l6: 0.001706
[epoch:  82/1000, batch:   228/ 1052, ite: 21360] train loss: 0.007560, tar: 0.000654 
l0: 0.000838, l1: 0.000888, l2: 0.000855, l3: 0.000817, l4: 0.001023, l5: 0.001609, l6: 0.002154
[epoch:  82/1000, batch:   308/ 1052, ite: 21380] train loss: 0.007572, tar: 0.000655 
l0: 0.000430, l1: 0.000420, l2: 0.000509, l3: 0.000605, l4: 0.000962, l5: 0.001266, l6: 0.002845
[epoch:  82/1000, batch:   388/ 1052, ite: 21400] train loss: 0.007536, tar: 0.000649 
l0: 0.000559, l1: 0.000507, l2: 0.000651, l3: 0.000871, l4: 0.001134, l5: 0.002164, l6: 0.003325
[epoch:  82/1000, batch:   468/ 1052, ite: 21420] train loss: 0.007708, tar: 0.000668 
l0: 0.001278, l1: 0.001342, l2: 0.001261, l3: 0.001263, l4: 0.001499, l5: 0.002187, l6: 0.003256
[epoch:  82/1000, batch:   548/ 1052, ite: 21440] train loss: 0.007697, tar: 0.000667 
l0: 0.000578, l1: 0.000578, l2: 0.000669, l3: 0.000695, l4: 0.000895, l5: 0.001567, l6: 0.002663
[epoch:  82/1000, batch:   628/ 1052, ite: 21460] train loss: 0.007679, tar: 0.000665 
l0: 0.000356, l1: 0.000361, l2: 0.000423, l3: 0.000503, l4: 0.000627, l5: 0.000828, l6: 0.001215
[epoch:  82/1000, batch:   708/ 1052, ite: 21480] train loss: 0.007697, tar: 0.000665 
l0: 0.000478, l1: 0.000484, l2: 0.000531, l3: 0.000560, l4: 0.000750, l5: 0.000953, l6: 0.001987
[epoch:  82/1000, batch:   788/ 1052, ite: 21500] train loss: 0.007671, tar: 0.000660 
l0: 0.000342, l1: 0.000340, l2: 0.000325, l3: 0.000500, l4: 0.000587, l5: 0.001052, l6: 0.001379
[epoch:  82/1000, batch:   868/ 1052, ite: 21520] train loss: 0.007666, tar: 0.000659 
l0: 0.000833, l1: 0.000831, l2: 0.000852, l3: 0.000893, l4: 0.001200, l5: 0.001916, l6: 0.002706
[epoch:  82/1000, batch:   948/ 1052, ite: 21540] train loss: 0.007670, tar: 0.000661 
l0: 0.000803, l1: 0.000712, l2: 0.000745, l3: 0.000746, l4: 0.001229, l5: 0.001907, l6: 0.002176
[epoch:  82/1000, batch:  1028/ 1052, ite: 21560] train loss: 0.007651, tar: 0.000658 
[Epoch 82/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000516, l1: 0.000519, l2: 0.000573, l3: 0.000577, l4: 0.000844, l5: 0.001265, l6: 0.002139
[epoch:  83/1000, batch:    56/ 1052, ite: 21580] train loss: 0.007649, tar: 0.000657 
l0: 0.000379, l1: 0.000404, l2: 0.000386, l3: 0.000516, l4: 0.000691, l5: 0.001098, l6: 0.001958
[epoch:  83/1000, batch:   136/ 1052, ite: 21600] train loss: 0.007646, tar: 0.000658 
l0: 0.000429, l1: 0.000477, l2: 0.000452, l3: 0.000603, l4: 0.000710, l5: 0.000956, l6: 0.002076
[epoch:  83/1000, batch:   216/ 1052, ite: 21620] train loss: 0.007636, tar: 0.000656 
l0: 0.000462, l1: 0.000449, l2: 0.000515, l3: 0.000515, l4: 0.000708, l5: 0.001017, l6: 0.001765
[epoch:  83/1000, batch:   296/ 1052, ite: 21640] train loss: 0.007621, tar: 0.000654 
l0: 0.000601, l1: 0.000615, l2: 0.000600, l3: 0.000663, l4: 0.000744, l5: 0.001289, l6: 0.001808
[epoch:  83/1000, batch:   376/ 1052, ite: 21660] train loss: 0.007597, tar: 0.000653 
l0: 0.001306, l1: 0.001414, l2: 0.001250, l3: 0.001575, l4: 0.001785, l5: 0.002324, l6: 0.003836
[epoch:  83/1000, batch:   456/ 1052, ite: 21680] train loss: 0.007615, tar: 0.000654 
l0: 0.000426, l1: 0.000418, l2: 0.000494, l3: 0.000633, l4: 0.001016, l5: 0.001443, l6: 0.002137
[epoch:  83/1000, batch:   536/ 1052, ite: 21700] train loss: 0.007601, tar: 0.000653 
l0: 0.000581, l1: 0.000586, l2: 0.000664, l3: 0.000727, l4: 0.000957, l5: 0.001683, l6: 0.003020
[epoch:  83/1000, batch:   616/ 1052, ite: 21720] train loss: 0.007610, tar: 0.000653 
l0: 0.000231, l1: 0.000228, l2: 0.000241, l3: 0.000251, l4: 0.000390, l5: 0.000750, l6: 0.001511
[epoch:  83/1000, batch:   696/ 1052, ite: 21740] train loss: 0.007586, tar: 0.000650 
l0: 0.000315, l1: 0.000296, l2: 0.000331, l3: 0.000491, l4: 0.000721, l5: 0.001160, l6: 0.001707
[epoch:  83/1000, batch:   776/ 1052, ite: 21760] train loss: 0.007565, tar: 0.000646 
l0: 0.000714, l1: 0.000693, l2: 0.000712, l3: 0.000878, l4: 0.000940, l5: 0.001465, l6: 0.002600
[epoch:  83/1000, batch:   856/ 1052, ite: 21780] train loss: 0.007538, tar: 0.000644 
l0: 0.000798, l1: 0.000850, l2: 0.000782, l3: 0.000654, l4: 0.001040, l5: 0.001830, l6: 0.002092
[epoch:  83/1000, batch:   936/ 1052, ite: 21800] train loss: 0.007601, tar: 0.000652 
l0: 0.000706, l1: 0.000743, l2: 0.000678, l3: 0.000766, l4: 0.000774, l5: 0.001869, l6: 0.002210
[epoch:  83/1000, batch:  1016/ 1052, ite: 21820] train loss: 0.007615, tar: 0.000653 
[Epoch 83/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000890, l1: 0.001006, l2: 0.000910, l3: 0.001112, l4: 0.001527, l5: 0.001932, l6: 0.003822
[epoch:  84/1000, batch:    44/ 1052, ite: 21840] train loss: 0.007634, tar: 0.000654 
l0: 0.000455, l1: 0.000478, l2: 0.000309, l3: 0.000365, l4: 0.000683, l5: 0.001056, l6: 0.001531
[epoch:  84/1000, batch:   124/ 1052, ite: 21860] train loss: 0.007614, tar: 0.000653 
l0: 0.000546, l1: 0.000529, l2: 0.000558, l3: 0.000612, l4: 0.000667, l5: 0.000891, l6: 0.001718
[epoch:  84/1000, batch:   204/ 1052, ite: 21880] train loss: 0.007638, tar: 0.000656 
l0: 0.000605, l1: 0.000600, l2: 0.000601, l3: 0.000612, l4: 0.000819, l5: 0.001549, l6: 0.001574
[epoch:  84/1000, batch:   284/ 1052, ite: 21900] train loss: 0.007621, tar: 0.000654 
l0: 0.000411, l1: 0.000420, l2: 0.000476, l3: 0.000465, l4: 0.000660, l5: 0.000848, l6: 0.001628
[epoch:  84/1000, batch:   364/ 1052, ite: 21920] train loss: 0.007636, tar: 0.000656 
l0: 0.000511, l1: 0.000510, l2: 0.000620, l3: 0.000793, l4: 0.001122, l5: 0.001603, l6: 0.002128
[epoch:  84/1000, batch:   444/ 1052, ite: 21940] train loss: 0.007624, tar: 0.000654 
l0: 0.000416, l1: 0.000405, l2: 0.000412, l3: 0.000520, l4: 0.000829, l5: 0.001332, l6: 0.002412
[epoch:  84/1000, batch:   524/ 1052, ite: 21960] train loss: 0.007647, tar: 0.000656 
l0: 0.000201, l1: 0.000205, l2: 0.000202, l3: 0.000280, l4: 0.000425, l5: 0.000494, l6: 0.000942
[epoch:  84/1000, batch:   604/ 1052, ite: 21980] train loss: 0.007657, tar: 0.000656 
l0: 0.000464, l1: 0.000499, l2: 0.000462, l3: 0.000539, l4: 0.000644, l5: 0.000797, l6: 0.001678
[epoch:  84/1000, batch:   684/ 1052, ite: 22000] train loss: 0.007667, tar: 0.000659 
l0: 0.000400, l1: 0.000381, l2: 0.000413, l3: 0.000533, l4: 0.000797, l5: 0.000939, l6: 0.002002
[epoch:  84/1000, batch:   764/ 1052, ite: 22020] train loss: 0.007646, tar: 0.000656 
l0: 0.001295, l1: 0.001472, l2: 0.001286, l3: 0.001520, l4: 0.002139, l5: 0.002094, l6: 0.002374
[epoch:  84/1000, batch:   844/ 1052, ite: 22040] train loss: 0.007635, tar: 0.000655 
l0: 0.001121, l1: 0.001179, l2: 0.001234, l3: 0.001392, l4: 0.001369, l5: 0.001742, l6: 0.002802
[epoch:  84/1000, batch:   924/ 1052, ite: 22060] train loss: 0.007618, tar: 0.000653 
l0: 0.000798, l1: 0.000757, l2: 0.000822, l3: 0.001097, l4: 0.001572, l5: 0.001801, l6: 0.001769
[epoch:  84/1000, batch:  1004/ 1052, ite: 22080] train loss: 0.007609, tar: 0.000652 
[Epoch 84/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000495, l1: 0.000487, l2: 0.000777, l3: 0.000615, l4: 0.000639, l5: 0.000928, l6: 0.001848
[epoch:  85/1000, batch:    32/ 1052, ite: 22100] train loss: 0.007652, tar: 0.000659 
l0: 0.000711, l1: 0.000695, l2: 0.000709, l3: 0.000715, l4: 0.000931, l5: 0.001471, l6: 0.002314
[epoch:  85/1000, batch:   112/ 1052, ite: 22120] train loss: 0.007660, tar: 0.000660 
l0: 0.000295, l1: 0.000314, l2: 0.000289, l3: 0.000343, l4: 0.000505, l5: 0.000924, l6: 0.001040
[epoch:  85/1000, batch:   192/ 1052, ite: 22140] train loss: 0.007650, tar: 0.000659 
l0: 0.000488, l1: 0.000507, l2: 0.000451, l3: 0.000608, l4: 0.000744, l5: 0.001203, l6: 0.002474
[epoch:  85/1000, batch:   272/ 1052, ite: 22160] train loss: 0.007636, tar: 0.000657 
l0: 0.000313, l1: 0.000306, l2: 0.000363, l3: 0.000470, l4: 0.000809, l5: 0.001255, l6: 0.001792
[epoch:  85/1000, batch:   352/ 1052, ite: 22180] train loss: 0.007644, tar: 0.000658 
l0: 0.000412, l1: 0.000391, l2: 0.000425, l3: 0.000478, l4: 0.000714, l5: 0.001101, l6: 0.001486
[epoch:  85/1000, batch:   432/ 1052, ite: 22200] train loss: 0.007648, tar: 0.000658 
l0: 0.000597, l1: 0.000708, l2: 0.000665, l3: 0.000685, l4: 0.000857, l5: 0.001207, l6: 0.002112
[epoch:  85/1000, batch:   512/ 1052, ite: 22220] train loss: 0.007664, tar: 0.000659 
l0: 0.000517, l1: 0.000392, l2: 0.000483, l3: 0.000624, l4: 0.000727, l5: 0.002857, l6: 0.002187
[epoch:  85/1000, batch:   592/ 1052, ite: 22240] train loss: 0.007663, tar: 0.000659 
l0: 0.001350, l1: 0.001378, l2: 0.001545, l3: 0.001540, l4: 0.001111, l5: 0.001432, l6: 0.002348
[epoch:  85/1000, batch:   672/ 1052, ite: 22260] train loss: 0.007674, tar: 0.000659 
l0: 0.000255, l1: 0.000246, l2: 0.000307, l3: 0.000364, l4: 0.000528, l5: 0.000780, l6: 0.001436
[epoch:  85/1000, batch:   752/ 1052, ite: 22280] train loss: 0.007650, tar: 0.000656 
l0: 0.000634, l1: 0.000668, l2: 0.000626, l3: 0.000784, l4: 0.000826, l5: 0.001358, l6: 0.001789
[epoch:  85/1000, batch:   832/ 1052, ite: 22300] train loss: 0.007640, tar: 0.000655 
l0: 0.000487, l1: 0.000502, l2: 0.000456, l3: 0.000618, l4: 0.000899, l5: 0.001153, l6: 0.002333
[epoch:  85/1000, batch:   912/ 1052, ite: 22320] train loss: 0.007623, tar: 0.000654 
l0: 0.000497, l1: 0.000509, l2: 0.000543, l3: 0.000726, l4: 0.000815, l5: 0.001006, l6: 0.002047
[epoch:  85/1000, batch:   992/ 1052, ite: 22340] train loss: 0.007611, tar: 0.000653 
[Epoch 85/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000828, l1: 0.000806, l2: 0.001008, l3: 0.001076, l4: 0.001497, l5: 0.002229, l6: 0.003534
[epoch:  86/1000, batch:    20/ 1052, ite: 22360] train loss: 0.007622, tar: 0.000654 
l0: 0.000642, l1: 0.000598, l2: 0.000624, l3: 0.000856, l4: 0.001136, l5: 0.001888, l6: 0.003316
[epoch:  86/1000, batch:   100/ 1052, ite: 22380] train loss: 0.007631, tar: 0.000656 
l0: 0.007362, l1: 0.003336, l2: 0.014430, l3: 0.013928, l4: 0.017767, l5: 0.022337, l6: 0.019090
[epoch:  86/1000, batch:   180/ 1052, ite: 22400] train loss: 0.007707, tar: 0.000661 
l0: 0.000827, l1: 0.001549, l2: 0.000832, l3: 0.000747, l4: 0.000881, l5: 0.001315, l6: 0.001663
[epoch:  86/1000, batch:   260/ 1052, ite: 22420] train loss: 0.007722, tar: 0.000663 
l0: 0.000831, l1: 0.000761, l2: 0.000859, l3: 0.001374, l4: 0.001291, l5: 0.002227, l6: 0.002785
[epoch:  86/1000, batch:   340/ 1052, ite: 22440] train loss: 0.007726, tar: 0.000663 
l0: 0.000441, l1: 0.000455, l2: 0.000514, l3: 0.000531, l4: 0.000745, l5: 0.001659, l6: 0.002183
[epoch:  86/1000, batch:   420/ 1052, ite: 22460] train loss: 0.007716, tar: 0.000662 
l0: 0.000429, l1: 0.000426, l2: 0.000450, l3: 0.000469, l4: 0.000539, l5: 0.000997, l6: 0.001528
[epoch:  86/1000, batch:   500/ 1052, ite: 22480] train loss: 0.007708, tar: 0.000662 
l0: 0.000518, l1: 0.000480, l2: 0.000521, l3: 0.000534, l4: 0.000843, l5: 0.000860, l6: 0.001950
[epoch:  86/1000, batch:   580/ 1052, ite: 22500] train loss: 0.007712, tar: 0.000661 
l0: 0.000426, l1: 0.000463, l2: 0.000450, l3: 0.000477, l4: 0.000593, l5: 0.001136, l6: 0.002066
[epoch:  86/1000, batch:   660/ 1052, ite: 22520] train loss: 0.007723, tar: 0.000662 
l0: 0.000318, l1: 0.000287, l2: 0.000344, l3: 0.000355, l4: 0.000517, l5: 0.000828, l6: 0.001510
[epoch:  86/1000, batch:   740/ 1052, ite: 22540] train loss: 0.007716, tar: 0.000662 
l0: 0.000707, l1: 0.000777, l2: 0.000870, l3: 0.000862, l4: 0.000868, l5: 0.001266, l6: 0.001885
[epoch:  86/1000, batch:   820/ 1052, ite: 22560] train loss: 0.007705, tar: 0.000661 
l0: 0.000754, l1: 0.000731, l2: 0.000914, l3: 0.001044, l4: 0.001229, l5: 0.001645, l6: 0.003589
[epoch:  86/1000, batch:   900/ 1052, ite: 22580] train loss: 0.007692, tar: 0.000659 
l0: 0.000262, l1: 0.000250, l2: 0.000295, l3: 0.000423, l4: 0.000583, l5: 0.000922, l6: 0.001991
[epoch:  86/1000, batch:   980/ 1052, ite: 22600] train loss: 0.007680, tar: 0.000657 
[Epoch 86/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000333, l1: 0.000324, l2: 0.000320, l3: 0.000360, l4: 0.000574, l5: 0.000754, l6: 0.001394
[epoch:  87/1000, batch:     8/ 1052, ite: 22620] train loss: 0.007678, tar: 0.000657 
l0: 0.000506, l1: 0.000498, l2: 0.000518, l3: 0.000585, l4: 0.000985, l5: 0.001939, l6: 0.002769
[epoch:  87/1000, batch:    88/ 1052, ite: 22640] train loss: 0.007675, tar: 0.000657 
l0: 0.000630, l1: 0.000600, l2: 0.000931, l3: 0.000637, l4: 0.000786, l5: 0.001327, l6: 0.001755
[epoch:  87/1000, batch:   168/ 1052, ite: 22660] train loss: 0.007680, tar: 0.000657 
l0: 0.000455, l1: 0.000485, l2: 0.000506, l3: 0.000483, l4: 0.000559, l5: 0.000848, l6: 0.001577
[epoch:  87/1000, batch:   248/ 1052, ite: 22680] train loss: 0.007673, tar: 0.000657 
l0: 0.000571, l1: 0.000588, l2: 0.000722, l3: 0.000746, l4: 0.000849, l5: 0.001181, l6: 0.001764
[epoch:  87/1000, batch:   328/ 1052, ite: 22700] train loss: 0.007673, tar: 0.000657 
l0: 0.000985, l1: 0.000988, l2: 0.000994, l3: 0.001005, l4: 0.001530, l5: 0.001730, l6: 0.001846
[epoch:  87/1000, batch:   408/ 1052, ite: 22720] train loss: 0.007674, tar: 0.000657 
l0: 0.000308, l1: 0.000300, l2: 0.000366, l3: 0.000475, l4: 0.000507, l5: 0.000961, l6: 0.001276
[epoch:  87/1000, batch:   488/ 1052, ite: 22740] train loss: 0.007665, tar: 0.000656 
l0: 0.000571, l1: 0.000552, l2: 0.000582, l3: 0.000736, l4: 0.000845, l5: 0.001218, l6: 0.002118
[epoch:  87/1000, batch:   568/ 1052, ite: 22760] train loss: 0.007664, tar: 0.000656 
l0: 0.000494, l1: 0.000516, l2: 0.000468, l3: 0.000458, l4: 0.000629, l5: 0.001007, l6: 0.001389
[epoch:  87/1000, batch:   648/ 1052, ite: 22780] train loss: 0.007648, tar: 0.000655 
l0: 0.000304, l1: 0.000308, l2: 0.000341, l3: 0.000443, l4: 0.000445, l5: 0.000783, l6: 0.001007
[epoch:  87/1000, batch:   728/ 1052, ite: 22800] train loss: 0.007644, tar: 0.000654 
l0: 0.000194, l1: 0.000191, l2: 0.000203, l3: 0.000253, l4: 0.000412, l5: 0.000447, l6: 0.001045
[epoch:  87/1000, batch:   808/ 1052, ite: 22820] train loss: 0.007644, tar: 0.000654 
l0: 0.000621, l1: 0.000584, l2: 0.000635, l3: 0.000662, l4: 0.000850, l5: 0.002095, l6: 0.003772
[epoch:  87/1000, batch:   888/ 1052, ite: 22840] train loss: 0.007630, tar: 0.000653 
l0: 0.000431, l1: 0.000437, l2: 0.000429, l3: 0.000441, l4: 0.000659, l5: 0.001083, l6: 0.001603
[epoch:  87/1000, batch:   968/ 1052, ite: 22860] train loss: 0.007627, tar: 0.000652 
l0: 0.000320, l1: 0.000339, l2: 0.000404, l3: 0.000398, l4: 0.000598, l5: 0.000872, l6: 0.001434
[epoch:  87/1000, batch:  1048/ 1052, ite: 22880] train loss: 0.007635, tar: 0.000653 
[Epoch 87/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000503, l1: 0.000549, l2: 0.000531, l3: 0.000486, l4: 0.000682, l5: 0.001223, l6: 0.002633
[epoch:  88/1000, batch:    76/ 1052, ite: 22900] train loss: 0.007637, tar: 0.000653 
l0: 0.000319, l1: 0.000323, l2: 0.000355, l3: 0.000412, l4: 0.000567, l5: 0.000964, l6: 0.002378
[epoch:  88/1000, batch:   156/ 1052, ite: 22920] train loss: 0.007629, tar: 0.000653 
l0: 0.000393, l1: 0.000376, l2: 0.000433, l3: 0.000444, l4: 0.000688, l5: 0.001468, l6: 0.002475
[epoch:  88/1000, batch:   236/ 1052, ite: 22940] train loss: 0.007634, tar: 0.000653 
l0: 0.000574, l1: 0.000568, l2: 0.000650, l3: 0.000659, l4: 0.000796, l5: 0.001118, l6: 0.001735
[epoch:  88/1000, batch:   316/ 1052, ite: 22960] train loss: 0.007644, tar: 0.000654 
l0: 0.000280, l1: 0.000297, l2: 0.000346, l3: 0.000398, l4: 0.000549, l5: 0.001196, l6: 0.001806
[epoch:  88/1000, batch:   396/ 1052, ite: 22980] train loss: 0.007629, tar: 0.000652 
l0: 0.000322, l1: 0.000363, l2: 0.000361, l3: 0.000359, l4: 0.000480, l5: 0.000956, l6: 0.001585
[epoch:  88/1000, batch:   476/ 1052, ite: 23000] train loss: 0.007623, tar: 0.000652 
l0: 0.000250, l1: 0.000237, l2: 0.000273, l3: 0.000357, l4: 0.000519, l5: 0.000878, l6: 0.001324
[epoch:  88/1000, batch:   556/ 1052, ite: 23020] train loss: 0.007613, tar: 0.000651 
l0: 0.000323, l1: 0.000319, l2: 0.000354, l3: 0.000370, l4: 0.000562, l5: 0.000631, l6: 0.001526
[epoch:  88/1000, batch:   636/ 1052, ite: 23040] train loss: 0.007601, tar: 0.000650 
l0: 0.000361, l1: 0.000345, l2: 0.000426, l3: 0.000618, l4: 0.000602, l5: 0.000678, l6: 0.000849
[epoch:  88/1000, batch:   716/ 1052, ite: 23060] train loss: 0.007593, tar: 0.000649 
l0: 0.000869, l1: 0.000825, l2: 0.001051, l3: 0.001474, l4: 0.001888, l5: 0.002672, l6: 0.003646
[epoch:  88/1000, batch:   796/ 1052, ite: 23080] train loss: 0.007597, tar: 0.000649 
l0: 0.000417, l1: 0.000423, l2: 0.000466, l3: 0.000563, l4: 0.000687, l5: 0.001210, l6: 0.001733
[epoch:  88/1000, batch:   876/ 1052, ite: 23100] train loss: 0.007597, tar: 0.000649 
l0: 0.000371, l1: 0.000339, l2: 0.000417, l3: 0.000461, l4: 0.000599, l5: 0.001053, l6: 0.001486
[epoch:  88/1000, batch:   956/ 1052, ite: 23120] train loss: 0.007592, tar: 0.000649 
l0: 0.000428, l1: 0.000436, l2: 0.000455, l3: 0.000523, l4: 0.000614, l5: 0.001300, l6: 0.001707
[epoch:  88/1000, batch:  1036/ 1052, ite: 23140] train loss: 0.007585, tar: 0.000648 
[Epoch 88/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000784, l1: 0.000786, l2: 0.000834, l3: 0.000691, l4: 0.001061, l5: 0.001735, l6: 0.002991
[epoch:  89/1000, batch:    64/ 1052, ite: 23160] train loss: 0.007573, tar: 0.000647 
l0: 0.000459, l1: 0.000451, l2: 0.000497, l3: 0.000533, l4: 0.000723, l5: 0.001440, l6: 0.002276
[epoch:  89/1000, batch:   144/ 1052, ite: 23180] train loss: 0.007568, tar: 0.000646 
l0: 0.000678, l1: 0.000707, l2: 0.000740, l3: 0.000723, l4: 0.000813, l5: 0.001350, l6: 0.002107
[epoch:  89/1000, batch:   224/ 1052, ite: 23200] train loss: 0.007557, tar: 0.000645 
l0: 0.000838, l1: 0.000819, l2: 0.000928, l3: 0.001121, l4: 0.001619, l5: 0.002786, l6: 0.005469
[epoch:  89/1000, batch:   304/ 1052, ite: 23220] train loss: 0.007552, tar: 0.000644 
l0: 0.000449, l1: 0.000438, l2: 0.000443, l3: 0.000517, l4: 0.000758, l5: 0.001301, l6: 0.001830
[epoch:  89/1000, batch:   384/ 1052, ite: 23240] train loss: 0.007554, tar: 0.000644 
l0: 0.000417, l1: 0.000406, l2: 0.000444, l3: 0.000507, l4: 0.000542, l5: 0.000878, l6: 0.001548
[epoch:  89/1000, batch:   464/ 1052, ite: 23260] train loss: 0.007556, tar: 0.000645 
l0: 0.000522, l1: 0.000466, l2: 0.000517, l3: 0.000744, l4: 0.001158, l5: 0.001569, l6: 0.002846
[epoch:  89/1000, batch:   544/ 1052, ite: 23280] train loss: 0.007554, tar: 0.000644 
l0: 0.000769, l1: 0.000780, l2: 0.000666, l3: 0.000711, l4: 0.000949, l5: 0.001644, l6: 0.002134
[epoch:  89/1000, batch:   624/ 1052, ite: 23300] train loss: 0.007546, tar: 0.000643 
l0: 0.000479, l1: 0.000455, l2: 0.000447, l3: 0.000538, l4: 0.000652, l5: 0.000947, l6: 0.001585
[epoch:  89/1000, batch:   704/ 1052, ite: 23320] train loss: 0.007541, tar: 0.000643 
l0: 0.000425, l1: 0.000422, l2: 0.000503, l3: 0.000556, l4: 0.000712, l5: 0.001011, l6: 0.001606
[epoch:  89/1000, batch:   784/ 1052, ite: 23340] train loss: 0.007537, tar: 0.000642 
l0: 0.000513, l1: 0.000574, l2: 0.000557, l3: 0.000595, l4: 0.000650, l5: 0.001089, l6: 0.001506
[epoch:  89/1000, batch:   864/ 1052, ite: 23360] train loss: 0.007531, tar: 0.000642 
l0: 0.000772, l1: 0.000760, l2: 0.000742, l3: 0.000839, l4: 0.000856, l5: 0.001577, l6: 0.003281
[epoch:  89/1000, batch:   944/ 1052, ite: 23380] train loss: 0.007534, tar: 0.000642 
l0: 0.000625, l1: 0.000628, l2: 0.000640, l3: 0.000796, l4: 0.000848, l5: 0.001073, l6: 0.002147
[epoch:  89/1000, batch:  1024/ 1052, ite: 23400] train loss: 0.007531, tar: 0.000642 
[Epoch 89/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000429, l1: 0.000432, l2: 0.000449, l3: 0.000577, l4: 0.000571, l5: 0.000912, l6: 0.001420
[epoch:  90/1000, batch:    52/ 1052, ite: 23420] train loss: 0.007524, tar: 0.000641 
l0: 0.000442, l1: 0.000433, l2: 0.000509, l3: 0.000712, l4: 0.000961, l5: 0.001125, l6: 0.002271
[epoch:  90/1000, batch:   132/ 1052, ite: 23440] train loss: 0.007514, tar: 0.000640 
l0: 0.000509, l1: 0.000485, l2: 0.000540, l3: 0.000643, l4: 0.000694, l5: 0.001096, l6: 0.002157
[epoch:  90/1000, batch:   212/ 1052, ite: 23460] train loss: 0.007514, tar: 0.000640 
l0: 0.000661, l1: 0.000666, l2: 0.000612, l3: 0.000768, l4: 0.000857, l5: 0.001365, l6: 0.002329
[epoch:  90/1000, batch:   292/ 1052, ite: 23480] train loss: 0.007507, tar: 0.000640 
l0: 0.000280, l1: 0.000272, l2: 0.000334, l3: 0.000417, l4: 0.000674, l5: 0.001123, l6: 0.001461
[epoch:  90/1000, batch:   372/ 1052, ite: 23500] train loss: 0.007502, tar: 0.000639 
l0: 0.001661, l1: 0.001883, l2: 0.002167, l3: 0.001874, l4: 0.002339, l5: 0.003134, l6: 0.002620
[epoch:  90/1000, batch:   452/ 1052, ite: 23520] train loss: 0.007503, tar: 0.000639 
l0: 0.000832, l1: 0.000853, l2: 0.000878, l3: 0.001066, l4: 0.001317, l5: 0.001569, l6: 0.002401
[epoch:  90/1000, batch:   532/ 1052, ite: 23540] train loss: 0.007500, tar: 0.000638 
l0: 0.000414, l1: 0.000404, l2: 0.000425, l3: 0.000458, l4: 0.000588, l5: 0.001213, l6: 0.001402
[epoch:  90/1000, batch:   612/ 1052, ite: 23560] train loss: 0.007496, tar: 0.000638 
l0: 0.000166, l1: 0.000168, l2: 0.000165, l3: 0.000207, l4: 0.000337, l5: 0.000683, l6: 0.000987
[epoch:  90/1000, batch:   692/ 1052, ite: 23580] train loss: 0.007495, tar: 0.000638 
l0: 0.000865, l1: 0.000886, l2: 0.000904, l3: 0.000783, l4: 0.001013, l5: 0.002177, l6: 0.003213
[epoch:  90/1000, batch:   772/ 1052, ite: 23600] train loss: 0.007497, tar: 0.000638 
l0: 0.000234, l1: 0.000238, l2: 0.000245, l3: 0.000261, l4: 0.000503, l5: 0.000968, l6: 0.001625
[epoch:  90/1000, batch:   852/ 1052, ite: 23620] train loss: 0.007488, tar: 0.000637 
l0: 0.000584, l1: 0.000581, l2: 0.000684, l3: 0.000812, l4: 0.001009, l5: 0.001577, l6: 0.002615
[epoch:  90/1000, batch:   932/ 1052, ite: 23640] train loss: 0.007486, tar: 0.000637 
l0: 0.000902, l1: 0.000952, l2: 0.001030, l3: 0.001436, l4: 0.001500, l5: 0.001953, l6: 0.003262
[epoch:  90/1000, batch:  1012/ 1052, ite: 23660] train loss: 0.007490, tar: 0.000637 
[Epoch 90/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000549, l1: 0.000558, l2: 0.000544, l3: 0.000650, l4: 0.000815, l5: 0.001577, l6: 0.002487
[epoch:  91/1000, batch:    40/ 1052, ite: 23680] train loss: 0.007495, tar: 0.000638 
l0: 0.000332, l1: 0.000345, l2: 0.000376, l3: 0.000479, l4: 0.000626, l5: 0.000929, l6: 0.001680
[epoch:  91/1000, batch:   120/ 1052, ite: 23700] train loss: 0.007528, tar: 0.000643 
l0: 0.000373, l1: 0.000426, l2: 0.000370, l3: 0.000468, l4: 0.000560, l5: 0.000886, l6: 0.001034
[epoch:  91/1000, batch:   200/ 1052, ite: 23720] train loss: 0.007539, tar: 0.000644 
l0: 0.000273, l1: 0.000288, l2: 0.000297, l3: 0.000306, l4: 0.000469, l5: 0.000661, l6: 0.001406
[epoch:  91/1000, batch:   280/ 1052, ite: 23740] train loss: 0.007540, tar: 0.000644 
l0: 0.000566, l1: 0.000572, l2: 0.000618, l3: 0.000699, l4: 0.000834, l5: 0.001197, l6: 0.002754
[epoch:  91/1000, batch:   360/ 1052, ite: 23760] train loss: 0.007533, tar: 0.000643 
l0: 0.000360, l1: 0.000362, l2: 0.000366, l3: 0.000567, l4: 0.000546, l5: 0.000768, l6: 0.001406
[epoch:  91/1000, batch:   440/ 1052, ite: 23780] train loss: 0.007543, tar: 0.000644 
l0: 0.000552, l1: 0.000573, l2: 0.000630, l3: 0.000559, l4: 0.000922, l5: 0.001536, l6: 0.002319
[epoch:  91/1000, batch:   520/ 1052, ite: 23800] train loss: 0.007543, tar: 0.000644 
l0: 0.000919, l1: 0.000882, l2: 0.000956, l3: 0.001060, l4: 0.001617, l5: 0.003021, l6: 0.004213
[epoch:  91/1000, batch:   600/ 1052, ite: 23820] train loss: 0.007549, tar: 0.000645 
l0: 0.000285, l1: 0.000281, l2: 0.000419, l3: 0.000355, l4: 0.000492, l5: 0.000907, l6: 0.001597
[epoch:  91/1000, batch:   680/ 1052, ite: 23840] train loss: 0.007545, tar: 0.000644 
l0: 0.000836, l1: 0.000806, l2: 0.000932, l3: 0.000910, l4: 0.000998, l5: 0.001641, l6: 0.002580
[epoch:  91/1000, batch:   760/ 1052, ite: 23860] train loss: 0.007543, tar: 0.000644 
l0: 0.000547, l1: 0.000542, l2: 0.000568, l3: 0.000628, l4: 0.000782, l5: 0.001410, l6: 0.002137
[epoch:  91/1000, batch:   840/ 1052, ite: 23880] train loss: 0.007537, tar: 0.000644 
l0: 0.000244, l1: 0.000361, l2: 0.000348, l3: 0.000307, l4: 0.000406, l5: 0.000821, l6: 0.001234
[epoch:  91/1000, batch:   920/ 1052, ite: 23900] train loss: 0.007538, tar: 0.000644 
l0: 0.000442, l1: 0.000435, l2: 0.000494, l3: 0.000525, l4: 0.000631, l5: 0.000834, l6: 0.001570
[epoch:  91/1000, batch:  1000/ 1052, ite: 23920] train loss: 0.007536, tar: 0.000644 
[Epoch 91/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000882, l1: 0.000891, l2: 0.000894, l3: 0.001179, l4: 0.001276, l5: 0.001845, l6: 0.002422
[epoch:  92/1000, batch:    28/ 1052, ite: 23940] train loss: 0.007526, tar: 0.000643 
l0: 0.000361, l1: 0.000350, l2: 0.000403, l3: 0.000474, l4: 0.000707, l5: 0.000994, l6: 0.001583
[epoch:  92/1000, batch:   108/ 1052, ite: 23960] train loss: 0.007521, tar: 0.000642 
l0: 0.000574, l1: 0.000578, l2: 0.000637, l3: 0.000718, l4: 0.000729, l5: 0.000953, l6: 0.001410
[epoch:  92/1000, batch:   188/ 1052, ite: 23980] train loss: 0.007531, tar: 0.000644 
l0: 0.000512, l1: 0.000546, l2: 0.000582, l3: 0.000668, l4: 0.000907, l5: 0.001353, l6: 0.002277
[epoch:  92/1000, batch:   268/ 1052, ite: 24000] train loss: 0.007542, tar: 0.000645 
l0: 0.000473, l1: 0.000527, l2: 0.000468, l3: 0.000643, l4: 0.000834, l5: 0.001050, l6: 0.001599
[epoch:  92/1000, batch:   348/ 1052, ite: 24020] train loss: 0.007537, tar: 0.000644 
l0: 0.001030, l1: 0.001096, l2: 0.001127, l3: 0.001198, l4: 0.001567, l5: 0.002004, l6: 0.002577
[epoch:  92/1000, batch:   428/ 1052, ite: 24040] train loss: 0.007534, tar: 0.000644 
l0: 0.002037, l1: 0.002240, l2: 0.002020, l3: 0.002477, l4: 0.002606, l5: 0.002997, l6: 0.004909
[epoch:  92/1000, batch:   508/ 1052, ite: 24060] train loss: 0.007526, tar: 0.000643 
l0: 0.000650, l1: 0.000999, l2: 0.000696, l3: 0.001320, l4: 0.001289, l5: 0.001827, l6: 0.002405
[epoch:  92/1000, batch:   588/ 1052, ite: 24080] train loss: 0.007524, tar: 0.000643 
l0: 0.001305, l1: 0.001451, l2: 0.001433, l3: 0.001359, l4: 0.001321, l5: 0.001937, l6: 0.004266
[epoch:  92/1000, batch:   668/ 1052, ite: 24100] train loss: 0.007525, tar: 0.000643 
l0: 0.000367, l1: 0.000366, l2: 0.000377, l3: 0.000457, l4: 0.000629, l5: 0.000942, l6: 0.001663
[epoch:  92/1000, batch:   748/ 1052, ite: 24120] train loss: 0.007515, tar: 0.000642 
l0: 0.000540, l1: 0.000508, l2: 0.000539, l3: 0.000566, l4: 0.000780, l5: 0.001190, l6: 0.001654
[epoch:  92/1000, batch:   828/ 1052, ite: 24140] train loss: 0.007514, tar: 0.000641 
l0: 0.000352, l1: 0.000335, l2: 0.000334, l3: 0.000443, l4: 0.000562, l5: 0.001069, l6: 0.001605
[epoch:  92/1000, batch:   908/ 1052, ite: 24160] train loss: 0.007513, tar: 0.000641 
l0: 0.000231, l1: 0.000229, l2: 0.000222, l3: 0.000272, l4: 0.000407, l5: 0.000771, l6: 0.001037
[epoch:  92/1000, batch:   988/ 1052, ite: 24180] train loss: 0.007507, tar: 0.000641 
[Epoch 92/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000463, l1: 0.000483, l2: 0.000475, l3: 0.000476, l4: 0.000726, l5: 0.001317, l6: 0.002154
[epoch:  93/1000, batch:    16/ 1052, ite: 24200] train loss: 0.007495, tar: 0.000639 
l0: 0.000739, l1: 0.000754, l2: 0.000727, l3: 0.000741, l4: 0.000845, l5: 0.001350, l6: 0.002004
[epoch:  93/1000, batch:    96/ 1052, ite: 24220] train loss: 0.007490, tar: 0.000638 
l0: 0.000421, l1: 0.000413, l2: 0.000462, l3: 0.000464, l4: 0.000741, l5: 0.001245, l6: 0.002576
[epoch:  93/1000, batch:   176/ 1052, ite: 24240] train loss: 0.007487, tar: 0.000638 
l0: 0.000648, l1: 0.000637, l2: 0.000605, l3: 0.000817, l4: 0.001113, l5: 0.001712, l6: 0.002623
[epoch:  93/1000, batch:   256/ 1052, ite: 24260] train loss: 0.007482, tar: 0.000638 
l0: 0.000948, l1: 0.000956, l2: 0.000987, l3: 0.001034, l4: 0.001032, l5: 0.001888, l6: 0.002844
[epoch:  93/1000, batch:   336/ 1052, ite: 24280] train loss: 0.007476, tar: 0.000637 
l0: 0.000851, l1: 0.000954, l2: 0.000863, l3: 0.000950, l4: 0.001512, l5: 0.002719, l6: 0.003532
[epoch:  93/1000, batch:   416/ 1052, ite: 24300] train loss: 0.007475, tar: 0.000637 
l0: 0.000553, l1: 0.000558, l2: 0.000589, l3: 0.000733, l4: 0.000925, l5: 0.001351, l6: 0.002383
[epoch:  93/1000, batch:   496/ 1052, ite: 24320] train loss: 0.007474, tar: 0.000637 
l0: 0.000606, l1: 0.000589, l2: 0.000625, l3: 0.000726, l4: 0.000970, l5: 0.001284, l6: 0.002129
[epoch:  93/1000, batch:   576/ 1052, ite: 24340] train loss: 0.007469, tar: 0.000636 
l0: 0.001136, l1: 0.001156, l2: 0.001188, l3: 0.001271, l4: 0.001686, l5: 0.002847, l6: 0.003229
[epoch:  93/1000, batch:   656/ 1052, ite: 24360] train loss: 0.007465, tar: 0.000636 
l0: 0.000397, l1: 0.000425, l2: 0.000430, l3: 0.000513, l4: 0.000748, l5: 0.001261, l6: 0.002408
[epoch:  93/1000, batch:   736/ 1052, ite: 24380] train loss: 0.007459, tar: 0.000635 
l0: 0.001343, l1: 0.001353, l2: 0.001335, l3: 0.001237, l4: 0.001230, l5: 0.001666, l6: 0.002246
[epoch:  93/1000, batch:   816/ 1052, ite: 24400] train loss: 0.007458, tar: 0.000635 
l0: 0.000384, l1: 0.000371, l2: 0.000355, l3: 0.000494, l4: 0.000777, l5: 0.001306, l6: 0.002690
[epoch:  93/1000, batch:   896/ 1052, ite: 24420] train loss: 0.007464, tar: 0.000636 
l0: 0.000725, l1: 0.000811, l2: 0.000920, l3: 0.001112, l4: 0.000630, l5: 0.001012, l6: 0.001482
[epoch:  93/1000, batch:   976/ 1052, ite: 24440] train loss: 0.007457, tar: 0.000635 
[Epoch 93/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000864, l1: 0.000851, l2: 0.000967, l3: 0.000807, l4: 0.000956, l5: 0.001333, l6: 0.002749
[epoch:  94/1000, batch:     4/ 1052, ite: 24460] train loss: 0.007466, tar: 0.000636 
l0: 0.000610, l1: 0.000602, l2: 0.000641, l3: 0.000650, l4: 0.001241, l5: 0.001787, l6: 0.003578
[epoch:  94/1000, batch:    84/ 1052, ite: 24480] train loss: 0.007462, tar: 0.000635 
l0: 0.000254, l1: 0.000238, l2: 0.000262, l3: 0.000332, l4: 0.000527, l5: 0.000782, l6: 0.001312
[epoch:  94/1000, batch:   164/ 1052, ite: 24500] train loss: 0.007454, tar: 0.000634 
l0: 0.000433, l1: 0.000471, l2: 0.000502, l3: 0.000560, l4: 0.000601, l5: 0.001222, l6: 0.001900
[epoch:  94/1000, batch:   244/ 1052, ite: 24520] train loss: 0.007451, tar: 0.000634 
l0: 0.000735, l1: 0.000709, l2: 0.000807, l3: 0.000863, l4: 0.000976, l5: 0.001920, l6: 0.002330
[epoch:  94/1000, batch:   324/ 1052, ite: 24540] train loss: 0.007446, tar: 0.000633 
l0: 0.000510, l1: 0.000538, l2: 0.000506, l3: 0.000609, l4: 0.000909, l5: 0.001308, l6: 0.002645
[epoch:  94/1000, batch:   404/ 1052, ite: 24560] train loss: 0.007445, tar: 0.000633 
l0: 0.000723, l1: 0.000729, l2: 0.000761, l3: 0.000732, l4: 0.000954, l5: 0.001667, l6: 0.003800
[epoch:  94/1000, batch:   484/ 1052, ite: 24580] train loss: 0.007440, tar: 0.000633 
l0: 0.000622, l1: 0.000604, l2: 0.000655, l3: 0.000735, l4: 0.001105, l5: 0.001683, l6: 0.003275
[epoch:  94/1000, batch:   564/ 1052, ite: 24600] train loss: 0.007436, tar: 0.000632 
l0: 0.002156, l1: 0.002320, l2: 0.002558, l3: 0.002343, l4: 0.002042, l5: 0.003252, l6: 0.005438
[epoch:  94/1000, batch:   644/ 1052, ite: 24620] train loss: 0.007449, tar: 0.000633 
l0: 0.000631, l1: 0.000607, l2: 0.000667, l3: 0.000717, l4: 0.001152, l5: 0.002336, l6: 0.003019
[epoch:  94/1000, batch:   724/ 1052, ite: 24640] train loss: 0.007440, tar: 0.000632 
l0: 0.000556, l1: 0.000512, l2: 0.000539, l3: 0.000686, l4: 0.001079, l5: 0.001510, l6: 0.002601
[epoch:  94/1000, batch:   804/ 1052, ite: 24660] train loss: 0.007431, tar: 0.000632 
l0: 0.000675, l1: 0.000680, l2: 0.000702, l3: 0.000761, l4: 0.001078, l5: 0.001917, l6: 0.002280
[epoch:  94/1000, batch:   884/ 1052, ite: 24680] train loss: 0.007432, tar: 0.000632 
l0: 0.000837, l1: 0.000898, l2: 0.000922, l3: 0.000881, l4: 0.001583, l5: 0.002317, l6: 0.003366
[epoch:  94/1000, batch:   964/ 1052, ite: 24700] train loss: 0.007430, tar: 0.000632 
l0: 0.000639, l1: 0.000678, l2: 0.000638, l3: 0.000750, l4: 0.000736, l5: 0.001136, l6: 0.002070
[epoch:  94/1000, batch:  1044/ 1052, ite: 24720] train loss: 0.007433, tar: 0.000632 
[Epoch 94/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000444, l1: 0.000456, l2: 0.000487, l3: 0.000515, l4: 0.000639, l5: 0.001180, l6: 0.003131
[epoch:  95/1000, batch:    72/ 1052, ite: 24740] train loss: 0.007434, tar: 0.000632 
l0: 0.001110, l1: 0.001230, l2: 0.001123, l3: 0.001442, l4: 0.001370, l5: 0.001813, l6: 0.003827
[epoch:  95/1000, batch:   152/ 1052, ite: 24760] train loss: 0.007433, tar: 0.000632 
l0: 0.000674, l1: 0.000647, l2: 0.000638, l3: 0.000851, l4: 0.001161, l5: 0.001861, l6: 0.003207
[epoch:  95/1000, batch:   232/ 1052, ite: 24780] train loss: 0.007431, tar: 0.000632 
l0: 0.000507, l1: 0.000524, l2: 0.000493, l3: 0.000542, l4: 0.000746, l5: 0.001472, l6: 0.002224
[epoch:  95/1000, batch:   312/ 1052, ite: 24800] train loss: 0.007432, tar: 0.000631 
l0: 0.000465, l1: 0.000445, l2: 0.000432, l3: 0.000557, l4: 0.000830, l5: 0.001204, l6: 0.002199
[epoch:  95/1000, batch:   392/ 1052, ite: 24820] train loss: 0.007429, tar: 0.000631 
l0: 0.000991, l1: 0.001041, l2: 0.001180, l3: 0.000859, l4: 0.001143, l5: 0.001496, l6: 0.003118
[epoch:  95/1000, batch:   472/ 1052, ite: 24840] train loss: 0.007423, tar: 0.000631 
l0: 0.000442, l1: 0.000420, l2: 0.000482, l3: 0.000557, l4: 0.000869, l5: 0.001369, l6: 0.001946
[epoch:  95/1000, batch:   552/ 1052, ite: 24860] train loss: 0.007414, tar: 0.000630 
l0: 0.000384, l1: 0.000377, l2: 0.000426, l3: 0.000518, l4: 0.000661, l5: 0.001028, l6: 0.001657
[epoch:  95/1000, batch:   632/ 1052, ite: 24880] train loss: 0.007413, tar: 0.000630 
l0: 0.000553, l1: 0.000557, l2: 0.000619, l3: 0.000756, l4: 0.000808, l5: 0.001558, l6: 0.002011
[epoch:  95/1000, batch:   712/ 1052, ite: 24900] train loss: 0.007407, tar: 0.000629 
l0: 0.000597, l1: 0.000590, l2: 0.000616, l3: 0.000634, l4: 0.000794, l5: 0.001254, l6: 0.002235
[epoch:  95/1000, batch:   792/ 1052, ite: 24920] train loss: 0.007405, tar: 0.000628 
l0: 0.000383, l1: 0.000383, l2: 0.000424, l3: 0.000422, l4: 0.000716, l5: 0.001343, l6: 0.001886
[epoch:  95/1000, batch:   872/ 1052, ite: 24940] train loss: 0.007399, tar: 0.000628 
l0: 0.000617, l1: 0.000722, l2: 0.000647, l3: 0.000710, l4: 0.000816, l5: 0.001514, l6: 0.002572
[epoch:  95/1000, batch:   952/ 1052, ite: 24960] train loss: 0.007406, tar: 0.000629 
l0: 0.000530, l1: 0.000502, l2: 0.000581, l3: 0.000648, l4: 0.000710, l5: 0.001348, l6: 0.002207
[epoch:  95/1000, batch:  1032/ 1052, ite: 24980] train loss: 0.007415, tar: 0.000631 
[Epoch 95/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000942, l1: 0.001220, l2: 0.001076, l3: 0.000929, l4: 0.001013, l5: 0.001298, l6: 0.002154
[epoch:  96/1000, batch:    60/ 1052, ite: 25000] train loss: 0.007423, tar: 0.000631 
l0: 0.000229, l1: 0.000214, l2: 0.000248, l3: 0.000327, l4: 0.000562, l5: 0.001076, l6: 0.001239
[epoch:  96/1000, batch:   140/ 1052, ite: 25020] train loss: 0.007423, tar: 0.000631 
l0: 0.000241, l1: 0.000255, l2: 0.000237, l3: 0.000278, l4: 0.000409, l5: 0.000674, l6: 0.001072
[epoch:  96/1000, batch:   220/ 1052, ite: 25040] train loss: 0.007428, tar: 0.000632 
l0: 0.000389, l1: 0.000378, l2: 0.000385, l3: 0.000409, l4: 0.000569, l5: 0.001122, l6: 0.002168
[epoch:  96/1000, batch:   300/ 1052, ite: 25060] train loss: 0.007425, tar: 0.000631 
l0: 0.000818, l1: 0.000818, l2: 0.000754, l3: 0.001046, l4: 0.001333, l5: 0.001881, l6: 0.002804
[epoch:  96/1000, batch:   380/ 1052, ite: 25080] train loss: 0.007456, tar: 0.000634 
l0: 0.000502, l1: 0.000537, l2: 0.000562, l3: 0.000601, l4: 0.000843, l5: 0.000958, l6: 0.001413
[epoch:  96/1000, batch:   460/ 1052, ite: 25100] train loss: 0.007457, tar: 0.000634 
l0: 0.000905, l1: 0.000981, l2: 0.000838, l3: 0.001369, l4: 0.001461, l5: 0.001427, l6: 0.001904
[epoch:  96/1000, batch:   540/ 1052, ite: 25120] train loss: 0.007459, tar: 0.000634 
l0: 0.000322, l1: 0.000354, l2: 0.000345, l3: 0.000352, l4: 0.000514, l5: 0.000981, l6: 0.001003
[epoch:  96/1000, batch:   620/ 1052, ite: 25140] train loss: 0.007465, tar: 0.000635 
l0: 0.000810, l1: 0.000846, l2: 0.000781, l3: 0.000781, l4: 0.000841, l5: 0.001370, l6: 0.001563
[epoch:  96/1000, batch:   700/ 1052, ite: 25160] train loss: 0.007463, tar: 0.000635 
l0: 0.000376, l1: 0.000381, l2: 0.000378, l3: 0.000473, l4: 0.000581, l5: 0.001127, l6: 0.002446
[epoch:  96/1000, batch:   780/ 1052, ite: 25180] train loss: 0.007466, tar: 0.000635 
l0: 0.000278, l1: 0.000278, l2: 0.000285, l3: 0.000286, l4: 0.000443, l5: 0.000845, l6: 0.001469
[epoch:  96/1000, batch:   860/ 1052, ite: 25200] train loss: 0.007461, tar: 0.000635 
l0: 0.000519, l1: 0.000493, l2: 0.000657, l3: 0.000695, l4: 0.000947, l5: 0.001380, l6: 0.002169
[epoch:  96/1000, batch:   940/ 1052, ite: 25220] train loss: 0.007456, tar: 0.000634 
l0: 0.001215, l1: 0.001829, l2: 0.001791, l3: 0.001109, l4: 0.001296, l5: 0.002184, l6: 0.003170
[epoch:  96/1000, batch:  1020/ 1052, ite: 25240] train loss: 0.007454, tar: 0.000634 
[Epoch 96/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000301, l1: 0.000300, l2: 0.000308, l3: 0.000389, l4: 0.000471, l5: 0.001163, l6: 0.001384
[epoch:  97/1000, batch:    48/ 1052, ite: 25260] train loss: 0.007452, tar: 0.000634 
l0: 0.000525, l1: 0.000519, l2: 0.000529, l3: 0.000604, l4: 0.000753, l5: 0.001287, l6: 0.001508
[epoch:  97/1000, batch:   128/ 1052, ite: 25280] train loss: 0.007446, tar: 0.000633 
l0: 0.000429, l1: 0.000441, l2: 0.000459, l3: 0.000420, l4: 0.000582, l5: 0.001229, l6: 0.002018
[epoch:  97/1000, batch:   208/ 1052, ite: 25300] train loss: 0.007443, tar: 0.000633 
l0: 0.000180, l1: 0.000190, l2: 0.000199, l3: 0.000212, l4: 0.000281, l5: 0.000653, l6: 0.001083
[epoch:  97/1000, batch:   288/ 1052, ite: 25320] train loss: 0.007439, tar: 0.000633 
l0: 0.000532, l1: 0.000521, l2: 0.000583, l3: 0.000622, l4: 0.000696, l5: 0.001280, l6: 0.002017
[epoch:  97/1000, batch:   368/ 1052, ite: 25340] train loss: 0.007435, tar: 0.000632 
l0: 0.000932, l1: 0.000891, l2: 0.000953, l3: 0.001112, l4: 0.001286, l5: 0.001834, l6: 0.001346
[epoch:  97/1000, batch:   448/ 1052, ite: 25360] train loss: 0.007438, tar: 0.000633 
l0: 0.000582, l1: 0.000620, l2: 0.000619, l3: 0.000735, l4: 0.000937, l5: 0.001689, l6: 0.003186
[epoch:  97/1000, batch:   528/ 1052, ite: 25380] train loss: 0.007437, tar: 0.000632 
l0: 0.000632, l1: 0.000636, l2: 0.000627, l3: 0.000638, l4: 0.000820, l5: 0.001442, l6: 0.002621
[epoch:  97/1000, batch:   608/ 1052, ite: 25400] train loss: 0.007440, tar: 0.000633 
l0: 0.000703, l1: 0.000667, l2: 0.000758, l3: 0.000814, l4: 0.001077, l5: 0.001437, l6: 0.002466
[epoch:  97/1000, batch:   688/ 1052, ite: 25420] train loss: 0.007440, tar: 0.000633 
l0: 0.000271, l1: 0.000263, l2: 0.000262, l3: 0.000333, l4: 0.000444, l5: 0.000813, l6: 0.001338
[epoch:  97/1000, batch:   768/ 1052, ite: 25440] train loss: 0.007438, tar: 0.000632 
l0: 0.000942, l1: 0.000921, l2: 0.001177, l3: 0.001321, l4: 0.001849, l5: 0.002432, l6: 0.003544
[epoch:  97/1000, batch:   848/ 1052, ite: 25460] train loss: 0.007451, tar: 0.000634 
l0: 0.000457, l1: 0.000444, l2: 0.000585, l3: 0.000409, l4: 0.000507, l5: 0.001113, l6: 0.001286
[epoch:  97/1000, batch:   928/ 1052, ite: 25480] train loss: 0.007449, tar: 0.000634 
l0: 0.000574, l1: 0.000553, l2: 0.000666, l3: 0.001166, l4: 0.001265, l5: 0.001619, l6: 0.003703
[epoch:  97/1000, batch:  1008/ 1052, ite: 25500] train loss: 0.007451, tar: 0.000634 
[Epoch 97/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000733, l1: 0.000734, l2: 0.000726, l3: 0.000853, l4: 0.001087, l5: 0.001736, l6: 0.003073
[epoch:  98/1000, batch:    36/ 1052, ite: 25520] train loss: 0.007461, tar: 0.000635 
l0: 0.000952, l1: 0.000900, l2: 0.001001, l3: 0.001194, l4: 0.001693, l5: 0.002148, l6: 0.003399
[epoch:  98/1000, batch:   116/ 1052, ite: 25540] train loss: 0.007457, tar: 0.000635 
l0: 0.000659, l1: 0.000695, l2: 0.000709, l3: 0.000697, l4: 0.000858, l5: 0.001524, l6: 0.002483
[epoch:  98/1000, batch:   196/ 1052, ite: 25560] train loss: 0.007457, tar: 0.000635 
l0: 0.001401, l1: 0.001982, l2: 0.001097, l3: 0.001205, l4: 0.001119, l5: 0.001080, l6: 0.002212
[epoch:  98/1000, batch:   276/ 1052, ite: 25580] train loss: 0.007464, tar: 0.000636 
l0: 0.000520, l1: 0.000517, l2: 0.000594, l3: 0.000637, l4: 0.000914, l5: 0.001416, l6: 0.002249
[epoch:  98/1000, batch:   356/ 1052, ite: 25600] train loss: 0.007465, tar: 0.000636 
l0: 0.000301, l1: 0.000303, l2: 0.000342, l3: 0.000405, l4: 0.000423, l5: 0.001038, l6: 0.001654
[epoch:  98/1000, batch:   436/ 1052, ite: 25620] train loss: 0.007463, tar: 0.000636 
l0: 0.000311, l1: 0.000316, l2: 0.000320, l3: 0.000355, l4: 0.000534, l5: 0.001000, l6: 0.001343
[epoch:  98/1000, batch:   516/ 1052, ite: 25640] train loss: 0.007462, tar: 0.000636 
l0: 0.000322, l1: 0.000397, l2: 0.000496, l3: 0.000465, l4: 0.000538, l5: 0.000961, l6: 0.001961
[epoch:  98/1000, batch:   596/ 1052, ite: 25660] train loss: 0.007461, tar: 0.000636 
l0: 0.000342, l1: 0.000362, l2: 0.000355, l3: 0.000382, l4: 0.000509, l5: 0.000954, l6: 0.001624
[epoch:  98/1000, batch:   676/ 1052, ite: 25680] train loss: 0.007462, tar: 0.000636 
l0: 0.000501, l1: 0.000499, l2: 0.000579, l3: 0.000663, l4: 0.000996, l5: 0.001596, l6: 0.002674
[epoch:  98/1000, batch:   756/ 1052, ite: 25700] train loss: 0.007461, tar: 0.000635 
l0: 0.000411, l1: 0.000440, l2: 0.000438, l3: 0.000466, l4: 0.000580, l5: 0.000935, l6: 0.001162
[epoch:  98/1000, batch:   836/ 1052, ite: 25720] train loss: 0.007460, tar: 0.000635 
l0: 0.000461, l1: 0.000483, l2: 0.000479, l3: 0.000505, l4: 0.000631, l5: 0.000983, l6: 0.001600
[epoch:  98/1000, batch:   916/ 1052, ite: 25740] train loss: 0.007457, tar: 0.000635 
l0: 0.001342, l1: 0.001455, l2: 0.001425, l3: 0.001525, l4: 0.001857, l5: 0.002210, l6: 0.003462
[epoch:  98/1000, batch:   996/ 1052, ite: 25760] train loss: 0.007456, tar: 0.000635 
[Epoch 98/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000441, l1: 0.000430, l2: 0.000481, l3: 0.000649, l4: 0.000746, l5: 0.001132, l6: 0.002093
[epoch:  99/1000, batch:    24/ 1052, ite: 25780] train loss: 0.007454, tar: 0.000635 
l0: 0.000268, l1: 0.000315, l2: 0.000286, l3: 0.000376, l4: 0.000636, l5: 0.000944, l6: 0.001821
[epoch:  99/1000, batch:   104/ 1052, ite: 25800] train loss: 0.007453, tar: 0.000635 
l0: 0.000535, l1: 0.000543, l2: 0.000523, l3: 0.000726, l4: 0.000803, l5: 0.001031, l6: 0.001545
[epoch:  99/1000, batch:   184/ 1052, ite: 25820] train loss: 0.007451, tar: 0.000635 
l0: 0.000434, l1: 0.000418, l2: 0.000437, l3: 0.000472, l4: 0.000606, l5: 0.000974, l6: 0.001748
[epoch:  99/1000, batch:   264/ 1052, ite: 25840] train loss: 0.007448, tar: 0.000634 
l0: 0.000340, l1: 0.000322, l2: 0.000360, l3: 0.000477, l4: 0.000625, l5: 0.000975, l6: 0.001755
[epoch:  99/1000, batch:   344/ 1052, ite: 25860] train loss: 0.007446, tar: 0.000634 
l0: 0.000604, l1: 0.000683, l2: 0.000587, l3: 0.000579, l4: 0.000643, l5: 0.000990, l6: 0.001648
[epoch:  99/1000, batch:   424/ 1052, ite: 25880] train loss: 0.007445, tar: 0.000634 
l0: 0.000456, l1: 0.000468, l2: 0.000505, l3: 0.000630, l4: 0.000660, l5: 0.001252, l6: 0.001977
[epoch:  99/1000, batch:   504/ 1052, ite: 25900] train loss: 0.007444, tar: 0.000634 
l0: 0.001036, l1: 0.000974, l2: 0.001031, l3: 0.001362, l4: 0.002198, l5: 0.003275, l6: 0.005333
[epoch:  99/1000, batch:   584/ 1052, ite: 25920] train loss: 0.007443, tar: 0.000633 
l0: 0.000372, l1: 0.000395, l2: 0.000376, l3: 0.000394, l4: 0.000504, l5: 0.000702, l6: 0.001530
[epoch:  99/1000, batch:   664/ 1052, ite: 25940] train loss: 0.007439, tar: 0.000633 
l0: 0.001166, l1: 0.001444, l2: 0.001530, l3: 0.001371, l4: 0.001928, l5: 0.002593, l6: 0.003457
[epoch:  99/1000, batch:   744/ 1052, ite: 25960] train loss: 0.007440, tar: 0.000633 
l0: 0.000693, l1: 0.000708, l2: 0.000771, l3: 0.000741, l4: 0.001144, l5: 0.002111, l6: 0.003265
[epoch:  99/1000, batch:   824/ 1052, ite: 25980] train loss: 0.007433, tar: 0.000632 
l0: 0.000439, l1: 0.000418, l2: 0.000536, l3: 0.000549, l4: 0.000729, l5: 0.001536, l6: 0.001605
[epoch:  99/1000, batch:   904/ 1052, ite: 26000] train loss: 0.007429, tar: 0.000632 
l0: 0.000304, l1: 0.000300, l2: 0.000332, l3: 0.000388, l4: 0.000425, l5: 0.000940, l6: 0.001499
[epoch:  99/1000, batch:   984/ 1052, ite: 26020] train loss: 0.007428, tar: 0.000632 
[Epoch 99/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000511, l1: 0.000515, l2: 0.000529, l3: 0.000608, l4: 0.000795, l5: 0.001599, l6: 0.002293
[epoch: 100/1000, batch:    12/ 1052, ite: 26040] train loss: 0.007424, tar: 0.000631 
l0: 0.000291, l1: 0.000306, l2: 0.000277, l3: 0.000306, l4: 0.000447, l5: 0.001062, l6: 0.001534
[epoch: 100/1000, batch:    92/ 1052, ite: 26060] train loss: 0.007419, tar: 0.000631 
l0: 0.000456, l1: 0.000473, l2: 0.000463, l3: 0.000528, l4: 0.000883, l5: 0.001438, l6: 0.002449
[epoch: 100/1000, batch:   172/ 1052, ite: 26080] train loss: 0.007415, tar: 0.000631 
l0: 0.000559, l1: 0.000599, l2: 0.000572, l3: 0.000620, l4: 0.000760, l5: 0.001226, l6: 0.002604
[epoch: 100/1000, batch:   252/ 1052, ite: 26100] train loss: 0.007411, tar: 0.000630 
l0: 0.000507, l1: 0.000507, l2: 0.000550, l3: 0.000706, l4: 0.000997, l5: 0.001367, l6: 0.001763
[epoch: 100/1000, batch:   332/ 1052, ite: 26120] train loss: 0.007411, tar: 0.000630 
l0: 0.000402, l1: 0.000416, l2: 0.000417, l3: 0.000496, l4: 0.000606, l5: 0.001224, l6: 0.001595
[epoch: 100/1000, batch:   412/ 1052, ite: 26140] train loss: 0.007414, tar: 0.000630 
l0: 0.000556, l1: 0.000742, l2: 0.000540, l3: 0.000676, l4: 0.000794, l5: 0.001436, l6: 0.001727
[epoch: 100/1000, batch:   492/ 1052, ite: 26160] train loss: 0.007412, tar: 0.000630 
l0: 0.000369, l1: 0.000355, l2: 0.000385, l3: 0.000415, l4: 0.000587, l5: 0.000883, l6: 0.001528
[epoch: 100/1000, batch:   572/ 1052, ite: 26180] train loss: 0.007409, tar: 0.000630 
l0: 0.000466, l1: 0.000487, l2: 0.000572, l3: 0.000583, l4: 0.000733, l5: 0.001145, l6: 0.002501
[epoch: 100/1000, batch:   652/ 1052, ite: 26200] train loss: 0.007408, tar: 0.000630 
l0: 0.002001, l1: 0.002450, l2: 0.002804, l3: 0.002325, l4: 0.002201, l5: 0.003534, l6: 0.007026
[epoch: 100/1000, batch:   732/ 1052, ite: 26220] train loss: 0.007407, tar: 0.000629 
l0: 0.001192, l1: 0.001262, l2: 0.001263, l3: 0.001184, l4: 0.001163, l5: 0.002169, l6: 0.003492
[epoch: 100/1000, batch:   812/ 1052, ite: 26240] train loss: 0.007402, tar: 0.000629 
l0: 0.001035, l1: 0.001147, l2: 0.001112, l3: 0.000963, l4: 0.001022, l5: 0.001872, l6: 0.003278
[epoch: 100/1000, batch:   892/ 1052, ite: 26260] train loss: 0.007404, tar: 0.000629 
l0: 0.000579, l1: 0.000589, l2: 0.000583, l3: 0.000606, l4: 0.001049, l5: 0.001568, l6: 0.002224
[epoch: 100/1000, batch:   972/ 1052, ite: 26280] train loss: 0.007400, tar: 0.000629 
l0: 0.000496, l1: 0.000486, l2: 0.000482, l3: 0.000683, l4: 0.000916, l5: 0.001455, l6: 0.001952
[epoch: 100/1000, batch:  1052/ 1052, ite: 26300] train loss: 0.007403, tar: 0.000629 
[Epoch 100/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000939, l1: 0.001082, l2: 0.000933, l3: 0.000946, l4: 0.001161, l5: 0.001605, l6: 0.003253
[epoch: 101/1000, batch:    80/ 1052, ite: 26320] train loss: 0.007404, tar: 0.000629 
l0: 0.000477, l1: 0.000477, l2: 0.000509, l3: 0.000570, l4: 0.000808, l5: 0.001693, l6: 0.002235
[epoch: 101/1000, batch:   160/ 1052, ite: 26340] train loss: 0.007401, tar: 0.000628 
l0: 0.000423, l1: 0.000467, l2: 0.000408, l3: 0.000437, l4: 0.000588, l5: 0.001096, l6: 0.001992
[epoch: 101/1000, batch:   240/ 1052, ite: 26360] train loss: 0.007396, tar: 0.000628 
l0: 0.000279, l1: 0.000277, l2: 0.000291, l3: 0.000321, l4: 0.000553, l5: 0.000726, l6: 0.001707
[epoch: 101/1000, batch:   320/ 1052, ite: 26380] train loss: 0.007392, tar: 0.000627 
l0: 0.000222, l1: 0.000218, l2: 0.000229, l3: 0.000285, l4: 0.000308, l5: 0.000684, l6: 0.001004
[epoch: 101/1000, batch:   400/ 1052, ite: 26400] train loss: 0.007388, tar: 0.000627 
l0: 0.000552, l1: 0.000596, l2: 0.000561, l3: 0.000565, l4: 0.000692, l5: 0.001417, l6: 0.002466
[epoch: 101/1000, batch:   480/ 1052, ite: 26420] train loss: 0.007385, tar: 0.000627 
l0: 0.000471, l1: 0.000461, l2: 0.000497, l3: 0.000537, l4: 0.000667, l5: 0.001198, l6: 0.002409
[epoch: 101/1000, batch:   560/ 1052, ite: 26440] train loss: 0.007383, tar: 0.000626 
l0: 0.000343, l1: 0.000330, l2: 0.000333, l3: 0.000376, l4: 0.000504, l5: 0.000429, l6: 0.001283
[epoch: 101/1000, batch:   640/ 1052, ite: 26460] train loss: 0.007383, tar: 0.000626 
l0: 0.000262, l1: 0.000272, l2: 0.000260, l3: 0.000345, l4: 0.000382, l5: 0.000676, l6: 0.001140
[epoch: 101/1000, batch:   720/ 1052, ite: 26480] train loss: 0.007380, tar: 0.000626 
l0: 0.000649, l1: 0.000635, l2: 0.000700, l3: 0.000856, l4: 0.001152, l5: 0.001555, l6: 0.002512
[epoch: 101/1000, batch:   800/ 1052, ite: 26500] train loss: 0.007378, tar: 0.000626 
l0: 0.000145, l1: 0.000151, l2: 0.000165, l3: 0.000180, l4: 0.000221, l5: 0.000524, l6: 0.000760
[epoch: 101/1000, batch:   880/ 1052, ite: 26520] train loss: 0.007374, tar: 0.000625 
l0: 0.000441, l1: 0.000474, l2: 0.000503, l3: 0.000504, l4: 0.000634, l5: 0.001010, l6: 0.002319
[epoch: 101/1000, batch:   960/ 1052, ite: 26540] train loss: 0.007371, tar: 0.000625 
l0: 0.000919, l1: 0.000954, l2: 0.000917, l3: 0.000913, l4: 0.001069, l5: 0.001464, l6: 0.001860
[epoch: 101/1000, batch:  1040/ 1052, ite: 26560] train loss: 0.007373, tar: 0.000625 
[Epoch 101/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000283, l1: 0.000249, l2: 0.000299, l3: 0.000445, l4: 0.000607, l5: 0.000971, l6: 0.001243
[epoch: 102/1000, batch:    68/ 1052, ite: 26580] train loss: 0.007379, tar: 0.000625 
l0: 0.000556, l1: 0.000547, l2: 0.000586, l3: 0.000742, l4: 0.001152, l5: 0.002327, l6: 0.004093
[epoch: 102/1000, batch:   148/ 1052, ite: 26600] train loss: 0.007375, tar: 0.000625 
l0: 0.000698, l1: 0.000740, l2: 0.000791, l3: 0.000803, l4: 0.001022, l5: 0.001959, l6: 0.003171
[epoch: 102/1000, batch:   228/ 1052, ite: 26620] train loss: 0.007369, tar: 0.000624 
l0: 0.000520, l1: 0.000509, l2: 0.000538, l3: 0.000604, l4: 0.000751, l5: 0.001329, l6: 0.001718
[epoch: 102/1000, batch:   308/ 1052, ite: 26640] train loss: 0.007367, tar: 0.000624 
l0: 0.000428, l1: 0.000397, l2: 0.000459, l3: 0.000538, l4: 0.000723, l5: 0.001216, l6: 0.001765
[epoch: 102/1000, batch:   388/ 1052, ite: 26660] train loss: 0.007374, tar: 0.000625 
l0: 0.000716, l1: 0.000701, l2: 0.000780, l3: 0.000961, l4: 0.001411, l5: 0.002525, l6: 0.003006
[epoch: 102/1000, batch:   468/ 1052, ite: 26680] train loss: 0.007375, tar: 0.000626 
l0: 0.000402, l1: 0.000398, l2: 0.000408, l3: 0.000368, l4: 0.000539, l5: 0.000870, l6: 0.001077
[epoch: 102/1000, batch:   548/ 1052, ite: 26700] train loss: 0.007381, tar: 0.000626 
l0: 0.000451, l1: 0.000546, l2: 0.000527, l3: 0.000531, l4: 0.000627, l5: 0.001152, l6: 0.001280
[epoch: 102/1000, batch:   628/ 1052, ite: 26720] train loss: 0.007385, tar: 0.000627 
l0: 0.000368, l1: 0.000355, l2: 0.000373, l3: 0.000515, l4: 0.000700, l5: 0.001296, l6: 0.002291
[epoch: 102/1000, batch:   708/ 1052, ite: 26740] train loss: 0.007385, tar: 0.000627 
l0: 0.001151, l1: 0.001183, l2: 0.001131, l3: 0.001308, l4: 0.001381, l5: 0.002187, l6: 0.003679
[epoch: 102/1000, batch:   788/ 1052, ite: 26760] train loss: 0.007381, tar: 0.000626 
l0: 0.001725, l1: 0.001897, l2: 0.001860, l3: 0.001531, l4: 0.001378, l5: 0.002609, l6: 0.004211
[epoch: 102/1000, batch:   868/ 1052, ite: 26780] train loss: 0.007383, tar: 0.000627 
l0: 0.000285, l1: 0.000288, l2: 0.000391, l3: 0.000422, l4: 0.000629, l5: 0.000936, l6: 0.001659
[epoch: 102/1000, batch:   948/ 1052, ite: 26800] train loss: 0.007382, tar: 0.000627 
l0: 0.000511, l1: 0.000523, l2: 0.000499, l3: 0.000600, l4: 0.000829, l5: 0.001467, l6: 0.002225
[epoch: 102/1000, batch:  1028/ 1052, ite: 26820] train loss: 0.007382, tar: 0.000626 
[Epoch 102/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.001004, l1: 0.000929, l2: 0.001049, l3: 0.001186, l4: 0.001463, l5: 0.002225, l6: 0.004199
[epoch: 103/1000, batch:    56/ 1052, ite: 26840] train loss: 0.007386, tar: 0.000627 
l0: 0.000353, l1: 0.000353, l2: 0.000378, l3: 0.000390, l4: 0.000621, l5: 0.001277, l6: 0.001660
[epoch: 103/1000, batch:   136/ 1052, ite: 26860] train loss: 0.007391, tar: 0.000627 
l0: 0.000301, l1: 0.000298, l2: 0.000319, l3: 0.000395, l4: 0.000564, l5: 0.001191, l6: 0.001199
[epoch: 103/1000, batch:   216/ 1052, ite: 26880] train loss: 0.007386, tar: 0.000627 
l0: 0.001097, l1: 0.001148, l2: 0.001289, l3: 0.001124, l4: 0.001315, l5: 0.001882, l6: 0.003326
[epoch: 103/1000, batch:   296/ 1052, ite: 26900] train loss: 0.007387, tar: 0.000627 
l0: 0.000420, l1: 0.000405, l2: 0.000481, l3: 0.000594, l4: 0.000620, l5: 0.000824, l6: 0.001980
[epoch: 103/1000, batch:   376/ 1052, ite: 26920] train loss: 0.007387, tar: 0.000627 
l0: 0.000482, l1: 0.000456, l2: 0.000526, l3: 0.000622, l4: 0.000731, l5: 0.001274, l6: 0.002383
[epoch: 103/1000, batch:   456/ 1052, ite: 26940] train loss: 0.007386, tar: 0.000627 
l0: 0.000636, l1: 0.000629, l2: 0.000686, l3: 0.000777, l4: 0.000957, l5: 0.001460, l6: 0.001611
[epoch: 103/1000, batch:   536/ 1052, ite: 26960] train loss: 0.007387, tar: 0.000627 
l0: 0.000535, l1: 0.000585, l2: 0.000578, l3: 0.000555, l4: 0.000531, l5: 0.000815, l6: 0.001292
[epoch: 103/1000, batch:   616/ 1052, ite: 26980] train loss: 0.007384, tar: 0.000627 
l0: 0.000340, l1: 0.000366, l2: 0.000432, l3: 0.000493, l4: 0.000542, l5: 0.001115, l6: 0.001608
[epoch: 103/1000, batch:   696/ 1052, ite: 27000] train loss: 0.007383, tar: 0.000626 
l0: 0.000704, l1: 0.000707, l2: 0.000714, l3: 0.000802, l4: 0.000998, l5: 0.001950, l6: 0.002957
[epoch: 103/1000, batch:   776/ 1052, ite: 27020] train loss: 0.007379, tar: 0.000626 
l0: 0.000529, l1: 0.000567, l2: 0.000538, l3: 0.000551, l4: 0.000610, l5: 0.001003, l6: 0.001901
[epoch: 103/1000, batch:   856/ 1052, ite: 27040] train loss: 0.007374, tar: 0.000625 
l0: 0.000392, l1: 0.000357, l2: 0.000427, l3: 0.000438, l4: 0.000603, l5: 0.000926, l6: 0.001169
[epoch: 103/1000, batch:   936/ 1052, ite: 27060] train loss: 0.007371, tar: 0.000625 
l0: 0.000361, l1: 0.000341, l2: 0.000386, l3: 0.000460, l4: 0.000584, l5: 0.001307, l6: 0.001552
[epoch: 103/1000, batch:  1016/ 1052, ite: 27080] train loss: 0.007370, tar: 0.000625 
[Epoch 103/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000512, l1: 0.000501, l2: 0.000518, l3: 0.000605, l4: 0.000732, l5: 0.001005, l6: 0.002099
[epoch: 104/1000, batch:    44/ 1052, ite: 27100] train loss: 0.007363, tar: 0.000624 
l0: 0.000721, l1: 0.000707, l2: 0.000773, l3: 0.000819, l4: 0.001191, l5: 0.001600, l6: 0.002865
[epoch: 104/1000, batch:   124/ 1052, ite: 27120] train loss: 0.007362, tar: 0.000624 
l0: 0.000640, l1: 0.000652, l2: 0.000620, l3: 0.000986, l4: 0.001400, l5: 0.001376, l6: 0.001954
[epoch: 104/1000, batch:   204/ 1052, ite: 27140] train loss: 0.007363, tar: 0.000624 
l0: 0.000327, l1: 0.000315, l2: 0.000350, l3: 0.000357, l4: 0.000576, l5: 0.001222, l6: 0.001698
[epoch: 104/1000, batch:   284/ 1052, ite: 27160] train loss: 0.007366, tar: 0.000624 
l0: 0.000336, l1: 0.000341, l2: 0.000395, l3: 0.000441, l4: 0.000805, l5: 0.001326, l6: 0.002876
[epoch: 104/1000, batch:   364/ 1052, ite: 27180] train loss: 0.007363, tar: 0.000624 
l0: 0.000339, l1: 0.000328, l2: 0.000360, l3: 0.000453, l4: 0.000618, l5: 0.000883, l6: 0.001157
[epoch: 104/1000, batch:   444/ 1052, ite: 27200] train loss: 0.007360, tar: 0.000624 
l0: 0.001982, l1: 0.002078, l2: 0.002040, l3: 0.001737, l4: 0.001822, l5: 0.002399, l6: 0.003659
[epoch: 104/1000, batch:   524/ 1052, ite: 27220] train loss: 0.007361, tar: 0.000624 
l0: 0.000296, l1: 0.000301, l2: 0.000296, l3: 0.000364, l4: 0.000506, l5: 0.000707, l6: 0.001450
[epoch: 104/1000, batch:   604/ 1052, ite: 27240] train loss: 0.007360, tar: 0.000624 
l0: 0.000258, l1: 0.000248, l2: 0.000245, l3: 0.000621, l4: 0.000559, l5: 0.001166, l6: 0.001625
[epoch: 104/1000, batch:   684/ 1052, ite: 27260] train loss: 0.007358, tar: 0.000623 
l0: 0.000929, l1: 0.001289, l2: 0.000628, l3: 0.000581, l4: 0.000795, l5: 0.001332, l6: 0.001635
[epoch: 104/1000, batch:   764/ 1052, ite: 27280] train loss: 0.007354, tar: 0.000623 
l0: 0.000351, l1: 0.000406, l2: 0.000387, l3: 0.000330, l4: 0.000508, l5: 0.001071, l6: 0.001115
[epoch: 104/1000, batch:   844/ 1052, ite: 27300] train loss: 0.007351, tar: 0.000623 
l0: 0.000370, l1: 0.000381, l2: 0.000387, l3: 0.000457, l4: 0.000626, l5: 0.001211, l6: 0.002625
[epoch: 104/1000, batch:   924/ 1052, ite: 27320] train loss: 0.007349, tar: 0.000623 
l0: 0.000850, l1: 0.001156, l2: 0.000960, l3: 0.000977, l4: 0.001286, l5: 0.001744, l6: 0.003404
[epoch: 104/1000, batch:  1004/ 1052, ite: 27340] train loss: 0.007347, tar: 0.000622 
[Epoch 104/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000463, l1: 0.000516, l2: 0.000496, l3: 0.000420, l4: 0.000643, l5: 0.000918, l6: 0.001472
[epoch: 105/1000, batch:    32/ 1052, ite: 27360] train loss: 0.007346, tar: 0.000622 
l0: 0.000256, l1: 0.000273, l2: 0.000276, l3: 0.000261, l4: 0.000422, l5: 0.000805, l6: 0.001594
[epoch: 105/1000, batch:   112/ 1052, ite: 27380] train loss: 0.007342, tar: 0.000622 
l0: 0.000263, l1: 0.000261, l2: 0.000290, l3: 0.000330, l4: 0.000492, l5: 0.001024, l6: 0.001355
[epoch: 105/1000, batch:   192/ 1052, ite: 27400] train loss: 0.007342, tar: 0.000622 
l0: 0.000489, l1: 0.000467, l2: 0.000490, l3: 0.000616, l4: 0.001023, l5: 0.001192, l6: 0.001833
[epoch: 105/1000, batch:   272/ 1052, ite: 27420] train loss: 0.007340, tar: 0.000622 
l0: 0.000732, l1: 0.000771, l2: 0.000739, l3: 0.000757, l4: 0.000846, l5: 0.001185, l6: 0.002467
[epoch: 105/1000, batch:   352/ 1052, ite: 27440] train loss: 0.007342, tar: 0.000622 
l0: 0.001006, l1: 0.000947, l2: 0.001055, l3: 0.000942, l4: 0.001411, l5: 0.001670, l6: 0.002716
[epoch: 105/1000, batch:   432/ 1052, ite: 27460] train loss: 0.007340, tar: 0.000621 
l0: 0.000424, l1: 0.000422, l2: 0.000439, l3: 0.000508, l4: 0.000705, l5: 0.001649, l6: 0.001770
[epoch: 105/1000, batch:   512/ 1052, ite: 27480] train loss: 0.007340, tar: 0.000621 
l0: 0.000542, l1: 0.000557, l2: 0.000584, l3: 0.000625, l4: 0.000778, l5: 0.001106, l6: 0.002263
[epoch: 105/1000, batch:   592/ 1052, ite: 27500] train loss: 0.007344, tar: 0.000622 
l0: 0.000329, l1: 0.000314, l2: 0.000390, l3: 0.000419, l4: 0.000615, l5: 0.000973, l6: 0.002158
[epoch: 105/1000, batch:   672/ 1052, ite: 27520] train loss: 0.007341, tar: 0.000621 
l0: 0.001332, l1: 0.001469, l2: 0.001435, l3: 0.001070, l4: 0.001384, l5: 0.002891, l6: 0.003611
[epoch: 105/1000, batch:   752/ 1052, ite: 27540] train loss: 0.007346, tar: 0.000622 
l0: 0.001454, l1: 0.001344, l2: 0.001744, l3: 0.001645, l4: 0.001822, l5: 0.003500, l6: 0.003815
[epoch: 105/1000, batch:   832/ 1052, ite: 27560] train loss: 0.007348, tar: 0.000622 
l0: 0.001451, l1: 0.001673, l2: 0.001476, l3: 0.001311, l4: 0.001368, l5: 0.002778, l6: 0.004496
[epoch: 105/1000, batch:   912/ 1052, ite: 27580] train loss: 0.007347, tar: 0.000622 
l0: 0.000461, l1: 0.000454, l2: 0.000463, l3: 0.000592, l4: 0.000799, l5: 0.001358, l6: 0.002406
[epoch: 105/1000, batch:   992/ 1052, ite: 27600] train loss: 0.007348, tar: 0.000622 
[Epoch 105/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000297, l1: 0.000301, l2: 0.000309, l3: 0.000345, l4: 0.000437, l5: 0.001017, l6: 0.001514
[epoch: 106/1000, batch:    20/ 1052, ite: 27620] train loss: 0.007352, tar: 0.000622 
l0: 0.000498, l1: 0.000469, l2: 0.000537, l3: 0.000795, l4: 0.000920, l5: 0.001313, l6: 0.002421
[epoch: 106/1000, batch:   100/ 1052, ite: 27640] train loss: 0.007352, tar: 0.000622 
l0: 0.000248, l1: 0.000233, l2: 0.000277, l3: 0.000311, l4: 0.000585, l5: 0.000817, l6: 0.001000
[epoch: 106/1000, batch:   180/ 1052, ite: 27660] train loss: 0.007353, tar: 0.000623 
l0: 0.000400, l1: 0.000373, l2: 0.000421, l3: 0.000446, l4: 0.000692, l5: 0.001019, l6: 0.001668
[epoch: 106/1000, batch:   260/ 1052, ite: 27680] train loss: 0.007353, tar: 0.000623 
l0: 0.001475, l1: 0.001485, l2: 0.001301, l3: 0.001672, l4: 0.002823, l5: 0.004515, l6: 0.003662
[epoch: 106/1000, batch:   340/ 1052, ite: 27700] train loss: 0.007353, tar: 0.000623 
l0: 0.001080, l1: 0.001137, l2: 0.001094, l3: 0.001305, l4: 0.001307, l5: 0.001944, l6: 0.003253
[epoch: 106/1000, batch:   420/ 1052, ite: 27720] train loss: 0.007351, tar: 0.000622 
l0: 0.000283, l1: 0.000268, l2: 0.000284, l3: 0.000432, l4: 0.000483, l5: 0.000768, l6: 0.001107
[epoch: 106/1000, batch:   500/ 1052, ite: 27740] train loss: 0.007348, tar: 0.000622 
l0: 0.000661, l1: 0.000657, l2: 0.000674, l3: 0.000747, l4: 0.001057, l5: 0.001521, l6: 0.003456
[epoch: 106/1000, batch:   580/ 1052, ite: 27760] train loss: 0.007350, tar: 0.000622 
l0: 0.000219, l1: 0.000220, l2: 0.000254, l3: 0.000231, l4: 0.000336, l5: 0.000796, l6: 0.001449
[epoch: 106/1000, batch:   660/ 1052, ite: 27780] train loss: 0.007345, tar: 0.000622 
l0: 0.000766, l1: 0.000876, l2: 0.000772, l3: 0.000776, l4: 0.000908, l5: 0.001490, l6: 0.002807
[epoch: 106/1000, batch:   740/ 1052, ite: 27800] train loss: 0.007342, tar: 0.000621 
l0: 0.000493, l1: 0.000482, l2: 0.000554, l3: 0.000583, l4: 0.001015, l5: 0.001404, l6: 0.002144
[epoch: 106/1000, batch:   820/ 1052, ite: 27820] train loss: 0.007344, tar: 0.000622 
l0: 0.000461, l1: 0.000456, l2: 0.000461, l3: 0.000486, l4: 0.000660, l5: 0.000885, l6: 0.001424
[epoch: 106/1000, batch:   900/ 1052, ite: 27840] train loss: 0.007346, tar: 0.000622 
l0: 0.001001, l1: 0.000995, l2: 0.000981, l3: 0.001268, l4: 0.001346, l5: 0.002176, l6: 0.003406
[epoch: 106/1000, batch:   980/ 1052, ite: 27860] train loss: 0.007344, tar: 0.000622 
[Epoch 106/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.001378, l1: 0.001443, l2: 0.001466, l3: 0.001575, l4: 0.001932, l5: 0.003102, l6: 0.004808
[epoch: 107/1000, batch:     8/ 1052, ite: 27880] train loss: 0.007349, tar: 0.000622 
l0: 0.000447, l1: 0.000446, l2: 0.000457, l3: 0.000537, l4: 0.000740, l5: 0.001617, l6: 0.002315
[epoch: 107/1000, batch:    88/ 1052, ite: 27900] train loss: 0.007347, tar: 0.000622 
l0: 0.000607, l1: 0.000603, l2: 0.000780, l3: 0.000883, l4: 0.000856, l5: 0.001235, l6: 0.001618
[epoch: 107/1000, batch:   168/ 1052, ite: 27920] train loss: 0.007348, tar: 0.000622 
l0: 0.000364, l1: 0.000368, l2: 0.000394, l3: 0.000467, l4: 0.000669, l5: 0.001258, l6: 0.001528
[epoch: 107/1000, batch:   248/ 1052, ite: 27940] train loss: 0.007345, tar: 0.000622 
l0: 0.000421, l1: 0.000416, l2: 0.000489, l3: 0.000480, l4: 0.000620, l5: 0.000723, l6: 0.002086
[epoch: 107/1000, batch:   328/ 1052, ite: 27960] train loss: 0.007346, tar: 0.000622 
l0: 0.000600, l1: 0.000619, l2: 0.000608, l3: 0.000662, l4: 0.001094, l5: 0.001794, l6: 0.003010
[epoch: 107/1000, batch:   408/ 1052, ite: 27980] train loss: 0.007343, tar: 0.000621 
l0: 0.000480, l1: 0.000445, l2: 0.000519, l3: 0.000609, l4: 0.000726, l5: 0.001212, l6: 0.001604
[epoch: 107/1000, batch:   488/ 1052, ite: 28000] train loss: 0.007340, tar: 0.000621 
l0: 0.001166, l1: 0.001229, l2: 0.001140, l3: 0.001526, l4: 0.001864, l5: 0.002122, l6: 0.003095
[epoch: 107/1000, batch:   568/ 1052, ite: 28020] train loss: 0.007345, tar: 0.000622 
l0: 0.000537, l1: 0.000555, l2: 0.000517, l3: 0.000548, l4: 0.000658, l5: 0.001435, l6: 0.001586
[epoch: 107/1000, batch:   648/ 1052, ite: 28040] train loss: 0.007344, tar: 0.000621 
l0: 0.000758, l1: 0.000797, l2: 0.000872, l3: 0.000853, l4: 0.000872, l5: 0.001275, l6: 0.001879
[epoch: 107/1000, batch:   728/ 1052, ite: 28060] train loss: 0.007341, tar: 0.000621 
l0: 0.000204, l1: 0.000198, l2: 0.000222, l3: 0.000263, l4: 0.000331, l5: 0.000890, l6: 0.001310
[epoch: 107/1000, batch:   808/ 1052, ite: 28080] train loss: 0.007342, tar: 0.000622 
l0: 0.000386, l1: 0.000385, l2: 0.000371, l3: 0.000405, l4: 0.000582, l5: 0.000988, l6: 0.001424
[epoch: 107/1000, batch:   888/ 1052, ite: 28100] train loss: 0.007341, tar: 0.000621 
l0: 0.000478, l1: 0.000490, l2: 0.000439, l3: 0.000554, l4: 0.000774, l5: 0.001302, l6: 0.002589
[epoch: 107/1000, batch:   968/ 1052, ite: 28120] train loss: 0.007338, tar: 0.000621 
l0: 0.000678, l1: 0.000673, l2: 0.000706, l3: 0.000792, l4: 0.001335, l5: 0.002310, l6: 0.003256
[epoch: 107/1000, batch:  1048/ 1052, ite: 28140] train loss: 0.007335, tar: 0.000621 
[Epoch 107/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000534, l1: 0.000521, l2: 0.000555, l3: 0.000642, l4: 0.001107, l5: 0.001815, l6: 0.002811
[epoch: 108/1000, batch:    76/ 1052, ite: 28160] train loss: 0.007334, tar: 0.000620 
l0: 0.000434, l1: 0.000481, l2: 0.000421, l3: 0.000491, l4: 0.000572, l5: 0.001172, l6: 0.002297
[epoch: 108/1000, batch:   156/ 1052, ite: 28180] train loss: 0.007330, tar: 0.000620 
l0: 0.000307, l1: 0.000306, l2: 0.000343, l3: 0.000424, l4: 0.000568, l5: 0.000959, l6: 0.001425
[epoch: 108/1000, batch:   236/ 1052, ite: 28200] train loss: 0.007328, tar: 0.000620 
l0: 0.000311, l1: 0.000323, l2: 0.000336, l3: 0.000416, l4: 0.000497, l5: 0.000850, l6: 0.001261
[epoch: 108/1000, batch:   316/ 1052, ite: 28220] train loss: 0.007325, tar: 0.000620 
l0: 0.000462, l1: 0.000477, l2: 0.000516, l3: 0.000496, l4: 0.000695, l5: 0.001094, l6: 0.001996
[epoch: 108/1000, batch:   396/ 1052, ite: 28240] train loss: 0.007324, tar: 0.000619 
l0: 0.000536, l1: 0.000509, l2: 0.000759, l3: 0.000758, l4: 0.000972, l5: 0.001667, l6: 0.002472
[epoch: 108/1000, batch:   476/ 1052, ite: 28260] train loss: 0.007325, tar: 0.000619 
l0: 0.000297, l1: 0.000319, l2: 0.000393, l3: 0.000351, l4: 0.000428, l5: 0.001034, l6: 0.001607
[epoch: 108/1000, batch:   556/ 1052, ite: 28280] train loss: 0.007326, tar: 0.000619 
l0: 0.000866, l1: 0.000963, l2: 0.001402, l3: 0.000695, l4: 0.000751, l5: 0.001249, l6: 0.002054
[epoch: 108/1000, batch:   636/ 1052, ite: 28300] train loss: 0.007331, tar: 0.000620 
l0: 0.000544, l1: 0.000592, l2: 0.000570, l3: 0.000642, l4: 0.000823, l5: 0.001285, l6: 0.001684
[epoch: 108/1000, batch:   716/ 1052, ite: 28320] train loss: 0.007338, tar: 0.000622 
l0: 0.000647, l1: 0.000663, l2: 0.000658, l3: 0.000928, l4: 0.001045, l5: 0.001609, l6: 0.002341
[epoch: 108/1000, batch:   796/ 1052, ite: 28340] train loss: 0.007336, tar: 0.000621 
l0: 0.000731, l1: 0.000845, l2: 0.000731, l3: 0.000769, l4: 0.000991, l5: 0.001319, l6: 0.001731
[epoch: 108/1000, batch:   876/ 1052, ite: 28360] train loss: 0.007336, tar: 0.000621 
l0: 0.000427, l1: 0.000394, l2: 0.000462, l3: 0.000490, l4: 0.000840, l5: 0.001327, l6: 0.002215
[epoch: 108/1000, batch:   956/ 1052, ite: 28380] train loss: 0.007336, tar: 0.000621 
l0: 0.000250, l1: 0.000252, l2: 0.000272, l3: 0.000295, l4: 0.000526, l5: 0.000768, l6: 0.001145
[epoch: 108/1000, batch:  1036/ 1052, ite: 28400] train loss: 0.007333, tar: 0.000621 
[Epoch 108/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000596, l1: 0.000533, l2: 0.000575, l3: 0.000761, l4: 0.001182, l5: 0.001246, l6: 0.002395
[epoch: 109/1000, batch:    64/ 1052, ite: 28420] train loss: 0.007334, tar: 0.000621 
l0: 0.000636, l1: 0.000646, l2: 0.000659, l3: 0.000774, l4: 0.000984, l5: 0.001378, l6: 0.002169
[epoch: 109/1000, batch:   144/ 1052, ite: 28440] train loss: 0.007333, tar: 0.000621 
l0: 0.000737, l1: 0.000831, l2: 0.000660, l3: 0.001095, l4: 0.001029, l5: 0.001607, l6: 0.001934
[epoch: 109/1000, batch:   224/ 1052, ite: 28460] train loss: 0.007333, tar: 0.000621 
l0: 0.000296, l1: 0.000275, l2: 0.000303, l3: 0.000434, l4: 0.000718, l5: 0.000872, l6: 0.001630
[epoch: 109/1000, batch:   304/ 1052, ite: 28480] train loss: 0.007333, tar: 0.000621 
l0: 0.000349, l1: 0.000360, l2: 0.000382, l3: 0.000516, l4: 0.000637, l5: 0.000802, l6: 0.001264
[epoch: 109/1000, batch:   384/ 1052, ite: 28500] train loss: 0.007334, tar: 0.000621 
l0: 0.000454, l1: 0.000462, l2: 0.000470, l3: 0.000478, l4: 0.000631, l5: 0.000761, l6: 0.001220
[epoch: 109/1000, batch:   464/ 1052, ite: 28520] train loss: 0.007332, tar: 0.000621 
l0: 0.000656, l1: 0.000588, l2: 0.000819, l3: 0.000835, l4: 0.001107, l5: 0.001567, l6: 0.002207
[epoch: 109/1000, batch:   544/ 1052, ite: 28540] train loss: 0.007331, tar: 0.000621 
l0: 0.000313, l1: 0.000330, l2: 0.000398, l3: 0.000387, l4: 0.000539, l5: 0.000743, l6: 0.001570
[epoch: 109/1000, batch:   624/ 1052, ite: 28560] train loss: 0.007329, tar: 0.000621 
l0: 0.000340, l1: 0.000340, l2: 0.000421, l3: 0.000484, l4: 0.000623, l5: 0.001032, l6: 0.001375
[epoch: 109/1000, batch:   704/ 1052, ite: 28580] train loss: 0.007325, tar: 0.000620 
l0: 0.000333, l1: 0.000351, l2: 0.000328, l3: 0.000354, l4: 0.000665, l5: 0.000845, l6: 0.001358
[epoch: 109/1000, batch:   784/ 1052, ite: 28600] train loss: 0.007322, tar: 0.000620 
l0: 0.000448, l1: 0.000410, l2: 0.000549, l3: 0.000646, l4: 0.000938, l5: 0.001990, l6: 0.002227
[epoch: 109/1000, batch:   864/ 1052, ite: 28620] train loss: 0.007320, tar: 0.000620 
l0: 0.001284, l1: 0.001313, l2: 0.001309, l3: 0.001190, l4: 0.001495, l5: 0.002228, l6: 0.003708
[epoch: 109/1000, batch:   944/ 1052, ite: 28640] train loss: 0.007320, tar: 0.000620 
l0: 0.000320, l1: 0.000311, l2: 0.000325, l3: 0.000386, l4: 0.000584, l5: 0.000820, l6: 0.001332
[epoch: 109/1000, batch:  1024/ 1052, ite: 28660] train loss: 0.007320, tar: 0.000620 
[Epoch 109/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000399, l1: 0.000398, l2: 0.000424, l3: 0.000494, l4: 0.000516, l5: 0.000805, l6: 0.001235
[epoch: 110/1000, batch:    52/ 1052, ite: 28680] train loss: 0.007317, tar: 0.000619 
l0: 0.000490, l1: 0.000553, l2: 0.000507, l3: 0.000471, l4: 0.000634, l5: 0.001255, l6: 0.001966
[epoch: 110/1000, batch:   132/ 1052, ite: 28700] train loss: 0.007313, tar: 0.000619 
l0: 0.000821, l1: 0.000840, l2: 0.000890, l3: 0.000906, l4: 0.001390, l5: 0.002016, l6: 0.004036
[epoch: 110/1000, batch:   212/ 1052, ite: 28720] train loss: 0.007311, tar: 0.000619 
l0: 0.000298, l1: 0.000320, l2: 0.000377, l3: 0.000349, l4: 0.000464, l5: 0.000799, l6: 0.001293
[epoch: 110/1000, batch:   292/ 1052, ite: 28740] train loss: 0.007307, tar: 0.000618 
l0: 0.000653, l1: 0.000676, l2: 0.000694, l3: 0.000758, l4: 0.001006, l5: 0.001698, l6: 0.002960
[epoch: 110/1000, batch:   372/ 1052, ite: 28760] train loss: 0.007306, tar: 0.000618 
l0: 0.000374, l1: 0.000359, l2: 0.000372, l3: 0.000431, l4: 0.000639, l5: 0.000959, l6: 0.000972
[epoch: 110/1000, batch:   452/ 1052, ite: 28780] train loss: 0.007305, tar: 0.000618 
l0: 0.000801, l1: 0.000730, l2: 0.000939, l3: 0.001013, l4: 0.001331, l5: 0.001406, l6: 0.001980
[epoch: 110/1000, batch:   532/ 1052, ite: 28800] train loss: 0.007307, tar: 0.000618 
l0: 0.000245, l1: 0.000289, l2: 0.000247, l3: 0.000239, l4: 0.000336, l5: 0.000434, l6: 0.000840
[epoch: 110/1000, batch:   612/ 1052, ite: 28820] train loss: 0.007305, tar: 0.000618 
l0: 0.000228, l1: 0.000215, l2: 0.000251, l3: 0.000289, l4: 0.000521, l5: 0.000871, l6: 0.001388
[epoch: 110/1000, batch:   692/ 1052, ite: 28840] train loss: 0.007302, tar: 0.000618 
l0: 0.000801, l1: 0.000826, l2: 0.000859, l3: 0.001151, l4: 0.001448, l5: 0.001763, l6: 0.002222
[epoch: 110/1000, batch:   772/ 1052, ite: 28860] train loss: 0.007299, tar: 0.000618 
l0: 0.000247, l1: 0.000279, l2: 0.000242, l3: 0.000233, l4: 0.000333, l5: 0.000640, l6: 0.000790
[epoch: 110/1000, batch:   852/ 1052, ite: 28880] train loss: 0.007299, tar: 0.000618 
l0: 0.000686, l1: 0.000720, l2: 0.000804, l3: 0.000854, l4: 0.000975, l5: 0.001070, l6: 0.002489
[epoch: 110/1000, batch:   932/ 1052, ite: 28900] train loss: 0.007301, tar: 0.000618 
l0: 0.000642, l1: 0.000647, l2: 0.000660, l3: 0.000753, l4: 0.000873, l5: 0.001549, l6: 0.002632
[epoch: 110/1000, batch:  1012/ 1052, ite: 28920] train loss: 0.007303, tar: 0.000618 
[Epoch 110/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.001201, l1: 0.001399, l2: 0.001184, l3: 0.001323, l4: 0.001411, l5: 0.002214, l6: 0.003507
[epoch: 111/1000, batch:    40/ 1052, ite: 28940] train loss: 0.007305, tar: 0.000619 
l0: 0.000802, l1: 0.000818, l2: 0.000795, l3: 0.000704, l4: 0.000732, l5: 0.001006, l6: 0.001625
[epoch: 111/1000, batch:   120/ 1052, ite: 28960] train loss: 0.007304, tar: 0.000619 
l0: 0.000736, l1: 0.000756, l2: 0.001100, l3: 0.000971, l4: 0.000992, l5: 0.001736, l6: 0.002682
[epoch: 111/1000, batch:   200/ 1052, ite: 28980] train loss: 0.007305, tar: 0.000619 
l0: 0.000619, l1: 0.000617, l2: 0.000673, l3: 0.000770, l4: 0.001013, l5: 0.001447, l6: 0.002975
[epoch: 111/1000, batch:   280/ 1052, ite: 29000] train loss: 0.007300, tar: 0.000618 
l0: 0.000417, l1: 0.000454, l2: 0.000462, l3: 0.000551, l4: 0.000805, l5: 0.001529, l6: 0.002771
[epoch: 111/1000, batch:   360/ 1052, ite: 29020] train loss: 0.007299, tar: 0.000618 
l0: 0.000515, l1: 0.000582, l2: 0.000607, l3: 0.000523, l4: 0.000606, l5: 0.000831, l6: 0.001087
[epoch: 111/1000, batch:   440/ 1052, ite: 29040] train loss: 0.007299, tar: 0.000618 
l0: 0.000303, l1: 0.000298, l2: 0.000357, l3: 0.000389, l4: 0.000663, l5: 0.001354, l6: 0.002101
[epoch: 111/1000, batch:   520/ 1052, ite: 29060] train loss: 0.007296, tar: 0.000618 
l0: 0.000378, l1: 0.000358, l2: 0.000407, l3: 0.000469, l4: 0.000770, l5: 0.001426, l6: 0.002647
[epoch: 111/1000, batch:   600/ 1052, ite: 29080] train loss: 0.007293, tar: 0.000617 
l0: 0.000636, l1: 0.000663, l2: 0.000623, l3: 0.000683, l4: 0.000856, l5: 0.001425, l6: 0.002354
[epoch: 111/1000, batch:   680/ 1052, ite: 29100] train loss: 0.007292, tar: 0.000617 
l0: 0.000639, l1: 0.000566, l2: 0.000625, l3: 0.000789, l4: 0.001267, l5: 0.001765, l6: 0.002357
[epoch: 111/1000, batch:   760/ 1052, ite: 29120] train loss: 0.007291, tar: 0.000617 
l0: 0.000260, l1: 0.000253, l2: 0.000284, l3: 0.000334, l4: 0.000516, l5: 0.001049, l6: 0.001928
[epoch: 111/1000, batch:   840/ 1052, ite: 29140] train loss: 0.007287, tar: 0.000616 
l0: 0.000270, l1: 0.000258, l2: 0.000274, l3: 0.000281, l4: 0.000519, l5: 0.000821, l6: 0.001334
[epoch: 111/1000, batch:   920/ 1052, ite: 29160] train loss: 0.007288, tar: 0.000617 
l0: 0.000674, l1: 0.000693, l2: 0.000671, l3: 0.000750, l4: 0.001058, l5: 0.001009, l6: 0.001710
[epoch: 111/1000, batch:  1000/ 1052, ite: 29180] train loss: 0.007285, tar: 0.000616 
[Epoch 111/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000560, l1: 0.000544, l2: 0.000597, l3: 0.000585, l4: 0.000732, l5: 0.001434, l6: 0.003008
[epoch: 112/1000, batch:    28/ 1052, ite: 29200] train loss: 0.007286, tar: 0.000616 
l0: 0.000354, l1: 0.000370, l2: 0.000370, l3: 0.000369, l4: 0.000474, l5: 0.001067, l6: 0.002347
[epoch: 112/1000, batch:   108/ 1052, ite: 29220] train loss: 0.007283, tar: 0.000616 
l0: 0.000594, l1: 0.000631, l2: 0.000610, l3: 0.000710, l4: 0.000724, l5: 0.001645, l6: 0.002205
[epoch: 112/1000, batch:   188/ 1052, ite: 29240] train loss: 0.007283, tar: 0.000616 
l0: 0.000238, l1: 0.000260, l2: 0.000293, l3: 0.000323, l4: 0.000487, l5: 0.000751, l6: 0.001358
[epoch: 112/1000, batch:   268/ 1052, ite: 29260] train loss: 0.007279, tar: 0.000616 
l0: 0.000457, l1: 0.000483, l2: 0.000473, l3: 0.000551, l4: 0.000691, l5: 0.001723, l6: 0.002991
[epoch: 112/1000, batch:   348/ 1052, ite: 29280] train loss: 0.007274, tar: 0.000615 
l0: 0.000653, l1: 0.000609, l2: 0.000615, l3: 0.000776, l4: 0.001289, l5: 0.001685, l6: 0.003168
[epoch: 112/1000, batch:   428/ 1052, ite: 29300] train loss: 0.007272, tar: 0.000615 
l0: 0.000430, l1: 0.000440, l2: 0.000405, l3: 0.000420, l4: 0.000796, l5: 0.001440, l6: 0.002204
[epoch: 112/1000, batch:   508/ 1052, ite: 29320] train loss: 0.007270, tar: 0.000615 
l0: 0.000829, l1: 0.000822, l2: 0.000845, l3: 0.000966, l4: 0.001035, l5: 0.001551, l6: 0.002945
[epoch: 112/1000, batch:   588/ 1052, ite: 29340] train loss: 0.007270, tar: 0.000615 
l0: 0.002065, l1: 0.002487, l2: 0.002517, l3: 0.002316, l4: 0.002713, l5: 0.002873, l6: 0.006052
[epoch: 112/1000, batch:   668/ 1052, ite: 29360] train loss: 0.007270, tar: 0.000614 
l0: 0.000934, l1: 0.000955, l2: 0.000981, l3: 0.001105, l4: 0.001298, l5: 0.001852, l6: 0.002727
[epoch: 112/1000, batch:   748/ 1052, ite: 29380] train loss: 0.007269, tar: 0.000614 
l0: 0.000360, l1: 0.000376, l2: 0.000417, l3: 0.000455, l4: 0.000647, l5: 0.000814, l6: 0.001944
[epoch: 112/1000, batch:   828/ 1052, ite: 29400] train loss: 0.007269, tar: 0.000614 
l0: 0.000438, l1: 0.000415, l2: 0.000470, l3: 0.000535, l4: 0.000669, l5: 0.001186, l6: 0.002422
[epoch: 112/1000, batch:   908/ 1052, ite: 29420] train loss: 0.007269, tar: 0.000614 
l0: 0.000346, l1: 0.000345, l2: 0.000366, l3: 0.000403, l4: 0.000605, l5: 0.001032, l6: 0.002244
[epoch: 112/1000, batch:   988/ 1052, ite: 29440] train loss: 0.007267, tar: 0.000614 
[Epoch 112/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000364, l1: 0.000346, l2: 0.000392, l3: 0.000520, l4: 0.000732, l5: 0.000844, l6: 0.002397
[epoch: 113/1000, batch:    16/ 1052, ite: 29460] train loss: 0.007266, tar: 0.000614 
l0: 0.000649, l1: 0.000619, l2: 0.000657, l3: 0.000754, l4: 0.001109, l5: 0.002364, l6: 0.003158
[epoch: 113/1000, batch:    96/ 1052, ite: 29480] train loss: 0.007264, tar: 0.000614 
l0: 0.000277, l1: 0.000272, l2: 0.000284, l3: 0.000382, l4: 0.000555, l5: 0.000933, l6: 0.001432
[epoch: 113/1000, batch:   176/ 1052, ite: 29500] train loss: 0.007263, tar: 0.000614 
l0: 0.000403, l1: 0.000388, l2: 0.000395, l3: 0.000444, l4: 0.000630, l5: 0.001139, l6: 0.001646
[epoch: 113/1000, batch:   256/ 1052, ite: 29520] train loss: 0.007259, tar: 0.000613 
l0: 0.000488, l1: 0.000467, l2: 0.000481, l3: 0.000504, l4: 0.000574, l5: 0.000719, l6: 0.001475
[epoch: 113/1000, batch:   336/ 1052, ite: 29540] train loss: 0.007259, tar: 0.000613 
l0: 0.000565, l1: 0.000525, l2: 0.000603, l3: 0.000708, l4: 0.001088, l5: 0.002094, l6: 0.001930
[epoch: 113/1000, batch:   416/ 1052, ite: 29560] train loss: 0.007257, tar: 0.000613 
l0: 0.000462, l1: 0.000447, l2: 0.000488, l3: 0.000538, l4: 0.000740, l5: 0.001025, l6: 0.002222
[epoch: 113/1000, batch:   496/ 1052, ite: 29580] train loss: 0.007256, tar: 0.000613 
l0: 0.000435, l1: 0.000445, l2: 0.000445, l3: 0.000522, l4: 0.000763, l5: 0.001274, l6: 0.001720
[epoch: 113/1000, batch:   576/ 1052, ite: 29600] train loss: 0.007255, tar: 0.000613 
l0: 0.000559, l1: 0.000577, l2: 0.000534, l3: 0.000560, l4: 0.000803, l5: 0.001093, l6: 0.002086
[epoch: 113/1000, batch:   656/ 1052, ite: 29620] train loss: 0.007256, tar: 0.000613 
l0: 0.000435, l1: 0.000454, l2: 0.000479, l3: 0.000529, l4: 0.000794, l5: 0.001170, l6: 0.002158
[epoch: 113/1000, batch:   736/ 1052, ite: 29640] train loss: 0.007255, tar: 0.000612 
l0: 0.000846, l1: 0.000818, l2: 0.000858, l3: 0.000925, l4: 0.001376, l5: 0.001949, l6: 0.002843
[epoch: 113/1000, batch:   816/ 1052, ite: 29660] train loss: 0.007255, tar: 0.000613 
l0: 0.000653, l1: 0.000627, l2: 0.000654, l3: 0.000731, l4: 0.001312, l5: 0.002302, l6: 0.003360
[epoch: 113/1000, batch:   896/ 1052, ite: 29680] train loss: 0.007255, tar: 0.000612 
l0: 0.000831, l1: 0.000868, l2: 0.000754, l3: 0.000825, l4: 0.001252, l5: 0.002093, l6: 0.003066
[epoch: 113/1000, batch:   976/ 1052, ite: 29700] train loss: 0.007252, tar: 0.000612 
[Epoch 113/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000505, l1: 0.000478, l2: 0.000637, l3: 0.000704, l4: 0.000934, l5: 0.001115, l6: 0.002212
[epoch: 114/1000, batch:     4/ 1052, ite: 29720] train loss: 0.007251, tar: 0.000612 
l0: 0.000240, l1: 0.000229, l2: 0.000268, l3: 0.000295, l4: 0.000416, l5: 0.001008, l6: 0.001358
[epoch: 114/1000, batch:    84/ 1052, ite: 29740] train loss: 0.007250, tar: 0.000612 
l0: 0.000657, l1: 0.000625, l2: 0.000669, l3: 0.000820, l4: 0.001040, l5: 0.001526, l6: 0.002224
[epoch: 114/1000, batch:   164/ 1052, ite: 29760] train loss: 0.007248, tar: 0.000612 
l0: 0.000587, l1: 0.000605, l2: 0.000549, l3: 0.000610, l4: 0.000932, l5: 0.001320, l6: 0.001819
[epoch: 114/1000, batch:   244/ 1052, ite: 29780] train loss: 0.007247, tar: 0.000612 
l0: 0.000626, l1: 0.000670, l2: 0.000625, l3: 0.000757, l4: 0.000975, l5: 0.001449, l6: 0.002155
[epoch: 114/1000, batch:   324/ 1052, ite: 29800] train loss: 0.007247, tar: 0.000612 
l0: 0.000264, l1: 0.000255, l2: 0.000276, l3: 0.000347, l4: 0.000576, l5: 0.000762, l6: 0.001433
[epoch: 114/1000, batch:   404/ 1052, ite: 29820] train loss: 0.007245, tar: 0.000612 
l0: 0.000634, l1: 0.000640, l2: 0.000670, l3: 0.000698, l4: 0.000946, l5: 0.000994, l6: 0.001707
[epoch: 114/1000, batch:   484/ 1052, ite: 29840] train loss: 0.007243, tar: 0.000611 
l0: 0.000549, l1: 0.000515, l2: 0.000569, l3: 0.000719, l4: 0.001004, l5: 0.001843, l6: 0.003255
[epoch: 114/1000, batch:   564/ 1052, ite: 29860] train loss: 0.007246, tar: 0.000611 
l0: 0.000438, l1: 0.000436, l2: 0.000494, l3: 0.000447, l4: 0.000581, l5: 0.000728, l6: 0.001054
[epoch: 114/1000, batch:   644/ 1052, ite: 29880] train loss: 0.007251, tar: 0.000612 
l0: 0.001159, l1: 0.001298, l2: 0.001413, l3: 0.001407, l4: 0.001536, l5: 0.003274, l6: 0.004807
[epoch: 114/1000, batch:   724/ 1052, ite: 29900] train loss: 0.007255, tar: 0.000612 
l0: 0.000666, l1: 0.000669, l2: 0.000751, l3: 0.000989, l4: 0.000807, l5: 0.001223, l6: 0.002075
[epoch: 114/1000, batch:   804/ 1052, ite: 29920] train loss: 0.007258, tar: 0.000613 
l0: 0.000366, l1: 0.000414, l2: 0.000364, l3: 0.000528, l4: 0.000580, l5: 0.000934, l6: 0.001653
[epoch: 114/1000, batch:   884/ 1052, ite: 29940] train loss: 0.007257, tar: 0.000613 
l0: 0.000443, l1: 0.000468, l2: 0.000433, l3: 0.000472, l4: 0.000712, l5: 0.000844, l6: 0.002122
[epoch: 114/1000, batch:   964/ 1052, ite: 29960] train loss: 0.007254, tar: 0.000612 
l0: 0.000508, l1: 0.000537, l2: 0.000530, l3: 0.000622, l4: 0.000709, l5: 0.001524, l6: 0.002074
[epoch: 114/1000, batch:  1044/ 1052, ite: 29980] train loss: 0.007253, tar: 0.000612 
[Epoch 114/1000] Test loss: 0.007, Test target loss: 0.001
l0: 0.000530, l1: 0.000521, l2: 0.000587, l3: 0.000600, l4: 0.000731, l5: 0.001574, l6: 0.002920
[epoch: 115/1000, batch:    72/ 1052, ite: 30000] train loss: 0.007252, tar: 0.000612 
l0: 0.000893, l1: 0.000944, l2: 0.001007, l3: 0.000955, l4: 0.001048, l5: 0.001453, l6: 0.002771
[epoch: 115/1000, batch:   152/ 1052, ite: 30020] train loss: 0.007252, tar: 0.000612 
l0: 0.000348, l1: 0.000347, l2: 0.000398, l3: 0.000404, l4: 0.000599, l5: 0.000829, l6: 0.001854
[epoch: 115/1000, batch:   232/ 1052, ite: 30040] train loss: 0.007251, tar: 0.000612 
l0: 0.000578, l1: 0.000722, l2: 0.000611, l3: 0.000490, l4: 0.000780, l5: 0.001168, l6: 0.001669
[epoch: 115/1000, batch:   312/ 1052, ite: 30060] train loss: 0.007249, tar: 0.000612 
l0: 0.000500, l1: 0.000494, l2: 0.000492, l3: 0.000563, l4: 0.000702, l5: 0.001051, l6: 0.001959
[epoch: 115/1000, batch:   392/ 1052, ite: 30080] train loss: 0.007252, tar: 0.000612 
l0: 0.000405, l1: 0.000412, l2: 0.000425, l3: 0.000525, l4: 0.000840, l5: 0.001165, l6: 0.001375
[epoch: 115/1000, batch:   472/ 1052, ite: 30100] train loss: 0.007251, tar: 0.000612 
l0: 0.000384, l1: 0.000398, l2: 0.000368, l3: 0.000450, l4: 0.000572, l5: 0.000859, l6: 0.001889
[epoch: 115/1000, batch:   552/ 1052, ite: 30120] train loss: 0.007250, tar: 0.000612 
l0: 0.000760, l1: 0.000738, l2: 0.000749, l3: 0.000809, l4: 0.001150, l5: 0.001782, l6: 0.003167
[epoch: 115/1000, batch:   632/ 1052, ite: 30140] train loss: 0.007251, tar: 0.000612 
l0: 0.000369, l1: 0.000379, l2: 0.000369, l3: 0.000449, l4: 0.000595, l5: 0.000983, l6: 0.002371
[epoch: 115/1000, batch:   712/ 1052, ite: 30160] train loss: 0.007251, tar: 0.000612 
l0: 0.000411, l1: 0.000406, l2: 0.000425, l3: 0.000449, l4: 0.000753, l5: 0.001203, l6: 0.002181
[epoch: 115/1000, batch:   792/ 1052, ite: 30180] train loss: 0.007248, tar: 0.000612 
l0: 0.000342, l1: 0.000350, l2: 0.000408, l3: 0.000361, l4: 0.000477, l5: 0.000816, l6: 0.001242
[epoch: 115/1000, batch:   872/ 1052, ite: 30200] train loss: 0.007248, tar: 0.000612 
l0: 0.000712, l1: 0.000765, l2: 0.000846, l3: 0.000771, l4: 0.000800, l5: 0.001044, l6: 0.002338
[epoch: 115/1000, batch:   952/ 1052, ite: 30220] train loss: 0.007248, tar: 0.000612 
l0: 0.000476, l1: 0.000493, l2: 0.000470, l3: 0.000566, l4: 0.000511, l5: 0.001196, l6: 0.001676
[epoch: 115/1000, batch:  1032/ 1052, ite: 30240] train loss: 0.007246, tar: 0.000611 
[Epoch 115/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.001076, l1: 0.001249, l2: 0.001202, l3: 0.001173, l4: 0.001377, l5: 0.002293, l6: 0.003852
[epoch: 116/1000, batch:    60/ 1052, ite: 30260] train loss: 0.007245, tar: 0.000611 
l0: 0.000868, l1: 0.000929, l2: 0.000891, l3: 0.001145, l4: 0.001054, l5: 0.001593, l6: 0.002588
[epoch: 116/1000, batch:   140/ 1052, ite: 30280] train loss: 0.007245, tar: 0.000611 
l0: 0.000779, l1: 0.000787, l2: 0.000820, l3: 0.001006, l4: 0.001266, l5: 0.001978, l6: 0.003233
[epoch: 116/1000, batch:   220/ 1052, ite: 30300] train loss: 0.007244, tar: 0.000611 
l0: 0.000455, l1: 0.000483, l2: 0.000485, l3: 0.000548, l4: 0.000837, l5: 0.001442, l6: 0.002885
[epoch: 116/1000, batch:   300/ 1052, ite: 30320] train loss: 0.007243, tar: 0.000611 
l0: 0.000692, l1: 0.000706, l2: 0.000698, l3: 0.000718, l4: 0.000970, l5: 0.001574, l6: 0.003043
[epoch: 116/1000, batch:   380/ 1052, ite: 30340] train loss: 0.007241, tar: 0.000611 
l0: 0.000879, l1: 0.000981, l2: 0.000781, l3: 0.001009, l4: 0.001017, l5: 0.001723, l6: 0.003011
[epoch: 116/1000, batch:   460/ 1052, ite: 30360] train loss: 0.007240, tar: 0.000610 
l0: 0.000752, l1: 0.000761, l2: 0.000752, l3: 0.000744, l4: 0.000946, l5: 0.001234, l6: 0.001913
[epoch: 116/1000, batch:   540/ 1052, ite: 30380] train loss: 0.007238, tar: 0.000610 
l0: 0.000525, l1: 0.000458, l2: 0.000578, l3: 0.000663, l4: 0.000715, l5: 0.001388, l6: 0.002014
[epoch: 116/1000, batch:   620/ 1052, ite: 30400] train loss: 0.007237, tar: 0.000610 
l0: 0.000299, l1: 0.000282, l2: 0.000335, l3: 0.000317, l4: 0.000594, l5: 0.001202, l6: 0.002025
[epoch: 116/1000, batch:   700/ 1052, ite: 30420] train loss: 0.007235, tar: 0.000610 
l0: 0.000551, l1: 0.000580, l2: 0.000615, l3: 0.000691, l4: 0.000824, l5: 0.001016, l6: 0.001900
[epoch: 116/1000, batch:   780/ 1052, ite: 30440] train loss: 0.007233, tar: 0.000610 
l0: 0.000563, l1: 0.000545, l2: 0.000589, l3: 0.000758, l4: 0.000957, l5: 0.001484, l6: 0.001858
[epoch: 116/1000, batch:   860/ 1052, ite: 30460] train loss: 0.007233, tar: 0.000610 
l0: 0.000485, l1: 0.000489, l2: 0.000481, l3: 0.000514, l4: 0.000868, l5: 0.001317, l6: 0.003398
[epoch: 116/1000, batch:   940/ 1052, ite: 30480] train loss: 0.007232, tar: 0.000610 
l0: 0.000219, l1: 0.000194, l2: 0.000203, l3: 0.000300, l4: 0.000441, l5: 0.000565, l6: 0.000659
[epoch: 116/1000, batch:  1020/ 1052, ite: 30500] train loss: 0.007231, tar: 0.000610 
[Epoch 116/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000304, l1: 0.000315, l2: 0.000292, l3: 0.000343, l4: 0.000402, l5: 0.000902, l6: 0.001258
[epoch: 117/1000, batch:    48/ 1052, ite: 30520] train loss: 0.007230, tar: 0.000609 
l0: 0.000859, l1: 0.000820, l2: 0.000927, l3: 0.001104, l4: 0.001455, l5: 0.002635, l6: 0.004867
[epoch: 117/1000, batch:   128/ 1052, ite: 30540] train loss: 0.007231, tar: 0.000609 
l0: 0.000719, l1: 0.000716, l2: 0.000855, l3: 0.000986, l4: 0.001242, l5: 0.001772, l6: 0.004187
[epoch: 117/1000, batch:   208/ 1052, ite: 30560] train loss: 0.007229, tar: 0.000609 
l0: 0.000617, l1: 0.000620, l2: 0.000724, l3: 0.000764, l4: 0.000974, l5: 0.001584, l6: 0.003025
[epoch: 117/1000, batch:   288/ 1052, ite: 30580] train loss: 0.007226, tar: 0.000609 
l0: 0.000424, l1: 0.000426, l2: 0.000464, l3: 0.000485, l4: 0.000771, l5: 0.001424, l6: 0.002127
[epoch: 117/1000, batch:   368/ 1052, ite: 30600] train loss: 0.007224, tar: 0.000609 
l0: 0.000668, l1: 0.000650, l2: 0.000653, l3: 0.000717, l4: 0.001087, l5: 0.001782, l6: 0.003197
[epoch: 117/1000, batch:   448/ 1052, ite: 30620] train loss: 0.007223, tar: 0.000608 
l0: 0.000502, l1: 0.000518, l2: 0.000582, l3: 0.000615, l4: 0.000899, l5: 0.001202, l6: 0.002979
[epoch: 117/1000, batch:   528/ 1052, ite: 30640] train loss: 0.007222, tar: 0.000608 
l0: 0.000399, l1: 0.000399, l2: 0.000381, l3: 0.000455, l4: 0.000817, l5: 0.001170, l6: 0.002401
[epoch: 117/1000, batch:   608/ 1052, ite: 30660] train loss: 0.007221, tar: 0.000608 
l0: 0.000541, l1: 0.000624, l2: 0.000592, l3: 0.000577, l4: 0.000843, l5: 0.001690, l6: 0.001946
[epoch: 117/1000, batch:   688/ 1052, ite: 30680] train loss: 0.007221, tar: 0.000608 
l0: 0.000299, l1: 0.000301, l2: 0.000308, l3: 0.000361, l4: 0.000504, l5: 0.001050, l6: 0.001694
[epoch: 117/1000, batch:   768/ 1052, ite: 30700] train loss: 0.007220, tar: 0.000608 
l0: 0.000324, l1: 0.000363, l2: 0.000299, l3: 0.000355, l4: 0.000454, l5: 0.000748, l6: 0.001584
[epoch: 117/1000, batch:   848/ 1052, ite: 30720] train loss: 0.007218, tar: 0.000608 
l0: 0.000254, l1: 0.000252, l2: 0.000240, l3: 0.000287, l4: 0.000414, l5: 0.000767, l6: 0.000952
[epoch: 117/1000, batch:   928/ 1052, ite: 30740] train loss: 0.007219, tar: 0.000608 
l0: 0.000315, l1: 0.000318, l2: 0.000331, l3: 0.000315, l4: 0.000397, l5: 0.000694, l6: 0.001211
[epoch: 117/1000, batch:  1008/ 1052, ite: 30760] train loss: 0.007217, tar: 0.000608 
[Epoch 117/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000717, l1: 0.000687, l2: 0.000671, l3: 0.000867, l4: 0.001186, l5: 0.001210, l6: 0.002197
[epoch: 118/1000, batch:    36/ 1052, ite: 30780] train loss: 0.007218, tar: 0.000608 
l0: 0.000325, l1: 0.000317, l2: 0.000305, l3: 0.000345, l4: 0.000421, l5: 0.000764, l6: 0.001167
[epoch: 118/1000, batch:   116/ 1052, ite: 30800] train loss: 0.007215, tar: 0.000608 
l0: 0.000586, l1: 0.000595, l2: 0.000556, l3: 0.000626, l4: 0.000874, l5: 0.001517, l6: 0.002761
[epoch: 118/1000, batch:   196/ 1052, ite: 30820] train loss: 0.007214, tar: 0.000608 
l0: 0.000421, l1: 0.000411, l2: 0.000474, l3: 0.000603, l4: 0.000737, l5: 0.001216, l6: 0.002023
[epoch: 118/1000, batch:   276/ 1052, ite: 30840] train loss: 0.007213, tar: 0.000607 
l0: 0.000294, l1: 0.000290, l2: 0.000345, l3: 0.000403, l4: 0.000432, l5: 0.000840, l6: 0.001086
[epoch: 118/1000, batch:   356/ 1052, ite: 30860] train loss: 0.007213, tar: 0.000607 
l0: 0.000690, l1: 0.000656, l2: 0.000640, l3: 0.000711, l4: 0.001032, l5: 0.001510, l6: 0.002492
[epoch: 118/1000, batch:   436/ 1052, ite: 30880] train loss: 0.007213, tar: 0.000607 
l0: 0.000700, l1: 0.000691, l2: 0.000687, l3: 0.000760, l4: 0.000970, l5: 0.001626, l6: 0.003083
[epoch: 118/1000, batch:   516/ 1052, ite: 30900] train loss: 0.007215, tar: 0.000608 
l0: 0.001096, l1: 0.001135, l2: 0.001130, l3: 0.001256, l4: 0.001680, l5: 0.002777, l6: 0.005506
[epoch: 118/1000, batch:   596/ 1052, ite: 30920] train loss: 0.007215, tar: 0.000607 
l0: 0.000647, l1: 0.000573, l2: 0.000685, l3: 0.000919, l4: 0.001348, l5: 0.001421, l6: 0.002392
[epoch: 118/1000, batch:   676/ 1052, ite: 30940] train loss: 0.007215, tar: 0.000607 
l0: 0.000293, l1: 0.000298, l2: 0.000299, l3: 0.000346, l4: 0.000412, l5: 0.000830, l6: 0.001179
[epoch: 118/1000, batch:   756/ 1052, ite: 30960] train loss: 0.007212, tar: 0.000607 
l0: 0.000355, l1: 0.000336, l2: 0.000402, l3: 0.000444, l4: 0.000598, l5: 0.001137, l6: 0.001754
[epoch: 118/1000, batch:   836/ 1052, ite: 30980] train loss: 0.007209, tar: 0.000607 
l0: 0.000556, l1: 0.000502, l2: 0.000552, l3: 0.000702, l4: 0.001079, l5: 0.001728, l6: 0.002774
[epoch: 118/1000, batch:   916/ 1052, ite: 31000] train loss: 0.007209, tar: 0.000607 
l0: 0.000333, l1: 0.000331, l2: 0.000381, l3: 0.000429, l4: 0.000595, l5: 0.001046, l6: 0.001588
[epoch: 118/1000, batch:   996/ 1052, ite: 31020] train loss: 0.007210, tar: 0.000607 
[Epoch 118/1000] Test loss: 0.012, Test target loss: 0.001
l0: 0.000844, l1: 0.000886, l2: 0.001195, l3: 0.000815, l4: 0.001358, l5: 0.002141, l6: 0.002974
[epoch: 119/1000, batch:    24/ 1052, ite: 31040] train loss: 0.007208, tar: 0.000607 
l0: 0.002171, l1: 0.002640, l2: 0.004022, l3: 0.002451, l4: 0.002002, l5: 0.002191, l6: 0.004154
[epoch: 119/1000, batch:   104/ 1052, ite: 31060] train loss: 0.007218, tar: 0.000608 
l0: 0.000510, l1: 0.000551, l2: 0.000688, l3: 0.000619, l4: 0.000738, l5: 0.001617, l6: 0.002615
[epoch: 119/1000, batch:   184/ 1052, ite: 31080] train loss: 0.007220, tar: 0.000609 
l0: 0.000438, l1: 0.000517, l2: 0.000483, l3: 0.000478, l4: 0.000569, l5: 0.001073, l6: 0.002152
[epoch: 119/1000, batch:   264/ 1052, ite: 31100] train loss: 0.007222, tar: 0.000609 
l0: 0.000471, l1: 0.000505, l2: 0.000515, l3: 0.000486, l4: 0.000722, l5: 0.001273, l6: 0.002829
[epoch: 119/1000, batch:   344/ 1052, ite: 31120] train loss: 0.007221, tar: 0.000609 
l0: 0.000327, l1: 0.000357, l2: 0.000310, l3: 0.000409, l4: 0.000482, l5: 0.001181, l6: 0.001484
[epoch: 119/1000, batch:   424/ 1052, ite: 31140] train loss: 0.007220, tar: 0.000609 
l0: 0.000566, l1: 0.000550, l2: 0.000553, l3: 0.000597, l4: 0.000888, l5: 0.001370, l6: 0.002097
[epoch: 119/1000, batch:   504/ 1052, ite: 31160] train loss: 0.007220, tar: 0.000609 
l0: 0.000378, l1: 0.000387, l2: 0.000385, l3: 0.000424, l4: 0.000511, l5: 0.001087, l6: 0.001796
[epoch: 119/1000, batch:   584/ 1052, ite: 31180] train loss: 0.007220, tar: 0.000609 
l0: 0.000307, l1: 0.000306, l2: 0.000309, l3: 0.000383, l4: 0.000513, l5: 0.001073, l6: 0.001147
[epoch: 119/1000, batch:   664/ 1052, ite: 31200] train loss: 0.007218, tar: 0.000608 
l0: 0.000616, l1: 0.000584, l2: 0.000715, l3: 0.000790, l4: 0.001100, l5: 0.001979, l6: 0.002733
[epoch: 119/1000, batch:   744/ 1052, ite: 31220] train loss: 0.007217, tar: 0.000608 
l0: 0.000987, l1: 0.001038, l2: 0.001028, l3: 0.001027, l4: 0.001047, l5: 0.001640, l6: 0.002362
[epoch: 119/1000, batch:   824/ 1052, ite: 31240] train loss: 0.007216, tar: 0.000608 
l0: 0.000878, l1: 0.000884, l2: 0.000872, l3: 0.001006, l4: 0.000950, l5: 0.001327, l6: 0.001938
[epoch: 119/1000, batch:   904/ 1052, ite: 31260] train loss: 0.007216, tar: 0.000608 
l0: 0.000923, l1: 0.000874, l2: 0.000960, l3: 0.001125, l4: 0.001460, l5: 0.001941, l6: 0.003848
[epoch: 119/1000, batch:   984/ 1052, ite: 31280] train loss: 0.007217, tar: 0.000608 
[Epoch 119/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000155, l1: 0.000153, l2: 0.000172, l3: 0.000197, l4: 0.000377, l5: 0.000476, l6: 0.000766
[epoch: 120/1000, batch:    12/ 1052, ite: 31300] train loss: 0.007216, tar: 0.000608 
l0: 0.000695, l1: 0.000695, l2: 0.000724, l3: 0.000691, l4: 0.001000, l5: 0.001009, l6: 0.002032
[epoch: 120/1000, batch:    92/ 1052, ite: 31320] train loss: 0.007216, tar: 0.000608 
l0: 0.000467, l1: 0.000500, l2: 0.000492, l3: 0.000496, l4: 0.000627, l5: 0.000980, l6: 0.002218
[epoch: 120/1000, batch:   172/ 1052, ite: 31340] train loss: 0.007214, tar: 0.000608 
l0: 0.000837, l1: 0.000786, l2: 0.000921, l3: 0.001047, l4: 0.001321, l5: 0.002029, l6: 0.003042
[epoch: 120/1000, batch:   252/ 1052, ite: 31360] train loss: 0.007213, tar: 0.000608 
l0: 0.000299, l1: 0.000365, l2: 0.000319, l3: 0.000406, l4: 0.000527, l5: 0.000960, l6: 0.001427
[epoch: 120/1000, batch:   332/ 1052, ite: 31380] train loss: 0.007212, tar: 0.000608 
l0: 0.000348, l1: 0.000371, l2: 0.000364, l3: 0.000364, l4: 0.000516, l5: 0.001072, l6: 0.001803
[epoch: 120/1000, batch:   412/ 1052, ite: 31400] train loss: 0.007211, tar: 0.000608 
l0: 0.000380, l1: 0.000381, l2: 0.000395, l3: 0.000487, l4: 0.000536, l5: 0.000786, l6: 0.001576
[epoch: 120/1000, batch:   492/ 1052, ite: 31420] train loss: 0.007211, tar: 0.000608 
l0: 0.000693, l1: 0.000625, l2: 0.000693, l3: 0.000902, l4: 0.001022, l5: 0.001582, l6: 0.003301
[epoch: 120/1000, batch:   572/ 1052, ite: 31440] train loss: 0.007212, tar: 0.000608 
l0: 0.000198, l1: 0.000229, l2: 0.000195, l3: 0.000236, l4: 0.000420, l5: 0.000652, l6: 0.001001
[epoch: 120/1000, batch:   652/ 1052, ite: 31460] train loss: 0.007210, tar: 0.000608 
l0: 0.000311, l1: 0.000298, l2: 0.000409, l3: 0.000396, l4: 0.000513, l5: 0.000874, l6: 0.001626
[epoch: 120/1000, batch:   732/ 1052, ite: 31480] train loss: 0.007208, tar: 0.000607 
l0: 0.000477, l1: 0.000504, l2: 0.000465, l3: 0.000486, l4: 0.000507, l5: 0.001072, l6: 0.001269
[epoch: 120/1000, batch:   812/ 1052, ite: 31500] train loss: 0.007209, tar: 0.000607 
l0: 0.000291, l1: 0.000272, l2: 0.000320, l3: 0.000360, l4: 0.000653, l5: 0.000847, l6: 0.001386
[epoch: 120/1000, batch:   892/ 1052, ite: 31520] train loss: 0.007208, tar: 0.000607 
l0: 0.000514, l1: 0.000578, l2: 0.000603, l3: 0.000723, l4: 0.000881, l5: 0.000999, l6: 0.001747
[epoch: 120/1000, batch:   972/ 1052, ite: 31540] train loss: 0.007208, tar: 0.000607 
l0: 0.000589, l1: 0.000576, l2: 0.000599, l3: 0.000648, l4: 0.000785, l5: 0.001558, l6: 0.002608
[epoch: 120/1000, batch:  1052/ 1052, ite: 31560] train loss: 0.007207, tar: 0.000607 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_31560.pkl
[Epoch 120/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.001041, l1: 0.001123, l2: 0.001075, l3: 0.000821, l4: 0.001103, l5: 0.001618, l6: 0.002433
[epoch: 121/1000, batch:    80/ 1052, ite: 31580] train loss: 0.006942, tar: 0.000560 
l0: 0.000241, l1: 0.000241, l2: 0.000237, l3: 0.000294, l4: 0.000453, l5: 0.000804, l6: 0.001645
[epoch: 121/1000, batch:   160/ 1052, ite: 31600] train loss: 0.007511, tar: 0.000649 
l0: 0.000449, l1: 0.000462, l2: 0.000502, l3: 0.000551, l4: 0.000784, l5: 0.000963, l6: 0.001602
[epoch: 121/1000, batch:   240/ 1052, ite: 31620] train loss: 0.006920, tar: 0.000584 
l0: 0.000479, l1: 0.000468, l2: 0.000598, l3: 0.000653, l4: 0.000787, l5: 0.001047, l6: 0.001822
[epoch: 121/1000, batch:   320/ 1052, ite: 31640] train loss: 0.006935, tar: 0.000576 
l0: 0.000411, l1: 0.000406, l2: 0.000426, l3: 0.000477, l4: 0.000745, l5: 0.001260, l6: 0.001929
[epoch: 121/1000, batch:   400/ 1052, ite: 31660] train loss: 0.006732, tar: 0.000556 
l0: 0.000439, l1: 0.000459, l2: 0.000414, l3: 0.000551, l4: 0.000637, l5: 0.001098, l6: 0.001747
[epoch: 121/1000, batch:   480/ 1052, ite: 31680] train loss: 0.006730, tar: 0.000557 
l0: 0.000882, l1: 0.000987, l2: 0.000872, l3: 0.000788, l4: 0.001144, l5: 0.001490, l6: 0.003178
[epoch: 121/1000, batch:   560/ 1052, ite: 31700] train loss: 0.006727, tar: 0.000557 
l0: 0.000495, l1: 0.000480, l2: 0.000521, l3: 0.000612, l4: 0.000902, l5: 0.001826, l6: 0.003219
[epoch: 121/1000, batch:   640/ 1052, ite: 31720] train loss: 0.006841, tar: 0.000568 
l0: 0.000380, l1: 0.000431, l2: 0.000407, l3: 0.000368, l4: 0.000464, l5: 0.001097, l6: 0.002228
[epoch: 121/1000, batch:   720/ 1052, ite: 31740] train loss: 0.006820, tar: 0.000564 
l0: 0.000278, l1: 0.000297, l2: 0.000304, l3: 0.000317, l4: 0.000341, l5: 0.000580, l6: 0.001364
[epoch: 121/1000, batch:   800/ 1052, ite: 31760] train loss: 0.006761, tar: 0.000559 
l0: 0.000761, l1: 0.000686, l2: 0.000800, l3: 0.000952, l4: 0.000994, l5: 0.001778, l6: 0.002720
[epoch: 121/1000, batch:   880/ 1052, ite: 31780] train loss: 0.006735, tar: 0.000555 
l0: 0.000337, l1: 0.000355, l2: 0.000371, l3: 0.000434, l4: 0.000494, l5: 0.000848, l6: 0.001660
[epoch: 121/1000, batch:   960/ 1052, ite: 31800] train loss: 0.006750, tar: 0.000557 
l0: 0.000304, l1: 0.000283, l2: 0.000378, l3: 0.000383, l4: 0.000604, l5: 0.001061, l6: 0.001152
[epoch: 121/1000, batch:  1040/ 1052, ite: 31820] train loss: 0.006726, tar: 0.000555 
[Epoch 121/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000492, l1: 0.000488, l2: 0.000534, l3: 0.000632, l4: 0.000896, l5: 0.001800, l6: 0.003622
[epoch: 122/1000, batch:    68/ 1052, ite: 31840] train loss: 0.006743, tar: 0.000553 
l0: 0.000868, l1: 0.001001, l2: 0.000965, l3: 0.000844, l4: 0.000827, l5: 0.001078, l6: 0.001956
[epoch: 122/1000, batch:   148/ 1052, ite: 31860] train loss: 0.006668, tar: 0.000548 
l0: 0.000245, l1: 0.000239, l2: 0.000261, l3: 0.000300, l4: 0.000376, l5: 0.000824, l6: 0.001336
[epoch: 122/1000, batch:   228/ 1052, ite: 31880] train loss: 0.006616, tar: 0.000543 
l0: 0.000627, l1: 0.000711, l2: 0.000746, l3: 0.000815, l4: 0.001249, l5: 0.002369, l6: 0.002992
[epoch: 122/1000, batch:   308/ 1052, ite: 31900] train loss: 0.006584, tar: 0.000538 
l0: 0.000329, l1: 0.000317, l2: 0.000320, l3: 0.000375, l4: 0.000473, l5: 0.000892, l6: 0.001265
[epoch: 122/1000, batch:   388/ 1052, ite: 31920] train loss: 0.006567, tar: 0.000538 
l0: 0.000288, l1: 0.000300, l2: 0.000300, l3: 0.000332, l4: 0.000543, l5: 0.000786, l6: 0.001502
[epoch: 122/1000, batch:   468/ 1052, ite: 31940] train loss: 0.006595, tar: 0.000539 
l0: 0.000603, l1: 0.000539, l2: 0.000571, l3: 0.000662, l4: 0.001073, l5: 0.002314, l6: 0.002423
[epoch: 122/1000, batch:   548/ 1052, ite: 31960] train loss: 0.006694, tar: 0.000548 
l0: 0.000459, l1: 0.000426, l2: 0.000482, l3: 0.000602, l4: 0.000961, l5: 0.001285, l6: 0.002420
[epoch: 122/1000, batch:   628/ 1052, ite: 31980] train loss: 0.006730, tar: 0.000552 
l0: 0.000667, l1: 0.000739, l2: 0.000688, l3: 0.000687, l4: 0.000960, l5: 0.001925, l6: 0.002671
[epoch: 122/1000, batch:   708/ 1052, ite: 32000] train loss: 0.006743, tar: 0.000552 
l0: 0.000239, l1: 0.000218, l2: 0.000253, l3: 0.000282, l4: 0.000440, l5: 0.000785, l6: 0.001134
[epoch: 122/1000, batch:   788/ 1052, ite: 32020] train loss: 0.006727, tar: 0.000550 
l0: 0.000736, l1: 0.000731, l2: 0.000785, l3: 0.000869, l4: 0.001177, l5: 0.001997, l6: 0.002886
[epoch: 122/1000, batch:   868/ 1052, ite: 32040] train loss: 0.006737, tar: 0.000550 
l0: 0.000590, l1: 0.000586, l2: 0.000652, l3: 0.000647, l4: 0.000830, l5: 0.001597, l6: 0.002245
[epoch: 122/1000, batch:   948/ 1052, ite: 32060] train loss: 0.006759, tar: 0.000553 
l0: 0.000685, l1: 0.000700, l2: 0.000725, l3: 0.000740, l4: 0.001125, l5: 0.001526, l6: 0.002783
[epoch: 122/1000, batch:  1028/ 1052, ite: 32080] train loss: 0.006743, tar: 0.000551 
[Epoch 122/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000311, l1: 0.000319, l2: 0.000383, l3: 0.000403, l4: 0.000680, l5: 0.001441, l6: 0.002118
[epoch: 123/1000, batch:    56/ 1052, ite: 32100] train loss: 0.006747, tar: 0.000552 
l0: 0.000510, l1: 0.000516, l2: 0.000500, l3: 0.000535, l4: 0.000809, l5: 0.001216, l6: 0.002244
[epoch: 123/1000, batch:   136/ 1052, ite: 32120] train loss: 0.006725, tar: 0.000549 
l0: 0.000713, l1: 0.000672, l2: 0.000742, l3: 0.000935, l4: 0.001508, l5: 0.001454, l6: 0.002498
[epoch: 123/1000, batch:   216/ 1052, ite: 32140] train loss: 0.006706, tar: 0.000548 
l0: 0.000252, l1: 0.000255, l2: 0.000292, l3: 0.000278, l4: 0.000504, l5: 0.000880, l6: 0.000997
[epoch: 123/1000, batch:   296/ 1052, ite: 32160] train loss: 0.006699, tar: 0.000545 
l0: 0.000619, l1: 0.000628, l2: 0.000701, l3: 0.000744, l4: 0.001005, l5: 0.001654, l6: 0.003463
[epoch: 123/1000, batch:   376/ 1052, ite: 32180] train loss: 0.006701, tar: 0.000545 
l0: 0.000450, l1: 0.000448, l2: 0.000425, l3: 0.000580, l4: 0.000990, l5: 0.001509, l6: 0.002170
[epoch: 123/1000, batch:   456/ 1052, ite: 32200] train loss: 0.006720, tar: 0.000547 
l0: 0.000872, l1: 0.000957, l2: 0.000890, l3: 0.000977, l4: 0.001154, l5: 0.002151, l6: 0.003673
[epoch: 123/1000, batch:   536/ 1052, ite: 32220] train loss: 0.006722, tar: 0.000548 
l0: 0.000426, l1: 0.000431, l2: 0.000441, l3: 0.000459, l4: 0.000580, l5: 0.001088, l6: 0.001658
[epoch: 123/1000, batch:   616/ 1052, ite: 32240] train loss: 0.006706, tar: 0.000547 
l0: 0.000279, l1: 0.000282, l2: 0.000302, l3: 0.000313, l4: 0.000444, l5: 0.001072, l6: 0.001748
[epoch: 123/1000, batch:   696/ 1052, ite: 32260] train loss: 0.006677, tar: 0.000543 
l0: 0.000798, l1: 0.000800, l2: 0.000824, l3: 0.001026, l4: 0.001442, l5: 0.002362, l6: 0.002672
[epoch: 123/1000, batch:   776/ 1052, ite: 32280] train loss: 0.006677, tar: 0.000544 
l0: 0.000491, l1: 0.000505, l2: 0.000493, l3: 0.000545, l4: 0.000647, l5: 0.001056, l6: 0.001860
[epoch: 123/1000, batch:   856/ 1052, ite: 32300] train loss: 0.006671, tar: 0.000544 
l0: 0.000358, l1: 0.000360, l2: 0.000370, l3: 0.000432, l4: 0.000682, l5: 0.001066, l6: 0.001725
[epoch: 123/1000, batch:   936/ 1052, ite: 32320] train loss: 0.006678, tar: 0.000544 
l0: 0.001857, l1: 0.002503, l2: 0.003268, l3: 0.001801, l4: 0.002081, l5: 0.003750, l6: 0.004173
[epoch: 123/1000, batch:  1016/ 1052, ite: 32340] train loss: 0.006688, tar: 0.000546 
[Epoch 123/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000506, l1: 0.000524, l2: 0.000575, l3: 0.000545, l4: 0.000828, l5: 0.000985, l6: 0.002006
[epoch: 124/1000, batch:    44/ 1052, ite: 32360] train loss: 0.006676, tar: 0.000544 
l0: 0.000568, l1: 0.000621, l2: 0.000584, l3: 0.000611, l4: 0.000777, l5: 0.001119, l6: 0.001503
[epoch: 124/1000, batch:   124/ 1052, ite: 32380] train loss: 0.006704, tar: 0.000552 
l0: 0.001662, l1: 0.001744, l2: 0.001694, l3: 0.001700, l4: 0.001926, l5: 0.001892, l6: 0.004512
[epoch: 124/1000, batch:   204/ 1052, ite: 32400] train loss: 0.006729, tar: 0.000553 
l0: 0.000851, l1: 0.000799, l2: 0.000567, l3: 0.000896, l4: 0.001126, l5: 0.001768, l6: 0.002774
[epoch: 124/1000, batch:   284/ 1052, ite: 32420] train loss: 0.006731, tar: 0.000554 
l0: 0.000910, l1: 0.000914, l2: 0.001535, l3: 0.000969, l4: 0.001081, l5: 0.001660, l6: 0.002400
[epoch: 124/1000, batch:   364/ 1052, ite: 32440] train loss: 0.006746, tar: 0.000555 
l0: 0.000924, l1: 0.000867, l2: 0.000941, l3: 0.001102, l4: 0.001223, l5: 0.001522, l6: 0.002816
[epoch: 124/1000, batch:   444/ 1052, ite: 32460] train loss: 0.006747, tar: 0.000555 
l0: 0.000276, l1: 0.000268, l2: 0.000266, l3: 0.000329, l4: 0.000510, l5: 0.001073, l6: 0.001700
[epoch: 124/1000, batch:   524/ 1052, ite: 32480] train loss: 0.006727, tar: 0.000553 
l0: 0.000467, l1: 0.000485, l2: 0.000522, l3: 0.000563, l4: 0.000762, l5: 0.001020, l6: 0.001916
[epoch: 124/1000, batch:   604/ 1052, ite: 32500] train loss: 0.006721, tar: 0.000553 
l0: 0.000280, l1: 0.000306, l2: 0.000314, l3: 0.000319, l4: 0.000481, l5: 0.000843, l6: 0.001359
[epoch: 124/1000, batch:   684/ 1052, ite: 32520] train loss: 0.006712, tar: 0.000551 
l0: 0.000458, l1: 0.000470, l2: 0.000493, l3: 0.000530, l4: 0.000621, l5: 0.001228, l6: 0.001691
[epoch: 124/1000, batch:   764/ 1052, ite: 32540] train loss: 0.006691, tar: 0.000549 
l0: 0.000632, l1: 0.000648, l2: 0.000706, l3: 0.000705, l4: 0.000871, l5: 0.001688, l6: 0.002350
[epoch: 124/1000, batch:   844/ 1052, ite: 32560] train loss: 0.006670, tar: 0.000547 
l0: 0.000316, l1: 0.000315, l2: 0.000385, l3: 0.000366, l4: 0.000856, l5: 0.001521, l6: 0.002107
[epoch: 124/1000, batch:   924/ 1052, ite: 32580] train loss: 0.006677, tar: 0.000547 
l0: 0.001849, l1: 0.001731, l2: 0.002433, l3: 0.002290, l4: 0.002290, l5: 0.002630, l6: 0.004217
[epoch: 124/1000, batch:  1004/ 1052, ite: 32600] train loss: 0.006686, tar: 0.000549 
[Epoch 124/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000482, l1: 0.000484, l2: 0.000485, l3: 0.000565, l4: 0.000700, l5: 0.001002, l6: 0.002535
[epoch: 125/1000, batch:    32/ 1052, ite: 32620] train loss: 0.006695, tar: 0.000550 
l0: 0.000626, l1: 0.000658, l2: 0.000667, l3: 0.000717, l4: 0.000844, l5: 0.001466, l6: 0.002707
[epoch: 125/1000, batch:   112/ 1052, ite: 32640] train loss: 0.006688, tar: 0.000550 
l0: 0.000465, l1: 0.000463, l2: 0.000470, l3: 0.000578, l4: 0.000807, l5: 0.001590, l6: 0.002189
[epoch: 125/1000, batch:   192/ 1052, ite: 32660] train loss: 0.006690, tar: 0.000549 
l0: 0.000408, l1: 0.000398, l2: 0.000432, l3: 0.000454, l4: 0.000801, l5: 0.001277, l6: 0.002264
[epoch: 125/1000, batch:   272/ 1052, ite: 32680] train loss: 0.006701, tar: 0.000550 
l0: 0.000671, l1: 0.000639, l2: 0.000616, l3: 0.000721, l4: 0.000723, l5: 0.001290, l6: 0.002654
[epoch: 125/1000, batch:   352/ 1052, ite: 32700] train loss: 0.006691, tar: 0.000549 
l0: 0.000205, l1: 0.000194, l2: 0.000205, l3: 0.000255, l4: 0.000408, l5: 0.000588, l6: 0.001062
[epoch: 125/1000, batch:   432/ 1052, ite: 32720] train loss: 0.006685, tar: 0.000548 
l0: 0.000268, l1: 0.000280, l2: 0.000318, l3: 0.000374, l4: 0.000606, l5: 0.000968, l6: 0.001587
[epoch: 125/1000, batch:   512/ 1052, ite: 32740] train loss: 0.006660, tar: 0.000546 
l0: 0.000477, l1: 0.000471, l2: 0.000505, l3: 0.000543, l4: 0.000793, l5: 0.001507, l6: 0.002427
[epoch: 125/1000, batch:   592/ 1052, ite: 32760] train loss: 0.006665, tar: 0.000546 
l0: 0.000370, l1: 0.000366, l2: 0.000377, l3: 0.000434, l4: 0.000640, l5: 0.001049, l6: 0.002019
[epoch: 125/1000, batch:   672/ 1052, ite: 32780] train loss: 0.006665, tar: 0.000547 
l0: 0.000366, l1: 0.000395, l2: 0.000396, l3: 0.000449, l4: 0.000645, l5: 0.001349, l6: 0.002605
[epoch: 125/1000, batch:   752/ 1052, ite: 32800] train loss: 0.006689, tar: 0.000550 
l0: 0.000433, l1: 0.000479, l2: 0.000458, l3: 0.000498, l4: 0.000698, l5: 0.001049, l6: 0.001320
[epoch: 125/1000, batch:   832/ 1052, ite: 32820] train loss: 0.006689, tar: 0.000549 
l0: 0.000364, l1: 0.000411, l2: 0.000487, l3: 0.000559, l4: 0.000480, l5: 0.000911, l6: 0.001381
[epoch: 125/1000, batch:   912/ 1052, ite: 32840] train loss: 0.006687, tar: 0.000549 
l0: 0.000416, l1: 0.000427, l2: 0.000440, l3: 0.000446, l4: 0.000809, l5: 0.001154, l6: 0.001477
[epoch: 125/1000, batch:   992/ 1052, ite: 32860] train loss: 0.006688, tar: 0.000549 
[Epoch 125/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000684, l1: 0.000723, l2: 0.000565, l3: 0.000655, l4: 0.000854, l5: 0.001029, l6: 0.001937
[epoch: 126/1000, batch:    20/ 1052, ite: 32880] train loss: 0.006688, tar: 0.000548 
l0: 0.000871, l1: 0.001016, l2: 0.000876, l3: 0.000962, l4: 0.001167, l5: 0.001718, l6: 0.003087
[epoch: 126/1000, batch:   100/ 1052, ite: 32900] train loss: 0.006687, tar: 0.000548 
l0: 0.000297, l1: 0.000291, l2: 0.000416, l3: 0.000442, l4: 0.000697, l5: 0.000848, l6: 0.001275
[epoch: 126/1000, batch:   180/ 1052, ite: 32920] train loss: 0.006671, tar: 0.000547 
l0: 0.000677, l1: 0.000783, l2: 0.000697, l3: 0.000624, l4: 0.000763, l5: 0.001694, l6: 0.002801
[epoch: 126/1000, batch:   260/ 1052, ite: 32940] train loss: 0.006674, tar: 0.000547 
l0: 0.000349, l1: 0.000383, l2: 0.000359, l3: 0.000365, l4: 0.000497, l5: 0.000825, l6: 0.002079
[epoch: 126/1000, batch:   340/ 1052, ite: 32960] train loss: 0.006672, tar: 0.000546 
l0: 0.000663, l1: 0.000658, l2: 0.000682, l3: 0.000674, l4: 0.001246, l5: 0.001787, l6: 0.002913
[epoch: 126/1000, batch:   420/ 1052, ite: 32980] train loss: 0.006669, tar: 0.000546 
l0: 0.000410, l1: 0.000413, l2: 0.000437, l3: 0.000534, l4: 0.000842, l5: 0.001193, l6: 0.002864
[epoch: 126/1000, batch:   500/ 1052, ite: 33000] train loss: 0.006672, tar: 0.000545 
l0: 0.000486, l1: 0.000476, l2: 0.000467, l3: 0.000660, l4: 0.000747, l5: 0.001561, l6: 0.002579
[epoch: 126/1000, batch:   580/ 1052, ite: 33020] train loss: 0.006667, tar: 0.000545 
l0: 0.000566, l1: 0.000559, l2: 0.000597, l3: 0.000651, l4: 0.000810, l5: 0.001104, l6: 0.002455
[epoch: 126/1000, batch:   660/ 1052, ite: 33040] train loss: 0.006683, tar: 0.000547 
l0: 0.000359, l1: 0.000363, l2: 0.000388, l3: 0.000482, l4: 0.000655, l5: 0.001082, l6: 0.002766
[epoch: 126/1000, batch:   740/ 1052, ite: 33060] train loss: 0.006676, tar: 0.000546 
l0: 0.000620, l1: 0.000616, l2: 0.000638, l3: 0.000652, l4: 0.000921, l5: 0.001446, l6: 0.003278
[epoch: 126/1000, batch:   820/ 1052, ite: 33080] train loss: 0.006684, tar: 0.000546 
l0: 0.000516, l1: 0.000537, l2: 0.000497, l3: 0.000567, l4: 0.000945, l5: 0.001208, l6: 0.001876
[epoch: 126/1000, batch:   900/ 1052, ite: 33100] train loss: 0.006681, tar: 0.000546 
l0: 0.000508, l1: 0.000494, l2: 0.000474, l3: 0.000568, l4: 0.000850, l5: 0.001326, l6: 0.002052
[epoch: 126/1000, batch:   980/ 1052, ite: 33120] train loss: 0.006679, tar: 0.000545 
[Epoch 126/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000359, l1: 0.000347, l2: 0.000358, l3: 0.000777, l4: 0.000787, l5: 0.000646, l6: 0.002265
[epoch: 127/1000, batch:     8/ 1052, ite: 33140] train loss: 0.006701, tar: 0.000547 
l0: 0.000291, l1: 0.000315, l2: 0.000348, l3: 0.000811, l4: 0.000568, l5: 0.000858, l6: 0.001113
[epoch: 127/1000, batch:    88/ 1052, ite: 33160] train loss: 0.006694, tar: 0.000546 
l0: 0.000312, l1: 0.000312, l2: 0.000327, l3: 0.000553, l4: 0.000655, l5: 0.000957, l6: 0.000717
[epoch: 127/1000, batch:   168/ 1052, ite: 33180] train loss: 0.006699, tar: 0.000546 
l0: 0.001038, l1: 0.001085, l2: 0.001094, l3: 0.001130, l4: 0.001542, l5: 0.001710, l6: 0.002142
[epoch: 127/1000, batch:   248/ 1052, ite: 33200] train loss: 0.006713, tar: 0.000547 
l0: 0.000799, l1: 0.000825, l2: 0.000764, l3: 0.000978, l4: 0.001257, l5: 0.001821, l6: 0.002340
[epoch: 127/1000, batch:   328/ 1052, ite: 33220] train loss: 0.006711, tar: 0.000547 
l0: 0.000491, l1: 0.000517, l2: 0.000522, l3: 0.000514, l4: 0.000793, l5: 0.000975, l6: 0.002141
[epoch: 127/1000, batch:   408/ 1052, ite: 33240] train loss: 0.006700, tar: 0.000546 
l0: 0.000514, l1: 0.000497, l2: 0.000590, l3: 0.000604, l4: 0.001069, l5: 0.001835, l6: 0.003046
[epoch: 127/1000, batch:   488/ 1052, ite: 33260] train loss: 0.006727, tar: 0.000550 
l0: 0.000605, l1: 0.000679, l2: 0.000596, l3: 0.001948, l4: 0.000803, l5: 0.001202, l6: 0.002202
[epoch: 127/1000, batch:   568/ 1052, ite: 33280] train loss: 0.006740, tar: 0.000551 
l0: 0.000308, l1: 0.000324, l2: 0.000372, l3: 0.000370, l4: 0.000557, l5: 0.001006, l6: 0.001536
[epoch: 127/1000, batch:   648/ 1052, ite: 33300] train loss: 0.006760, tar: 0.000553 
l0: 0.000450, l1: 0.000486, l2: 0.000398, l3: 0.000425, l4: 0.000592, l5: 0.000918, l6: 0.001816
[epoch: 127/1000, batch:   728/ 1052, ite: 33320] train loss: 0.006760, tar: 0.000553 
l0: 0.000303, l1: 0.000286, l2: 0.000314, l3: 0.000458, l4: 0.000549, l5: 0.000862, l6: 0.001540
[epoch: 127/1000, batch:   808/ 1052, ite: 33340] train loss: 0.006765, tar: 0.000554 
l0: 0.000733, l1: 0.000754, l2: 0.000793, l3: 0.000824, l4: 0.001094, l5: 0.001601, l6: 0.002894
[epoch: 127/1000, batch:   888/ 1052, ite: 33360] train loss: 0.006771, tar: 0.000555 
l0: 0.000941, l1: 0.000882, l2: 0.001144, l3: 0.001121, l4: 0.001514, l5: 0.002179, l6: 0.003766
[epoch: 127/1000, batch:   968/ 1052, ite: 33380] train loss: 0.006795, tar: 0.000557 
l0: 0.000724, l1: 0.000817, l2: 0.000788, l3: 0.000931, l4: 0.000975, l5: 0.001309, l6: 0.002012
[epoch: 127/1000, batch:  1048/ 1052, ite: 33400] train loss: 0.006787, tar: 0.000557 
[Epoch 127/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000513, l1: 0.000531, l2: 0.000560, l3: 0.000611, l4: 0.000737, l5: 0.001035, l6: 0.001686
[epoch: 128/1000, batch:    76/ 1052, ite: 33420] train loss: 0.006795, tar: 0.000558 
l0: 0.000515, l1: 0.000509, l2: 0.000427, l3: 0.000646, l4: 0.000549, l5: 0.001225, l6: 0.001177
[epoch: 128/1000, batch:   156/ 1052, ite: 33440] train loss: 0.006792, tar: 0.000557 
l0: 0.000430, l1: 0.000427, l2: 0.000454, l3: 0.000510, l4: 0.000724, l5: 0.001068, l6: 0.001470
[epoch: 128/1000, batch:   236/ 1052, ite: 33460] train loss: 0.006787, tar: 0.000557 
l0: 0.000570, l1: 0.000544, l2: 0.000590, l3: 0.000795, l4: 0.001054, l5: 0.001862, l6: 0.002987
[epoch: 128/1000, batch:   316/ 1052, ite: 33480] train loss: 0.006784, tar: 0.000556 
l0: 0.000692, l1: 0.000672, l2: 0.000729, l3: 0.000839, l4: 0.001119, l5: 0.001835, l6: 0.002626
[epoch: 128/1000, batch:   396/ 1052, ite: 33500] train loss: 0.006774, tar: 0.000556 
l0: 0.000661, l1: 0.000716, l2: 0.000705, l3: 0.000744, l4: 0.000924, l5: 0.001784, l6: 0.002750
[epoch: 128/1000, batch:   476/ 1052, ite: 33520] train loss: 0.006768, tar: 0.000555 
l0: 0.000538, l1: 0.000543, l2: 0.000579, l3: 0.000653, l4: 0.000749, l5: 0.001336, l6: 0.001548
[epoch: 128/1000, batch:   556/ 1052, ite: 33540] train loss: 0.006769, tar: 0.000555 
l0: 0.000188, l1: 0.000184, l2: 0.000189, l3: 0.000233, l4: 0.000396, l5: 0.000680, l6: 0.000872
[epoch: 128/1000, batch:   636/ 1052, ite: 33560] train loss: 0.006772, tar: 0.000555 
l0: 0.000371, l1: 0.000416, l2: 0.000439, l3: 0.000393, l4: 0.000406, l5: 0.000705, l6: 0.000727
[epoch: 128/1000, batch:   716/ 1052, ite: 33580] train loss: 0.006775, tar: 0.000556 
l0: 0.000357, l1: 0.000326, l2: 0.000440, l3: 0.000535, l4: 0.000764, l5: 0.000818, l6: 0.001602
[epoch: 128/1000, batch:   796/ 1052, ite: 33600] train loss: 0.006784, tar: 0.000557 
l0: 0.001604, l1: 0.001764, l2: 0.001613, l3: 0.002096, l4: 0.001942, l5: 0.002468, l6: 0.004745
[epoch: 128/1000, batch:   876/ 1052, ite: 33620] train loss: 0.006786, tar: 0.000558 
l0: 0.000524, l1: 0.000545, l2: 0.000515, l3: 0.000669, l4: 0.000916, l5: 0.001795, l6: 0.002377
[epoch: 128/1000, batch:   956/ 1052, ite: 33640] train loss: 0.006782, tar: 0.000557 
l0: 0.000961, l1: 0.000952, l2: 0.001045, l3: 0.001112, l4: 0.001282, l5: 0.002505, l6: 0.004059
[epoch: 128/1000, batch:  1036/ 1052, ite: 33660] train loss: 0.006785, tar: 0.000557 
[Epoch 128/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000392, l1: 0.000377, l2: 0.000431, l3: 0.000450, l4: 0.000713, l5: 0.001426, l6: 0.002242
[epoch: 129/1000, batch:    64/ 1052, ite: 33680] train loss: 0.006790, tar: 0.000557 
l0: 0.000578, l1: 0.000550, l2: 0.000613, l3: 0.000701, l4: 0.000991, l5: 0.001297, l6: 0.002525
[epoch: 129/1000, batch:   144/ 1052, ite: 33700] train loss: 0.006785, tar: 0.000557 
l0: 0.000409, l1: 0.000396, l2: 0.000416, l3: 0.000489, l4: 0.000738, l5: 0.001195, l6: 0.001621
[epoch: 129/1000, batch:   224/ 1052, ite: 33720] train loss: 0.006789, tar: 0.000557 
l0: 0.000778, l1: 0.000732, l2: 0.000721, l3: 0.000910, l4: 0.001193, l5: 0.002255, l6: 0.003111
[epoch: 129/1000, batch:   304/ 1052, ite: 33740] train loss: 0.006790, tar: 0.000558 
l0: 0.000268, l1: 0.000252, l2: 0.000284, l3: 0.000363, l4: 0.000524, l5: 0.000828, l6: 0.001382
[epoch: 129/1000, batch:   384/ 1052, ite: 33760] train loss: 0.006783, tar: 0.000557 
l0: 0.000431, l1: 0.000419, l2: 0.000471, l3: 0.000521, l4: 0.000595, l5: 0.001246, l6: 0.002567
[epoch: 129/1000, batch:   464/ 1052, ite: 33780] train loss: 0.006780, tar: 0.000556 
l0: 0.000475, l1: 0.000481, l2: 0.000474, l3: 0.000527, l4: 0.000686, l5: 0.000937, l6: 0.001987
[epoch: 129/1000, batch:   544/ 1052, ite: 33800] train loss: 0.006779, tar: 0.000556 
l0: 0.000408, l1: 0.000409, l2: 0.000438, l3: 0.000490, l4: 0.000687, l5: 0.000983, l6: 0.001831
[epoch: 129/1000, batch:   624/ 1052, ite: 33820] train loss: 0.006785, tar: 0.000556 
l0: 0.000612, l1: 0.000602, l2: 0.000646, l3: 0.000700, l4: 0.001155, l5: 0.001659, l6: 0.002941
[epoch: 129/1000, batch:   704/ 1052, ite: 33840] train loss: 0.006773, tar: 0.000555 
l0: 0.000239, l1: 0.000225, l2: 0.000267, l3: 0.000294, l4: 0.000541, l5: 0.000763, l6: 0.001268
[epoch: 129/1000, batch:   784/ 1052, ite: 33860] train loss: 0.006768, tar: 0.000555 
l0: 0.000339, l1: 0.000342, l2: 0.000315, l3: 0.000371, l4: 0.000517, l5: 0.000907, l6: 0.001250
[epoch: 129/1000, batch:   864/ 1052, ite: 33880] train loss: 0.006762, tar: 0.000554 
l0: 0.000206, l1: 0.000210, l2: 0.000218, l3: 0.000256, l4: 0.000337, l5: 0.000614, l6: 0.001139
[epoch: 129/1000, batch:   944/ 1052, ite: 33900] train loss: 0.006751, tar: 0.000553 
l0: 0.000625, l1: 0.000688, l2: 0.000650, l3: 0.000644, l4: 0.000957, l5: 0.001335, l6: 0.003501
[epoch: 129/1000, batch:  1024/ 1052, ite: 33920] train loss: 0.006755, tar: 0.000553 
[Epoch 129/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000504, l1: 0.000504, l2: 0.000562, l3: 0.000617, l4: 0.000599, l5: 0.000982, l6: 0.002951
[epoch: 130/1000, batch:    52/ 1052, ite: 33940] train loss: 0.006757, tar: 0.000554 
l0: 0.000995, l1: 0.000961, l2: 0.000985, l3: 0.001210, l4: 0.001384, l5: 0.002032, l6: 0.003957
[epoch: 130/1000, batch:   132/ 1052, ite: 33960] train loss: 0.006762, tar: 0.000555 
l0: 0.000761, l1: 0.000728, l2: 0.000807, l3: 0.000909, l4: 0.000931, l5: 0.002010, l6: 0.002167
[epoch: 130/1000, batch:   212/ 1052, ite: 33980] train loss: 0.006759, tar: 0.000555 
l0: 0.001249, l1: 0.001371, l2: 0.001409, l3: 0.001414, l4: 0.001613, l5: 0.002269, l6: 0.004196
[epoch: 130/1000, batch:   292/ 1052, ite: 34000] train loss: 0.006760, tar: 0.000555 
l0: 0.000862, l1: 0.000871, l2: 0.000933, l3: 0.000948, l4: 0.001183, l5: 0.001734, l6: 0.003388
[epoch: 130/1000, batch:   372/ 1052, ite: 34020] train loss: 0.006755, tar: 0.000555 
l0: 0.000743, l1: 0.000781, l2: 0.000816, l3: 0.000749, l4: 0.000899, l5: 0.000975, l6: 0.001500
[epoch: 130/1000, batch:   452/ 1052, ite: 34040] train loss: 0.006753, tar: 0.000554 
l0: 0.000424, l1: 0.000368, l2: 0.000337, l3: 0.000393, l4: 0.000797, l5: 0.001216, l6: 0.002327
[epoch: 130/1000, batch:   532/ 1052, ite: 34060] train loss: 0.006749, tar: 0.000554 
l0: 0.000662, l1: 0.000558, l2: 0.000731, l3: 0.000782, l4: 0.001133, l5: 0.001782, l6: 0.002999
[epoch: 130/1000, batch:   612/ 1052, ite: 34080] train loss: 0.006752, tar: 0.000554 
l0: 0.000560, l1: 0.000569, l2: 0.000544, l3: 0.000607, l4: 0.000774, l5: 0.001585, l6: 0.002446
[epoch: 130/1000, batch:   692/ 1052, ite: 34100] train loss: 0.006757, tar: 0.000554 
l0: 0.000921, l1: 0.000931, l2: 0.000978, l3: 0.000990, l4: 0.001455, l5: 0.003794, l6: 0.004835
[epoch: 130/1000, batch:   772/ 1052, ite: 34120] train loss: 0.006752, tar: 0.000554 
l0: 0.000105, l1: 0.000127, l2: 0.000140, l3: 0.000142, l4: 0.000150, l5: 0.000380, l6: 0.000719
[epoch: 130/1000, batch:   852/ 1052, ite: 34140] train loss: 0.006753, tar: 0.000554 
l0: 0.000742, l1: 0.000760, l2: 0.000810, l3: 0.000849, l4: 0.001265, l5: 0.002037, l6: 0.002998
[epoch: 130/1000, batch:   932/ 1052, ite: 34160] train loss: 0.006756, tar: 0.000555 
l0: 0.000637, l1: 0.000670, l2: 0.000637, l3: 0.000621, l4: 0.000860, l5: 0.001709, l6: 0.002663
[epoch: 130/1000, batch:  1012/ 1052, ite: 34180] train loss: 0.006752, tar: 0.000554 
[Epoch 130/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000293, l1: 0.000297, l2: 0.000268, l3: 0.000336, l4: 0.000486, l5: 0.000909, l6: 0.001772
[epoch: 131/1000, batch:    40/ 1052, ite: 34200] train loss: 0.006747, tar: 0.000554 
l0: 0.000538, l1: 0.000535, l2: 0.000674, l3: 0.000483, l4: 0.000799, l5: 0.000908, l6: 0.001421
[epoch: 131/1000, batch:   120/ 1052, ite: 34220] train loss: 0.006743, tar: 0.000553 
l0: 0.000565, l1: 0.000630, l2: 0.000601, l3: 0.000521, l4: 0.000802, l5: 0.000683, l6: 0.001569
[epoch: 131/1000, batch:   200/ 1052, ite: 34240] train loss: 0.006738, tar: 0.000552 
l0: 0.000747, l1: 0.000721, l2: 0.000788, l3: 0.000914, l4: 0.001079, l5: 0.001881, l6: 0.002701
[epoch: 131/1000, batch:   280/ 1052, ite: 34260] train loss: 0.006736, tar: 0.000552 
l0: 0.000733, l1: 0.000713, l2: 0.000738, l3: 0.000916, l4: 0.001362, l5: 0.002218, l6: 0.002724
[epoch: 131/1000, batch:   360/ 1052, ite: 34280] train loss: 0.006741, tar: 0.000553 
l0: 0.000377, l1: 0.000366, l2: 0.000402, l3: 0.000454, l4: 0.000612, l5: 0.001285, l6: 0.001960
[epoch: 131/1000, batch:   440/ 1052, ite: 34300] train loss: 0.006741, tar: 0.000552 
l0: 0.000294, l1: 0.000288, l2: 0.000349, l3: 0.000471, l4: 0.000501, l5: 0.000820, l6: 0.001444
[epoch: 131/1000, batch:   520/ 1052, ite: 34320] train loss: 0.006728, tar: 0.000551 
l0: 0.000397, l1: 0.000388, l2: 0.000425, l3: 0.000433, l4: 0.000659, l5: 0.000864, l6: 0.001756
[epoch: 131/1000, batch:   600/ 1052, ite: 34340] train loss: 0.006729, tar: 0.000551 
l0: 0.000369, l1: 0.000395, l2: 0.000405, l3: 0.000389, l4: 0.000655, l5: 0.001144, l6: 0.001920
[epoch: 131/1000, batch:   680/ 1052, ite: 34360] train loss: 0.006733, tar: 0.000552 
l0: 0.000665, l1: 0.000697, l2: 0.000666, l3: 0.000654, l4: 0.000856, l5: 0.001409, l6: 0.002190
[epoch: 131/1000, batch:   760/ 1052, ite: 34380] train loss: 0.006739, tar: 0.000552 
l0: 0.000415, l1: 0.000408, l2: 0.000463, l3: 0.000503, l4: 0.000650, l5: 0.001457, l6: 0.002436
[epoch: 131/1000, batch:   840/ 1052, ite: 34400] train loss: 0.006734, tar: 0.000552 
l0: 0.000433, l1: 0.000406, l2: 0.000478, l3: 0.000555, l4: 0.000753, l5: 0.001281, l6: 0.002156
[epoch: 131/1000, batch:   920/ 1052, ite: 34420] train loss: 0.006739, tar: 0.000552 
l0: 0.000459, l1: 0.000444, l2: 0.000462, l3: 0.000553, l4: 0.000771, l5: 0.001247, l6: 0.002043
[epoch: 131/1000, batch:  1000/ 1052, ite: 34440] train loss: 0.006736, tar: 0.000552 
[Epoch 131/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000690, l1: 0.000748, l2: 0.000772, l3: 0.000814, l4: 0.001027, l5: 0.001771, l6: 0.002571
[epoch: 132/1000, batch:    28/ 1052, ite: 34460] train loss: 0.006730, tar: 0.000551 
l0: 0.001208, l1: 0.001151, l2: 0.001211, l3: 0.001423, l4: 0.001597, l5: 0.002601, l6: 0.003722
[epoch: 132/1000, batch:   108/ 1052, ite: 34480] train loss: 0.006729, tar: 0.000551 
l0: 0.000303, l1: 0.000301, l2: 0.000290, l3: 0.000336, l4: 0.000512, l5: 0.000976, l6: 0.001462
[epoch: 132/1000, batch:   188/ 1052, ite: 34500] train loss: 0.006728, tar: 0.000551 
l0: 0.000810, l1: 0.000802, l2: 0.000797, l3: 0.000983, l4: 0.001188, l5: 0.001658, l6: 0.003265
[epoch: 132/1000, batch:   268/ 1052, ite: 34520] train loss: 0.006730, tar: 0.000551 
l0: 0.000144, l1: 0.000150, l2: 0.000146, l3: 0.000182, l4: 0.000413, l5: 0.000581, l6: 0.000561
[epoch: 132/1000, batch:   348/ 1052, ite: 34540] train loss: 0.006720, tar: 0.000550 
l0: 0.000336, l1: 0.000326, l2: 0.000325, l3: 0.000460, l4: 0.000639, l5: 0.001170, l6: 0.001995
[epoch: 132/1000, batch:   428/ 1052, ite: 34560] train loss: 0.006720, tar: 0.000550 
l0: 0.001165, l1: 0.001299, l2: 0.001185, l3: 0.001130, l4: 0.001371, l5: 0.002499, l6: 0.004218
[epoch: 132/1000, batch:   508/ 1052, ite: 34580] train loss: 0.006721, tar: 0.000550 
l0: 0.000339, l1: 0.000360, l2: 0.000370, l3: 0.000363, l4: 0.000576, l5: 0.001367, l6: 0.001725
[epoch: 132/1000, batch:   588/ 1052, ite: 34600] train loss: 0.006718, tar: 0.000550 
l0: 0.000398, l1: 0.000414, l2: 0.000430, l3: 0.000501, l4: 0.000605, l5: 0.001265, l6: 0.001690
[epoch: 132/1000, batch:   668/ 1052, ite: 34620] train loss: 0.006716, tar: 0.000550 
l0: 0.000549, l1: 0.000544, l2: 0.000556, l3: 0.000660, l4: 0.000818, l5: 0.001356, l6: 0.002087
[epoch: 132/1000, batch:   748/ 1052, ite: 34640] train loss: 0.006724, tar: 0.000550 
l0: 0.000390, l1: 0.000388, l2: 0.000451, l3: 0.000518, l4: 0.000516, l5: 0.000800, l6: 0.001486
[epoch: 132/1000, batch:   828/ 1052, ite: 34660] train loss: 0.006717, tar: 0.000550 
l0: 0.000630, l1: 0.000654, l2: 0.000643, l3: 0.000599, l4: 0.000678, l5: 0.001136, l6: 0.001714
[epoch: 132/1000, batch:   908/ 1052, ite: 34680] train loss: 0.006711, tar: 0.000549 
l0: 0.000372, l1: 0.000370, l2: 0.000442, l3: 0.000438, l4: 0.000522, l5: 0.001106, l6: 0.000957
[epoch: 132/1000, batch:   988/ 1052, ite: 34700] train loss: 0.006714, tar: 0.000549 
[Epoch 132/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000771, l1: 0.000779, l2: 0.000762, l3: 0.000826, l4: 0.001069, l5: 0.002094, l6: 0.002535
[epoch: 133/1000, batch:    16/ 1052, ite: 34720] train loss: 0.006709, tar: 0.000549 
l0: 0.000457, l1: 0.000459, l2: 0.000460, l3: 0.000524, l4: 0.000678, l5: 0.000991, l6: 0.001593
[epoch: 133/1000, batch:    96/ 1052, ite: 34740] train loss: 0.006705, tar: 0.000548 
l0: 0.000798, l1: 0.000804, l2: 0.000862, l3: 0.000954, l4: 0.001510, l5: 0.001767, l6: 0.003008
[epoch: 133/1000, batch:   176/ 1052, ite: 34760] train loss: 0.006705, tar: 0.000548 
l0: 0.000393, l1: 0.000368, l2: 0.000408, l3: 0.000435, l4: 0.000525, l5: 0.000700, l6: 0.001365
[epoch: 133/1000, batch:   256/ 1052, ite: 34780] train loss: 0.006708, tar: 0.000548 
l0: 0.000542, l1: 0.000561, l2: 0.000556, l3: 0.000565, l4: 0.000699, l5: 0.001055, l6: 0.001899
[epoch: 133/1000, batch:   336/ 1052, ite: 34800] train loss: 0.006706, tar: 0.000548 
l0: 0.000337, l1: 0.000339, l2: 0.000325, l3: 0.000377, l4: 0.000540, l5: 0.000894, l6: 0.001360
[epoch: 133/1000, batch:   416/ 1052, ite: 34820] train loss: 0.006697, tar: 0.000547 
l0: 0.000320, l1: 0.000296, l2: 0.000315, l3: 0.000395, l4: 0.000551, l5: 0.000692, l6: 0.001188
[epoch: 133/1000, batch:   496/ 1052, ite: 34840] train loss: 0.006694, tar: 0.000547 
l0: 0.000281, l1: 0.000284, l2: 0.000282, l3: 0.000276, l4: 0.000389, l5: 0.000849, l6: 0.001101
[epoch: 133/1000, batch:   576/ 1052, ite: 34860] train loss: 0.006686, tar: 0.000546 
l0: 0.001575, l1: 0.001765, l2: 0.001601, l3: 0.001621, l4: 0.001849, l5: 0.003739, l6: 0.005210
[epoch: 133/1000, batch:   656/ 1052, ite: 34880] train loss: 0.006689, tar: 0.000546 
l0: 0.000259, l1: 0.000255, l2: 0.000251, l3: 0.000341, l4: 0.000499, l5: 0.000980, l6: 0.001305
[epoch: 133/1000, batch:   736/ 1052, ite: 34900] train loss: 0.006688, tar: 0.000546 
l0: 0.000635, l1: 0.000633, l2: 0.000693, l3: 0.000884, l4: 0.000871, l5: 0.001606, l6: 0.002722
[epoch: 133/1000, batch:   816/ 1052, ite: 34920] train loss: 0.006681, tar: 0.000545 
l0: 0.000256, l1: 0.000269, l2: 0.000289, l3: 0.000407, l4: 0.000543, l5: 0.001227, l6: 0.001475
[epoch: 133/1000, batch:   896/ 1052, ite: 34940] train loss: 0.006680, tar: 0.000545 
l0: 0.000511, l1: 0.000483, l2: 0.000542, l3: 0.000565, l4: 0.001078, l5: 0.001808, l6: 0.003018
[epoch: 133/1000, batch:   976/ 1052, ite: 34960] train loss: 0.006680, tar: 0.000545 
[Epoch 133/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000312, l1: 0.000304, l2: 0.000319, l3: 0.000391, l4: 0.000477, l5: 0.001139, l6: 0.001948
[epoch: 134/1000, batch:     4/ 1052, ite: 34980] train loss: 0.006680, tar: 0.000545 
l0: 0.001084, l1: 0.001091, l2: 0.001231, l3: 0.001397, l4: 0.002290, l5: 0.004186, l6: 0.006715
[epoch: 134/1000, batch:    84/ 1052, ite: 35000] train loss: 0.006680, tar: 0.000545 
l0: 0.000522, l1: 0.000490, l2: 0.000479, l3: 0.000648, l4: 0.000751, l5: 0.001129, l6: 0.002706
[epoch: 134/1000, batch:   164/ 1052, ite: 35020] train loss: 0.006679, tar: 0.000545 
l0: 0.000363, l1: 0.000364, l2: 0.000392, l3: 0.000499, l4: 0.000667, l5: 0.001044, l6: 0.001394
[epoch: 134/1000, batch:   244/ 1052, ite: 35040] train loss: 0.006678, tar: 0.000544 
l0: 0.000557, l1: 0.000600, l2: 0.000633, l3: 0.000528, l4: 0.000685, l5: 0.001228, l6: 0.001655
[epoch: 134/1000, batch:   324/ 1052, ite: 35060] train loss: 0.006674, tar: 0.000544 
l0: 0.000303, l1: 0.000318, l2: 0.000359, l3: 0.000352, l4: 0.000506, l5: 0.000940, l6: 0.001776
[epoch: 134/1000, batch:   404/ 1052, ite: 35080] train loss: 0.006669, tar: 0.000544 
l0: 0.000529, l1: 0.000552, l2: 0.000537, l3: 0.000582, l4: 0.000648, l5: 0.001384, l6: 0.003436
[epoch: 134/1000, batch:   484/ 1052, ite: 35100] train loss: 0.006669, tar: 0.000543 
l0: 0.000209, l1: 0.000204, l2: 0.000248, l3: 0.000298, l4: 0.000438, l5: 0.000710, l6: 0.001048
[epoch: 134/1000, batch:   564/ 1052, ite: 35120] train loss: 0.006665, tar: 0.000543 
l0: 0.000486, l1: 0.000554, l2: 0.000618, l3: 0.000511, l4: 0.000808, l5: 0.001400, l6: 0.002460
[epoch: 134/1000, batch:   644/ 1052, ite: 35140] train loss: 0.006669, tar: 0.000543 
l0: 0.000619, l1: 0.000673, l2: 0.000567, l3: 0.000573, l4: 0.000743, l5: 0.001149, l6: 0.002720
[epoch: 134/1000, batch:   724/ 1052, ite: 35160] train loss: 0.006669, tar: 0.000543 
l0: 0.000623, l1: 0.000637, l2: 0.000641, l3: 0.000721, l4: 0.000980, l5: 0.001685, l6: 0.002409
[epoch: 134/1000, batch:   804/ 1052, ite: 35180] train loss: 0.006664, tar: 0.000543 
l0: 0.000481, l1: 0.000498, l2: 0.000500, l3: 0.000538, l4: 0.000754, l5: 0.001535, l6: 0.002386
[epoch: 134/1000, batch:   884/ 1052, ite: 35200] train loss: 0.006664, tar: 0.000543 
l0: 0.000529, l1: 0.000522, l2: 0.000555, l3: 0.000710, l4: 0.001073, l5: 0.002098, l6: 0.003564
[epoch: 134/1000, batch:   964/ 1052, ite: 35220] train loss: 0.006663, tar: 0.000543 
l0: 0.000376, l1: 0.000386, l2: 0.000417, l3: 0.000465, l4: 0.000710, l5: 0.001207, l6: 0.001402
[epoch: 134/1000, batch:  1044/ 1052, ite: 35240] train loss: 0.006658, tar: 0.000542 
[Epoch 134/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000221, l1: 0.000226, l2: 0.000242, l3: 0.000274, l4: 0.000424, l5: 0.000640, l6: 0.001027
[epoch: 135/1000, batch:    72/ 1052, ite: 35260] train loss: 0.006653, tar: 0.000542 
l0: 0.000336, l1: 0.000335, l2: 0.000358, l3: 0.000391, l4: 0.000545, l5: 0.000950, l6: 0.001945
[epoch: 135/1000, batch:   152/ 1052, ite: 35280] train loss: 0.006650, tar: 0.000542 
l0: 0.000838, l1: 0.000886, l2: 0.000894, l3: 0.001012, l4: 0.001544, l5: 0.002109, l6: 0.003333
[epoch: 135/1000, batch:   232/ 1052, ite: 35300] train loss: 0.006645, tar: 0.000541 
l0: 0.000523, l1: 0.000504, l2: 0.000534, l3: 0.000592, l4: 0.000809, l5: 0.001762, l6: 0.003084
[epoch: 135/1000, batch:   312/ 1052, ite: 35320] train loss: 0.006639, tar: 0.000540 
l0: 0.000541, l1: 0.000548, l2: 0.000567, l3: 0.000591, l4: 0.000910, l5: 0.001541, l6: 0.002049
[epoch: 135/1000, batch:   392/ 1052, ite: 35340] train loss: 0.006637, tar: 0.000540 
l0: 0.000524, l1: 0.000521, l2: 0.000487, l3: 0.000506, l4: 0.000688, l5: 0.001140, l6: 0.001435
[epoch: 135/1000, batch:   472/ 1052, ite: 35360] train loss: 0.006635, tar: 0.000540 
l0: 0.000306, l1: 0.000290, l2: 0.000328, l3: 0.000410, l4: 0.000556, l5: 0.001048, l6: 0.001693
[epoch: 135/1000, batch:   552/ 1052, ite: 35380] train loss: 0.006635, tar: 0.000540 
l0: 0.000557, l1: 0.000477, l2: 0.000533, l3: 0.000570, l4: 0.000759, l5: 0.001097, l6: 0.002735
[epoch: 135/1000, batch:   632/ 1052, ite: 35400] train loss: 0.006637, tar: 0.000540 
l0: 0.001225, l1: 0.001279, l2: 0.001247, l3: 0.001522, l4: 0.001392, l5: 0.002127, l6: 0.003569
[epoch: 135/1000, batch:   712/ 1052, ite: 35420] train loss: 0.006642, tar: 0.000540 
l0: 0.000448, l1: 0.000467, l2: 0.000507, l3: 0.000492, l4: 0.000711, l5: 0.001196, l6: 0.002714
[epoch: 135/1000, batch:   792/ 1052, ite: 35440] train loss: 0.006638, tar: 0.000540 
l0: 0.000559, l1: 0.000544, l2: 0.000631, l3: 0.000679, l4: 0.000900, l5: 0.001623, l6: 0.002621
[epoch: 135/1000, batch:   872/ 1052, ite: 35460] train loss: 0.006640, tar: 0.000540 
l0: 0.000319, l1: 0.000320, l2: 0.000332, l3: 0.000389, l4: 0.000601, l5: 0.001198, l6: 0.002306
[epoch: 135/1000, batch:   952/ 1052, ite: 35480] train loss: 0.006641, tar: 0.000540 
l0: 0.000385, l1: 0.000385, l2: 0.000345, l3: 0.000460, l4: 0.000520, l5: 0.000829, l6: 0.001365
[epoch: 135/1000, batch:  1032/ 1052, ite: 35500] train loss: 0.006640, tar: 0.000540 
[Epoch 135/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000470, l1: 0.000460, l2: 0.000511, l3: 0.000563, l4: 0.000800, l5: 0.001620, l6: 0.001684
[epoch: 136/1000, batch:    60/ 1052, ite: 35520] train loss: 0.006642, tar: 0.000540 
l0: 0.000486, l1: 0.000499, l2: 0.000543, l3: 0.000512, l4: 0.000727, l5: 0.001579, l6: 0.002306
[epoch: 136/1000, batch:   140/ 1052, ite: 35540] train loss: 0.006638, tar: 0.000540 
l0: 0.000331, l1: 0.000344, l2: 0.000415, l3: 0.000376, l4: 0.000469, l5: 0.000838, l6: 0.001330
[epoch: 136/1000, batch:   220/ 1052, ite: 35560] train loss: 0.006631, tar: 0.000539 
l0: 0.000261, l1: 0.000260, l2: 0.000303, l3: 0.000351, l4: 0.000546, l5: 0.000748, l6: 0.001896
[epoch: 136/1000, batch:   300/ 1052, ite: 35580] train loss: 0.006629, tar: 0.000539 
l0: 0.001553, l1: 0.001497, l2: 0.001541, l3: 0.001472, l4: 0.001800, l5: 0.002043, l6: 0.002883
[epoch: 136/1000, batch:   380/ 1052, ite: 35600] train loss: 0.006632, tar: 0.000539 
l0: 0.000517, l1: 0.000501, l2: 0.000550, l3: 0.000684, l4: 0.000801, l5: 0.001239, l6: 0.002540
[epoch: 136/1000, batch:   460/ 1052, ite: 35620] train loss: 0.006635, tar: 0.000539 
l0: 0.001035, l1: 0.001368, l2: 0.001172, l3: 0.001324, l4: 0.001322, l5: 0.001534, l6: 0.003004
[epoch: 136/1000, batch:   540/ 1052, ite: 35640] train loss: 0.006638, tar: 0.000540 
l0: 0.000367, l1: 0.000353, l2: 0.000354, l3: 0.000439, l4: 0.000551, l5: 0.001047, l6: 0.001712
[epoch: 136/1000, batch:   620/ 1052, ite: 35660] train loss: 0.006637, tar: 0.000539 
l0: 0.000425, l1: 0.000433, l2: 0.000454, l3: 0.000547, l4: 0.000836, l5: 0.001239, l6: 0.002025
[epoch: 136/1000, batch:   700/ 1052, ite: 35680] train loss: 0.006638, tar: 0.000539 
l0: 0.000261, l1: 0.000272, l2: 0.000267, l3: 0.000290, l4: 0.000446, l5: 0.000906, l6: 0.001402
[epoch: 136/1000, batch:   780/ 1052, ite: 35700] train loss: 0.006634, tar: 0.000539 
l0: 0.000436, l1: 0.000410, l2: 0.000453, l3: 0.000538, l4: 0.000869, l5: 0.001581, l6: 0.002247
[epoch: 136/1000, batch:   860/ 1052, ite: 35720] train loss: 0.006636, tar: 0.000539 
l0: 0.000539, l1: 0.000548, l2: 0.000523, l3: 0.000625, l4: 0.000861, l5: 0.001426, l6: 0.002365
[epoch: 136/1000, batch:   940/ 1052, ite: 35740] train loss: 0.006635, tar: 0.000539 
l0: 0.000596, l1: 0.000622, l2: 0.000631, l3: 0.000637, l4: 0.001012, l5: 0.001685, l6: 0.002770
[epoch: 136/1000, batch:  1020/ 1052, ite: 35760] train loss: 0.006634, tar: 0.000539 
[Epoch 136/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000310, l1: 0.000309, l2: 0.000298, l3: 0.000341, l4: 0.000506, l5: 0.001004, l6: 0.001369
[epoch: 137/1000, batch:    48/ 1052, ite: 35780] train loss: 0.006630, tar: 0.000538 
l0: 0.000509, l1: 0.000525, l2: 0.000512, l3: 0.000536, l4: 0.000710, l5: 0.000775, l6: 0.002200
[epoch: 137/1000, batch:   128/ 1052, ite: 35800] train loss: 0.006631, tar: 0.000538 
l0: 0.000383, l1: 0.000381, l2: 0.000404, l3: 0.000458, l4: 0.000720, l5: 0.001621, l6: 0.001907
[epoch: 137/1000, batch:   208/ 1052, ite: 35820] train loss: 0.006628, tar: 0.000538 
l0: 0.000223, l1: 0.000234, l2: 0.000217, l3: 0.000303, l4: 0.000317, l5: 0.000581, l6: 0.000823
[epoch: 137/1000, batch:   288/ 1052, ite: 35840] train loss: 0.006624, tar: 0.000537 
l0: 0.000490, l1: 0.000487, l2: 0.000576, l3: 0.000582, l4: 0.000733, l5: 0.001092, l6: 0.001641
[epoch: 137/1000, batch:   368/ 1052, ite: 35860] train loss: 0.006627, tar: 0.000537 
l0: 0.000421, l1: 0.000433, l2: 0.000475, l3: 0.000537, l4: 0.000822, l5: 0.001224, l6: 0.002137
[epoch: 137/1000, batch:   448/ 1052, ite: 35880] train loss: 0.006626, tar: 0.000538 
l0: 0.000795, l1: 0.000958, l2: 0.000888, l3: 0.000749, l4: 0.000931, l5: 0.001322, l6: 0.002903
[epoch: 137/1000, batch:   528/ 1052, ite: 35900] train loss: 0.006626, tar: 0.000537 
l0: 0.000758, l1: 0.000708, l2: 0.000738, l3: 0.000946, l4: 0.001061, l5: 0.001363, l6: 0.002364
[epoch: 137/1000, batch:   608/ 1052, ite: 35920] train loss: 0.006620, tar: 0.000537 
l0: 0.000975, l1: 0.000936, l2: 0.000998, l3: 0.001020, l4: 0.001357, l5: 0.002473, l6: 0.004240
[epoch: 137/1000, batch:   688/ 1052, ite: 35940] train loss: 0.006622, tar: 0.000537 
l0: 0.000959, l1: 0.000957, l2: 0.000984, l3: 0.001041, l4: 0.001879, l5: 0.002503, l6: 0.004333
[epoch: 137/1000, batch:   768/ 1052, ite: 35960] train loss: 0.006629, tar: 0.000537 
l0: 0.001214, l1: 0.001263, l2: 0.001318, l3: 0.001227, l4: 0.001554, l5: 0.002422, l6: 0.003464
[epoch: 137/1000, batch:   848/ 1052, ite: 35980] train loss: 0.006626, tar: 0.000537 
l0: 0.001611, l1: 0.001563, l2: 0.001922, l3: 0.001865, l4: 0.002069, l5: 0.003031, l6: 0.005200
[epoch: 137/1000, batch:   928/ 1052, ite: 36000] train loss: 0.006623, tar: 0.000537 
l0: 0.000461, l1: 0.000495, l2: 0.000427, l3: 0.000415, l4: 0.000628, l5: 0.000721, l6: 0.001339
[epoch: 137/1000, batch:  1008/ 1052, ite: 36020] train loss: 0.006621, tar: 0.000536 
[Epoch 137/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000665, l1: 0.000666, l2: 0.000717, l3: 0.000808, l4: 0.001245, l5: 0.001632, l6: 0.003532
[epoch: 138/1000, batch:    36/ 1052, ite: 36040] train loss: 0.006622, tar: 0.000536 
l0: 0.000547, l1: 0.000523, l2: 0.000587, l3: 0.000641, l4: 0.000849, l5: 0.001433, l6: 0.002353
[epoch: 138/1000, batch:   116/ 1052, ite: 36060] train loss: 0.006624, tar: 0.000537 
l0: 0.000367, l1: 0.000369, l2: 0.000435, l3: 0.000474, l4: 0.000640, l5: 0.001316, l6: 0.002121
[epoch: 138/1000, batch:   196/ 1052, ite: 36080] train loss: 0.006621, tar: 0.000536 
l0: 0.000710, l1: 0.000745, l2: 0.000740, l3: 0.000765, l4: 0.000848, l5: 0.002202, l6: 0.002585
[epoch: 138/1000, batch:   276/ 1052, ite: 36100] train loss: 0.006623, tar: 0.000536 
l0: 0.000558, l1: 0.000550, l2: 0.000562, l3: 0.000619, l4: 0.000960, l5: 0.001066, l6: 0.001918
[epoch: 138/1000, batch:   356/ 1052, ite: 36120] train loss: 0.006630, tar: 0.000537 
l0: 0.000594, l1: 0.000592, l2: 0.000542, l3: 0.000635, l4: 0.000746, l5: 0.001619, l6: 0.002627
[epoch: 138/1000, batch:   436/ 1052, ite: 36140] train loss: 0.006635, tar: 0.000538 
l0: 0.000655, l1: 0.000679, l2: 0.000720, l3: 0.000662, l4: 0.000785, l5: 0.000955, l6: 0.001610
[epoch: 138/1000, batch:   516/ 1052, ite: 36160] train loss: 0.006635, tar: 0.000538 
l0: 0.000446, l1: 0.000432, l2: 0.000490, l3: 0.000525, l4: 0.000685, l5: 0.001372, l6: 0.002467
[epoch: 138/1000, batch:   596/ 1052, ite: 36180] train loss: 0.006630, tar: 0.000537 
l0: 0.000317, l1: 0.000317, l2: 0.000363, l3: 0.000404, l4: 0.000510, l5: 0.000869, l6: 0.001102
[epoch: 138/1000, batch:   676/ 1052, ite: 36200] train loss: 0.006630, tar: 0.000537 
l0: 0.000288, l1: 0.000280, l2: 0.000328, l3: 0.000347, l4: 0.000553, l5: 0.000874, l6: 0.001341
[epoch: 138/1000, batch:   756/ 1052, ite: 36220] train loss: 0.006629, tar: 0.000537 
l0: 0.000333, l1: 0.000326, l2: 0.000343, l3: 0.000403, l4: 0.000516, l5: 0.000910, l6: 0.001194
[epoch: 138/1000, batch:   836/ 1052, ite: 36240] train loss: 0.006629, tar: 0.000537 
l0: 0.000633, l1: 0.000653, l2: 0.000710, l3: 0.000716, l4: 0.000962, l5: 0.001579, l6: 0.001818
[epoch: 138/1000, batch:   916/ 1052, ite: 36260] train loss: 0.006627, tar: 0.000537 
l0: 0.000464, l1: 0.000489, l2: 0.000498, l3: 0.000565, l4: 0.000743, l5: 0.001279, l6: 0.003127
[epoch: 138/1000, batch:   996/ 1052, ite: 36280] train loss: 0.006631, tar: 0.000537 
[Epoch 138/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000762, l1: 0.000986, l2: 0.000726, l3: 0.000667, l4: 0.000879, l5: 0.000957, l6: 0.002019
[epoch: 139/1000, batch:    24/ 1052, ite: 36300] train loss: 0.006632, tar: 0.000537 
l0: 0.000446, l1: 0.000463, l2: 0.000462, l3: 0.000491, l4: 0.000618, l5: 0.000997, l6: 0.002230
[epoch: 139/1000, batch:   104/ 1052, ite: 36320] train loss: 0.006638, tar: 0.000538 
l0: 0.000320, l1: 0.000327, l2: 0.000337, l3: 0.000354, l4: 0.000475, l5: 0.000931, l6: 0.001729
[epoch: 139/1000, batch:   184/ 1052, ite: 36340] train loss: 0.006636, tar: 0.000538 
l0: 0.000395, l1: 0.000365, l2: 0.000393, l3: 0.000579, l4: 0.000485, l5: 0.000929, l6: 0.001722
[epoch: 139/1000, batch:   264/ 1052, ite: 36360] train loss: 0.006633, tar: 0.000537 
l0: 0.000804, l1: 0.000779, l2: 0.000832, l3: 0.001001, l4: 0.001307, l5: 0.002006, l6: 0.003357
[epoch: 139/1000, batch:   344/ 1052, ite: 36380] train loss: 0.006631, tar: 0.000537 
l0: 0.000282, l1: 0.000285, l2: 0.000303, l3: 0.000387, l4: 0.000604, l5: 0.001003, l6: 0.001491
[epoch: 139/1000, batch:   424/ 1052, ite: 36400] train loss: 0.006630, tar: 0.000537 
l0: 0.000597, l1: 0.000587, l2: 0.000558, l3: 0.000606, l4: 0.000891, l5: 0.001306, l6: 0.002906
[epoch: 139/1000, batch:   504/ 1052, ite: 36420] train loss: 0.006629, tar: 0.000537 
l0: 0.000399, l1: 0.000439, l2: 0.000423, l3: 0.000387, l4: 0.000865, l5: 0.001328, l6: 0.002096
[epoch: 139/1000, batch:   584/ 1052, ite: 36440] train loss: 0.006629, tar: 0.000537 
l0: 0.000950, l1: 0.001042, l2: 0.000959, l3: 0.000992, l4: 0.001097, l5: 0.001691, l6: 0.002318
[epoch: 139/1000, batch:   664/ 1052, ite: 36460] train loss: 0.006628, tar: 0.000537 
l0: 0.000466, l1: 0.000497, l2: 0.000470, l3: 0.000495, l4: 0.000604, l5: 0.001265, l6: 0.002352
[epoch: 139/1000, batch:   744/ 1052, ite: 36480] train loss: 0.006626, tar: 0.000536 
l0: 0.000276, l1: 0.000269, l2: 0.000273, l3: 0.000363, l4: 0.000478, l5: 0.000554, l6: 0.001246
[epoch: 139/1000, batch:   824/ 1052, ite: 36500] train loss: 0.006624, tar: 0.000536 
l0: 0.000627, l1: 0.000673, l2: 0.000602, l3: 0.000690, l4: 0.000887, l5: 0.001401, l6: 0.002589
[epoch: 139/1000, batch:   904/ 1052, ite: 36520] train loss: 0.006622, tar: 0.000536 
l0: 0.000413, l1: 0.000426, l2: 0.000453, l3: 0.000515, l4: 0.000657, l5: 0.001138, l6: 0.001876
[epoch: 139/1000, batch:   984/ 1052, ite: 36540] train loss: 0.006619, tar: 0.000536 
[Epoch 139/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000258, l1: 0.000264, l2: 0.000249, l3: 0.000298, l4: 0.000433, l5: 0.000974, l6: 0.001806
[epoch: 140/1000, batch:    12/ 1052, ite: 36560] train loss: 0.006620, tar: 0.000536 
l0: 0.000466, l1: 0.000421, l2: 0.000527, l3: 0.000628, l4: 0.000708, l5: 0.000999, l6: 0.002097
[epoch: 140/1000, batch:    92/ 1052, ite: 36580] train loss: 0.006621, tar: 0.000536 
l0: 0.000271, l1: 0.000246, l2: 0.000256, l3: 0.000345, l4: 0.000518, l5: 0.000793, l6: 0.001352
[epoch: 140/1000, batch:   172/ 1052, ite: 36600] train loss: 0.006621, tar: 0.000535 
l0: 0.000473, l1: 0.000478, l2: 0.000489, l3: 0.000554, l4: 0.000711, l5: 0.000876, l6: 0.002502
[epoch: 140/1000, batch:   252/ 1052, ite: 36620] train loss: 0.006620, tar: 0.000535 
l0: 0.000372, l1: 0.000359, l2: 0.000383, l3: 0.000501, l4: 0.000649, l5: 0.001480, l6: 0.001821
[epoch: 140/1000, batch:   332/ 1052, ite: 36640] train loss: 0.006617, tar: 0.000535 
l0: 0.000468, l1: 0.000455, l2: 0.000491, l3: 0.000527, l4: 0.000684, l5: 0.001022, l6: 0.002168
[epoch: 140/1000, batch:   412/ 1052, ite: 36660] train loss: 0.006614, tar: 0.000534 
l0: 0.000679, l1: 0.000683, l2: 0.000746, l3: 0.000709, l4: 0.000936, l5: 0.001031, l6: 0.002201
[epoch: 140/1000, batch:   492/ 1052, ite: 36680] train loss: 0.006615, tar: 0.000534 
l0: 0.000262, l1: 0.000271, l2: 0.000273, l3: 0.000301, l4: 0.000456, l5: 0.000667, l6: 0.001107
[epoch: 140/1000, batch:   572/ 1052, ite: 36700] train loss: 0.006616, tar: 0.000535 
l0: 0.000354, l1: 0.000341, l2: 0.000392, l3: 0.000429, l4: 0.000613, l5: 0.001008, l6: 0.001770
[epoch: 140/1000, batch:   652/ 1052, ite: 36720] train loss: 0.006615, tar: 0.000534 
l0: 0.000528, l1: 0.000558, l2: 0.000528, l3: 0.000666, l4: 0.000982, l5: 0.002015, l6: 0.002371
[epoch: 140/1000, batch:   732/ 1052, ite: 36740] train loss: 0.006617, tar: 0.000535 
l0: 0.000264, l1: 0.000275, l2: 0.000270, l3: 0.000274, l4: 0.000453, l5: 0.000824, l6: 0.001225
[epoch: 140/1000, batch:   812/ 1052, ite: 36760] train loss: 0.006619, tar: 0.000535 
l0: 0.000395, l1: 0.000354, l2: 0.000339, l3: 0.000467, l4: 0.000759, l5: 0.001220, l6: 0.001360
[epoch: 140/1000, batch:   892/ 1052, ite: 36780] train loss: 0.006617, tar: 0.000535 
l0: 0.000403, l1: 0.000372, l2: 0.000474, l3: 0.000568, l4: 0.000638, l5: 0.001298, l6: 0.002203
[epoch: 140/1000, batch:   972/ 1052, ite: 36800] train loss: 0.006614, tar: 0.000534 
l0: 0.000472, l1: 0.000448, l2: 0.000496, l3: 0.000748, l4: 0.001115, l5: 0.001745, l6: 0.002174
[epoch: 140/1000, batch:  1052/ 1052, ite: 36820] train loss: 0.006611, tar: 0.000534 
[Epoch 140/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000850, l1: 0.000761, l2: 0.000992, l3: 0.001350, l4: 0.001501, l5: 0.002422, l6: 0.003904
[epoch: 141/1000, batch:    80/ 1052, ite: 36840] train loss: 0.006610, tar: 0.000534 
l0: 0.000437, l1: 0.000442, l2: 0.000467, l3: 0.000528, l4: 0.000650, l5: 0.001023, l6: 0.001750
[epoch: 141/1000, batch:   160/ 1052, ite: 36860] train loss: 0.006607, tar: 0.000534 
l0: 0.000650, l1: 0.000650, l2: 0.000727, l3: 0.000858, l4: 0.001170, l5: 0.002185, l6: 0.003389
[epoch: 141/1000, batch:   240/ 1052, ite: 36880] train loss: 0.006607, tar: 0.000534 
l0: 0.000471, l1: 0.000473, l2: 0.000495, l3: 0.000534, l4: 0.000638, l5: 0.001154, l6: 0.002091
[epoch: 141/1000, batch:   320/ 1052, ite: 36900] train loss: 0.006605, tar: 0.000533 
l0: 0.000274, l1: 0.000286, l2: 0.000311, l3: 0.000312, l4: 0.000445, l5: 0.000791, l6: 0.001810
[epoch: 141/1000, batch:   400/ 1052, ite: 36920] train loss: 0.006603, tar: 0.000533 
l0: 0.000363, l1: 0.000352, l2: 0.000372, l3: 0.000500, l4: 0.000722, l5: 0.001264, l6: 0.001729
[epoch: 141/1000, batch:   480/ 1052, ite: 36940] train loss: 0.006605, tar: 0.000534 
l0: 0.000404, l1: 0.000481, l2: 0.000564, l3: 0.000469, l4: 0.000595, l5: 0.000784, l6: 0.001966
[epoch: 141/1000, batch:   560/ 1052, ite: 36960] train loss: 0.006605, tar: 0.000534 
l0: 0.000438, l1: 0.000433, l2: 0.000465, l3: 0.000605, l4: 0.000890, l5: 0.001680, l6: 0.002050
[epoch: 141/1000, batch:   640/ 1052, ite: 36980] train loss: 0.006605, tar: 0.000533 
l0: 0.000868, l1: 0.000899, l2: 0.000835, l3: 0.001081, l4: 0.001311, l5: 0.001645, l6: 0.002401
[epoch: 141/1000, batch:   720/ 1052, ite: 37000] train loss: 0.006605, tar: 0.000533 
l0: 0.000653, l1: 0.000699, l2: 0.000697, l3: 0.000714, l4: 0.000980, l5: 0.001384, l6: 0.003776
[epoch: 141/1000, batch:   800/ 1052, ite: 37020] train loss: 0.006608, tar: 0.000534 
l0: 0.000604, l1: 0.000635, l2: 0.000593, l3: 0.000604, l4: 0.000746, l5: 0.001299, l6: 0.002157
[epoch: 141/1000, batch:   880/ 1052, ite: 37040] train loss: 0.006609, tar: 0.000534 
l0: 0.000203, l1: 0.000206, l2: 0.000236, l3: 0.000250, l4: 0.000409, l5: 0.000720, l6: 0.001276
[epoch: 141/1000, batch:   960/ 1052, ite: 37060] train loss: 0.006605, tar: 0.000534 
l0: 0.000592, l1: 0.000588, l2: 0.000642, l3: 0.000610, l4: 0.000850, l5: 0.001163, l6: 0.002066
[epoch: 141/1000, batch:  1040/ 1052, ite: 37080] train loss: 0.006602, tar: 0.000533 
[Epoch 141/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000395, l1: 0.000389, l2: 0.000467, l3: 0.000516, l4: 0.000583, l5: 0.001038, l6: 0.001303
[epoch: 142/1000, batch:    68/ 1052, ite: 37100] train loss: 0.006600, tar: 0.000533 
l0: 0.000512, l1: 0.000519, l2: 0.000500, l3: 0.000564, l4: 0.000834, l5: 0.001367, l6: 0.001981
[epoch: 142/1000, batch:   148/ 1052, ite: 37120] train loss: 0.006599, tar: 0.000533 
l0: 0.000436, l1: 0.000430, l2: 0.000434, l3: 0.000530, l4: 0.000830, l5: 0.001542, l6: 0.003256
[epoch: 142/1000, batch:   228/ 1052, ite: 37140] train loss: 0.006598, tar: 0.000533 
l0: 0.000888, l1: 0.000922, l2: 0.000930, l3: 0.001073, l4: 0.001200, l5: 0.001508, l6: 0.002509
[epoch: 142/1000, batch:   308/ 1052, ite: 37160] train loss: 0.006598, tar: 0.000533 
l0: 0.000477, l1: 0.000488, l2: 0.000538, l3: 0.000502, l4: 0.000715, l5: 0.001189, l6: 0.001989
[epoch: 142/1000, batch:   388/ 1052, ite: 37180] train loss: 0.006600, tar: 0.000533 
l0: 0.000302, l1: 0.000296, l2: 0.000340, l3: 0.000359, l4: 0.000548, l5: 0.000829, l6: 0.001868
[epoch: 142/1000, batch:   468/ 1052, ite: 37200] train loss: 0.006600, tar: 0.000533 
l0: 0.000353, l1: 0.000359, l2: 0.000364, l3: 0.000405, l4: 0.000614, l5: 0.001333, l6: 0.001305
[epoch: 142/1000, batch:   548/ 1052, ite: 37220] train loss: 0.006597, tar: 0.000532 
l0: 0.000372, l1: 0.000355, l2: 0.000407, l3: 0.000523, l4: 0.000794, l5: 0.001311, l6: 0.002516
[epoch: 142/1000, batch:   628/ 1052, ite: 37240] train loss: 0.006594, tar: 0.000532 
l0: 0.000218, l1: 0.000220, l2: 0.000246, l3: 0.000331, l4: 0.000497, l5: 0.000809, l6: 0.001355
[epoch: 142/1000, batch:   708/ 1052, ite: 37260] train loss: 0.006589, tar: 0.000532 
l0: 0.000335, l1: 0.000323, l2: 0.000374, l3: 0.000446, l4: 0.000636, l5: 0.000971, l6: 0.001717
[epoch: 142/1000, batch:   788/ 1052, ite: 37280] train loss: 0.006589, tar: 0.000531 
l0: 0.000294, l1: 0.000292, l2: 0.000336, l3: 0.000386, l4: 0.000509, l5: 0.000787, l6: 0.001646
[epoch: 142/1000, batch:   868/ 1052, ite: 37300] train loss: 0.006587, tar: 0.000531 
l0: 0.000602, l1: 0.000579, l2: 0.000607, l3: 0.000780, l4: 0.000861, l5: 0.001386, l6: 0.002377
[epoch: 142/1000, batch:   948/ 1052, ite: 37320] train loss: 0.006587, tar: 0.000531 
l0: 0.000285, l1: 0.000272, l2: 0.000302, l3: 0.000373, l4: 0.000582, l5: 0.000922, l6: 0.001508
[epoch: 142/1000, batch:  1028/ 1052, ite: 37340] train loss: 0.006589, tar: 0.000531 
[Epoch 142/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000469, l1: 0.000455, l2: 0.000455, l3: 0.000544, l4: 0.000796, l5: 0.001073, l6: 0.002337
[epoch: 143/1000, batch:    56/ 1052, ite: 37360] train loss: 0.006586, tar: 0.000531 
l0: 0.000759, l1: 0.000694, l2: 0.000819, l3: 0.000902, l4: 0.000981, l5: 0.001329, l6: 0.001347
[epoch: 143/1000, batch:   136/ 1052, ite: 37380] train loss: 0.006588, tar: 0.000531 
l0: 0.000756, l1: 0.000712, l2: 0.000820, l3: 0.001010, l4: 0.001164, l5: 0.001792, l6: 0.004255
[epoch: 143/1000, batch:   216/ 1052, ite: 37400] train loss: 0.006587, tar: 0.000531 
l0: 0.000267, l1: 0.000281, l2: 0.000286, l3: 0.000286, l4: 0.000508, l5: 0.001160, l6: 0.001622
[epoch: 143/1000, batch:   296/ 1052, ite: 37420] train loss: 0.006586, tar: 0.000531 
l0: 0.000488, l1: 0.000495, l2: 0.000518, l3: 0.000567, l4: 0.000679, l5: 0.001359, l6: 0.003028
[epoch: 143/1000, batch:   376/ 1052, ite: 37440] train loss: 0.006586, tar: 0.000531 
l0: 0.000974, l1: 0.001006, l2: 0.001064, l3: 0.001204, l4: 0.001664, l5: 0.002577, l6: 0.003766
[epoch: 143/1000, batch:   456/ 1052, ite: 37460] train loss: 0.006586, tar: 0.000531 
l0: 0.000464, l1: 0.000554, l2: 0.000474, l3: 0.000386, l4: 0.000406, l5: 0.000747, l6: 0.001952
[epoch: 143/1000, batch:   536/ 1052, ite: 37480] train loss: 0.006585, tar: 0.000531 
l0: 0.000397, l1: 0.000373, l2: 0.000442, l3: 0.000524, l4: 0.000687, l5: 0.001435, l6: 0.001853
[epoch: 143/1000, batch:   616/ 1052, ite: 37500] train loss: 0.006584, tar: 0.000531 
l0: 0.000338, l1: 0.000316, l2: 0.000381, l3: 0.000386, l4: 0.000668, l5: 0.000666, l6: 0.001162
[epoch: 143/1000, batch:   696/ 1052, ite: 37520] train loss: 0.006578, tar: 0.000530 
l0: 0.000334, l1: 0.000307, l2: 0.000401, l3: 0.000534, l4: 0.000823, l5: 0.001307, l6: 0.001777
[epoch: 143/1000, batch:   776/ 1052, ite: 37540] train loss: 0.006579, tar: 0.000530 
l0: 0.000220, l1: 0.000224, l2: 0.000232, l3: 0.000259, l4: 0.000337, l5: 0.000606, l6: 0.000945
[epoch: 143/1000, batch:   856/ 1052, ite: 37560] train loss: 0.006578, tar: 0.000530 
l0: 0.000172, l1: 0.000180, l2: 0.000179, l3: 0.000192, l4: 0.000292, l5: 0.000585, l6: 0.000985
[epoch: 143/1000, batch:   936/ 1052, ite: 37580] train loss: 0.006576, tar: 0.000530 
l0: 0.000421, l1: 0.000427, l2: 0.000416, l3: 0.000490, l4: 0.000819, l5: 0.001031, l6: 0.002297
[epoch: 143/1000, batch:  1016/ 1052, ite: 37600] train loss: 0.006574, tar: 0.000530 
[Epoch 143/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000447, l1: 0.000434, l2: 0.000489, l3: 0.000645, l4: 0.000756, l5: 0.001309, l6: 0.001687
[epoch: 144/1000, batch:    44/ 1052, ite: 37620] train loss: 0.006577, tar: 0.000530 
l0: 0.000315, l1: 0.000318, l2: 0.000359, l3: 0.000491, l4: 0.000744, l5: 0.001040, l6: 0.001689
[epoch: 144/1000, batch:   124/ 1052, ite: 37640] train loss: 0.006575, tar: 0.000530 
l0: 0.000368, l1: 0.000347, l2: 0.000382, l3: 0.000558, l4: 0.000680, l5: 0.000795, l6: 0.001915
[epoch: 144/1000, batch:   204/ 1052, ite: 37660] train loss: 0.006572, tar: 0.000529 
l0: 0.000273, l1: 0.000275, l2: 0.000332, l3: 0.000383, l4: 0.000524, l5: 0.000889, l6: 0.001150
[epoch: 144/1000, batch:   284/ 1052, ite: 37680] train loss: 0.006571, tar: 0.000529 
l0: 0.000537, l1: 0.000546, l2: 0.000606, l3: 0.000592, l4: 0.000609, l5: 0.000928, l6: 0.001972
[epoch: 144/1000, batch:   364/ 1052, ite: 37700] train loss: 0.006572, tar: 0.000529 
l0: 0.000374, l1: 0.000390, l2: 0.000418, l3: 0.000422, l4: 0.000538, l5: 0.000804, l6: 0.001776
[epoch: 144/1000, batch:   444/ 1052, ite: 37720] train loss: 0.006570, tar: 0.000529 
l0: 0.000176, l1: 0.000169, l2: 0.000178, l3: 0.000230, l4: 0.000312, l5: 0.000656, l6: 0.001302
[epoch: 144/1000, batch:   524/ 1052, ite: 37740] train loss: 0.006565, tar: 0.000528 
l0: 0.000384, l1: 0.000391, l2: 0.000386, l3: 0.000439, l4: 0.000688, l5: 0.001296, l6: 0.002358
[epoch: 144/1000, batch:   604/ 1052, ite: 37760] train loss: 0.006568, tar: 0.000529 
l0: 0.000465, l1: 0.000466, l2: 0.000497, l3: 0.000634, l4: 0.000786, l5: 0.001671, l6: 0.001828
[epoch: 144/1000, batch:   684/ 1052, ite: 37780] train loss: 0.006567, tar: 0.000529 
l0: 0.000300, l1: 0.000295, l2: 0.000359, l3: 0.000386, l4: 0.000666, l5: 0.000928, l6: 0.001234
[epoch: 144/1000, batch:   764/ 1052, ite: 37800] train loss: 0.006566, tar: 0.000529 
l0: 0.000685, l1: 0.000663, l2: 0.000751, l3: 0.000884, l4: 0.001083, l5: 0.001321, l6: 0.001733
[epoch: 144/1000, batch:   844/ 1052, ite: 37820] train loss: 0.006567, tar: 0.000529 
l0: 0.000236, l1: 0.000239, l2: 0.000246, l3: 0.000299, l4: 0.000359, l5: 0.000826, l6: 0.001414
[epoch: 144/1000, batch:   924/ 1052, ite: 37840] train loss: 0.006566, tar: 0.000528 
l0: 0.000416, l1: 0.000404, l2: 0.000392, l3: 0.000522, l4: 0.000629, l5: 0.001094, l6: 0.001760
[epoch: 144/1000, batch:  1004/ 1052, ite: 37860] train loss: 0.006568, tar: 0.000529 
[Epoch 144/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000241, l1: 0.000236, l2: 0.000234, l3: 0.000321, l4: 0.000329, l5: 0.000626, l6: 0.000804
[epoch: 145/1000, batch:    32/ 1052, ite: 37880] train loss: 0.006567, tar: 0.000528 
l0: 0.000490, l1: 0.000528, l2: 0.000548, l3: 0.000524, l4: 0.000716, l5: 0.001120, l6: 0.003820
[epoch: 145/1000, batch:   112/ 1052, ite: 37900] train loss: 0.006565, tar: 0.000528 
l0: 0.000218, l1: 0.000224, l2: 0.000215, l3: 0.000260, l4: 0.000410, l5: 0.000664, l6: 0.001144
[epoch: 145/1000, batch:   192/ 1052, ite: 37920] train loss: 0.006564, tar: 0.000528 
l0: 0.000374, l1: 0.000341, l2: 0.000383, l3: 0.000537, l4: 0.000690, l5: 0.001135, l6: 0.001927
[epoch: 145/1000, batch:   272/ 1052, ite: 37940] train loss: 0.006563, tar: 0.000528 
l0: 0.000521, l1: 0.000515, l2: 0.000512, l3: 0.000594, l4: 0.000845, l5: 0.001213, l6: 0.002174
[epoch: 145/1000, batch:   352/ 1052, ite: 37960] train loss: 0.006562, tar: 0.000528 
l0: 0.000503, l1: 0.000502, l2: 0.000564, l3: 0.000542, l4: 0.000583, l5: 0.000990, l6: 0.002268
[epoch: 145/1000, batch:   432/ 1052, ite: 37980] train loss: 0.006566, tar: 0.000528 
l0: 0.000283, l1: 0.000288, l2: 0.000321, l3: 0.000354, l4: 0.000699, l5: 0.001161, l6: 0.001598
[epoch: 145/1000, batch:   512/ 1052, ite: 38000] train loss: 0.006567, tar: 0.000528 
l0: 0.000342, l1: 0.000343, l2: 0.000374, l3: 0.000391, l4: 0.000526, l5: 0.000878, l6: 0.001478
[epoch: 145/1000, batch:   592/ 1052, ite: 38020] train loss: 0.006567, tar: 0.000528 
l0: 0.000401, l1: 0.000392, l2: 0.000428, l3: 0.000541, l4: 0.000685, l5: 0.001096, l6: 0.002015
[epoch: 145/1000, batch:   672/ 1052, ite: 38040] train loss: 0.006563, tar: 0.000528 
l0: 0.000524, l1: 0.000546, l2: 0.000498, l3: 0.000511, l4: 0.000536, l5: 0.001036, l6: 0.001735
[epoch: 145/1000, batch:   752/ 1052, ite: 38060] train loss: 0.006562, tar: 0.000528 
l0: 0.000919, l1: 0.000912, l2: 0.000970, l3: 0.001147, l4: 0.001631, l5: 0.004192, l6: 0.005924
[epoch: 145/1000, batch:   832/ 1052, ite: 38080] train loss: 0.006561, tar: 0.000528 
l0: 0.000849, l1: 0.000852, l2: 0.000855, l3: 0.000893, l4: 0.001153, l5: 0.001916, l6: 0.002967
[epoch: 145/1000, batch:   912/ 1052, ite: 38100] train loss: 0.006562, tar: 0.000528 
l0: 0.000413, l1: 0.000408, l2: 0.000468, l3: 0.000493, l4: 0.000675, l5: 0.001434, l6: 0.002914
[epoch: 145/1000, batch:   992/ 1052, ite: 38120] train loss: 0.006561, tar: 0.000527 
[Epoch 145/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000540, l1: 0.000539, l2: 0.000575, l3: 0.000566, l4: 0.000602, l5: 0.001061, l6: 0.001386
[epoch: 146/1000, batch:    20/ 1052, ite: 38140] train loss: 0.006559, tar: 0.000527 
l0: 0.000387, l1: 0.000387, l2: 0.000481, l3: 0.000531, l4: 0.000692, l5: 0.000965, l6: 0.001138
[epoch: 146/1000, batch:   100/ 1052, ite: 38160] train loss: 0.006557, tar: 0.000527 
l0: 0.001165, l1: 0.001277, l2: 0.001227, l3: 0.001230, l4: 0.001504, l5: 0.002188, l6: 0.003867
[epoch: 146/1000, batch:   180/ 1052, ite: 38180] train loss: 0.006555, tar: 0.000527 
l0: 0.000167, l1: 0.000170, l2: 0.000166, l3: 0.000203, l4: 0.000312, l5: 0.000550, l6: 0.001146
[epoch: 146/1000, batch:   260/ 1052, ite: 38200] train loss: 0.006552, tar: 0.000526 
l0: 0.001308, l1: 0.001358, l2: 0.001248, l3: 0.001181, l4: 0.001791, l5: 0.002995, l6: 0.004316
[epoch: 146/1000, batch:   340/ 1052, ite: 38220] train loss: 0.006554, tar: 0.000527 
l0: 0.000196, l1: 0.000187, l2: 0.000198, l3: 0.000231, l4: 0.000418, l5: 0.000782, l6: 0.001345
[epoch: 146/1000, batch:   420/ 1052, ite: 38240] train loss: 0.006553, tar: 0.000527 
l0: 0.000366, l1: 0.000374, l2: 0.000389, l3: 0.000551, l4: 0.000811, l5: 0.000960, l6: 0.001552
[epoch: 146/1000, batch:   500/ 1052, ite: 38260] train loss: 0.006553, tar: 0.000526 
l0: 0.000312, l1: 0.000317, l2: 0.000331, l3: 0.000385, l4: 0.000548, l5: 0.000676, l6: 0.001738
[epoch: 146/1000, batch:   580/ 1052, ite: 38280] train loss: 0.006551, tar: 0.000526 
l0: 0.000450, l1: 0.000440, l2: 0.000489, l3: 0.000565, l4: 0.000657, l5: 0.001085, l6: 0.001670
[epoch: 146/1000, batch:   660/ 1052, ite: 38300] train loss: 0.006552, tar: 0.000526 
l0: 0.000206, l1: 0.000214, l2: 0.000218, l3: 0.000267, l4: 0.000371, l5: 0.000533, l6: 0.001029
[epoch: 146/1000, batch:   740/ 1052, ite: 38320] train loss: 0.006553, tar: 0.000526 
l0: 0.000442, l1: 0.000443, l2: 0.000478, l3: 0.000594, l4: 0.000820, l5: 0.001236, l6: 0.002336
[epoch: 146/1000, batch:   820/ 1052, ite: 38340] train loss: 0.006556, tar: 0.000527 
l0: 0.000861, l1: 0.000886, l2: 0.000779, l3: 0.000991, l4: 0.001318, l5: 0.001315, l6: 0.001981
[epoch: 146/1000, batch:   900/ 1052, ite: 38360] train loss: 0.006557, tar: 0.000527 
l0: 0.000679, l1: 0.000728, l2: 0.000712, l3: 0.000741, l4: 0.000687, l5: 0.001056, l6: 0.001475
[epoch: 146/1000, batch:   980/ 1052, ite: 38380] train loss: 0.006557, tar: 0.000527 
[Epoch 146/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000819, l1: 0.000830, l2: 0.000908, l3: 0.001076, l4: 0.001270, l5: 0.001711, l6: 0.002930
[epoch: 147/1000, batch:     8/ 1052, ite: 38400] train loss: 0.006556, tar: 0.000527 
l0: 0.000542, l1: 0.000586, l2: 0.000667, l3: 0.000617, l4: 0.000789, l5: 0.001735, l6: 0.001984
[epoch: 147/1000, batch:    88/ 1052, ite: 38420] train loss: 0.006556, tar: 0.000527 
l0: 0.000368, l1: 0.000377, l2: 0.000374, l3: 0.000415, l4: 0.000550, l5: 0.000812, l6: 0.001729
[epoch: 147/1000, batch:   168/ 1052, ite: 38440] train loss: 0.006556, tar: 0.000527 
l0: 0.000379, l1: 0.000385, l2: 0.000370, l3: 0.000455, l4: 0.000637, l5: 0.001066, l6: 0.002067
[epoch: 147/1000, batch:   248/ 1052, ite: 38460] train loss: 0.006556, tar: 0.000527 
l0: 0.000601, l1: 0.000644, l2: 0.000611, l3: 0.000558, l4: 0.000573, l5: 0.001123, l6: 0.002247
[epoch: 147/1000, batch:   328/ 1052, ite: 38480] train loss: 0.006557, tar: 0.000527 
l0: 0.001296, l1: 0.001368, l2: 0.001475, l3: 0.001323, l4: 0.001421, l5: 0.001653, l6: 0.002475
[epoch: 147/1000, batch:   408/ 1052, ite: 38500] train loss: 0.006556, tar: 0.000527 
l0: 0.000795, l1: 0.000784, l2: 0.000830, l3: 0.000877, l4: 0.001098, l5: 0.001321, l6: 0.003124
[epoch: 147/1000, batch:   488/ 1052, ite: 38520] train loss: 0.006557, tar: 0.000527 
l0: 0.000251, l1: 0.000248, l2: 0.000290, l3: 0.000318, l4: 0.000403, l5: 0.000774, l6: 0.001383
[epoch: 147/1000, batch:   568/ 1052, ite: 38540] train loss: 0.006557, tar: 0.000527 
l0: 0.000225, l1: 0.000219, l2: 0.000282, l3: 0.000326, l4: 0.000470, l5: 0.000774, l6: 0.001539
[epoch: 147/1000, batch:   648/ 1052, ite: 38560] train loss: 0.006559, tar: 0.000527 
l0: 0.000278, l1: 0.000295, l2: 0.000312, l3: 0.000317, l4: 0.000496, l5: 0.001310, l6: 0.001495
[epoch: 147/1000, batch:   728/ 1052, ite: 38580] train loss: 0.006557, tar: 0.000527 
l0: 0.000267, l1: 0.000301, l2: 0.000286, l3: 0.000335, l4: 0.000466, l5: 0.000783, l6: 0.001917
[epoch: 147/1000, batch:   808/ 1052, ite: 38600] train loss: 0.006556, tar: 0.000527 
l0: 0.000571, l1: 0.000570, l2: 0.000692, l3: 0.000789, l4: 0.001122, l5: 0.001859, l6: 0.004309
[epoch: 147/1000, batch:   888/ 1052, ite: 38620] train loss: 0.006555, tar: 0.000527 
l0: 0.000365, l1: 0.000385, l2: 0.000375, l3: 0.000435, l4: 0.000651, l5: 0.001284, l6: 0.001786
[epoch: 147/1000, batch:   968/ 1052, ite: 38640] train loss: 0.006554, tar: 0.000526 
l0: 0.002315, l1: 0.002042, l2: 0.002217, l3: 0.004145, l4: 0.005871, l5: 0.003168, l6: 0.006031
[epoch: 147/1000, batch:  1048/ 1052, ite: 38660] train loss: 0.006558, tar: 0.000527 
[Epoch 147/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.001164, l1: 0.001411, l2: 0.001218, l3: 0.000927, l4: 0.001158, l5: 0.001754, l6: 0.003852
[epoch: 148/1000, batch:    76/ 1052, ite: 38680] train loss: 0.006559, tar: 0.000527 
l0: 0.000472, l1: 0.000466, l2: 0.000516, l3: 0.000632, l4: 0.000808, l5: 0.000819, l6: 0.001542
[epoch: 148/1000, batch:   156/ 1052, ite: 38700] train loss: 0.006558, tar: 0.000527 
l0: 0.000360, l1: 0.000373, l2: 0.000388, l3: 0.000393, l4: 0.000554, l5: 0.001054, l6: 0.001937
[epoch: 148/1000, batch:   236/ 1052, ite: 38720] train loss: 0.006562, tar: 0.000527 
l0: 0.000714, l1: 0.000732, l2: 0.001444, l3: 0.001308, l4: 0.001893, l5: 0.001654, l6: 0.002731
[epoch: 148/1000, batch:   316/ 1052, ite: 38740] train loss: 0.006571, tar: 0.000528 
l0: 0.000773, l1: 0.000806, l2: 0.000611, l3: 0.000932, l4: 0.000846, l5: 0.001349, l6: 0.001910
[epoch: 148/1000, batch:   396/ 1052, ite: 38760] train loss: 0.006581, tar: 0.000529 
l0: 0.000680, l1: 0.000663, l2: 0.000976, l3: 0.000836, l4: 0.001201, l5: 0.001826, l6: 0.003137
[epoch: 148/1000, batch:   476/ 1052, ite: 38780] train loss: 0.006588, tar: 0.000530 
l0: 0.000205, l1: 0.000200, l2: 0.000222, l3: 0.000240, l4: 0.000419, l5: 0.000863, l6: 0.001384
[epoch: 148/1000, batch:   556/ 1052, ite: 38800] train loss: 0.006591, tar: 0.000530 
l0: 0.000354, l1: 0.000431, l2: 0.000430, l3: 0.000688, l4: 0.000737, l5: 0.001224, l6: 0.001651
[epoch: 148/1000, batch:   636/ 1052, ite: 38820] train loss: 0.006593, tar: 0.000530 
l0: 0.000829, l1: 0.000972, l2: 0.001010, l3: 0.000837, l4: 0.001164, l5: 0.002006, l6: 0.003950
[epoch: 148/1000, batch:   716/ 1052, ite: 38840] train loss: 0.006594, tar: 0.000530 
l0: 0.000397, l1: 0.000419, l2: 0.000384, l3: 0.000457, l4: 0.000706, l5: 0.001574, l6: 0.002417
[epoch: 148/1000, batch:   796/ 1052, ite: 38860] train loss: 0.006595, tar: 0.000531 
l0: 0.000649, l1: 0.000738, l2: 0.000609, l3: 0.000682, l4: 0.001021, l5: 0.001054, l6: 0.002180
[epoch: 148/1000, batch:   876/ 1052, ite: 38880] train loss: 0.006595, tar: 0.000531 
l0: 0.001319, l1: 0.001407, l2: 0.001570, l3: 0.001868, l4: 0.001366, l5: 0.002108, l6: 0.002515
[epoch: 148/1000, batch:   956/ 1052, ite: 38900] train loss: 0.006596, tar: 0.000531 
l0: 0.000879, l1: 0.000833, l2: 0.000868, l3: 0.000951, l4: 0.001411, l5: 0.002275, l6: 0.002987
[epoch: 148/1000, batch:  1036/ 1052, ite: 38920] train loss: 0.006595, tar: 0.000531 
[Epoch 148/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000367, l1: 0.000373, l2: 0.000421, l3: 0.000446, l4: 0.000720, l5: 0.001193, l6: 0.002279
[epoch: 149/1000, batch:    64/ 1052, ite: 38940] train loss: 0.006600, tar: 0.000531 
l0: 0.000669, l1: 0.000938, l2: 0.000762, l3: 0.000751, l4: 0.000906, l5: 0.001264, l6: 0.002247
[epoch: 149/1000, batch:   144/ 1052, ite: 38960] train loss: 0.006602, tar: 0.000532 
l0: 0.000483, l1: 0.000493, l2: 0.000585, l3: 0.000583, l4: 0.000806, l5: 0.001004, l6: 0.002626
[epoch: 149/1000, batch:   224/ 1052, ite: 38980] train loss: 0.006604, tar: 0.000532 
l0: 0.000321, l1: 0.000305, l2: 0.000320, l3: 0.000405, l4: 0.000749, l5: 0.000845, l6: 0.001550
[epoch: 149/1000, batch:   304/ 1052, ite: 39000] train loss: 0.006605, tar: 0.000532 
l0: 0.000421, l1: 0.000434, l2: 0.000416, l3: 0.000490, l4: 0.000578, l5: 0.000780, l6: 0.002062
[epoch: 149/1000, batch:   384/ 1052, ite: 39020] train loss: 0.006604, tar: 0.000532 
l0: 0.000363, l1: 0.000369, l2: 0.000365, l3: 0.000465, l4: 0.000593, l5: 0.000992, l6: 0.001971
[epoch: 149/1000, batch:   464/ 1052, ite: 39040] train loss: 0.006606, tar: 0.000532 
l0: 0.000448, l1: 0.000462, l2: 0.000419, l3: 0.000463, l4: 0.000775, l5: 0.001219, l6: 0.002437
[epoch: 149/1000, batch:   544/ 1052, ite: 39060] train loss: 0.006606, tar: 0.000532 
l0: 0.000553, l1: 0.000571, l2: 0.000543, l3: 0.000520, l4: 0.000660, l5: 0.000650, l6: 0.001718
[epoch: 149/1000, batch:   624/ 1052, ite: 39080] train loss: 0.006606, tar: 0.000532 
l0: 0.000363, l1: 0.000401, l2: 0.000397, l3: 0.000367, l4: 0.000618, l5: 0.000812, l6: 0.001441
[epoch: 149/1000, batch:   704/ 1052, ite: 39100] train loss: 0.006607, tar: 0.000532 
l0: 0.000425, l1: 0.000407, l2: 0.000429, l3: 0.000513, l4: 0.000823, l5: 0.001493, l6: 0.002439
[epoch: 149/1000, batch:   784/ 1052, ite: 39120] train loss: 0.006610, tar: 0.000532 
l0: 0.000527, l1: 0.000550, l2: 0.000564, l3: 0.000577, l4: 0.000827, l5: 0.000890, l6: 0.002377
[epoch: 149/1000, batch:   864/ 1052, ite: 39140] train loss: 0.006609, tar: 0.000532 
l0: 0.000560, l1: 0.000551, l2: 0.000654, l3: 0.000749, l4: 0.001124, l5: 0.001601, l6: 0.002622
[epoch: 149/1000, batch:   944/ 1052, ite: 39160] train loss: 0.006610, tar: 0.000532 
l0: 0.000364, l1: 0.000353, l2: 0.000371, l3: 0.000475, l4: 0.000556, l5: 0.001083, l6: 0.001892
[epoch: 149/1000, batch:  1024/ 1052, ite: 39180] train loss: 0.006611, tar: 0.000533 
[Epoch 149/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000602, l1: 0.000594, l2: 0.000593, l3: 0.000678, l4: 0.000968, l5: 0.001601, l6: 0.002626
[epoch: 150/1000, batch:    52/ 1052, ite: 39200] train loss: 0.006611, tar: 0.000533 
l0: 0.000576, l1: 0.000568, l2: 0.000611, l3: 0.000691, l4: 0.000939, l5: 0.001113, l6: 0.003026
[epoch: 150/1000, batch:   132/ 1052, ite: 39220] train loss: 0.006613, tar: 0.000533 
l0: 0.000759, l1: 0.000730, l2: 0.000793, l3: 0.000874, l4: 0.001434, l5: 0.002242, l6: 0.003238
[epoch: 150/1000, batch:   212/ 1052, ite: 39240] train loss: 0.006612, tar: 0.000533 
l0: 0.000663, l1: 0.000657, l2: 0.000747, l3: 0.000750, l4: 0.000909, l5: 0.001532, l6: 0.003098
[epoch: 150/1000, batch:   292/ 1052, ite: 39260] train loss: 0.006612, tar: 0.000533 
l0: 0.000350, l1: 0.000339, l2: 0.000346, l3: 0.000383, l4: 0.000512, l5: 0.000913, l6: 0.001419
[epoch: 150/1000, batch:   372/ 1052, ite: 39280] train loss: 0.006609, tar: 0.000532 
l0: 0.000663, l1: 0.000718, l2: 0.000575, l3: 0.000611, l4: 0.000738, l5: 0.000883, l6: 0.001094
[epoch: 150/1000, batch:   452/ 1052, ite: 39300] train loss: 0.006608, tar: 0.000532 
l0: 0.000767, l1: 0.000888, l2: 0.000823, l3: 0.000824, l4: 0.000874, l5: 0.001391, l6: 0.002372
[epoch: 150/1000, batch:   532/ 1052, ite: 39320] train loss: 0.006608, tar: 0.000532 
l0: 0.000512, l1: 0.000503, l2: 0.000602, l3: 0.000592, l4: 0.000902, l5: 0.001618, l6: 0.002406
[epoch: 150/1000, batch:   612/ 1052, ite: 39340] train loss: 0.006607, tar: 0.000532 
l0: 0.000353, l1: 0.000362, l2: 0.000365, l3: 0.000423, l4: 0.000543, l5: 0.000675, l6: 0.001785
[epoch: 150/1000, batch:   692/ 1052, ite: 39360] train loss: 0.006605, tar: 0.000532 
l0: 0.000533, l1: 0.000562, l2: 0.000547, l3: 0.000647, l4: 0.000751, l5: 0.001861, l6: 0.002756
[epoch: 150/1000, batch:   772/ 1052, ite: 39380] train loss: 0.006607, tar: 0.000532 
l0: 0.000383, l1: 0.000395, l2: 0.000397, l3: 0.000485, l4: 0.000586, l5: 0.000855, l6: 0.001963
[epoch: 150/1000, batch:   852/ 1052, ite: 39400] train loss: 0.006608, tar: 0.000532 
l0: 0.000349, l1: 0.000346, l2: 0.000363, l3: 0.000429, l4: 0.000527, l5: 0.000891, l6: 0.002147
[epoch: 150/1000, batch:   932/ 1052, ite: 39420] train loss: 0.006606, tar: 0.000532 
l0: 0.000365, l1: 0.000399, l2: 0.000379, l3: 0.000359, l4: 0.000484, l5: 0.000850, l6: 0.001615
[epoch: 150/1000, batch:  1012/ 1052, ite: 39440] train loss: 0.006606, tar: 0.000532 
[Epoch 150/1000] Test loss: 0.008, Test target loss: 0.001
l0: 0.000324, l1: 0.000338, l2: 0.000368, l3: 0.000410, l4: 0.000690, l5: 0.001025, l6: 0.001798
[epoch: 151/1000, batch:    40/ 1052, ite: 39460] train loss: 0.006606, tar: 0.000532 
l0: 0.000183, l1: 0.000194, l2: 0.000207, l3: 0.000227, l4: 0.000327, l5: 0.000551, l6: 0.000820
[epoch: 151/1000, batch:   120/ 1052, ite: 39480] train loss: 0.006605, tar: 0.000532 
l0: 0.000683, l1: 0.000612, l2: 0.000662, l3: 0.000880, l4: 0.001023, l5: 0.001539, l6: 0.002733
[epoch: 151/1000, batch:   200/ 1052, ite: 39500] train loss: 0.006605, tar: 0.000532 
l0: 0.000524, l1: 0.000520, l2: 0.000539, l3: 0.000694, l4: 0.000903, l5: 0.001634, l6: 0.002587
[epoch: 151/1000, batch:   280/ 1052, ite: 39520] train loss: 0.006605, tar: 0.000532 
l0: 0.000658, l1: 0.000666, l2: 0.000885, l3: 0.000837, l4: 0.000917, l5: 0.001818, l6: 0.002742
[epoch: 151/1000, batch:   360/ 1052, ite: 39540] train loss: 0.006604, tar: 0.000532 
l0: 0.000482, l1: 0.000501, l2: 0.000547, l3: 0.000587, l4: 0.000953, l5: 0.001650, l6: 0.002929
[epoch: 151/1000, batch:   440/ 1052, ite: 39560] train loss: 0.006604, tar: 0.000532 
l0: 0.000459, l1: 0.000461, l2: 0.000471, l3: 0.000581, l4: 0.000724, l5: 0.001382, l6: 0.002497
[epoch: 151/1000, batch:   520/ 1052, ite: 39580] train loss: 0.006604, tar: 0.000532 
l0: 0.000569, l1: 0.000541, l2: 0.000673, l3: 0.000716, l4: 0.000862, l5: 0.000955, l6: 0.001542
[epoch: 151/1000, batch:   600/ 1052, ite: 39600] train loss: 0.006602, tar: 0.000532 
l0: 0.000987, l1: 0.000928, l2: 0.000975, l3: 0.001013, l4: 0.001301, l5: 0.001873, l6: 0.003198
[epoch: 151/1000, batch:   680/ 1052, ite: 39620] train loss: 0.006603, tar: 0.000532 
l0: 0.000707, l1: 0.000752, l2: 0.000767, l3: 0.000936, l4: 0.001318, l5: 0.001550, l6: 0.003439
[epoch: 151/1000, batch:   760/ 1052, ite: 39640] train loss: 0.006603, tar: 0.000531 
l0: 0.000551, l1: 0.000509, l2: 0.000542, l3: 0.000606, l4: 0.000921, l5: 0.001183, l6: 0.001725
[epoch: 151/1000, batch:   840/ 1052, ite: 39660] train loss: 0.006602, tar: 0.000531 
l0: 0.000794, l1: 0.000803, l2: 0.000828, l3: 0.000854, l4: 0.000968, l5: 0.001296, l6: 0.002848
[epoch: 151/1000, batch:   920/ 1052, ite: 39680] train loss: 0.006601, tar: 0.000531 
l0: 0.000502, l1: 0.000493, l2: 0.000554, l3: 0.000639, l4: 0.000811, l5: 0.001614, l6: 0.002012
[epoch: 151/1000, batch:  1000/ 1052, ite: 39700] train loss: 0.006601, tar: 0.000531 
[Epoch 151/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000234, l1: 0.000242, l2: 0.000264, l3: 0.000287, l4: 0.000342, l5: 0.000685, l6: 0.000898
[epoch: 152/1000, batch:    28/ 1052, ite: 39720] train loss: 0.006599, tar: 0.000531 
l0: 0.000240, l1: 0.000229, l2: 0.000237, l3: 0.000296, l4: 0.000491, l5: 0.000727, l6: 0.001400
[epoch: 152/1000, batch:   108/ 1052, ite: 39740] train loss: 0.006599, tar: 0.000531 
l0: 0.000218, l1: 0.000207, l2: 0.000212, l3: 0.000292, l4: 0.000454, l5: 0.000576, l6: 0.001039
[epoch: 152/1000, batch:   188/ 1052, ite: 39760] train loss: 0.006596, tar: 0.000531 
l0: 0.000437, l1: 0.000427, l2: 0.000456, l3: 0.000522, l4: 0.000806, l5: 0.001656, l6: 0.002809
[epoch: 152/1000, batch:   268/ 1052, ite: 39780] train loss: 0.006595, tar: 0.000531 
l0: 0.000350, l1: 0.000365, l2: 0.000367, l3: 0.000398, l4: 0.000535, l5: 0.000925, l6: 0.001925
[epoch: 152/1000, batch:   348/ 1052, ite: 39800] train loss: 0.006595, tar: 0.000531 
l0: 0.000432, l1: 0.000426, l2: 0.000436, l3: 0.000508, l4: 0.000734, l5: 0.001070, l6: 0.002697
[epoch: 152/1000, batch:   428/ 1052, ite: 39820] train loss: 0.006593, tar: 0.000530 
l0: 0.000316, l1: 0.000311, l2: 0.000363, l3: 0.000427, l4: 0.000549, l5: 0.001066, l6: 0.002003
[epoch: 152/1000, batch:   508/ 1052, ite: 39840] train loss: 0.006591, tar: 0.000530 
l0: 0.000384, l1: 0.000405, l2: 0.000405, l3: 0.000383, l4: 0.000551, l5: 0.000974, l6: 0.001817
[epoch: 152/1000, batch:   588/ 1052, ite: 39860] train loss: 0.006593, tar: 0.000530 
l0: 0.000352, l1: 0.000360, l2: 0.000343, l3: 0.000374, l4: 0.000530, l5: 0.000691, l6: 0.001513
[epoch: 152/1000, batch:   668/ 1052, ite: 39880] train loss: 0.006593, tar: 0.000530 
l0: 0.001009, l1: 0.001056, l2: 0.001068, l3: 0.001208, l4: 0.001538, l5: 0.002410, l6: 0.004015
[epoch: 152/1000, batch:   748/ 1052, ite: 39900] train loss: 0.006593, tar: 0.000530 
l0: 0.000466, l1: 0.000480, l2: 0.000506, l3: 0.000541, l4: 0.000686, l5: 0.001152, l6: 0.001758
[epoch: 152/1000, batch:   828/ 1052, ite: 39920] train loss: 0.006593, tar: 0.000530 
l0: 0.000644, l1: 0.000612, l2: 0.000633, l3: 0.000827, l4: 0.000904, l5: 0.001439, l6: 0.001847
[epoch: 152/1000, batch:   908/ 1052, ite: 39940] train loss: 0.006593, tar: 0.000530 
l0: 0.000271, l1: 0.000293, l2: 0.000369, l3: 0.000366, l4: 0.000569, l5: 0.001097, l6: 0.001622
[epoch: 152/1000, batch:   988/ 1052, ite: 39960] train loss: 0.006590, tar: 0.000530 
[Epoch 152/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000464, l1: 0.000493, l2: 0.000426, l3: 0.000581, l4: 0.000797, l5: 0.001153, l6: 0.001374
[epoch: 153/1000, batch:    16/ 1052, ite: 39980] train loss: 0.006589, tar: 0.000530 
l0: 0.000618, l1: 0.000602, l2: 0.000639, l3: 0.000810, l4: 0.001003, l5: 0.001628, l6: 0.002872
[epoch: 153/1000, batch:    96/ 1052, ite: 40000] train loss: 0.006588, tar: 0.000530 
l0: 0.000727, l1: 0.000757, l2: 0.000709, l3: 0.000802, l4: 0.000797, l5: 0.001460, l6: 0.002047
[epoch: 153/1000, batch:   176/ 1052, ite: 40020] train loss: 0.006586, tar: 0.000529 
l0: 0.000262, l1: 0.000273, l2: 0.000276, l3: 0.000265, l4: 0.000471, l5: 0.000882, l6: 0.001313
[epoch: 153/1000, batch:   256/ 1052, ite: 40040] train loss: 0.006586, tar: 0.000529 
l0: 0.000811, l1: 0.000822, l2: 0.000994, l3: 0.000908, l4: 0.000974, l5: 0.001524, l6: 0.002517
[epoch: 153/1000, batch:   336/ 1052, ite: 40060] train loss: 0.006586, tar: 0.000529 
l0: 0.000362, l1: 0.000371, l2: 0.000405, l3: 0.000434, l4: 0.000622, l5: 0.000780, l6: 0.001421
[epoch: 153/1000, batch:   416/ 1052, ite: 40080] train loss: 0.006586, tar: 0.000529 
l0: 0.000284, l1: 0.000282, l2: 0.000279, l3: 0.000315, l4: 0.000512, l5: 0.000705, l6: 0.001615
[epoch: 153/1000, batch:   496/ 1052, ite: 40100] train loss: 0.006586, tar: 0.000529 
l0: 0.000523, l1: 0.000497, l2: 0.000562, l3: 0.000610, l4: 0.001033, l5: 0.002066, l6: 0.002703
[epoch: 153/1000, batch:   576/ 1052, ite: 40120] train loss: 0.006585, tar: 0.000529 
l0: 0.000300, l1: 0.000300, l2: 0.000305, l3: 0.000371, l4: 0.000527, l5: 0.000938, l6: 0.001437
[epoch: 153/1000, batch:   656/ 1052, ite: 40140] train loss: 0.006586, tar: 0.000529 
l0: 0.000337, l1: 0.000345, l2: 0.000363, l3: 0.000376, l4: 0.000653, l5: 0.001163, l6: 0.001944
[epoch: 153/1000, batch:   736/ 1052, ite: 40160] train loss: 0.006585, tar: 0.000529 
l0: 0.000311, l1: 0.000327, l2: 0.000327, l3: 0.000361, l4: 0.000505, l5: 0.001138, l6: 0.001469
[epoch: 153/1000, batch:   816/ 1052, ite: 40180] train loss: 0.006581, tar: 0.000528 
l0: 0.000535, l1: 0.000500, l2: 0.000533, l3: 0.000695, l4: 0.001037, l5: 0.001944, l6: 0.002532
[epoch: 153/1000, batch:   896/ 1052, ite: 40200] train loss: 0.006580, tar: 0.000528 
l0: 0.000198, l1: 0.000221, l2: 0.000199, l3: 0.000203, l4: 0.000306, l5: 0.000620, l6: 0.000441
[epoch: 153/1000, batch:   976/ 1052, ite: 40220] train loss: 0.006578, tar: 0.000528 
[Epoch 153/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000995, l1: 0.001025, l2: 0.001027, l3: 0.000986, l4: 0.001388, l5: 0.002490, l6: 0.004542
[epoch: 154/1000, batch:     4/ 1052, ite: 40240] train loss: 0.006580, tar: 0.000528 
l0: 0.000622, l1: 0.000649, l2: 0.000580, l3: 0.000612, l4: 0.000683, l5: 0.000803, l6: 0.001803
[epoch: 154/1000, batch:    84/ 1052, ite: 40260] train loss: 0.006579, tar: 0.000528 
l0: 0.000463, l1: 0.000496, l2: 0.000467, l3: 0.000518, l4: 0.000687, l5: 0.001153, l6: 0.001924
[epoch: 154/1000, batch:   164/ 1052, ite: 40280] train loss: 0.006578, tar: 0.000528 
l0: 0.000480, l1: 0.000472, l2: 0.000540, l3: 0.000580, l4: 0.000833, l5: 0.001708, l6: 0.003101
[epoch: 154/1000, batch:   244/ 1052, ite: 40300] train loss: 0.006577, tar: 0.000528 
l0: 0.000347, l1: 0.000355, l2: 0.000405, l3: 0.000431, l4: 0.000911, l5: 0.001212, l6: 0.002537
[epoch: 154/1000, batch:   324/ 1052, ite: 40320] train loss: 0.006577, tar: 0.000528 
l0: 0.001040, l1: 0.001119, l2: 0.001044, l3: 0.001023, l4: 0.001423, l5: 0.002208, l6: 0.004455
[epoch: 154/1000, batch:   404/ 1052, ite: 40340] train loss: 0.006579, tar: 0.000528 
l0: 0.000335, l1: 0.000341, l2: 0.000371, l3: 0.000406, l4: 0.000602, l5: 0.000946, l6: 0.001886
[epoch: 154/1000, batch:   484/ 1052, ite: 40360] train loss: 0.006577, tar: 0.000528 
l0: 0.000603, l1: 0.000584, l2: 0.000605, l3: 0.000789, l4: 0.001149, l5: 0.001606, l6: 0.003032
[epoch: 154/1000, batch:   564/ 1052, ite: 40380] train loss: 0.006578, tar: 0.000528 
l0: 0.000237, l1: 0.000237, l2: 0.000258, l3: 0.000404, l4: 0.000567, l5: 0.000465, l6: 0.000904
[epoch: 154/1000, batch:   644/ 1052, ite: 40400] train loss: 0.006576, tar: 0.000528 
l0: 0.000330, l1: 0.000365, l2: 0.000387, l3: 0.000356, l4: 0.000471, l5: 0.001262, l6: 0.001862
[epoch: 154/1000, batch:   724/ 1052, ite: 40420] train loss: 0.006576, tar: 0.000528 
l0: 0.000475, l1: 0.000486, l2: 0.000548, l3: 0.000575, l4: 0.000584, l5: 0.001013, l6: 0.002172
[epoch: 154/1000, batch:   804/ 1052, ite: 40440] train loss: 0.006576, tar: 0.000528 
l0: 0.000243, l1: 0.000240, l2: 0.000261, l3: 0.000287, l4: 0.000362, l5: 0.001110, l6: 0.001670
[epoch: 154/1000, batch:   884/ 1052, ite: 40460] train loss: 0.006574, tar: 0.000527 
l0: 0.000367, l1: 0.000381, l2: 0.000381, l3: 0.000399, l4: 0.000862, l5: 0.001542, l6: 0.002014
[epoch: 154/1000, batch:   964/ 1052, ite: 40480] train loss: 0.006573, tar: 0.000527 
l0: 0.000743, l1: 0.000781, l2: 0.000827, l3: 0.000784, l4: 0.000824, l5: 0.001628, l6: 0.003093
[epoch: 154/1000, batch:  1044/ 1052, ite: 40500] train loss: 0.006570, tar: 0.000527 
[Epoch 154/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000429, l1: 0.000436, l2: 0.000399, l3: 0.000524, l4: 0.000638, l5: 0.000836, l6: 0.001509
[epoch: 155/1000, batch:    72/ 1052, ite: 40520] train loss: 0.006569, tar: 0.000527 
l0: 0.000221, l1: 0.000235, l2: 0.000241, l3: 0.000224, l4: 0.000366, l5: 0.000668, l6: 0.001451
[epoch: 155/1000, batch:   152/ 1052, ite: 40540] train loss: 0.006569, tar: 0.000527 
l0: 0.000609, l1: 0.000639, l2: 0.000648, l3: 0.000679, l4: 0.001143, l5: 0.001681, l6: 0.003745
[epoch: 155/1000, batch:   232/ 1052, ite: 40560] train loss: 0.006567, tar: 0.000527 
l0: 0.000725, l1: 0.000741, l2: 0.000721, l3: 0.000726, l4: 0.001070, l5: 0.001944, l6: 0.003739
[epoch: 155/1000, batch:   312/ 1052, ite: 40580] train loss: 0.006566, tar: 0.000527 
l0: 0.000921, l1: 0.000881, l2: 0.001063, l3: 0.001289, l4: 0.001777, l5: 0.003755, l6: 0.004922
[epoch: 155/1000, batch:   392/ 1052, ite: 40600] train loss: 0.006564, tar: 0.000526 
l0: 0.000611, l1: 0.000636, l2: 0.000652, l3: 0.000691, l4: 0.001001, l5: 0.001617, l6: 0.002706
[epoch: 155/1000, batch:   472/ 1052, ite: 40620] train loss: 0.006565, tar: 0.000527 
l0: 0.000323, l1: 0.000322, l2: 0.000366, l3: 0.000401, l4: 0.000594, l5: 0.000935, l6: 0.001672
[epoch: 155/1000, batch:   552/ 1052, ite: 40640] train loss: 0.006564, tar: 0.000526 
l0: 0.000792, l1: 0.000839, l2: 0.000812, l3: 0.000815, l4: 0.000981, l5: 0.001377, l6: 0.002870
[epoch: 155/1000, batch:   632/ 1052, ite: 40660] train loss: 0.006563, tar: 0.000526 
l0: 0.000330, l1: 0.000336, l2: 0.000432, l3: 0.000435, l4: 0.000576, l5: 0.000949, l6: 0.001213
[epoch: 155/1000, batch:   712/ 1052, ite: 40680] train loss: 0.006564, tar: 0.000526 
l0: 0.000781, l1: 0.000781, l2: 0.000810, l3: 0.000791, l4: 0.001164, l5: 0.002643, l6: 0.003518
[epoch: 155/1000, batch:   792/ 1052, ite: 40700] train loss: 0.006564, tar: 0.000526 
l0: 0.000542, l1: 0.000532, l2: 0.000642, l3: 0.000610, l4: 0.000715, l5: 0.001370, l6: 0.002445
[epoch: 155/1000, batch:   872/ 1052, ite: 40720] train loss: 0.006565, tar: 0.000526 
l0: 0.000531, l1: 0.000528, l2: 0.000585, l3: 0.000571, l4: 0.000739, l5: 0.001524, l6: 0.002014
[epoch: 155/1000, batch:   952/ 1052, ite: 40740] train loss: 0.006564, tar: 0.000526 
l0: 0.000271, l1: 0.000257, l2: 0.000301, l3: 0.000378, l4: 0.000592, l5: 0.000886, l6: 0.001378
[epoch: 155/1000, batch:  1032/ 1052, ite: 40760] train loss: 0.006562, tar: 0.000526 
[Epoch 155/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000332, l1: 0.000339, l2: 0.000357, l3: 0.000403, l4: 0.000492, l5: 0.001027, l6: 0.002079
[epoch: 156/1000, batch:    60/ 1052, ite: 40780] train loss: 0.006559, tar: 0.000526 
l0: 0.000736, l1: 0.000789, l2: 0.000788, l3: 0.000886, l4: 0.001160, l5: 0.001759, l6: 0.002704
[epoch: 156/1000, batch:   140/ 1052, ite: 40800] train loss: 0.006560, tar: 0.000526 
l0: 0.000707, l1: 0.000706, l2: 0.000690, l3: 0.000786, l4: 0.001173, l5: 0.002144, l6: 0.002539
[epoch: 156/1000, batch:   220/ 1052, ite: 40820] train loss: 0.006559, tar: 0.000526 
l0: 0.000654, l1: 0.000630, l2: 0.000716, l3: 0.000769, l4: 0.001129, l5: 0.001786, l6: 0.002672
[epoch: 156/1000, batch:   300/ 1052, ite: 40840] train loss: 0.006558, tar: 0.000526 
l0: 0.000177, l1: 0.000174, l2: 0.000184, l3: 0.000219, l4: 0.000276, l5: 0.000569, l6: 0.000755
[epoch: 156/1000, batch:   380/ 1052, ite: 40860] train loss: 0.006559, tar: 0.000526 
l0: 0.000817, l1: 0.000782, l2: 0.000867, l3: 0.000949, l4: 0.001675, l5: 0.002363, l6: 0.003960
[epoch: 156/1000, batch:   460/ 1052, ite: 40880] train loss: 0.006559, tar: 0.000526 
l0: 0.000306, l1: 0.000307, l2: 0.000317, l3: 0.000343, l4: 0.000515, l5: 0.000926, l6: 0.001796
[epoch: 156/1000, batch:   540/ 1052, ite: 40900] train loss: 0.006557, tar: 0.000525 
l0: 0.000441, l1: 0.000450, l2: 0.000404, l3: 0.000530, l4: 0.000618, l5: 0.001209, l6: 0.002100
[epoch: 156/1000, batch:   620/ 1052, ite: 40920] train loss: 0.006556, tar: 0.000525 
l0: 0.000479, l1: 0.000510, l2: 0.000533, l3: 0.000592, l4: 0.000775, l5: 0.001488, l6: 0.003047
[epoch: 156/1000, batch:   700/ 1052, ite: 40940] train loss: 0.006556, tar: 0.000525 
l0: 0.001423, l1: 0.001542, l2: 0.001456, l3: 0.001367, l4: 0.001631, l5: 0.002618, l6: 0.005127
[epoch: 156/1000, batch:   780/ 1052, ite: 40960] train loss: 0.006556, tar: 0.000525 
l0: 0.000397, l1: 0.000398, l2: 0.000400, l3: 0.000455, l4: 0.000672, l5: 0.001125, l6: 0.002231
[epoch: 156/1000, batch:   860/ 1052, ite: 40980] train loss: 0.006554, tar: 0.000525 
l0: 0.000266, l1: 0.000275, l2: 0.000275, l3: 0.000323, l4: 0.000554, l5: 0.000453, l6: 0.001447
[epoch: 156/1000, batch:   940/ 1052, ite: 41000] train loss: 0.006551, tar: 0.000525 
l0: 0.000316, l1: 0.000303, l2: 0.000320, l3: 0.000399, l4: 0.000512, l5: 0.001060, l6: 0.001419
[epoch: 156/1000, batch:  1020/ 1052, ite: 41020] train loss: 0.006550, tar: 0.000525 
[Epoch 156/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000445, l1: 0.000431, l2: 0.000538, l3: 0.000691, l4: 0.000926, l5: 0.001626, l6: 0.003127
[epoch: 157/1000, batch:    48/ 1052, ite: 41040] train loss: 0.006550, tar: 0.000525 
l0: 0.000580, l1: 0.000593, l2: 0.000515, l3: 0.000559, l4: 0.000618, l5: 0.000957, l6: 0.001529
[epoch: 157/1000, batch:   128/ 1052, ite: 41060] train loss: 0.006549, tar: 0.000524 
l0: 0.000459, l1: 0.000463, l2: 0.000428, l3: 0.000485, l4: 0.000764, l5: 0.000843, l6: 0.001903
[epoch: 157/1000, batch:   208/ 1052, ite: 41080] train loss: 0.006548, tar: 0.000524 
l0: 0.000327, l1: 0.000332, l2: 0.000337, l3: 0.000311, l4: 0.000446, l5: 0.000763, l6: 0.001492
[epoch: 157/1000, batch:   288/ 1052, ite: 41100] train loss: 0.006546, tar: 0.000524 
l0: 0.000492, l1: 0.000493, l2: 0.000487, l3: 0.000574, l4: 0.000707, l5: 0.001309, l6: 0.002772
[epoch: 157/1000, batch:   368/ 1052, ite: 41120] train loss: 0.006545, tar: 0.000524 
l0: 0.000809, l1: 0.000886, l2: 0.000661, l3: 0.000737, l4: 0.000962, l5: 0.001513, l6: 0.002515
[epoch: 157/1000, batch:   448/ 1052, ite: 41140] train loss: 0.006545, tar: 0.000524 
l0: 0.000535, l1: 0.000530, l2: 0.000592, l3: 0.000651, l4: 0.000865, l5: 0.001324, l6: 0.002854
[epoch: 157/1000, batch:   528/ 1052, ite: 41160] train loss: 0.006544, tar: 0.000524 
l0: 0.000799, l1: 0.000819, l2: 0.000794, l3: 0.000903, l4: 0.001535, l5: 0.002189, l6: 0.002942
[epoch: 157/1000, batch:   608/ 1052, ite: 41180] train loss: 0.006543, tar: 0.000524 
l0: 0.000414, l1: 0.000413, l2: 0.000456, l3: 0.000483, l4: 0.000588, l5: 0.001203, l6: 0.002126
[epoch: 157/1000, batch:   688/ 1052, ite: 41200] train loss: 0.006542, tar: 0.000524 
l0: 0.000539, l1: 0.000536, l2: 0.000602, l3: 0.000770, l4: 0.000991, l5: 0.001867, l6: 0.002351
[epoch: 157/1000, batch:   768/ 1052, ite: 41220] train loss: 0.006542, tar: 0.000524 
l0: 0.000286, l1: 0.000281, l2: 0.000306, l3: 0.000389, l4: 0.000593, l5: 0.001061, l6: 0.002020
[epoch: 157/1000, batch:   848/ 1052, ite: 41240] train loss: 0.006540, tar: 0.000523 
l0: 0.000329, l1: 0.000323, l2: 0.000361, l3: 0.000371, l4: 0.000550, l5: 0.000852, l6: 0.001564
[epoch: 157/1000, batch:   928/ 1052, ite: 41260] train loss: 0.006539, tar: 0.000523 
l0: 0.000410, l1: 0.000403, l2: 0.000429, l3: 0.000568, l4: 0.000830, l5: 0.001531, l6: 0.001782
[epoch: 157/1000, batch:  1008/ 1052, ite: 41280] train loss: 0.006540, tar: 0.000523 
[Epoch 157/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000924, l1: 0.000972, l2: 0.000967, l3: 0.000919, l4: 0.001086, l5: 0.001732, l6: 0.002391
[epoch: 158/1000, batch:    36/ 1052, ite: 41300] train loss: 0.006539, tar: 0.000523 
l0: 0.000555, l1: 0.000539, l2: 0.000657, l3: 0.000676, l4: 0.000923, l5: 0.001251, l6: 0.002476
[epoch: 158/1000, batch:   116/ 1052, ite: 41320] train loss: 0.006538, tar: 0.000523 
l0: 0.000325, l1: 0.000314, l2: 0.000340, l3: 0.000421, l4: 0.000522, l5: 0.000732, l6: 0.001779
[epoch: 158/1000, batch:   196/ 1052, ite: 41340] train loss: 0.006538, tar: 0.000523 
l0: 0.000469, l1: 0.000419, l2: 0.000506, l3: 0.000685, l4: 0.000919, l5: 0.000874, l6: 0.001132
[epoch: 158/1000, batch:   276/ 1052, ite: 41360] train loss: 0.006537, tar: 0.000523 
l0: 0.000353, l1: 0.000388, l2: 0.000402, l3: 0.000449, l4: 0.000741, l5: 0.001243, l6: 0.001506
[epoch: 158/1000, batch:   356/ 1052, ite: 41380] train loss: 0.006536, tar: 0.000523 
l0: 0.000359, l1: 0.000372, l2: 0.000449, l3: 0.000492, l4: 0.000642, l5: 0.001005, l6: 0.002176
[epoch: 158/1000, batch:   436/ 1052, ite: 41400] train loss: 0.006536, tar: 0.000523 
l0: 0.000554, l1: 0.000558, l2: 0.000569, l3: 0.000655, l4: 0.000735, l5: 0.001699, l6: 0.002630
[epoch: 158/1000, batch:   516/ 1052, ite: 41420] train loss: 0.006535, tar: 0.000522 
l0: 0.000253, l1: 0.000239, l2: 0.000263, l3: 0.000358, l4: 0.000481, l5: 0.001097, l6: 0.002278
[epoch: 158/1000, batch:   596/ 1052, ite: 41440] train loss: 0.006534, tar: 0.000522 
l0: 0.000165, l1: 0.000170, l2: 0.000173, l3: 0.000220, l4: 0.000425, l5: 0.000667, l6: 0.001119
[epoch: 158/1000, batch:   676/ 1052, ite: 41460] train loss: 0.006532, tar: 0.000522 
l0: 0.000357, l1: 0.000345, l2: 0.000373, l3: 0.000451, l4: 0.000579, l5: 0.000819, l6: 0.001965
[epoch: 158/1000, batch:   756/ 1052, ite: 41480] train loss: 0.006531, tar: 0.000522 
l0: 0.000384, l1: 0.000420, l2: 0.000436, l3: 0.000457, l4: 0.000705, l5: 0.001293, l6: 0.001889
[epoch: 158/1000, batch:   836/ 1052, ite: 41500] train loss: 0.006530, tar: 0.000522 
l0: 0.000331, l1: 0.000318, l2: 0.000369, l3: 0.000466, l4: 0.000785, l5: 0.000995, l6: 0.001923
[epoch: 158/1000, batch:   916/ 1052, ite: 41520] train loss: 0.006529, tar: 0.000522 
l0: 0.000830, l1: 0.000878, l2: 0.000837, l3: 0.000883, l4: 0.000998, l5: 0.001200, l6: 0.003175
[epoch: 158/1000, batch:   996/ 1052, ite: 41540] train loss: 0.006531, tar: 0.000522 
[Epoch 158/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000361, l1: 0.000349, l2: 0.000370, l3: 0.000518, l4: 0.000657, l5: 0.001203, l6: 0.002048
[epoch: 159/1000, batch:    24/ 1052, ite: 41560] train loss: 0.006530, tar: 0.000522 
l0: 0.001208, l1: 0.001220, l2: 0.001235, l3: 0.001466, l4: 0.002503, l5: 0.004079, l6: 0.008155
[epoch: 159/1000, batch:   104/ 1052, ite: 41580] train loss: 0.006533, tar: 0.000522 
l0: 0.000332, l1: 0.000333, l2: 0.000364, l3: 0.000406, l4: 0.000677, l5: 0.001362, l6: 0.002422
[epoch: 159/1000, batch:   184/ 1052, ite: 41600] train loss: 0.006531, tar: 0.000522 
l0: 0.000488, l1: 0.000517, l2: 0.000529, l3: 0.000567, l4: 0.000841, l5: 0.001673, l6: 0.002382
[epoch: 159/1000, batch:   264/ 1052, ite: 41620] train loss: 0.006531, tar: 0.000522 
l0: 0.000325, l1: 0.000335, l2: 0.000346, l3: 0.000344, l4: 0.000567, l5: 0.000700, l6: 0.001471
[epoch: 159/1000, batch:   344/ 1052, ite: 41640] train loss: 0.006531, tar: 0.000522 
l0: 0.000620, l1: 0.000641, l2: 0.000710, l3: 0.000673, l4: 0.000769, l5: 0.001004, l6: 0.002517
[epoch: 159/1000, batch:   424/ 1052, ite: 41660] train loss: 0.006530, tar: 0.000522 
l0: 0.000566, l1: 0.001209, l2: 0.000994, l3: 0.000484, l4: 0.000611, l5: 0.000933, l6: 0.001743
[epoch: 159/1000, batch:   504/ 1052, ite: 41680] train loss: 0.006533, tar: 0.000522 
l0: 0.000518, l1: 0.000525, l2: 0.000510, l3: 0.000683, l4: 0.000774, l5: 0.001089, l6: 0.001935
[epoch: 159/1000, batch:   584/ 1052, ite: 41700] train loss: 0.006533, tar: 0.000522 
l0: 0.000407, l1: 0.000411, l2: 0.000428, l3: 0.000496, l4: 0.000716, l5: 0.001306, l6: 0.001597
[epoch: 159/1000, batch:   664/ 1052, ite: 41720] train loss: 0.006534, tar: 0.000522 
l0: 0.000464, l1: 0.000456, l2: 0.000452, l3: 0.000617, l4: 0.000700, l5: 0.001107, l6: 0.001698
[epoch: 159/1000, batch:   744/ 1052, ite: 41740] train loss: 0.006532, tar: 0.000522 
l0: 0.000268, l1: 0.000276, l2: 0.000300, l3: 0.000356, l4: 0.000406, l5: 0.001014, l6: 0.001284
[epoch: 159/1000, batch:   824/ 1052, ite: 41760] train loss: 0.006532, tar: 0.000522 
l0: 0.000169, l1: 0.000204, l2: 0.000230, l3: 0.000236, l4: 0.000458, l5: 0.000717, l6: 0.001440
[epoch: 159/1000, batch:   904/ 1052, ite: 41780] train loss: 0.006544, tar: 0.000524 
l0: 0.000633, l1: 0.000668, l2: 0.000708, l3: 0.000731, l4: 0.001254, l5: 0.001250, l6: 0.002182
[epoch: 159/1000, batch:   984/ 1052, ite: 41800] train loss: 0.006549, tar: 0.000524 
[Epoch 159/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000755, l1: 0.001004, l2: 0.000442, l3: 0.001073, l4: 0.001296, l5: 0.001562, l6: 0.001635
[epoch: 160/1000, batch:    12/ 1052, ite: 41820] train loss: 0.006549, tar: 0.000524 
l0: 0.000441, l1: 0.000406, l2: 0.000544, l3: 0.000558, l4: 0.000753, l5: 0.001609, l6: 0.002419
[epoch: 160/1000, batch:    92/ 1052, ite: 41840] train loss: 0.006551, tar: 0.000524 
l0: 0.000287, l1: 0.000316, l2: 0.000334, l3: 0.000488, l4: 0.000644, l5: 0.000932, l6: 0.001136
[epoch: 160/1000, batch:   172/ 1052, ite: 41860] train loss: 0.006556, tar: 0.000525 
l0: 0.000833, l1: 0.000876, l2: 0.000906, l3: 0.000936, l4: 0.000947, l5: 0.001727, l6: 0.002937
[epoch: 160/1000, batch:   252/ 1052, ite: 41880] train loss: 0.006557, tar: 0.000525 
l0: 0.000391, l1: 0.000386, l2: 0.000390, l3: 0.000580, l4: 0.000798, l5: 0.001166, l6: 0.002029
[epoch: 160/1000, batch:   332/ 1052, ite: 41900] train loss: 0.006562, tar: 0.000526 
l0: 0.000784, l1: 0.000771, l2: 0.000858, l3: 0.000911, l4: 0.001019, l5: 0.001838, l6: 0.002953
[epoch: 160/1000, batch:   412/ 1052, ite: 41920] train loss: 0.006565, tar: 0.000526 
l0: 0.000575, l1: 0.000593, l2: 0.000652, l3: 0.000612, l4: 0.001078, l5: 0.001082, l6: 0.002224
[epoch: 160/1000, batch:   492/ 1052, ite: 41940] train loss: 0.006566, tar: 0.000526 
l0: 0.000638, l1: 0.000655, l2: 0.000644, l3: 0.000757, l4: 0.000846, l5: 0.001354, l6: 0.001853
[epoch: 160/1000, batch:   572/ 1052, ite: 41960] train loss: 0.006566, tar: 0.000526 
l0: 0.000484, l1: 0.000473, l2: 0.000554, l3: 0.000597, l4: 0.000751, l5: 0.001022, l6: 0.001657
[epoch: 160/1000, batch:   652/ 1052, ite: 41980] train loss: 0.006567, tar: 0.000526 
l0: 0.000517, l1: 0.000507, l2: 0.000528, l3: 0.000621, l4: 0.000819, l5: 0.001340, l6: 0.003035
[epoch: 160/1000, batch:   732/ 1052, ite: 42000] train loss: 0.006567, tar: 0.000526 
l0: 0.000796, l1: 0.000796, l2: 0.000860, l3: 0.000921, l4: 0.001146, l5: 0.001490, l6: 0.003051
[epoch: 160/1000, batch:   812/ 1052, ite: 42020] train loss: 0.006571, tar: 0.000527 
l0: 0.000730, l1: 0.000713, l2: 0.000749, l3: 0.000884, l4: 0.001479, l5: 0.001840, l6: 0.003536
[epoch: 160/1000, batch:   892/ 1052, ite: 42040] train loss: 0.006572, tar: 0.000527 
l0: 0.000376, l1: 0.000409, l2: 0.000389, l3: 0.000375, l4: 0.000496, l5: 0.000668, l6: 0.001589
[epoch: 160/1000, batch:   972/ 1052, ite: 42060] train loss: 0.006571, tar: 0.000527 
l0: 0.000279, l1: 0.000297, l2: 0.000280, l3: 0.000303, l4: 0.000345, l5: 0.000460, l6: 0.000776
[epoch: 160/1000, batch:  1052/ 1052, ite: 42080] train loss: 0.006571, tar: 0.000527 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_42080.pkl
[Epoch 160/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000666, l1: 0.000661, l2: 0.000704, l3: 0.000726, l4: 0.001305, l5: 0.001682, l6: 0.003104
[epoch: 161/1000, batch:    80/ 1052, ite: 42100] train loss: 0.007713, tar: 0.000596 
l0: 0.000288, l1: 0.000279, l2: 0.000291, l3: 0.000322, l4: 0.000453, l5: 0.000823, l6: 0.001375
[epoch: 161/1000, batch:   160/ 1052, ite: 42120] train loss: 0.007113, tar: 0.000549 
l0: 0.000182, l1: 0.000189, l2: 0.000199, l3: 0.000196, l4: 0.000276, l5: 0.000744, l6: 0.000931
[epoch: 161/1000, batch:   240/ 1052, ite: 42140] train loss: 0.007130, tar: 0.000559 
l0: 0.000424, l1: 0.000425, l2: 0.000429, l3: 0.000418, l4: 0.000640, l5: 0.001081, l6: 0.001392
[epoch: 161/1000, batch:   320/ 1052, ite: 42160] train loss: 0.007036, tar: 0.000552 
l0: 0.001829, l1: 0.001554, l2: 0.001976, l3: 0.004149, l4: 0.001912, l5: 0.003365, l6: 0.005267
[epoch: 161/1000, batch:   400/ 1052, ite: 42180] train loss: 0.007145, tar: 0.000583 
l0: 0.000464, l1: 0.000439, l2: 0.000441, l3: 0.000569, l4: 0.000918, l5: 0.001345, l6: 0.003282
[epoch: 161/1000, batch:   480/ 1052, ite: 42200] train loss: 0.006840, tar: 0.000557 
l0: 0.000348, l1: 0.000324, l2: 0.000346, l3: 0.000369, l4: 0.000865, l5: 0.001215, l6: 0.001150
[epoch: 161/1000, batch:   560/ 1052, ite: 42220] train loss: 0.006818, tar: 0.000556 
l0: 0.000733, l1: 0.000804, l2: 0.000847, l3: 0.000730, l4: 0.000967, l5: 0.001854, l6: 0.002104
[epoch: 161/1000, batch:   640/ 1052, ite: 42240] train loss: 0.006923, tar: 0.000571 
l0: 0.000352, l1: 0.000385, l2: 0.000392, l3: 0.000354, l4: 0.000531, l5: 0.001064, l6: 0.001712
[epoch: 161/1000, batch:   720/ 1052, ite: 42260] train loss: 0.006866, tar: 0.000566 
l0: 0.000457, l1: 0.000474, l2: 0.000498, l3: 0.000542, l4: 0.000683, l5: 0.001354, l6: 0.001664
[epoch: 161/1000, batch:   800/ 1052, ite: 42280] train loss: 0.006853, tar: 0.000562 
l0: 0.000523, l1: 0.000544, l2: 0.000495, l3: 0.000582, l4: 0.000585, l5: 0.000912, l6: 0.002182
[epoch: 161/1000, batch:   880/ 1052, ite: 42300] train loss: 0.006754, tar: 0.000551 
l0: 0.000407, l1: 0.000402, l2: 0.000393, l3: 0.000477, l4: 0.000647, l5: 0.001074, l6: 0.001789
[epoch: 161/1000, batch:   960/ 1052, ite: 42320] train loss: 0.006665, tar: 0.000542 
l0: 0.000615, l1: 0.000664, l2: 0.000676, l3: 0.000738, l4: 0.001185, l5: 0.001915, l6: 0.002793
[epoch: 161/1000, batch:  1040/ 1052, ite: 42340] train loss: 0.006574, tar: 0.000535 
[Epoch 161/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000698, l1: 0.000698, l2: 0.000729, l3: 0.000760, l4: 0.000867, l5: 0.001202, l6: 0.001789
[epoch: 162/1000, batch:    68/ 1052, ite: 42360] train loss: 0.006549, tar: 0.000531 
l0: 0.000567, l1: 0.000584, l2: 0.000564, l3: 0.000634, l4: 0.000799, l5: 0.001270, l6: 0.002362
[epoch: 162/1000, batch:   148/ 1052, ite: 42380] train loss: 0.006599, tar: 0.000534 
l0: 0.000404, l1: 0.000433, l2: 0.000375, l3: 0.000453, l4: 0.000667, l5: 0.001099, l6: 0.001928
[epoch: 162/1000, batch:   228/ 1052, ite: 42400] train loss: 0.006574, tar: 0.000530 
l0: 0.000662, l1: 0.000667, l2: 0.000629, l3: 0.000720, l4: 0.000982, l5: 0.001355, l6: 0.002406
[epoch: 162/1000, batch:   308/ 1052, ite: 42420] train loss: 0.006568, tar: 0.000529 
l0: 0.000310, l1: 0.000301, l2: 0.000305, l3: 0.000319, l4: 0.000608, l5: 0.001213, l6: 0.002020
[epoch: 162/1000, batch:   388/ 1052, ite: 42440] train loss: 0.006535, tar: 0.000525 
l0: 0.000383, l1: 0.000371, l2: 0.000418, l3: 0.000451, l4: 0.000687, l5: 0.001351, l6: 0.002294
[epoch: 162/1000, batch:   468/ 1052, ite: 42460] train loss: 0.006515, tar: 0.000522 
l0: 0.000403, l1: 0.000449, l2: 0.000443, l3: 0.000448, l4: 0.000608, l5: 0.001151, l6: 0.001613
[epoch: 162/1000, batch:   548/ 1052, ite: 42480] train loss: 0.006513, tar: 0.000519 
l0: 0.000300, l1: 0.000271, l2: 0.000318, l3: 0.000460, l4: 0.000539, l5: 0.000791, l6: 0.001242
[epoch: 162/1000, batch:   628/ 1052, ite: 42500] train loss: 0.006486, tar: 0.000517 
l0: 0.000409, l1: 0.000410, l2: 0.000428, l3: 0.000449, l4: 0.000621, l5: 0.000788, l6: 0.001708
[epoch: 162/1000, batch:   708/ 1052, ite: 42520] train loss: 0.006446, tar: 0.000513 
l0: 0.000783, l1: 0.000756, l2: 0.000844, l3: 0.000981, l4: 0.000932, l5: 0.001678, l6: 0.002836
[epoch: 162/1000, batch:   788/ 1052, ite: 42540] train loss: 0.006430, tar: 0.000512 
l0: 0.000238, l1: 0.000237, l2: 0.000281, l3: 0.000363, l4: 0.000419, l5: 0.001055, l6: 0.000930
[epoch: 162/1000, batch:   868/ 1052, ite: 42560] train loss: 0.006398, tar: 0.000508 
l0: 0.000694, l1: 0.000655, l2: 0.000676, l3: 0.000825, l4: 0.001117, l5: 0.001720, l6: 0.002442
[epoch: 162/1000, batch:   948/ 1052, ite: 42580] train loss: 0.006396, tar: 0.000510 
l0: 0.000384, l1: 0.000379, l2: 0.000380, l3: 0.000423, l4: 0.000655, l5: 0.001217, l6: 0.001682
[epoch: 162/1000, batch:  1028/ 1052, ite: 42600] train loss: 0.006439, tar: 0.000516 
[Epoch 162/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000828, l1: 0.000894, l2: 0.000713, l3: 0.000816, l4: 0.000976, l5: 0.001589, l6: 0.002280
[epoch: 163/1000, batch:    56/ 1052, ite: 42620] train loss: 0.006474, tar: 0.000519 
l0: 0.000517, l1: 0.000513, l2: 0.000491, l3: 0.000590, l4: 0.000822, l5: 0.001835, l6: 0.002783
[epoch: 163/1000, batch:   136/ 1052, ite: 42640] train loss: 0.006464, tar: 0.000516 
l0: 0.000421, l1: 0.000406, l2: 0.000432, l3: 0.000483, l4: 0.000683, l5: 0.001279, l6: 0.002139
[epoch: 163/1000, batch:   216/ 1052, ite: 42660] train loss: 0.006430, tar: 0.000513 
l0: 0.000355, l1: 0.000340, l2: 0.000396, l3: 0.000433, l4: 0.000580, l5: 0.000994, l6: 0.001618
[epoch: 163/1000, batch:   296/ 1052, ite: 42680] train loss: 0.006410, tar: 0.000511 
l0: 0.000553, l1: 0.000573, l2: 0.000608, l3: 0.000624, l4: 0.000710, l5: 0.001594, l6: 0.001746
[epoch: 163/1000, batch:   376/ 1052, ite: 42700] train loss: 0.006432, tar: 0.000514 
l0: 0.000404, l1: 0.000424, l2: 0.000460, l3: 0.000487, l4: 0.000644, l5: 0.000919, l6: 0.001687
[epoch: 163/1000, batch:   456/ 1052, ite: 42720] train loss: 0.006432, tar: 0.000513 
l0: 0.000483, l1: 0.000481, l2: 0.000500, l3: 0.000530, l4: 0.000687, l5: 0.001251, l6: 0.001684
[epoch: 163/1000, batch:   536/ 1052, ite: 42740] train loss: 0.006408, tar: 0.000511 
l0: 0.000299, l1: 0.000277, l2: 0.000311, l3: 0.000402, l4: 0.000567, l5: 0.000952, l6: 0.001102
[epoch: 163/1000, batch:   616/ 1052, ite: 42760] train loss: 0.006411, tar: 0.000512 
l0: 0.000248, l1: 0.000248, l2: 0.000302, l3: 0.000331, l4: 0.000310, l5: 0.000858, l6: 0.001222
[epoch: 163/1000, batch:   696/ 1052, ite: 42780] train loss: 0.006400, tar: 0.000510 
l0: 0.000220, l1: 0.000230, l2: 0.000237, l3: 0.000221, l4: 0.000390, l5: 0.000662, l6: 0.001406
[epoch: 163/1000, batch:   776/ 1052, ite: 42800] train loss: 0.006356, tar: 0.000506 
l0: 0.000946, l1: 0.001039, l2: 0.001018, l3: 0.001002, l4: 0.001441, l5: 0.003373, l6: 0.005259
[epoch: 163/1000, batch:   856/ 1052, ite: 42820] train loss: 0.006375, tar: 0.000506 
l0: 0.000754, l1: 0.000747, l2: 0.000784, l3: 0.000789, l4: 0.001068, l5: 0.002276, l6: 0.003152
[epoch: 163/1000, batch:   936/ 1052, ite: 42840] train loss: 0.006389, tar: 0.000507 
l0: 0.000371, l1: 0.000369, l2: 0.000415, l3: 0.000444, l4: 0.000766, l5: 0.001357, l6: 0.001734
[epoch: 163/1000, batch:  1016/ 1052, ite: 42860] train loss: 0.006396, tar: 0.000508 
[Epoch 163/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000391, l1: 0.000399, l2: 0.000401, l3: 0.000456, l4: 0.000551, l5: 0.000833, l6: 0.001345
[epoch: 164/1000, batch:    44/ 1052, ite: 42880] train loss: 0.006374, tar: 0.000506 
l0: 0.000279, l1: 0.000271, l2: 0.000279, l3: 0.000317, l4: 0.000420, l5: 0.000857, l6: 0.001629
[epoch: 164/1000, batch:   124/ 1052, ite: 42900] train loss: 0.006353, tar: 0.000505 
l0: 0.000842, l1: 0.000881, l2: 0.000883, l3: 0.001033, l4: 0.001470, l5: 0.001927, l6: 0.004267
[epoch: 164/1000, batch:   204/ 1052, ite: 42920] train loss: 0.006359, tar: 0.000504 
l0: 0.000311, l1: 0.000325, l2: 0.000357, l3: 0.000390, l4: 0.000587, l5: 0.000892, l6: 0.001748
[epoch: 164/1000, batch:   284/ 1052, ite: 42940] train loss: 0.006350, tar: 0.000503 
l0: 0.000506, l1: 0.000486, l2: 0.000499, l3: 0.000626, l4: 0.000750, l5: 0.001231, l6: 0.002192
[epoch: 164/1000, batch:   364/ 1052, ite: 42960] train loss: 0.006348, tar: 0.000503 
l0: 0.000202, l1: 0.000202, l2: 0.000224, l3: 0.000225, l4: 0.000396, l5: 0.000677, l6: 0.001212
[epoch: 164/1000, batch:   444/ 1052, ite: 42980] train loss: 0.006364, tar: 0.000504 
l0: 0.001318, l1: 0.001424, l2: 0.001450, l3: 0.001262, l4: 0.001597, l5: 0.003248, l6: 0.004947
[epoch: 164/1000, batch:   524/ 1052, ite: 43000] train loss: 0.006347, tar: 0.000503 
l0: 0.000517, l1: 0.000423, l2: 0.000469, l3: 0.000749, l4: 0.001312, l5: 0.001465, l6: 0.001634
[epoch: 164/1000, batch:   604/ 1052, ite: 43020] train loss: 0.006346, tar: 0.000503 
l0: 0.000548, l1: 0.000548, l2: 0.000625, l3: 0.000664, l4: 0.001051, l5: 0.001604, l6: 0.003085
[epoch: 164/1000, batch:   684/ 1052, ite: 43040] train loss: 0.006358, tar: 0.000504 
l0: 0.000216, l1: 0.000224, l2: 0.000219, l3: 0.000228, l4: 0.000362, l5: 0.000480, l6: 0.000638
[epoch: 164/1000, batch:   764/ 1052, ite: 43060] train loss: 0.006351, tar: 0.000502 
l0: 0.000284, l1: 0.000286, l2: 0.000310, l3: 0.000338, l4: 0.000436, l5: 0.000572, l6: 0.000976
[epoch: 164/1000, batch:   844/ 1052, ite: 43080] train loss: 0.006355, tar: 0.000502 
l0: 0.000339, l1: 0.000302, l2: 0.000344, l3: 0.000489, l4: 0.000713, l5: 0.001241, l6: 0.001898
[epoch: 164/1000, batch:   924/ 1052, ite: 43100] train loss: 0.006332, tar: 0.000499 
l0: 0.000452, l1: 0.000527, l2: 0.000423, l3: 0.000409, l4: 0.000483, l5: 0.000719, l6: 0.001868
[epoch: 164/1000, batch:  1004/ 1052, ite: 43120] train loss: 0.006324, tar: 0.000499 
[Epoch 164/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000503, l1: 0.000508, l2: 0.000494, l3: 0.000602, l4: 0.000735, l5: 0.000954, l6: 0.001696
[epoch: 165/1000, batch:    32/ 1052, ite: 43140] train loss: 0.006306, tar: 0.000497 
l0: 0.000353, l1: 0.000347, l2: 0.000376, l3: 0.000416, l4: 0.000561, l5: 0.001300, l6: 0.001302
[epoch: 165/1000, batch:   112/ 1052, ite: 43160] train loss: 0.006313, tar: 0.000498 
l0: 0.000330, l1: 0.000334, l2: 0.000324, l3: 0.000360, l4: 0.000464, l5: 0.000873, l6: 0.001614
[epoch: 165/1000, batch:   192/ 1052, ite: 43180] train loss: 0.006304, tar: 0.000497 
l0: 0.000251, l1: 0.000249, l2: 0.000277, l3: 0.000378, l4: 0.000549, l5: 0.001103, l6: 0.001670
[epoch: 165/1000, batch:   272/ 1052, ite: 43200] train loss: 0.006303, tar: 0.000496 
l0: 0.000679, l1: 0.000702, l2: 0.000707, l3: 0.000824, l4: 0.001121, l5: 0.001855, l6: 0.002436
[epoch: 165/1000, batch:   352/ 1052, ite: 43220] train loss: 0.006305, tar: 0.000496 
l0: 0.000243, l1: 0.000235, l2: 0.000261, l3: 0.000339, l4: 0.000599, l5: 0.001134, l6: 0.001737
[epoch: 165/1000, batch:   432/ 1052, ite: 43240] train loss: 0.006302, tar: 0.000495 
l0: 0.001016, l1: 0.001089, l2: 0.001086, l3: 0.001084, l4: 0.001200, l5: 0.002237, l6: 0.003309
[epoch: 165/1000, batch:   512/ 1052, ite: 43260] train loss: 0.006306, tar: 0.000495 
l0: 0.000816, l1: 0.000849, l2: 0.000927, l3: 0.000889, l4: 0.001239, l5: 0.001714, l6: 0.002983
[epoch: 165/1000, batch:   592/ 1052, ite: 43280] train loss: 0.006321, tar: 0.000496 
l0: 0.000315, l1: 0.000308, l2: 0.000310, l3: 0.000390, l4: 0.000477, l5: 0.001428, l6: 0.002001
[epoch: 165/1000, batch:   672/ 1052, ite: 43300] train loss: 0.006316, tar: 0.000496 
l0: 0.001070, l1: 0.001083, l2: 0.001032, l3: 0.001257, l4: 0.001441, l5: 0.002352, l6: 0.003238
[epoch: 165/1000, batch:   752/ 1052, ite: 43320] train loss: 0.006309, tar: 0.000495 
l0: 0.000333, l1: 0.000323, l2: 0.000334, l3: 0.000393, l4: 0.000547, l5: 0.001513, l6: 0.001841
[epoch: 165/1000, batch:   832/ 1052, ite: 43340] train loss: 0.006304, tar: 0.000495 
l0: 0.000498, l1: 0.000502, l2: 0.000527, l3: 0.000627, l4: 0.000889, l5: 0.001277, l6: 0.002247
[epoch: 165/1000, batch:   912/ 1052, ite: 43360] train loss: 0.006300, tar: 0.000494 
l0: 0.000897, l1: 0.001072, l2: 0.001009, l3: 0.000874, l4: 0.000915, l5: 0.002027, l6: 0.002828
[epoch: 165/1000, batch:   992/ 1052, ite: 43380] train loss: 0.006285, tar: 0.000493 
[Epoch 165/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000249, l1: 0.000247, l2: 0.000281, l3: 0.000357, l4: 0.000482, l5: 0.000722, l6: 0.001962
[epoch: 166/1000, batch:    20/ 1052, ite: 43400] train loss: 0.006278, tar: 0.000493 
l0: 0.000197, l1: 0.000200, l2: 0.000196, l3: 0.000240, l4: 0.000406, l5: 0.000737, l6: 0.001037
[epoch: 166/1000, batch:   100/ 1052, ite: 43420] train loss: 0.006278, tar: 0.000492 
l0: 0.000361, l1: 0.000366, l2: 0.000375, l3: 0.000351, l4: 0.000632, l5: 0.001310, l6: 0.002121
[epoch: 166/1000, batch:   180/ 1052, ite: 43440] train loss: 0.006267, tar: 0.000491 
l0: 0.000344, l1: 0.000348, l2: 0.000391, l3: 0.000460, l4: 0.000761, l5: 0.001119, l6: 0.001780
[epoch: 166/1000, batch:   260/ 1052, ite: 43460] train loss: 0.006268, tar: 0.000491 
l0: 0.000548, l1: 0.000500, l2: 0.000516, l3: 0.000597, l4: 0.000570, l5: 0.000932, l6: 0.001109
[epoch: 166/1000, batch:   340/ 1052, ite: 43480] train loss: 0.006262, tar: 0.000490 
l0: 0.000463, l1: 0.000453, l2: 0.000412, l3: 0.000454, l4: 0.000647, l5: 0.001629, l6: 0.001928
[epoch: 166/1000, batch:   420/ 1052, ite: 43500] train loss: 0.006263, tar: 0.000490 
l0: 0.000272, l1: 0.000293, l2: 0.000296, l3: 0.000329, l4: 0.000566, l5: 0.000799, l6: 0.001367
[epoch: 166/1000, batch:   500/ 1052, ite: 43520] train loss: 0.006258, tar: 0.000489 
l0: 0.000656, l1: 0.000650, l2: 0.000710, l3: 0.000764, l4: 0.000906, l5: 0.001295, l6: 0.002495
[epoch: 166/1000, batch:   580/ 1052, ite: 43540] train loss: 0.006254, tar: 0.000489 
l0: 0.000575, l1: 0.000623, l2: 0.000616, l3: 0.000641, l4: 0.001013, l5: 0.001710, l6: 0.002361
[epoch: 166/1000, batch:   660/ 1052, ite: 43560] train loss: 0.006251, tar: 0.000489 
l0: 0.000278, l1: 0.000264, l2: 0.000371, l3: 0.000447, l4: 0.000526, l5: 0.000904, l6: 0.001759
[epoch: 166/1000, batch:   740/ 1052, ite: 43580] train loss: 0.006240, tar: 0.000488 
l0: 0.000329, l1: 0.000333, l2: 0.000385, l3: 0.000387, l4: 0.000467, l5: 0.000781, l6: 0.001469
[epoch: 166/1000, batch:   820/ 1052, ite: 43600] train loss: 0.006251, tar: 0.000489 
l0: 0.000493, l1: 0.000524, l2: 0.000557, l3: 0.000547, l4: 0.000691, l5: 0.001461, l6: 0.001620
[epoch: 166/1000, batch:   900/ 1052, ite: 43620] train loss: 0.006252, tar: 0.000489 
l0: 0.000219, l1: 0.000218, l2: 0.000267, l3: 0.000274, l4: 0.000444, l5: 0.000770, l6: 0.001165
[epoch: 166/1000, batch:   980/ 1052, ite: 43640] train loss: 0.006244, tar: 0.000488 
[Epoch 166/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000250, l1: 0.000250, l2: 0.000272, l3: 0.000284, l4: 0.000439, l5: 0.000998, l6: 0.001256
[epoch: 167/1000, batch:     8/ 1052, ite: 43660] train loss: 0.006240, tar: 0.000488 
l0: 0.000763, l1: 0.000755, l2: 0.000907, l3: 0.000986, l4: 0.001315, l5: 0.001965, l6: 0.003608
[epoch: 167/1000, batch:    88/ 1052, ite: 43680] train loss: 0.006246, tar: 0.000487 
l0: 0.000877, l1: 0.000904, l2: 0.000856, l3: 0.000893, l4: 0.001104, l5: 0.001444, l6: 0.002641
[epoch: 167/1000, batch:   168/ 1052, ite: 43700] train loss: 0.006242, tar: 0.000487 
l0: 0.000819, l1: 0.000774, l2: 0.000858, l3: 0.001100, l4: 0.001476, l5: 0.002603, l6: 0.004068
[epoch: 167/1000, batch:   248/ 1052, ite: 43720] train loss: 0.006233, tar: 0.000487 
l0: 0.000419, l1: 0.000442, l2: 0.000438, l3: 0.000357, l4: 0.000504, l5: 0.000952, l6: 0.000974
[epoch: 167/1000, batch:   328/ 1052, ite: 43740] train loss: 0.006219, tar: 0.000485 
l0: 0.000389, l1: 0.000390, l2: 0.000395, l3: 0.000462, l4: 0.000514, l5: 0.000779, l6: 0.001268
[epoch: 167/1000, batch:   408/ 1052, ite: 43760] train loss: 0.006220, tar: 0.000485 
l0: 0.000782, l1: 0.000797, l2: 0.000840, l3: 0.000944, l4: 0.001276, l5: 0.001622, l6: 0.003087
[epoch: 167/1000, batch:   488/ 1052, ite: 43780] train loss: 0.006234, tar: 0.000486 
l0: 0.000296, l1: 0.000305, l2: 0.000304, l3: 0.000343, l4: 0.000604, l5: 0.001008, l6: 0.001721
[epoch: 167/1000, batch:   568/ 1052, ite: 43800] train loss: 0.006232, tar: 0.000485 
l0: 0.000641, l1: 0.000689, l2: 0.000663, l3: 0.000662, l4: 0.000903, l5: 0.001427, l6: 0.002602
[epoch: 167/1000, batch:   648/ 1052, ite: 43820] train loss: 0.006234, tar: 0.000485 
l0: 0.000493, l1: 0.000489, l2: 0.000538, l3: 0.000597, l4: 0.000872, l5: 0.001284, l6: 0.003034
[epoch: 167/1000, batch:   728/ 1052, ite: 43840] train loss: 0.006226, tar: 0.000484 
l0: 0.000601, l1: 0.000654, l2: 0.000626, l3: 0.000685, l4: 0.001098, l5: 0.001721, l6: 0.003142
[epoch: 167/1000, batch:   808/ 1052, ite: 43860] train loss: 0.006223, tar: 0.000484 
l0: 0.000811, l1: 0.000797, l2: 0.000790, l3: 0.000859, l4: 0.000856, l5: 0.001075, l6: 0.001948
[epoch: 167/1000, batch:   888/ 1052, ite: 43880] train loss: 0.006233, tar: 0.000484 
l0: 0.000462, l1: 0.000515, l2: 0.000514, l3: 0.000457, l4: 0.000623, l5: 0.001342, l6: 0.002052
[epoch: 167/1000, batch:   968/ 1052, ite: 43900] train loss: 0.006229, tar: 0.000484 
l0: 0.000283, l1: 0.000278, l2: 0.000314, l3: 0.000340, l4: 0.000437, l5: 0.001028, l6: 0.001299
[epoch: 167/1000, batch:  1048/ 1052, ite: 43920] train loss: 0.006229, tar: 0.000484 
[Epoch 167/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000415, l1: 0.000439, l2: 0.000408, l3: 0.000456, l4: 0.000664, l5: 0.001126, l6: 0.002455
[epoch: 168/1000, batch:    76/ 1052, ite: 43940] train loss: 0.006233, tar: 0.000484 
l0: 0.000586, l1: 0.000570, l2: 0.000610, l3: 0.000692, l4: 0.000732, l5: 0.001395, l6: 0.002016
[epoch: 168/1000, batch:   156/ 1052, ite: 43960] train loss: 0.006224, tar: 0.000483 
l0: 0.000590, l1: 0.000595, l2: 0.000628, l3: 0.000667, l4: 0.000889, l5: 0.001198, l6: 0.001906
[epoch: 168/1000, batch:   236/ 1052, ite: 43980] train loss: 0.006213, tar: 0.000482 
l0: 0.000413, l1: 0.000475, l2: 0.000442, l3: 0.000439, l4: 0.000624, l5: 0.001124, l6: 0.001133
[epoch: 168/1000, batch:   316/ 1052, ite: 44000] train loss: 0.006214, tar: 0.000482 
l0: 0.000637, l1: 0.000633, l2: 0.000665, l3: 0.000628, l4: 0.000857, l5: 0.001453, l6: 0.002370
[epoch: 168/1000, batch:   396/ 1052, ite: 44020] train loss: 0.006216, tar: 0.000482 
l0: 0.000192, l1: 0.000193, l2: 0.000200, l3: 0.000228, l4: 0.000366, l5: 0.000755, l6: 0.001136
[epoch: 168/1000, batch:   476/ 1052, ite: 44040] train loss: 0.006221, tar: 0.000482 
l0: 0.000619, l1: 0.000623, l2: 0.000639, l3: 0.000707, l4: 0.001033, l5: 0.001537, l6: 0.002570
[epoch: 168/1000, batch:   556/ 1052, ite: 44060] train loss: 0.006223, tar: 0.000482 
l0: 0.000375, l1: 0.000361, l2: 0.000452, l3: 0.000516, l4: 0.000612, l5: 0.001311, l6: 0.001733
[epoch: 168/1000, batch:   636/ 1052, ite: 44080] train loss: 0.006221, tar: 0.000482 
l0: 0.000319, l1: 0.000298, l2: 0.000382, l3: 0.000500, l4: 0.000763, l5: 0.001263, l6: 0.001864
[epoch: 168/1000, batch:   716/ 1052, ite: 44100] train loss: 0.006213, tar: 0.000481 
l0: 0.000619, l1: 0.000634, l2: 0.000594, l3: 0.000732, l4: 0.000709, l5: 0.001136, l6: 0.002106
[epoch: 168/1000, batch:   796/ 1052, ite: 44120] train loss: 0.006213, tar: 0.000481 
l0: 0.000304, l1: 0.000315, l2: 0.000306, l3: 0.000351, l4: 0.000569, l5: 0.000902, l6: 0.001670
[epoch: 168/1000, batch:   876/ 1052, ite: 44140] train loss: 0.006210, tar: 0.000480 
l0: 0.000365, l1: 0.000355, l2: 0.000420, l3: 0.000510, l4: 0.000700, l5: 0.000939, l6: 0.001820
[epoch: 168/1000, batch:   956/ 1052, ite: 44160] train loss: 0.006204, tar: 0.000480 
l0: 0.000263, l1: 0.000269, l2: 0.000271, l3: 0.000338, l4: 0.000517, l5: 0.000837, l6: 0.001316
[epoch: 168/1000, batch:  1036/ 1052, ite: 44180] train loss: 0.006205, tar: 0.000480 
[Epoch 168/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000507, l1: 0.000531, l2: 0.000515, l3: 0.000542, l4: 0.000655, l5: 0.001273, l6: 0.002070
[epoch: 169/1000, batch:    64/ 1052, ite: 44200] train loss: 0.006198, tar: 0.000480 
l0: 0.000426, l1: 0.000426, l2: 0.000495, l3: 0.000482, l4: 0.000692, l5: 0.001021, l6: 0.002286
[epoch: 169/1000, batch:   144/ 1052, ite: 44220] train loss: 0.006202, tar: 0.000480 
l0: 0.000251, l1: 0.000259, l2: 0.000285, l3: 0.000362, l4: 0.000411, l5: 0.000637, l6: 0.001631
[epoch: 169/1000, batch:   224/ 1052, ite: 44240] train loss: 0.006195, tar: 0.000480 
l0: 0.000245, l1: 0.000259, l2: 0.000256, l3: 0.000270, l4: 0.000378, l5: 0.000602, l6: 0.001339
[epoch: 169/1000, batch:   304/ 1052, ite: 44260] train loss: 0.006201, tar: 0.000480 
l0: 0.000231, l1: 0.000238, l2: 0.000232, l3: 0.000261, l4: 0.000428, l5: 0.000560, l6: 0.001353
[epoch: 169/1000, batch:   384/ 1052, ite: 44280] train loss: 0.006201, tar: 0.000480 
l0: 0.000251, l1: 0.000233, l2: 0.000269, l3: 0.000359, l4: 0.000554, l5: 0.000831, l6: 0.001391
[epoch: 169/1000, batch:   464/ 1052, ite: 44300] train loss: 0.006195, tar: 0.000480 
l0: 0.000869, l1: 0.000832, l2: 0.000797, l3: 0.000952, l4: 0.001185, l5: 0.001711, l6: 0.002482
[epoch: 169/1000, batch:   544/ 1052, ite: 44320] train loss: 0.006189, tar: 0.000479 
l0: 0.000272, l1: 0.000256, l2: 0.000293, l3: 0.000306, l4: 0.000556, l5: 0.000808, l6: 0.001267
[epoch: 169/1000, batch:   624/ 1052, ite: 44340] train loss: 0.006185, tar: 0.000478 
l0: 0.000666, l1: 0.000652, l2: 0.000747, l3: 0.000914, l4: 0.001409, l5: 0.002317, l6: 0.002805
[epoch: 169/1000, batch:   704/ 1052, ite: 44360] train loss: 0.006184, tar: 0.000478 
l0: 0.000614, l1: 0.000570, l2: 0.000605, l3: 0.000812, l4: 0.001344, l5: 0.001711, l6: 0.003035
[epoch: 169/1000, batch:   784/ 1052, ite: 44380] train loss: 0.006182, tar: 0.000478 
l0: 0.000535, l1: 0.000579, l2: 0.000505, l3: 0.000595, l4: 0.000631, l5: 0.001332, l6: 0.002052
[epoch: 169/1000, batch:   864/ 1052, ite: 44400] train loss: 0.006183, tar: 0.000478 
l0: 0.000644, l1: 0.000695, l2: 0.000640, l3: 0.000636, l4: 0.001041, l5: 0.002023, l6: 0.003120
[epoch: 169/1000, batch:   944/ 1052, ite: 44420] train loss: 0.006184, tar: 0.000478 
l0: 0.000524, l1: 0.000540, l2: 0.000481, l3: 0.000513, l4: 0.000537, l5: 0.001181, l6: 0.002321
[epoch: 169/1000, batch:  1024/ 1052, ite: 44440] train loss: 0.006179, tar: 0.000477 
[Epoch 169/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000261, l1: 0.000269, l2: 0.000277, l3: 0.000277, l4: 0.000393, l5: 0.000883, l6: 0.001116
[epoch: 170/1000, batch:    52/ 1052, ite: 44460] train loss: 0.006179, tar: 0.000477 
l0: 0.000272, l1: 0.000372, l2: 0.000280, l3: 0.000253, l4: 0.000393, l5: 0.000676, l6: 0.001508
[epoch: 170/1000, batch:   132/ 1052, ite: 44480] train loss: 0.006178, tar: 0.000477 
l0: 0.000911, l1: 0.000933, l2: 0.000946, l3: 0.000905, l4: 0.001250, l5: 0.001654, l6: 0.002683
[epoch: 170/1000, batch:   212/ 1052, ite: 44500] train loss: 0.006187, tar: 0.000478 
l0: 0.000374, l1: 0.000376, l2: 0.000434, l3: 0.000461, l4: 0.000832, l5: 0.001777, l6: 0.001819
[epoch: 170/1000, batch:   292/ 1052, ite: 44520] train loss: 0.006189, tar: 0.000478 
l0: 0.000458, l1: 0.000435, l2: 0.000483, l3: 0.000576, l4: 0.000633, l5: 0.001357, l6: 0.002287
[epoch: 170/1000, batch:   372/ 1052, ite: 44540] train loss: 0.006186, tar: 0.000478 
l0: 0.000403, l1: 0.000401, l2: 0.000445, l3: 0.000487, l4: 0.000621, l5: 0.001354, l6: 0.001873
[epoch: 170/1000, batch:   452/ 1052, ite: 44560] train loss: 0.006186, tar: 0.000478 
l0: 0.000681, l1: 0.000770, l2: 0.000676, l3: 0.000794, l4: 0.001020, l5: 0.001936, l6: 0.003339
[epoch: 170/1000, batch:   532/ 1052, ite: 44580] train loss: 0.006187, tar: 0.000477 
l0: 0.000179, l1: 0.000186, l2: 0.000191, l3: 0.000225, l4: 0.000354, l5: 0.000645, l6: 0.000746
[epoch: 170/1000, batch:   612/ 1052, ite: 44600] train loss: 0.006182, tar: 0.000477 
l0: 0.000337, l1: 0.000335, l2: 0.000349, l3: 0.000364, l4: 0.000511, l5: 0.001037, l6: 0.002126
[epoch: 170/1000, batch:   692/ 1052, ite: 44620] train loss: 0.006185, tar: 0.000477 
l0: 0.000531, l1: 0.000485, l2: 0.000612, l3: 0.000818, l4: 0.000946, l5: 0.001836, l6: 0.002622
[epoch: 170/1000, batch:   772/ 1052, ite: 44640] train loss: 0.006185, tar: 0.000477 
l0: 0.000368, l1: 0.000372, l2: 0.000385, l3: 0.000414, l4: 0.000582, l5: 0.001073, l6: 0.001982
[epoch: 170/1000, batch:   852/ 1052, ite: 44660] train loss: 0.006195, tar: 0.000478 
l0: 0.000542, l1: 0.000561, l2: 0.000560, l3: 0.000512, l4: 0.000781, l5: 0.000973, l6: 0.001758
[epoch: 170/1000, batch:   932/ 1052, ite: 44680] train loss: 0.006195, tar: 0.000478 
l0: 0.000219, l1: 0.000215, l2: 0.000225, l3: 0.000288, l4: 0.000669, l5: 0.001188, l6: 0.001808
[epoch: 170/1000, batch:  1012/ 1052, ite: 44700] train loss: 0.006190, tar: 0.000477 
[Epoch 170/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000459, l1: 0.000497, l2: 0.000475, l3: 0.000488, l4: 0.000719, l5: 0.001180, l6: 0.002249
[epoch: 171/1000, batch:    40/ 1052, ite: 44720] train loss: 0.006184, tar: 0.000477 
l0: 0.000192, l1: 0.000203, l2: 0.000221, l3: 0.000228, l4: 0.000393, l5: 0.000649, l6: 0.000922
[epoch: 171/1000, batch:   120/ 1052, ite: 44740] train loss: 0.006180, tar: 0.000476 
l0: 0.000365, l1: 0.000348, l2: 0.000362, l3: 0.000484, l4: 0.000711, l5: 0.000864, l6: 0.001978
[epoch: 171/1000, batch:   200/ 1052, ite: 44760] train loss: 0.006175, tar: 0.000476 
l0: 0.000268, l1: 0.000276, l2: 0.000296, l3: 0.000320, l4: 0.000487, l5: 0.000846, l6: 0.002419
[epoch: 171/1000, batch:   280/ 1052, ite: 44780] train loss: 0.006173, tar: 0.000475 
l0: 0.000277, l1: 0.000276, l2: 0.000336, l3: 0.000355, l4: 0.000531, l5: 0.000895, l6: 0.001725
[epoch: 171/1000, batch:   360/ 1052, ite: 44800] train loss: 0.006176, tar: 0.000475 
l0: 0.000514, l1: 0.000629, l2: 0.000652, l3: 0.000484, l4: 0.000757, l5: 0.000863, l6: 0.001759
[epoch: 171/1000, batch:   440/ 1052, ite: 44820] train loss: 0.006175, tar: 0.000475 
l0: 0.000200, l1: 0.000212, l2: 0.000211, l3: 0.000239, l4: 0.000346, l5: 0.000788, l6: 0.000866
[epoch: 171/1000, batch:   520/ 1052, ite: 44840] train loss: 0.006175, tar: 0.000475 
l0: 0.000465, l1: 0.000519, l2: 0.000515, l3: 0.000529, l4: 0.000560, l5: 0.000928, l6: 0.001865
[epoch: 171/1000, batch:   600/ 1052, ite: 44860] train loss: 0.006174, tar: 0.000475 
l0: 0.001429, l1: 0.001962, l2: 0.001528, l3: 0.001044, l4: 0.001145, l5: 0.001802, l6: 0.003730
[epoch: 171/1000, batch:   680/ 1052, ite: 44880] train loss: 0.006171, tar: 0.000475 
l0: 0.000396, l1: 0.000470, l2: 0.000475, l3: 0.000398, l4: 0.000553, l5: 0.000896, l6: 0.001541
[epoch: 171/1000, batch:   760/ 1052, ite: 44900] train loss: 0.006173, tar: 0.000475 
l0: 0.000418, l1: 0.000437, l2: 0.000445, l3: 0.000483, l4: 0.000615, l5: 0.001148, l6: 0.001583
[epoch: 171/1000, batch:   840/ 1052, ite: 44920] train loss: 0.006183, tar: 0.000476 
l0: 0.000469, l1: 0.000463, l2: 0.000480, l3: 0.000522, l4: 0.000887, l5: 0.000951, l6: 0.002357
[epoch: 171/1000, batch:   920/ 1052, ite: 44940] train loss: 0.006184, tar: 0.000476 
l0: 0.000446, l1: 0.000465, l2: 0.000500, l3: 0.000538, l4: 0.000678, l5: 0.001012, l6: 0.001874
[epoch: 171/1000, batch:  1000/ 1052, ite: 44960] train loss: 0.006184, tar: 0.000476 
[Epoch 171/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000320, l1: 0.000334, l2: 0.000360, l3: 0.000357, l4: 0.000494, l5: 0.001081, l6: 0.001901
[epoch: 172/1000, batch:    28/ 1052, ite: 44980] train loss: 0.006187, tar: 0.000477 
l0: 0.000426, l1: 0.000471, l2: 0.000487, l3: 0.000459, l4: 0.000568, l5: 0.000970, l6: 0.002148
[epoch: 172/1000, batch:   108/ 1052, ite: 45000] train loss: 0.006184, tar: 0.000476 
l0: 0.001096, l1: 0.001026, l2: 0.000922, l3: 0.000985, l4: 0.001564, l5: 0.002952, l6: 0.003645
[epoch: 172/1000, batch:   188/ 1052, ite: 45020] train loss: 0.006184, tar: 0.000476 
l0: 0.000615, l1: 0.000632, l2: 0.000683, l3: 0.000714, l4: 0.001217, l5: 0.001988, l6: 0.002019
[epoch: 172/1000, batch:   268/ 1052, ite: 45040] train loss: 0.006189, tar: 0.000476 
l0: 0.000413, l1: 0.000404, l2: 0.000395, l3: 0.000481, l4: 0.000678, l5: 0.000930, l6: 0.001293
[epoch: 172/1000, batch:   348/ 1052, ite: 45060] train loss: 0.006194, tar: 0.000477 
l0: 0.000403, l1: 0.000398, l2: 0.000479, l3: 0.000560, l4: 0.000814, l5: 0.001507, l6: 0.002624
[epoch: 172/1000, batch:   428/ 1052, ite: 45080] train loss: 0.006202, tar: 0.000478 
l0: 0.000534, l1: 0.000568, l2: 0.000565, l3: 0.000635, l4: 0.000956, l5: 0.001650, l6: 0.002420
[epoch: 172/1000, batch:   508/ 1052, ite: 45100] train loss: 0.006202, tar: 0.000478 
l0: 0.000313, l1: 0.000310, l2: 0.000307, l3: 0.000408, l4: 0.000449, l5: 0.000964, l6: 0.001698
[epoch: 172/1000, batch:   588/ 1052, ite: 45120] train loss: 0.006205, tar: 0.000478 
l0: 0.000536, l1: 0.000598, l2: 0.000565, l3: 0.000520, l4: 0.000735, l5: 0.001242, l6: 0.003053
[epoch: 172/1000, batch:   668/ 1052, ite: 45140] train loss: 0.006204, tar: 0.000478 
l0: 0.000415, l1: 0.000477, l2: 0.000426, l3: 0.000396, l4: 0.000513, l5: 0.000894, l6: 0.001397
[epoch: 172/1000, batch:   748/ 1052, ite: 45160] train loss: 0.006200, tar: 0.000478 
l0: 0.000793, l1: 0.000687, l2: 0.000773, l3: 0.001175, l4: 0.001418, l5: 0.002383, l6: 0.003599
[epoch: 172/1000, batch:   828/ 1052, ite: 45180] train loss: 0.006197, tar: 0.000477 
l0: 0.000283, l1: 0.000294, l2: 0.000296, l3: 0.000318, l4: 0.000469, l5: 0.000807, l6: 0.001879
[epoch: 172/1000, batch:   908/ 1052, ite: 45200] train loss: 0.006196, tar: 0.000477 
l0: 0.000404, l1: 0.000424, l2: 0.000465, l3: 0.000410, l4: 0.000745, l5: 0.000963, l6: 0.001685
[epoch: 172/1000, batch:   988/ 1052, ite: 45220] train loss: 0.006195, tar: 0.000477 
[Epoch 172/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000418, l1: 0.000406, l2: 0.000446, l3: 0.000558, l4: 0.000897, l5: 0.001427, l6: 0.002855
[epoch: 173/1000, batch:    16/ 1052, ite: 45240] train loss: 0.006200, tar: 0.000478 
l0: 0.000538, l1: 0.000513, l2: 0.000574, l3: 0.000642, l4: 0.000676, l5: 0.001095, l6: 0.002265
[epoch: 173/1000, batch:    96/ 1052, ite: 45260] train loss: 0.006199, tar: 0.000478 
l0: 0.000292, l1: 0.000291, l2: 0.000326, l3: 0.000385, l4: 0.000580, l5: 0.000793, l6: 0.001732
[epoch: 173/1000, batch:   176/ 1052, ite: 45280] train loss: 0.006199, tar: 0.000478 
l0: 0.000884, l1: 0.000863, l2: 0.000842, l3: 0.000951, l4: 0.001164, l5: 0.001750, l6: 0.001880
[epoch: 173/1000, batch:   256/ 1052, ite: 45300] train loss: 0.006195, tar: 0.000478 
l0: 0.000297, l1: 0.000307, l2: 0.000314, l3: 0.000392, l4: 0.000579, l5: 0.001166, l6: 0.001698
[epoch: 173/1000, batch:   336/ 1052, ite: 45320] train loss: 0.006190, tar: 0.000477 
l0: 0.000609, l1: 0.000621, l2: 0.000616, l3: 0.000763, l4: 0.000792, l5: 0.001095, l6: 0.001877
[epoch: 173/1000, batch:   416/ 1052, ite: 45340] train loss: 0.006198, tar: 0.000478 
l0: 0.000352, l1: 0.000338, l2: 0.000399, l3: 0.000524, l4: 0.000709, l5: 0.000959, l6: 0.001911
[epoch: 173/1000, batch:   496/ 1052, ite: 45360] train loss: 0.006196, tar: 0.000478 
l0: 0.000427, l1: 0.000468, l2: 0.000455, l3: 0.000462, l4: 0.000582, l5: 0.001562, l6: 0.001515
[epoch: 173/1000, batch:   576/ 1052, ite: 45380] train loss: 0.006200, tar: 0.000478 
l0: 0.000370, l1: 0.000388, l2: 0.000392, l3: 0.000393, l4: 0.000606, l5: 0.001041, l6: 0.001659
[epoch: 173/1000, batch:   656/ 1052, ite: 45400] train loss: 0.006203, tar: 0.000478 
l0: 0.000452, l1: 0.000512, l2: 0.000547, l3: 0.000516, l4: 0.000625, l5: 0.001224, l6: 0.001493
[epoch: 173/1000, batch:   736/ 1052, ite: 45420] train loss: 0.006208, tar: 0.000479 
l0: 0.000584, l1: 0.000619, l2: 0.000642, l3: 0.000737, l4: 0.001085, l5: 0.002295, l6: 0.002431
[epoch: 173/1000, batch:   816/ 1052, ite: 45440] train loss: 0.006203, tar: 0.000478 
l0: 0.000443, l1: 0.000446, l2: 0.000480, l3: 0.000524, l4: 0.000639, l5: 0.001247, l6: 0.001867
[epoch: 173/1000, batch:   896/ 1052, ite: 45460] train loss: 0.006206, tar: 0.000479 
l0: 0.001088, l1: 0.001146, l2: 0.001156, l3: 0.001253, l4: 0.001474, l5: 0.001939, l6: 0.003564
[epoch: 173/1000, batch:   976/ 1052, ite: 45480] train loss: 0.006206, tar: 0.000479 
[Epoch 173/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000430, l1: 0.000486, l2: 0.000452, l3: 0.000406, l4: 0.000525, l5: 0.000956, l6: 0.001411
[epoch: 174/1000, batch:     4/ 1052, ite: 45500] train loss: 0.006214, tar: 0.000479 
l0: 0.000521, l1: 0.000517, l2: 0.000698, l3: 0.000615, l4: 0.000820, l5: 0.001507, l6: 0.002200
[epoch: 174/1000, batch:    84/ 1052, ite: 45520] train loss: 0.006212, tar: 0.000479 
l0: 0.000530, l1: 0.000512, l2: 0.000551, l3: 0.000610, l4: 0.000859, l5: 0.001788, l6: 0.002102
[epoch: 174/1000, batch:   164/ 1052, ite: 45540] train loss: 0.006211, tar: 0.000479 
l0: 0.000476, l1: 0.000487, l2: 0.000532, l3: 0.000632, l4: 0.000861, l5: 0.001424, l6: 0.002427
[epoch: 174/1000, batch:   244/ 1052, ite: 45560] train loss: 0.006210, tar: 0.000479 
l0: 0.000708, l1: 0.000680, l2: 0.000717, l3: 0.000653, l4: 0.001013, l5: 0.001466, l6: 0.002861
[epoch: 174/1000, batch:   324/ 1052, ite: 45580] train loss: 0.006213, tar: 0.000479 
l0: 0.000529, l1: 0.000584, l2: 0.000571, l3: 0.000503, l4: 0.000654, l5: 0.001279, l6: 0.002356
[epoch: 174/1000, batch:   404/ 1052, ite: 45600] train loss: 0.006212, tar: 0.000479 
l0: 0.000289, l1: 0.000275, l2: 0.000313, l3: 0.000331, l4: 0.000538, l5: 0.000846, l6: 0.001825
[epoch: 174/1000, batch:   484/ 1052, ite: 45620] train loss: 0.006209, tar: 0.000478 
l0: 0.000455, l1: 0.000455, l2: 0.000562, l3: 0.000612, l4: 0.001044, l5: 0.001567, l6: 0.002763
[epoch: 174/1000, batch:   564/ 1052, ite: 45640] train loss: 0.006211, tar: 0.000478 
l0: 0.000377, l1: 0.000366, l2: 0.000406, l3: 0.000389, l4: 0.000506, l5: 0.000741, l6: 0.001177
[epoch: 174/1000, batch:   644/ 1052, ite: 45660] train loss: 0.006210, tar: 0.000478 
l0: 0.000390, l1: 0.000401, l2: 0.000402, l3: 0.000433, l4: 0.000675, l5: 0.000846, l6: 0.001849
[epoch: 174/1000, batch:   724/ 1052, ite: 45680] train loss: 0.006209, tar: 0.000478 
l0: 0.000311, l1: 0.000312, l2: 0.000337, l3: 0.000394, l4: 0.000608, l5: 0.001189, l6: 0.001704
[epoch: 174/1000, batch:   804/ 1052, ite: 45700] train loss: 0.006216, tar: 0.000479 
l0: 0.000154, l1: 0.000173, l2: 0.000181, l3: 0.000207, l4: 0.000303, l5: 0.000801, l6: 0.000736
[epoch: 174/1000, batch:   884/ 1052, ite: 45720] train loss: 0.006216, tar: 0.000479 
l0: 0.001125, l1: 0.001192, l2: 0.001157, l3: 0.000986, l4: 0.001049, l5: 0.001474, l6: 0.001896
[epoch: 174/1000, batch:   964/ 1052, ite: 45740] train loss: 0.006216, tar: 0.000479 
l0: 0.000307, l1: 0.000323, l2: 0.000312, l3: 0.000370, l4: 0.000401, l5: 0.000957, l6: 0.001756
[epoch: 174/1000, batch:  1044/ 1052, ite: 45760] train loss: 0.006214, tar: 0.000479 
[Epoch 174/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000303, l1: 0.000319, l2: 0.000368, l3: 0.000357, l4: 0.000773, l5: 0.001174, l6: 0.001716
[epoch: 175/1000, batch:    72/ 1052, ite: 45780] train loss: 0.006216, tar: 0.000479 
l0: 0.000567, l1: 0.000566, l2: 0.000557, l3: 0.000642, l4: 0.000822, l5: 0.001375, l6: 0.002131
[epoch: 175/1000, batch:   152/ 1052, ite: 45800] train loss: 0.006217, tar: 0.000479 
l0: 0.000627, l1: 0.000662, l2: 0.000607, l3: 0.000554, l4: 0.000602, l5: 0.001189, l6: 0.001760
[epoch: 175/1000, batch:   232/ 1052, ite: 45820] train loss: 0.006215, tar: 0.000478 
l0: 0.000608, l1: 0.000625, l2: 0.000655, l3: 0.000639, l4: 0.000656, l5: 0.001155, l6: 0.002301
[epoch: 175/1000, batch:   312/ 1052, ite: 45840] train loss: 0.006211, tar: 0.000478 
l0: 0.000413, l1: 0.000413, l2: 0.000425, l3: 0.000478, l4: 0.000633, l5: 0.001064, l6: 0.001722
[epoch: 175/1000, batch:   392/ 1052, ite: 45860] train loss: 0.006209, tar: 0.000478 
l0: 0.000887, l1: 0.000913, l2: 0.000920, l3: 0.000895, l4: 0.001035, l5: 0.001844, l6: 0.003330
[epoch: 175/1000, batch:   472/ 1052, ite: 45880] train loss: 0.006211, tar: 0.000478 
l0: 0.000260, l1: 0.000301, l2: 0.000334, l3: 0.000303, l4: 0.000441, l5: 0.001092, l6: 0.001281
[epoch: 175/1000, batch:   552/ 1052, ite: 45900] train loss: 0.006210, tar: 0.000478 
l0: 0.000493, l1: 0.000517, l2: 0.000540, l3: 0.000539, l4: 0.000895, l5: 0.001718, l6: 0.002350
[epoch: 175/1000, batch:   632/ 1052, ite: 45920] train loss: 0.006208, tar: 0.000478 
l0: 0.000360, l1: 0.000385, l2: 0.000344, l3: 0.000317, l4: 0.000550, l5: 0.000548, l6: 0.001076
[epoch: 175/1000, batch:   712/ 1052, ite: 45940] train loss: 0.006208, tar: 0.000478 
l0: 0.000523, l1: 0.000488, l2: 0.000567, l3: 0.000581, l4: 0.000851, l5: 0.001298, l6: 0.002177
[epoch: 175/1000, batch:   792/ 1052, ite: 45960] train loss: 0.006209, tar: 0.000478 
l0: 0.000324, l1: 0.000344, l2: 0.000358, l3: 0.000356, l4: 0.000450, l5: 0.000689, l6: 0.001557
[epoch: 175/1000, batch:   872/ 1052, ite: 45980] train loss: 0.006211, tar: 0.000478 
l0: 0.000249, l1: 0.000236, l2: 0.000280, l3: 0.000329, l4: 0.000703, l5: 0.001139, l6: 0.001379
[epoch: 175/1000, batch:   952/ 1052, ite: 46000] train loss: 0.006211, tar: 0.000478 
l0: 0.000324, l1: 0.000346, l2: 0.000345, l3: 0.000404, l4: 0.000460, l5: 0.000670, l6: 0.001101
[epoch: 175/1000, batch:  1032/ 1052, ite: 46020] train loss: 0.006210, tar: 0.000478 
[Epoch 175/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000259, l1: 0.000255, l2: 0.000270, l3: 0.000321, l4: 0.000537, l5: 0.001208, l6: 0.001858
[epoch: 176/1000, batch:    60/ 1052, ite: 46040] train loss: 0.006212, tar: 0.000478 
l0: 0.000257, l1: 0.000265, l2: 0.000266, l3: 0.000280, l4: 0.000499, l5: 0.001242, l6: 0.001788
[epoch: 176/1000, batch:   140/ 1052, ite: 46060] train loss: 0.006212, tar: 0.000478 
l0: 0.000318, l1: 0.000312, l2: 0.000382, l3: 0.000465, l4: 0.000594, l5: 0.000961, l6: 0.001746
[epoch: 176/1000, batch:   220/ 1052, ite: 46080] train loss: 0.006210, tar: 0.000478 
l0: 0.000339, l1: 0.000333, l2: 0.000388, l3: 0.000453, l4: 0.000726, l5: 0.001352, l6: 0.001723
[epoch: 176/1000, batch:   300/ 1052, ite: 46100] train loss: 0.006208, tar: 0.000478 
l0: 0.000593, l1: 0.000587, l2: 0.000611, l3: 0.000584, l4: 0.000818, l5: 0.001796, l6: 0.003026
[epoch: 176/1000, batch:   380/ 1052, ite: 46120] train loss: 0.006204, tar: 0.000477 
l0: 0.001382, l1: 0.001460, l2: 0.001473, l3: 0.001275, l4: 0.001767, l5: 0.002631, l6: 0.004068
[epoch: 176/1000, batch:   460/ 1052, ite: 46140] train loss: 0.006204, tar: 0.000477 
l0: 0.000297, l1: 0.000293, l2: 0.000319, l3: 0.000368, l4: 0.000625, l5: 0.001161, l6: 0.001971
[epoch: 176/1000, batch:   540/ 1052, ite: 46160] train loss: 0.006204, tar: 0.000477 
l0: 0.000204, l1: 0.000196, l2: 0.000216, l3: 0.000286, l4: 0.000516, l5: 0.000705, l6: 0.001377
[epoch: 176/1000, batch:   620/ 1052, ite: 46180] train loss: 0.006203, tar: 0.000477 
l0: 0.000254, l1: 0.000248, l2: 0.000279, l3: 0.000334, l4: 0.000493, l5: 0.000753, l6: 0.001235
[epoch: 176/1000, batch:   700/ 1052, ite: 46200] train loss: 0.006202, tar: 0.000477 
l0: 0.000344, l1: 0.000346, l2: 0.000396, l3: 0.000464, l4: 0.000576, l5: 0.001046, l6: 0.001689
[epoch: 176/1000, batch:   780/ 1052, ite: 46220] train loss: 0.006201, tar: 0.000476 
l0: 0.000139, l1: 0.000121, l2: 0.000145, l3: 0.000243, l4: 0.000397, l5: 0.000582, l6: 0.000652
[epoch: 176/1000, batch:   860/ 1052, ite: 46240] train loss: 0.006201, tar: 0.000476 
l0: 0.000514, l1: 0.000534, l2: 0.000661, l3: 0.000567, l4: 0.001066, l5: 0.001948, l6: 0.003020
[epoch: 176/1000, batch:   940/ 1052, ite: 46260] train loss: 0.006205, tar: 0.000477 
l0: 0.000256, l1: 0.000267, l2: 0.000261, l3: 0.000324, l4: 0.000448, l5: 0.000633, l6: 0.001934
[epoch: 176/1000, batch:  1020/ 1052, ite: 46280] train loss: 0.006201, tar: 0.000476 
[Epoch 176/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000201, l1: 0.000223, l2: 0.000202, l3: 0.000228, l4: 0.000341, l5: 0.000599, l6: 0.000588
[epoch: 177/1000, batch:    48/ 1052, ite: 46300] train loss: 0.006200, tar: 0.000476 
l0: 0.000546, l1: 0.000534, l2: 0.000560, l3: 0.000715, l4: 0.001282, l5: 0.001629, l6: 0.002438
[epoch: 177/1000, batch:   128/ 1052, ite: 46320] train loss: 0.006198, tar: 0.000476 
l0: 0.000642, l1: 0.000653, l2: 0.000671, l3: 0.000756, l4: 0.000900, l5: 0.001413, l6: 0.002784
[epoch: 177/1000, batch:   208/ 1052, ite: 46340] train loss: 0.006197, tar: 0.000476 
l0: 0.000315, l1: 0.000314, l2: 0.000337, l3: 0.000372, l4: 0.000512, l5: 0.000789, l6: 0.001436
[epoch: 177/1000, batch:   288/ 1052, ite: 46360] train loss: 0.006194, tar: 0.000476 
l0: 0.000565, l1: 0.000627, l2: 0.000602, l3: 0.000516, l4: 0.000781, l5: 0.001675, l6: 0.003261
[epoch: 177/1000, batch:   368/ 1052, ite: 46380] train loss: 0.006194, tar: 0.000475 
l0: 0.000245, l1: 0.000241, l2: 0.000249, l3: 0.000299, l4: 0.000489, l5: 0.001195, l6: 0.001360
[epoch: 177/1000, batch:   448/ 1052, ite: 46400] train loss: 0.006194, tar: 0.000475 
l0: 0.000362, l1: 0.000360, l2: 0.000431, l3: 0.000450, l4: 0.000681, l5: 0.000792, l6: 0.001629
[epoch: 177/1000, batch:   528/ 1052, ite: 46420] train loss: 0.006195, tar: 0.000476 
l0: 0.000378, l1: 0.000388, l2: 0.000414, l3: 0.000435, l4: 0.000474, l5: 0.001052, l6: 0.001064
[epoch: 177/1000, batch:   608/ 1052, ite: 46440] train loss: 0.006196, tar: 0.000476 
l0: 0.000565, l1: 0.000561, l2: 0.000632, l3: 0.000654, l4: 0.000818, l5: 0.001555, l6: 0.002111
[epoch: 177/1000, batch:   688/ 1052, ite: 46460] train loss: 0.006197, tar: 0.000476 
l0: 0.000385, l1: 0.000383, l2: 0.000414, l3: 0.000462, l4: 0.000814, l5: 0.000850, l6: 0.001885
[epoch: 177/1000, batch:   768/ 1052, ite: 46480] train loss: 0.006196, tar: 0.000476 
l0: 0.000815, l1: 0.001087, l2: 0.000962, l3: 0.000596, l4: 0.000817, l5: 0.001255, l6: 0.002369
[epoch: 177/1000, batch:   848/ 1052, ite: 46500] train loss: 0.006195, tar: 0.000475 
l0: 0.000531, l1: 0.000553, l2: 0.000505, l3: 0.000631, l4: 0.000790, l5: 0.001333, l6: 0.003383
[epoch: 177/1000, batch:   928/ 1052, ite: 46520] train loss: 0.006191, tar: 0.000475 
l0: 0.000659, l1: 0.000653, l2: 0.000692, l3: 0.000607, l4: 0.000920, l5: 0.001604, l6: 0.003282
[epoch: 177/1000, batch:  1008/ 1052, ite: 46540] train loss: 0.006192, tar: 0.000475 
[Epoch 177/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000347, l1: 0.000367, l2: 0.000352, l3: 0.000359, l4: 0.000581, l5: 0.001064, l6: 0.001711
[epoch: 178/1000, batch:    36/ 1052, ite: 46560] train loss: 0.006192, tar: 0.000475 
l0: 0.000446, l1: 0.000462, l2: 0.000447, l3: 0.000505, l4: 0.000926, l5: 0.001611, l6: 0.002850
[epoch: 178/1000, batch:   116/ 1052, ite: 46580] train loss: 0.006188, tar: 0.000475 
l0: 0.000755, l1: 0.000794, l2: 0.000848, l3: 0.000762, l4: 0.001198, l5: 0.002236, l6: 0.004340
[epoch: 178/1000, batch:   196/ 1052, ite: 46600] train loss: 0.006184, tar: 0.000474 
l0: 0.000795, l1: 0.000869, l2: 0.000881, l3: 0.000923, l4: 0.001211, l5: 0.002361, l6: 0.003431
[epoch: 178/1000, batch:   276/ 1052, ite: 46620] train loss: 0.006193, tar: 0.000475 
l0: 0.000498, l1: 0.000517, l2: 0.000503, l3: 0.000527, l4: 0.000812, l5: 0.001762, l6: 0.002210
[epoch: 178/1000, batch:   356/ 1052, ite: 46640] train loss: 0.006193, tar: 0.000475 
l0: 0.000290, l1: 0.000311, l2: 0.000321, l3: 0.000293, l4: 0.000480, l5: 0.000788, l6: 0.001778
[epoch: 178/1000, batch:   436/ 1052, ite: 46660] train loss: 0.006196, tar: 0.000475 
l0: 0.000367, l1: 0.000378, l2: 0.000347, l3: 0.000413, l4: 0.000582, l5: 0.001106, l6: 0.001778
[epoch: 178/1000, batch:   516/ 1052, ite: 46680] train loss: 0.006191, tar: 0.000475 
l0: 0.000812, l1: 0.000793, l2: 0.000812, l3: 0.001050, l4: 0.001522, l5: 0.002788, l6: 0.004120
[epoch: 178/1000, batch:   596/ 1052, ite: 46700] train loss: 0.006194, tar: 0.000475 
l0: 0.000367, l1: 0.000347, l2: 0.000391, l3: 0.000581, l4: 0.000903, l5: 0.001454, l6: 0.002206
[epoch: 178/1000, batch:   676/ 1052, ite: 46720] train loss: 0.006190, tar: 0.000475 
l0: 0.000726, l1: 0.000774, l2: 0.000764, l3: 0.000719, l4: 0.001087, l5: 0.002231, l6: 0.003375
[epoch: 178/1000, batch:   756/ 1052, ite: 46740] train loss: 0.006191, tar: 0.000475 
l0: 0.000502, l1: 0.000482, l2: 0.000524, l3: 0.000704, l4: 0.001095, l5: 0.001575, l6: 0.002593
[epoch: 178/1000, batch:   836/ 1052, ite: 46760] train loss: 0.006190, tar: 0.000475 
l0: 0.000451, l1: 0.000438, l2: 0.000487, l3: 0.000517, l4: 0.000682, l5: 0.001269, l6: 0.002008
[epoch: 178/1000, batch:   916/ 1052, ite: 46780] train loss: 0.006186, tar: 0.000474 
l0: 0.000263, l1: 0.000265, l2: 0.000249, l3: 0.000311, l4: 0.000465, l5: 0.001065, l6: 0.001907
[epoch: 178/1000, batch:   996/ 1052, ite: 46800] train loss: 0.006185, tar: 0.000474 
[Epoch 178/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000305, l1: 0.000333, l2: 0.000358, l3: 0.000356, l4: 0.000614, l5: 0.001225, l6: 0.001887
[epoch: 179/1000, batch:    24/ 1052, ite: 46820] train loss: 0.006185, tar: 0.000474 
l0: 0.000526, l1: 0.000520, l2: 0.000579, l3: 0.000642, l4: 0.000649, l5: 0.001063, l6: 0.001981
[epoch: 179/1000, batch:   104/ 1052, ite: 46840] train loss: 0.006183, tar: 0.000474 
l0: 0.000364, l1: 0.000368, l2: 0.000353, l3: 0.000467, l4: 0.000711, l5: 0.001488, l6: 0.001839
[epoch: 179/1000, batch:   184/ 1052, ite: 46860] train loss: 0.006185, tar: 0.000474 
l0: 0.000716, l1: 0.000737, l2: 0.000859, l3: 0.000964, l4: 0.001441, l5: 0.002038, l6: 0.004222
[epoch: 179/1000, batch:   264/ 1052, ite: 46880] train loss: 0.006186, tar: 0.000474 
l0: 0.000542, l1: 0.000533, l2: 0.000550, l3: 0.000647, l4: 0.000790, l5: 0.001381, l6: 0.003260
[epoch: 179/1000, batch:   344/ 1052, ite: 46900] train loss: 0.006185, tar: 0.000474 
l0: 0.000501, l1: 0.000466, l2: 0.000550, l3: 0.000740, l4: 0.001011, l5: 0.001825, l6: 0.002582
[epoch: 179/1000, batch:   424/ 1052, ite: 46920] train loss: 0.006186, tar: 0.000474 
l0: 0.000484, l1: 0.000569, l2: 0.000531, l3: 0.000532, l4: 0.000740, l5: 0.001324, l6: 0.002080
[epoch: 179/1000, batch:   504/ 1052, ite: 46940] train loss: 0.006185, tar: 0.000474 
l0: 0.000206, l1: 0.000202, l2: 0.000210, l3: 0.000256, l4: 0.000508, l5: 0.000714, l6: 0.001453
[epoch: 179/1000, batch:   584/ 1052, ite: 46960] train loss: 0.006185, tar: 0.000474 
l0: 0.000364, l1: 0.000375, l2: 0.000404, l3: 0.000367, l4: 0.000609, l5: 0.000911, l6: 0.001044
[epoch: 179/1000, batch:   664/ 1052, ite: 46980] train loss: 0.006183, tar: 0.000473 
l0: 0.000373, l1: 0.000397, l2: 0.000404, l3: 0.000371, l4: 0.000534, l5: 0.000773, l6: 0.001994
[epoch: 179/1000, batch:   744/ 1052, ite: 47000] train loss: 0.006182, tar: 0.000473 
l0: 0.000295, l1: 0.000294, l2: 0.000312, l3: 0.000324, l4: 0.000577, l5: 0.000937, l6: 0.001736
[epoch: 179/1000, batch:   824/ 1052, ite: 47020] train loss: 0.006180, tar: 0.000473 
l0: 0.000241, l1: 0.000245, l2: 0.000273, l3: 0.000332, l4: 0.000473, l5: 0.000620, l6: 0.001176
[epoch: 179/1000, batch:   904/ 1052, ite: 47040] train loss: 0.006179, tar: 0.000473 
l0: 0.000562, l1: 0.000528, l2: 0.000544, l3: 0.000778, l4: 0.000792, l5: 0.001564, l6: 0.002721
[epoch: 179/1000, batch:   984/ 1052, ite: 47060] train loss: 0.006180, tar: 0.000473 
[Epoch 179/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000175, l1: 0.000197, l2: 0.000181, l3: 0.000183, l4: 0.000259, l5: 0.000775, l6: 0.001146
[epoch: 180/1000, batch:    12/ 1052, ite: 47080] train loss: 0.006179, tar: 0.000473 
l0: 0.000469, l1: 0.000470, l2: 0.000545, l3: 0.000705, l4: 0.000914, l5: 0.001510, l6: 0.002977
[epoch: 180/1000, batch:    92/ 1052, ite: 47100] train loss: 0.006182, tar: 0.000473 
l0: 0.000379, l1: 0.000395, l2: 0.000446, l3: 0.000463, l4: 0.000713, l5: 0.001095, l6: 0.002102
[epoch: 180/1000, batch:   172/ 1052, ite: 47120] train loss: 0.006178, tar: 0.000473 
l0: 0.000482, l1: 0.000487, l2: 0.000510, l3: 0.000587, l4: 0.001099, l5: 0.001703, l6: 0.003150
[epoch: 180/1000, batch:   252/ 1052, ite: 47140] train loss: 0.006178, tar: 0.000473 
l0: 0.000222, l1: 0.000220, l2: 0.000228, l3: 0.000280, l4: 0.000507, l5: 0.001148, l6: 0.001543
[epoch: 180/1000, batch:   332/ 1052, ite: 47160] train loss: 0.006176, tar: 0.000473 
l0: 0.000385, l1: 0.000384, l2: 0.000405, l3: 0.000448, l4: 0.000596, l5: 0.001346, l6: 0.001860
[epoch: 180/1000, batch:   412/ 1052, ite: 47180] train loss: 0.006175, tar: 0.000472 
l0: 0.000894, l1: 0.000922, l2: 0.000881, l3: 0.000913, l4: 0.001274, l5: 0.002388, l6: 0.004412
[epoch: 180/1000, batch:   492/ 1052, ite: 47200] train loss: 0.006171, tar: 0.000472 
l0: 0.000286, l1: 0.000279, l2: 0.000261, l3: 0.000394, l4: 0.000484, l5: 0.000736, l6: 0.001796
[epoch: 180/1000, batch:   572/ 1052, ite: 47220] train loss: 0.006170, tar: 0.000472 
l0: 0.000367, l1: 0.000365, l2: 0.000384, l3: 0.000446, l4: 0.000622, l5: 0.000580, l6: 0.001409
[epoch: 180/1000, batch:   652/ 1052, ite: 47240] train loss: 0.006167, tar: 0.000472 
l0: 0.000751, l1: 0.000817, l2: 0.000808, l3: 0.000831, l4: 0.000911, l5: 0.001181, l6: 0.002179
[epoch: 180/1000, batch:   732/ 1052, ite: 47260] train loss: 0.006173, tar: 0.000472 
l0: 0.000586, l1: 0.000645, l2: 0.000598, l3: 0.000763, l4: 0.001192, l5: 0.001759, l6: 0.003344
[epoch: 180/1000, batch:   812/ 1052, ite: 47280] train loss: 0.006174, tar: 0.000472 
l0: 0.000793, l1: 0.000806, l2: 0.001021, l3: 0.000962, l4: 0.001191, l5: 0.001682, l6: 0.003677
[epoch: 180/1000, batch:   892/ 1052, ite: 47300] train loss: 0.006175, tar: 0.000472 
l0: 0.000508, l1: 0.000487, l2: 0.000463, l3: 0.000772, l4: 0.000919, l5: 0.001431, l6: 0.002169
[epoch: 180/1000, batch:   972/ 1052, ite: 47320] train loss: 0.006177, tar: 0.000472 
l0: 0.000543, l1: 0.000564, l2: 0.000597, l3: 0.000609, l4: 0.000880, l5: 0.001299, l6: 0.002984
[epoch: 180/1000, batch:  1052/ 1052, ite: 47340] train loss: 0.006177, tar: 0.000472 
[Epoch 180/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000490, l1: 0.000523, l2: 0.000515, l3: 0.000555, l4: 0.000760, l5: 0.001086, l6: 0.002915
[epoch: 181/1000, batch:    80/ 1052, ite: 47360] train loss: 0.006177, tar: 0.000472 
l0: 0.000611, l1: 0.000607, l2: 0.000589, l3: 0.000627, l4: 0.000685, l5: 0.001220, l6: 0.001319
[epoch: 181/1000, batch:   160/ 1052, ite: 47380] train loss: 0.006179, tar: 0.000472 
l0: 0.000335, l1: 0.000356, l2: 0.000422, l3: 0.000403, l4: 0.000459, l5: 0.001374, l6: 0.002082
[epoch: 181/1000, batch:   240/ 1052, ite: 47400] train loss: 0.006178, tar: 0.000472 
l0: 0.000324, l1: 0.000327, l2: 0.000359, l3: 0.000392, l4: 0.000607, l5: 0.001390, l6: 0.002759
[epoch: 181/1000, batch:   320/ 1052, ite: 47420] train loss: 0.006176, tar: 0.000472 
l0: 0.000324, l1: 0.000334, l2: 0.000356, l3: 0.000479, l4: 0.000733, l5: 0.001022, l6: 0.001658
[epoch: 181/1000, batch:   400/ 1052, ite: 47440] train loss: 0.006175, tar: 0.000472 
l0: 0.000946, l1: 0.001101, l2: 0.000865, l3: 0.000822, l4: 0.001057, l5: 0.001580, l6: 0.002522
[epoch: 181/1000, batch:   480/ 1052, ite: 47460] train loss: 0.006174, tar: 0.000472 
l0: 0.000256, l1: 0.000265, l2: 0.000280, l3: 0.000306, l4: 0.000375, l5: 0.000925, l6: 0.001450
[epoch: 181/1000, batch:   560/ 1052, ite: 47480] train loss: 0.006176, tar: 0.000472 
l0: 0.000939, l1: 0.000987, l2: 0.000920, l3: 0.000979, l4: 0.001596, l5: 0.002860, l6: 0.003974
[epoch: 181/1000, batch:   640/ 1052, ite: 47500] train loss: 0.006179, tar: 0.000472 
l0: 0.000695, l1: 0.000715, l2: 0.000670, l3: 0.000869, l4: 0.000996, l5: 0.001360, l6: 0.002432
[epoch: 181/1000, batch:   720/ 1052, ite: 47520] train loss: 0.006180, tar: 0.000473 
l0: 0.000250, l1: 0.000256, l2: 0.000276, l3: 0.000342, l4: 0.000530, l5: 0.000811, l6: 0.001312
[epoch: 181/1000, batch:   800/ 1052, ite: 47540] train loss: 0.006178, tar: 0.000472 
l0: 0.000892, l1: 0.000910, l2: 0.001032, l3: 0.000882, l4: 0.000910, l5: 0.001358, l6: 0.002080
[epoch: 181/1000, batch:   880/ 1052, ite: 47560] train loss: 0.006175, tar: 0.000472 
l0: 0.000582, l1: 0.000583, l2: 0.000629, l3: 0.000680, l4: 0.001195, l5: 0.002005, l6: 0.003345
[epoch: 181/1000, batch:   960/ 1052, ite: 47580] train loss: 0.006179, tar: 0.000473 
l0: 0.000819, l1: 0.000747, l2: 0.000727, l3: 0.001083, l4: 0.001245, l5: 0.001381, l6: 0.002640
[epoch: 181/1000, batch:  1040/ 1052, ite: 47600] train loss: 0.006178, tar: 0.000473 
[Epoch 181/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000307, l1: 0.000300, l2: 0.000313, l3: 0.000389, l4: 0.000473, l5: 0.001031, l6: 0.001646
[epoch: 182/1000, batch:    68/ 1052, ite: 47620] train loss: 0.006179, tar: 0.000473 
l0: 0.000548, l1: 0.000561, l2: 0.000582, l3: 0.000615, l4: 0.001075, l5: 0.001591, l6: 0.002990
[epoch: 182/1000, batch:   148/ 1052, ite: 47640] train loss: 0.006178, tar: 0.000473 
l0: 0.000812, l1: 0.000883, l2: 0.000930, l3: 0.000781, l4: 0.001130, l5: 0.001841, l6: 0.003516
[epoch: 182/1000, batch:   228/ 1052, ite: 47660] train loss: 0.006180, tar: 0.000473 
l0: 0.000326, l1: 0.000327, l2: 0.000342, l3: 0.000343, l4: 0.000454, l5: 0.000888, l6: 0.001067
[epoch: 182/1000, batch:   308/ 1052, ite: 47680] train loss: 0.006180, tar: 0.000473 
l0: 0.000301, l1: 0.000306, l2: 0.000341, l3: 0.000380, l4: 0.000530, l5: 0.001022, l6: 0.002551
[epoch: 182/1000, batch:   388/ 1052, ite: 47700] train loss: 0.006174, tar: 0.000472 
l0: 0.000233, l1: 0.000237, l2: 0.000225, l3: 0.000262, l4: 0.000420, l5: 0.001030, l6: 0.001398
[epoch: 182/1000, batch:   468/ 1052, ite: 47720] train loss: 0.006175, tar: 0.000472 
l0: 0.000200, l1: 0.000187, l2: 0.000210, l3: 0.000281, l4: 0.000473, l5: 0.000877, l6: 0.000759
[epoch: 182/1000, batch:   548/ 1052, ite: 47740] train loss: 0.006173, tar: 0.000472 
l0: 0.000400, l1: 0.000408, l2: 0.000453, l3: 0.000496, l4: 0.000671, l5: 0.001289, l6: 0.002353
[epoch: 182/1000, batch:   628/ 1052, ite: 47760] train loss: 0.006169, tar: 0.000472 
l0: 0.000637, l1: 0.000657, l2: 0.000718, l3: 0.000838, l4: 0.001085, l5: 0.001727, l6: 0.002263
[epoch: 182/1000, batch:   708/ 1052, ite: 47780] train loss: 0.006172, tar: 0.000472 
l0: 0.001193, l1: 0.001313, l2: 0.001396, l3: 0.001260, l4: 0.001699, l5: 0.002973, l6: 0.006484
[epoch: 182/1000, batch:   788/ 1052, ite: 47800] train loss: 0.006174, tar: 0.000472 
l0: 0.000356, l1: 0.000375, l2: 0.000384, l3: 0.000433, l4: 0.000602, l5: 0.001227, l6: 0.002154
[epoch: 182/1000, batch:   868/ 1052, ite: 47820] train loss: 0.006170, tar: 0.000472 
l0: 0.000449, l1: 0.000462, l2: 0.000535, l3: 0.000627, l4: 0.001109, l5: 0.001858, l6: 0.002813
[epoch: 182/1000, batch:   948/ 1052, ite: 47840] train loss: 0.006168, tar: 0.000471 
l0: 0.000587, l1: 0.000610, l2: 0.000641, l3: 0.000645, l4: 0.001016, l5: 0.001502, l6: 0.002460
[epoch: 182/1000, batch:  1028/ 1052, ite: 47860] train loss: 0.006169, tar: 0.000471 
[Epoch 182/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000265, l1: 0.000285, l2: 0.000307, l3: 0.000336, l4: 0.000290, l5: 0.000827, l6: 0.000985
[epoch: 183/1000, batch:    56/ 1052, ite: 47880] train loss: 0.006169, tar: 0.000471 
l0: 0.000341, l1: 0.000336, l2: 0.000363, l3: 0.000356, l4: 0.000504, l5: 0.001027, l6: 0.001174
[epoch: 183/1000, batch:   136/ 1052, ite: 47900] train loss: 0.006167, tar: 0.000471 
l0: 0.000356, l1: 0.000367, l2: 0.000430, l3: 0.000457, l4: 0.000525, l5: 0.000749, l6: 0.001291
[epoch: 183/1000, batch:   216/ 1052, ite: 47920] train loss: 0.006167, tar: 0.000471 
l0: 0.000358, l1: 0.000406, l2: 0.000370, l3: 0.000375, l4: 0.000531, l5: 0.001287, l6: 0.002489
[epoch: 183/1000, batch:   296/ 1052, ite: 47940] train loss: 0.006168, tar: 0.000471 
l0: 0.000434, l1: 0.000432, l2: 0.000447, l3: 0.000549, l4: 0.000753, l5: 0.001002, l6: 0.002527
[epoch: 183/1000, batch:   376/ 1052, ite: 47960] train loss: 0.006167, tar: 0.000471 
l0: 0.000578, l1: 0.000617, l2: 0.000626, l3: 0.000554, l4: 0.000661, l5: 0.001071, l6: 0.001526
[epoch: 183/1000, batch:   456/ 1052, ite: 47980] train loss: 0.006171, tar: 0.000471 
l0: 0.000529, l1: 0.000569, l2: 0.000562, l3: 0.000615, l4: 0.000928, l5: 0.001562, l6: 0.003783
[epoch: 183/1000, batch:   536/ 1052, ite: 48000] train loss: 0.006171, tar: 0.000471 
l0: 0.000257, l1: 0.000267, l2: 0.000266, l3: 0.000307, l4: 0.000479, l5: 0.000995, l6: 0.001236
[epoch: 183/1000, batch:   616/ 1052, ite: 48020] train loss: 0.006170, tar: 0.000471 
l0: 0.000412, l1: 0.000449, l2: 0.000441, l3: 0.000551, l4: 0.000785, l5: 0.001317, l6: 0.001915
[epoch: 183/1000, batch:   696/ 1052, ite: 48040] train loss: 0.006169, tar: 0.000471 
l0: 0.000260, l1: 0.000276, l2: 0.000303, l3: 0.000330, l4: 0.000483, l5: 0.000767, l6: 0.001716
[epoch: 183/1000, batch:   776/ 1052, ite: 48060] train loss: 0.006170, tar: 0.000471 
l0: 0.000369, l1: 0.000365, l2: 0.000378, l3: 0.000437, l4: 0.000593, l5: 0.001241, l6: 0.001361
[epoch: 183/1000, batch:   856/ 1052, ite: 48080] train loss: 0.006166, tar: 0.000471 
l0: 0.000262, l1: 0.000267, l2: 0.000290, l3: 0.000327, l4: 0.000494, l5: 0.000924, l6: 0.001253
[epoch: 183/1000, batch:   936/ 1052, ite: 48100] train loss: 0.006165, tar: 0.000471 
l0: 0.000225, l1: 0.000234, l2: 0.000238, l3: 0.000266, l4: 0.000422, l5: 0.001093, l6: 0.001172
[epoch: 183/1000, batch:  1016/ 1052, ite: 48120] train loss: 0.006164, tar: 0.000471 
[Epoch 183/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000433, l1: 0.000472, l2: 0.000447, l3: 0.000556, l4: 0.001116, l5: 0.002208, l6: 0.002725
[epoch: 184/1000, batch:    44/ 1052, ite: 48140] train loss: 0.006163, tar: 0.000471 
l0: 0.000673, l1: 0.000666, l2: 0.000732, l3: 0.000890, l4: 0.001015, l5: 0.001923, l6: 0.002900
[epoch: 184/1000, batch:   124/ 1052, ite: 48160] train loss: 0.006164, tar: 0.000471 
l0: 0.000114, l1: 0.000107, l2: 0.000130, l3: 0.000161, l4: 0.000372, l5: 0.000309, l6: 0.000701
[epoch: 184/1000, batch:   204/ 1052, ite: 48180] train loss: 0.006166, tar: 0.000471 
l0: 0.000166, l1: 0.000171, l2: 0.000211, l3: 0.000238, l4: 0.000397, l5: 0.000557, l6: 0.001103
[epoch: 184/1000, batch:   284/ 1052, ite: 48200] train loss: 0.006165, tar: 0.000471 
l0: 0.000462, l1: 0.000463, l2: 0.000456, l3: 0.000553, l4: 0.000565, l5: 0.001368, l6: 0.001879
[epoch: 184/1000, batch:   364/ 1052, ite: 48220] train loss: 0.006164, tar: 0.000471 
l0: 0.000391, l1: 0.000383, l2: 0.000409, l3: 0.000797, l4: 0.000838, l5: 0.001222, l6: 0.001930
[epoch: 184/1000, batch:   444/ 1052, ite: 48240] train loss: 0.006167, tar: 0.000471 
l0: 0.000688, l1: 0.000699, l2: 0.000761, l3: 0.000837, l4: 0.001135, l5: 0.002143, l6: 0.003145
[epoch: 184/1000, batch:   524/ 1052, ite: 48260] train loss: 0.006165, tar: 0.000471 
l0: 0.000390, l1: 0.000409, l2: 0.000434, l3: 0.000427, l4: 0.000537, l5: 0.001039, l6: 0.001276
[epoch: 184/1000, batch:   604/ 1052, ite: 48280] train loss: 0.006164, tar: 0.000471 
l0: 0.000711, l1: 0.000817, l2: 0.000776, l3: 0.000637, l4: 0.000856, l5: 0.001222, l6: 0.002394
[epoch: 184/1000, batch:   684/ 1052, ite: 48300] train loss: 0.006164, tar: 0.000471 
l0: 0.000332, l1: 0.000328, l2: 0.000340, l3: 0.000396, l4: 0.000577, l5: 0.000659, l6: 0.001685
[epoch: 184/1000, batch:   764/ 1052, ite: 48320] train loss: 0.006162, tar: 0.000470 
l0: 0.000434, l1: 0.000443, l2: 0.000462, l3: 0.000529, l4: 0.000546, l5: 0.001430, l6: 0.003024
[epoch: 184/1000, batch:   844/ 1052, ite: 48340] train loss: 0.006165, tar: 0.000471 
l0: 0.000656, l1: 0.000652, l2: 0.000654, l3: 0.000784, l4: 0.000980, l5: 0.001192, l6: 0.002609
[epoch: 184/1000, batch:   924/ 1052, ite: 48360] train loss: 0.006163, tar: 0.000470 
l0: 0.000488, l1: 0.000493, l2: 0.000519, l3: 0.000481, l4: 0.000753, l5: 0.001411, l6: 0.002502
[epoch: 184/1000, batch:  1004/ 1052, ite: 48380] train loss: 0.006167, tar: 0.000470 
[Epoch 184/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000457, l1: 0.000467, l2: 0.000469, l3: 0.000505, l4: 0.000631, l5: 0.001207, l6: 0.001946
[epoch: 185/1000, batch:    32/ 1052, ite: 48400] train loss: 0.006165, tar: 0.000470 
l0: 0.000285, l1: 0.000259, l2: 0.000300, l3: 0.000418, l4: 0.000476, l5: 0.001164, l6: 0.001909
[epoch: 185/1000, batch:   112/ 1052, ite: 48420] train loss: 0.006164, tar: 0.000470 
l0: 0.000467, l1: 0.000486, l2: 0.000586, l3: 0.000646, l4: 0.000793, l5: 0.001016, l6: 0.001284
[epoch: 185/1000, batch:   192/ 1052, ite: 48440] train loss: 0.006165, tar: 0.000470 
l0: 0.000565, l1: 0.000576, l2: 0.000577, l3: 0.000576, l4: 0.000731, l5: 0.001399, l6: 0.002501
[epoch: 185/1000, batch:   272/ 1052, ite: 48460] train loss: 0.006167, tar: 0.000470 
l0: 0.001128, l1: 0.001319, l2: 0.001141, l3: 0.001096, l4: 0.001492, l5: 0.002543, l6: 0.003849
[epoch: 185/1000, batch:   352/ 1052, ite: 48480] train loss: 0.006168, tar: 0.000470 
l0: 0.000069, l1: 0.000071, l2: 0.000062, l3: 0.000084, l4: 0.000223, l5: 0.000416, l6: 0.000467
[epoch: 185/1000, batch:   432/ 1052, ite: 48500] train loss: 0.006167, tar: 0.000470 
l0: 0.000565, l1: 0.000576, l2: 0.000684, l3: 0.000851, l4: 0.001334, l5: 0.001768, l6: 0.002436
[epoch: 185/1000, batch:   512/ 1052, ite: 48520] train loss: 0.006166, tar: 0.000470 
l0: 0.000384, l1: 0.000366, l2: 0.000397, l3: 0.000511, l4: 0.000666, l5: 0.001255, l6: 0.002333
[epoch: 185/1000, batch:   592/ 1052, ite: 48540] train loss: 0.006166, tar: 0.000470 
l0: 0.000258, l1: 0.000259, l2: 0.000255, l3: 0.000286, l4: 0.000448, l5: 0.000951, l6: 0.001236
[epoch: 185/1000, batch:   672/ 1052, ite: 48560] train loss: 0.006164, tar: 0.000470 
l0: 0.000277, l1: 0.000265, l2: 0.000339, l3: 0.000372, l4: 0.000444, l5: 0.000764, l6: 0.001395
[epoch: 185/1000, batch:   752/ 1052, ite: 48580] train loss: 0.006162, tar: 0.000470 
l0: 0.000628, l1: 0.000630, l2: 0.000643, l3: 0.000662, l4: 0.000991, l5: 0.001751, l6: 0.002946
[epoch: 185/1000, batch:   832/ 1052, ite: 48600] train loss: 0.006162, tar: 0.000470 
l0: 0.000223, l1: 0.000211, l2: 0.000227, l3: 0.000297, l4: 0.000587, l5: 0.000854, l6: 0.001710
[epoch: 185/1000, batch:   912/ 1052, ite: 48620] train loss: 0.006159, tar: 0.000470 
l0: 0.000374, l1: 0.000387, l2: 0.000386, l3: 0.000392, l4: 0.000634, l5: 0.001178, l6: 0.002201
[epoch: 185/1000, batch:   992/ 1052, ite: 48640] train loss: 0.006158, tar: 0.000469 
[Epoch 185/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000519, l1: 0.000525, l2: 0.000508, l3: 0.000666, l4: 0.001086, l5: 0.001876, l6: 0.004198
[epoch: 186/1000, batch:    20/ 1052, ite: 48660] train loss: 0.006158, tar: 0.000469 
l0: 0.000919, l1: 0.001043, l2: 0.001000, l3: 0.001064, l4: 0.000997, l5: 0.001522, l6: 0.003676
[epoch: 186/1000, batch:   100/ 1052, ite: 48680] train loss: 0.006157, tar: 0.000469 
l0: 0.000330, l1: 0.000342, l2: 0.000378, l3: 0.000418, l4: 0.000607, l5: 0.001456, l6: 0.001856
[epoch: 186/1000, batch:   180/ 1052, ite: 48700] train loss: 0.006156, tar: 0.000469 
l0: 0.000305, l1: 0.000342, l2: 0.000350, l3: 0.000307, l4: 0.000603, l5: 0.000735, l6: 0.001507
[epoch: 186/1000, batch:   260/ 1052, ite: 48720] train loss: 0.006157, tar: 0.000469 
l0: 0.000362, l1: 0.000373, l2: 0.000369, l3: 0.000393, l4: 0.000607, l5: 0.001074, l6: 0.001527
[epoch: 186/1000, batch:   340/ 1052, ite: 48740] train loss: 0.006157, tar: 0.000469 
l0: 0.000302, l1: 0.000313, l2: 0.000349, l3: 0.000371, l4: 0.000576, l5: 0.001302, l6: 0.002069
[epoch: 186/1000, batch:   420/ 1052, ite: 48760] train loss: 0.006156, tar: 0.000469 
l0: 0.000590, l1: 0.000606, l2: 0.000609, l3: 0.000635, l4: 0.000983, l5: 0.002049, l6: 0.003749
[epoch: 186/1000, batch:   500/ 1052, ite: 48780] train loss: 0.006156, tar: 0.000469 
l0: 0.000488, l1: 0.000483, l2: 0.000489, l3: 0.000682, l4: 0.000976, l5: 0.001981, l6: 0.004357
[epoch: 186/1000, batch:   580/ 1052, ite: 48800] train loss: 0.006156, tar: 0.000469 
l0: 0.000369, l1: 0.000381, l2: 0.000472, l3: 0.000416, l4: 0.000606, l5: 0.001123, l6: 0.001581
[epoch: 186/1000, batch:   660/ 1052, ite: 48820] train loss: 0.006155, tar: 0.000469 
l0: 0.000280, l1: 0.000286, l2: 0.000290, l3: 0.000349, l4: 0.000598, l5: 0.000833, l6: 0.001389
[epoch: 186/1000, batch:   740/ 1052, ite: 48840] train loss: 0.006154, tar: 0.000469 
l0: 0.000458, l1: 0.000453, l2: 0.000483, l3: 0.000519, l4: 0.000936, l5: 0.001296, l6: 0.002151
[epoch: 186/1000, batch:   820/ 1052, ite: 48860] train loss: 0.006153, tar: 0.000468 
l0: 0.000721, l1: 0.000686, l2: 0.000704, l3: 0.000837, l4: 0.001049, l5: 0.001570, l6: 0.002250
[epoch: 186/1000, batch:   900/ 1052, ite: 48880] train loss: 0.006152, tar: 0.000468 
l0: 0.000510, l1: 0.000567, l2: 0.000472, l3: 0.000382, l4: 0.000720, l5: 0.001075, l6: 0.001958
[epoch: 186/1000, batch:   980/ 1052, ite: 48900] train loss: 0.006149, tar: 0.000468 
[Epoch 186/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000242, l1: 0.000240, l2: 0.000287, l3: 0.000391, l4: 0.000376, l5: 0.000916, l6: 0.001185
[epoch: 187/1000, batch:     8/ 1052, ite: 48920] train loss: 0.006150, tar: 0.000468 
l0: 0.000404, l1: 0.000393, l2: 0.000457, l3: 0.000505, l4: 0.000731, l5: 0.000891, l6: 0.002083
[epoch: 187/1000, batch:    88/ 1052, ite: 48940] train loss: 0.006151, tar: 0.000468 
l0: 0.000230, l1: 0.000220, l2: 0.000240, l3: 0.000293, l4: 0.000496, l5: 0.000646, l6: 0.001362
[epoch: 187/1000, batch:   168/ 1052, ite: 48960] train loss: 0.006150, tar: 0.000468 
l0: 0.000749, l1: 0.000801, l2: 0.000840, l3: 0.000855, l4: 0.001218, l5: 0.001959, l6: 0.003364
[epoch: 187/1000, batch:   248/ 1052, ite: 48980] train loss: 0.006148, tar: 0.000468 
l0: 0.000286, l1: 0.000284, l2: 0.000290, l3: 0.000306, l4: 0.000527, l5: 0.000787, l6: 0.001132
[epoch: 187/1000, batch:   328/ 1052, ite: 49000] train loss: 0.006150, tar: 0.000468 
l0: 0.000725, l1: 0.000719, l2: 0.000776, l3: 0.000734, l4: 0.000873, l5: 0.001730, l6: 0.002294
[epoch: 187/1000, batch:   408/ 1052, ite: 49020] train loss: 0.006150, tar: 0.000468 
l0: 0.000245, l1: 0.000244, l2: 0.000248, l3: 0.000274, l4: 0.000446, l5: 0.000745, l6: 0.001660
[epoch: 187/1000, batch:   488/ 1052, ite: 49040] train loss: 0.006152, tar: 0.000468 
l0: 0.000347, l1: 0.000370, l2: 0.000371, l3: 0.000385, l4: 0.000570, l5: 0.001304, l6: 0.002552
[epoch: 187/1000, batch:   568/ 1052, ite: 49060] train loss: 0.006150, tar: 0.000468 
l0: 0.000398, l1: 0.000418, l2: 0.000422, l3: 0.000470, l4: 0.000704, l5: 0.001476, l6: 0.002811
[epoch: 187/1000, batch:   648/ 1052, ite: 49080] train loss: 0.006148, tar: 0.000468 
l0: 0.000488, l1: 0.000510, l2: 0.000509, l3: 0.000563, l4: 0.000823, l5: 0.001034, l6: 0.003864
[epoch: 187/1000, batch:   728/ 1052, ite: 49100] train loss: 0.006145, tar: 0.000467 
l0: 0.000215, l1: 0.000225, l2: 0.000269, l3: 0.000253, l4: 0.000536, l5: 0.000680, l6: 0.001146
[epoch: 187/1000, batch:   808/ 1052, ite: 49120] train loss: 0.006144, tar: 0.000467 
l0: 0.000593, l1: 0.000586, l2: 0.000632, l3: 0.000723, l4: 0.000997, l5: 0.001469, l6: 0.003367
[epoch: 187/1000, batch:   888/ 1052, ite: 49140] train loss: 0.006143, tar: 0.000467 
l0: 0.000448, l1: 0.000456, l2: 0.000448, l3: 0.000497, l4: 0.000911, l5: 0.001464, l6: 0.002703
[epoch: 187/1000, batch:   968/ 1052, ite: 49160] train loss: 0.006141, tar: 0.000467 
l0: 0.000486, l1: 0.000493, l2: 0.000540, l3: 0.000534, l4: 0.000775, l5: 0.001038, l6: 0.001515
[epoch: 187/1000, batch:  1048/ 1052, ite: 49180] train loss: 0.006141, tar: 0.000467 
[Epoch 187/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000493, l1: 0.000498, l2: 0.000503, l3: 0.000570, l4: 0.000781, l5: 0.001836, l6: 0.002833
[epoch: 188/1000, batch:    76/ 1052, ite: 49200] train loss: 0.006141, tar: 0.000467 
l0: 0.000340, l1: 0.000346, l2: 0.000354, l3: 0.000388, l4: 0.000656, l5: 0.001125, l6: 0.001383
[epoch: 188/1000, batch:   156/ 1052, ite: 49220] train loss: 0.006138, tar: 0.000467 
l0: 0.000567, l1: 0.000570, l2: 0.000527, l3: 0.000703, l4: 0.001161, l5: 0.001577, l6: 0.003289
[epoch: 188/1000, batch:   236/ 1052, ite: 49240] train loss: 0.006138, tar: 0.000467 
l0: 0.000569, l1: 0.000600, l2: 0.000629, l3: 0.000599, l4: 0.000732, l5: 0.001399, l6: 0.002881
[epoch: 188/1000, batch:   316/ 1052, ite: 49260] train loss: 0.006135, tar: 0.000466 
l0: 0.000634, l1: 0.000550, l2: 0.000637, l3: 0.000881, l4: 0.001047, l5: 0.001479, l6: 0.002214
[epoch: 188/1000, batch:   396/ 1052, ite: 49280] train loss: 0.006134, tar: 0.000466 
l0: 0.000682, l1: 0.000671, l2: 0.000864, l3: 0.000820, l4: 0.000909, l5: 0.001938, l6: 0.002840
[epoch: 188/1000, batch:   476/ 1052, ite: 49300] train loss: 0.006136, tar: 0.000466 
l0: 0.000235, l1: 0.000247, l2: 0.000232, l3: 0.000248, l4: 0.000394, l5: 0.000758, l6: 0.001319
[epoch: 188/1000, batch:   556/ 1052, ite: 49320] train loss: 0.006136, tar: 0.000466 
l0: 0.000293, l1: 0.000301, l2: 0.000323, l3: 0.000343, l4: 0.000555, l5: 0.001208, l6: 0.001831
[epoch: 188/1000, batch:   636/ 1052, ite: 49340] train loss: 0.006134, tar: 0.000466 
l0: 0.000358, l1: 0.000358, l2: 0.000356, l3: 0.000434, l4: 0.000649, l5: 0.001215, l6: 0.001610
[epoch: 188/1000, batch:   716/ 1052, ite: 49360] train loss: 0.006132, tar: 0.000466 
l0: 0.000551, l1: 0.000572, l2: 0.000555, l3: 0.000594, l4: 0.000927, l5: 0.002003, l6: 0.002690
[epoch: 188/1000, batch:   796/ 1052, ite: 49380] train loss: 0.006132, tar: 0.000466 
l0: 0.000538, l1: 0.000606, l2: 0.000639, l3: 0.000604, l4: 0.000884, l5: 0.001351, l6: 0.003022
[epoch: 188/1000, batch:   876/ 1052, ite: 49400] train loss: 0.006134, tar: 0.000466 
l0: 0.000522, l1: 0.000520, l2: 0.000552, l3: 0.000607, l4: 0.001151, l5: 0.001622, l6: 0.002735
[epoch: 188/1000, batch:   956/ 1052, ite: 49420] train loss: 0.006135, tar: 0.000466 
l0: 0.000682, l1: 0.000749, l2: 0.000709, l3: 0.000659, l4: 0.000592, l5: 0.001430, l6: 0.001879
[epoch: 188/1000, batch:  1036/ 1052, ite: 49440] train loss: 0.006135, tar: 0.000466 
[Epoch 188/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000680, l1: 0.000718, l2: 0.000766, l3: 0.000718, l4: 0.000946, l5: 0.001338, l6: 0.002495
[epoch: 189/1000, batch:    64/ 1052, ite: 49460] train loss: 0.006133, tar: 0.000466 
l0: 0.000435, l1: 0.000444, l2: 0.000467, l3: 0.000539, l4: 0.000734, l5: 0.001175, l6: 0.002479
[epoch: 189/1000, batch:   144/ 1052, ite: 49480] train loss: 0.006132, tar: 0.000466 
l0: 0.000546, l1: 0.000584, l2: 0.000566, l3: 0.000566, l4: 0.000785, l5: 0.001995, l6: 0.003284
[epoch: 189/1000, batch:   224/ 1052, ite: 49500] train loss: 0.006130, tar: 0.000466 
l0: 0.000211, l1: 0.000179, l2: 0.000237, l3: 0.000281, l4: 0.000431, l5: 0.000733, l6: 0.001090
[epoch: 189/1000, batch:   304/ 1052, ite: 49520] train loss: 0.006130, tar: 0.000466 
l0: 0.000238, l1: 0.000245, l2: 0.000276, l3: 0.000302, l4: 0.000517, l5: 0.001181, l6: 0.001804
[epoch: 189/1000, batch:   384/ 1052, ite: 49540] train loss: 0.006128, tar: 0.000465 
l0: 0.000244, l1: 0.000251, l2: 0.000297, l3: 0.000313, l4: 0.000511, l5: 0.000767, l6: 0.002015
[epoch: 189/1000, batch:   464/ 1052, ite: 49560] train loss: 0.006128, tar: 0.000465 
l0: 0.000198, l1: 0.000197, l2: 0.000244, l3: 0.000253, l4: 0.000379, l5: 0.000949, l6: 0.001028
[epoch: 189/1000, batch:   544/ 1052, ite: 49580] train loss: 0.006130, tar: 0.000465 
l0: 0.000141, l1: 0.000143, l2: 0.000130, l3: 0.000159, l4: 0.000258, l5: 0.000198, l6: 0.000748
[epoch: 189/1000, batch:   624/ 1052, ite: 49600] train loss: 0.006128, tar: 0.000465 
l0: 0.000541, l1: 0.000556, l2: 0.000597, l3: 0.000578, l4: 0.000834, l5: 0.002149, l6: 0.003524
[epoch: 189/1000, batch:   704/ 1052, ite: 49620] train loss: 0.006129, tar: 0.000465 
l0: 0.000514, l1: 0.000584, l2: 0.000578, l3: 0.000532, l4: 0.000748, l5: 0.001716, l6: 0.002645
[epoch: 189/1000, batch:   784/ 1052, ite: 49640] train loss: 0.006128, tar: 0.000465 
l0: 0.000441, l1: 0.000443, l2: 0.000445, l3: 0.000530, l4: 0.000744, l5: 0.002073, l6: 0.001991
[epoch: 189/1000, batch:   864/ 1052, ite: 49660] train loss: 0.006126, tar: 0.000465 
l0: 0.000404, l1: 0.000431, l2: 0.000423, l3: 0.000467, l4: 0.000586, l5: 0.001393, l6: 0.001230
[epoch: 189/1000, batch:   944/ 1052, ite: 49680] train loss: 0.006126, tar: 0.000465 
l0: 0.000436, l1: 0.000443, l2: 0.000467, l3: 0.000585, l4: 0.000780, l5: 0.001921, l6: 0.002718
[epoch: 189/1000, batch:  1024/ 1052, ite: 49700] train loss: 0.006125, tar: 0.000465 
[Epoch 189/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000186, l1: 0.000195, l2: 0.000211, l3: 0.000304, l4: 0.000428, l5: 0.000605, l6: 0.001409
[epoch: 190/1000, batch:    52/ 1052, ite: 49720] train loss: 0.006124, tar: 0.000465 
l0: 0.000434, l1: 0.000423, l2: 0.000456, l3: 0.000545, l4: 0.000821, l5: 0.001110, l6: 0.002168
[epoch: 190/1000, batch:   132/ 1052, ite: 49740] train loss: 0.006125, tar: 0.000465 
l0: 0.000397, l1: 0.000404, l2: 0.000419, l3: 0.000496, l4: 0.000739, l5: 0.001152, l6: 0.002046
[epoch: 190/1000, batch:   212/ 1052, ite: 49760] train loss: 0.006124, tar: 0.000465 
l0: 0.000565, l1: 0.000573, l2: 0.000606, l3: 0.000638, l4: 0.000871, l5: 0.001307, l6: 0.003218
[epoch: 190/1000, batch:   292/ 1052, ite: 49780] train loss: 0.006124, tar: 0.000465 
l0: 0.000314, l1: 0.000308, l2: 0.000347, l3: 0.000420, l4: 0.000662, l5: 0.001115, l6: 0.001756
[epoch: 190/1000, batch:   372/ 1052, ite: 49800] train loss: 0.006125, tar: 0.000465 
l0: 0.000287, l1: 0.000291, l2: 0.000324, l3: 0.000352, l4: 0.000467, l5: 0.000918, l6: 0.002015
[epoch: 190/1000, batch:   452/ 1052, ite: 49820] train loss: 0.006124, tar: 0.000465 
l0: 0.000634, l1: 0.000667, l2: 0.000703, l3: 0.000728, l4: 0.001118, l5: 0.001935, l6: 0.003062
[epoch: 190/1000, batch:   532/ 1052, ite: 49840] train loss: 0.006123, tar: 0.000464 
l0: 0.000500, l1: 0.000515, l2: 0.000500, l3: 0.000572, l4: 0.000747, l5: 0.001457, l6: 0.002288
[epoch: 190/1000, batch:   612/ 1052, ite: 49860] train loss: 0.006121, tar: 0.000464 
l0: 0.000404, l1: 0.000429, l2: 0.000427, l3: 0.000386, l4: 0.000596, l5: 0.000738, l6: 0.001682
[epoch: 190/1000, batch:   692/ 1052, ite: 49880] train loss: 0.006120, tar: 0.000464 
l0: 0.000433, l1: 0.000449, l2: 0.000530, l3: 0.000586, l4: 0.000714, l5: 0.001197, l6: 0.002170
[epoch: 190/1000, batch:   772/ 1052, ite: 49900] train loss: 0.006120, tar: 0.000464 
l0: 0.000337, l1: 0.000331, l2: 0.000355, l3: 0.000424, l4: 0.000617, l5: 0.001121, l6: 0.001518
[epoch: 190/1000, batch:   852/ 1052, ite: 49920] train loss: 0.006118, tar: 0.000464 
l0: 0.000385, l1: 0.000408, l2: 0.000415, l3: 0.000413, l4: 0.000620, l5: 0.001125, l6: 0.002284
[epoch: 190/1000, batch:   932/ 1052, ite: 49940] train loss: 0.006118, tar: 0.000464 
l0: 0.000441, l1: 0.000461, l2: 0.000440, l3: 0.000472, l4: 0.000643, l5: 0.001260, l6: 0.002259
[epoch: 190/1000, batch:  1012/ 1052, ite: 49960] train loss: 0.006119, tar: 0.000464 
[Epoch 190/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000826, l1: 0.000886, l2: 0.000947, l3: 0.000866, l4: 0.001236, l5: 0.002388, l6: 0.002762
[epoch: 191/1000, batch:    40/ 1052, ite: 49980] train loss: 0.006118, tar: 0.000464 
l0: 0.000300, l1: 0.000303, l2: 0.000309, l3: 0.000368, l4: 0.000498, l5: 0.000924, l6: 0.001270
[epoch: 191/1000, batch:   120/ 1052, ite: 50000] train loss: 0.006115, tar: 0.000463 
l0: 0.000469, l1: 0.000489, l2: 0.000454, l3: 0.000508, l4: 0.000632, l5: 0.001585, l6: 0.003065
[epoch: 191/1000, batch:   200/ 1052, ite: 50020] train loss: 0.006117, tar: 0.000463 
l0: 0.000807, l1: 0.000844, l2: 0.000852, l3: 0.000921, l4: 0.001118, l5: 0.001916, l6: 0.003224
[epoch: 191/1000, batch:   280/ 1052, ite: 50040] train loss: 0.006116, tar: 0.000463 
l0: 0.000550, l1: 0.000575, l2: 0.000520, l3: 0.000590, l4: 0.000685, l5: 0.001270, l6: 0.002270
[epoch: 191/1000, batch:   360/ 1052, ite: 50060] train loss: 0.006115, tar: 0.000463 
l0: 0.000222, l1: 0.000222, l2: 0.000312, l3: 0.000295, l4: 0.000700, l5: 0.000895, l6: 0.001382
[epoch: 191/1000, batch:   440/ 1052, ite: 50080] train loss: 0.006114, tar: 0.000463 
l0: 0.000881, l1: 0.000880, l2: 0.001031, l3: 0.001053, l4: 0.001348, l5: 0.001711, l6: 0.003761
[epoch: 191/1000, batch:   520/ 1052, ite: 50100] train loss: 0.006115, tar: 0.000463 
l0: 0.000645, l1: 0.000663, l2: 0.000621, l3: 0.000698, l4: 0.000915, l5: 0.001821, l6: 0.002976
[epoch: 191/1000, batch:   600/ 1052, ite: 50120] train loss: 0.006116, tar: 0.000463 
l0: 0.000212, l1: 0.000220, l2: 0.000218, l3: 0.000255, l4: 0.000361, l5: 0.000433, l6: 0.001301
[epoch: 191/1000, batch:   680/ 1052, ite: 50140] train loss: 0.006114, tar: 0.000463 
l0: 0.000402, l1: 0.000415, l2: 0.000474, l3: 0.000411, l4: 0.000602, l5: 0.001281, l6: 0.002358
[epoch: 191/1000, batch:   760/ 1052, ite: 50160] train loss: 0.006113, tar: 0.000463 
l0: 0.000582, l1: 0.000608, l2: 0.000646, l3: 0.000586, l4: 0.001071, l5: 0.001815, l6: 0.002390
[epoch: 191/1000, batch:   840/ 1052, ite: 50180] train loss: 0.006112, tar: 0.000463 
l0: 0.000601, l1: 0.000589, l2: 0.000587, l3: 0.000718, l4: 0.000987, l5: 0.002107, l6: 0.003349
[epoch: 191/1000, batch:   920/ 1052, ite: 50200] train loss: 0.006111, tar: 0.000463 
l0: 0.000354, l1: 0.000358, l2: 0.000431, l3: 0.000421, l4: 0.000605, l5: 0.000928, l6: 0.001842
[epoch: 191/1000, batch:  1000/ 1052, ite: 50220] train loss: 0.006110, tar: 0.000462 
[Epoch 191/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000224, l1: 0.000217, l2: 0.000244, l3: 0.000246, l4: 0.000492, l5: 0.000603, l6: 0.000762
[epoch: 192/1000, batch:    28/ 1052, ite: 50240] train loss: 0.006109, tar: 0.000462 
l0: 0.000590, l1: 0.000585, l2: 0.000580, l3: 0.000616, l4: 0.000803, l5: 0.001414, l6: 0.002322
[epoch: 192/1000, batch:   108/ 1052, ite: 50260] train loss: 0.006109, tar: 0.000462 
l0: 0.000603, l1: 0.000625, l2: 0.000684, l3: 0.000694, l4: 0.000783, l5: 0.001857, l6: 0.003056
[epoch: 192/1000, batch:   188/ 1052, ite: 50280] train loss: 0.006107, tar: 0.000462 
l0: 0.000474, l1: 0.000469, l2: 0.000576, l3: 0.000646, l4: 0.001031, l5: 0.001513, l6: 0.001868
[epoch: 192/1000, batch:   268/ 1052, ite: 50300] train loss: 0.006106, tar: 0.000462 
l0: 0.000869, l1: 0.000915, l2: 0.000890, l3: 0.000862, l4: 0.001247, l5: 0.001140, l6: 0.002288
[epoch: 192/1000, batch:   348/ 1052, ite: 50320] train loss: 0.006108, tar: 0.000462 
l0: 0.000213, l1: 0.000215, l2: 0.000214, l3: 0.000213, l4: 0.000395, l5: 0.000674, l6: 0.000790
[epoch: 192/1000, batch:   428/ 1052, ite: 50340] train loss: 0.006108, tar: 0.000462 
l0: 0.000489, l1: 0.000469, l2: 0.000542, l3: 0.000567, l4: 0.000908, l5: 0.001403, l6: 0.002199
[epoch: 192/1000, batch:   508/ 1052, ite: 50360] train loss: 0.006107, tar: 0.000462 
l0: 0.000591, l1: 0.000605, l2: 0.000628, l3: 0.000743, l4: 0.001031, l5: 0.001620, l6: 0.002225
[epoch: 192/1000, batch:   588/ 1052, ite: 50380] train loss: 0.006105, tar: 0.000462 
l0: 0.000416, l1: 0.000457, l2: 0.000441, l3: 0.000438, l4: 0.000908, l5: 0.001281, l6: 0.002254
[epoch: 192/1000, batch:   668/ 1052, ite: 50400] train loss: 0.006103, tar: 0.000462 
l0: 0.000761, l1: 0.000790, l2: 0.000805, l3: 0.000862, l4: 0.001046, l5: 0.002410, l6: 0.004090
[epoch: 192/1000, batch:   748/ 1052, ite: 50420] train loss: 0.006102, tar: 0.000461 
l0: 0.000444, l1: 0.000466, l2: 0.000454, l3: 0.000515, l4: 0.000785, l5: 0.001066, l6: 0.002425
[epoch: 192/1000, batch:   828/ 1052, ite: 50440] train loss: 0.006102, tar: 0.000461 
l0: 0.000572, l1: 0.000585, l2: 0.000592, l3: 0.000661, l4: 0.000859, l5: 0.002131, l6: 0.003025
[epoch: 192/1000, batch:   908/ 1052, ite: 50460] train loss: 0.006103, tar: 0.000462 
l0: 0.000402, l1: 0.000380, l2: 0.000494, l3: 0.000655, l4: 0.000777, l5: 0.001910, l6: 0.002652
[epoch: 192/1000, batch:   988/ 1052, ite: 50480] train loss: 0.006102, tar: 0.000461 
[Epoch 192/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000486, l1: 0.000489, l2: 0.000588, l3: 0.000689, l4: 0.001134, l5: 0.002173, l6: 0.003067
[epoch: 193/1000, batch:    16/ 1052, ite: 50500] train loss: 0.006101, tar: 0.000461 
l0: 0.000403, l1: 0.000411, l2: 0.000460, l3: 0.000502, l4: 0.000880, l5: 0.001504, l6: 0.002227
[epoch: 193/1000, batch:    96/ 1052, ite: 50520] train loss: 0.006101, tar: 0.000461 
l0: 0.000260, l1: 0.000265, l2: 0.000298, l3: 0.000296, l4: 0.000445, l5: 0.000730, l6: 0.001472
[epoch: 193/1000, batch:   176/ 1052, ite: 50540] train loss: 0.006100, tar: 0.000461 
l0: 0.000660, l1: 0.000666, l2: 0.000694, l3: 0.000773, l4: 0.001203, l5: 0.003667, l6: 0.005880
[epoch: 193/1000, batch:   256/ 1052, ite: 50560] train loss: 0.006099, tar: 0.000461 
l0: 0.000292, l1: 0.000294, l2: 0.000289, l3: 0.000373, l4: 0.000730, l5: 0.001184, l6: 0.001759
[epoch: 193/1000, batch:   336/ 1052, ite: 50580] train loss: 0.006098, tar: 0.000461 
l0: 0.000273, l1: 0.000271, l2: 0.000260, l3: 0.000305, l4: 0.000596, l5: 0.001016, l6: 0.001683
[epoch: 193/1000, batch:   416/ 1052, ite: 50600] train loss: 0.006098, tar: 0.000461 
l0: 0.000554, l1: 0.000590, l2: 0.000548, l3: 0.000623, l4: 0.000734, l5: 0.001926, l6: 0.002979
[epoch: 193/1000, batch:   496/ 1052, ite: 50620] train loss: 0.006101, tar: 0.000461 
l0: 0.000314, l1: 0.000323, l2: 0.000307, l3: 0.000412, l4: 0.000575, l5: 0.000936, l6: 0.001150
[epoch: 193/1000, batch:   576/ 1052, ite: 50640] train loss: 0.006101, tar: 0.000461 
l0: 0.000146, l1: 0.000141, l2: 0.000133, l3: 0.000234, l4: 0.000428, l5: 0.000824, l6: 0.001181
[epoch: 193/1000, batch:   656/ 1052, ite: 50660] train loss: 0.006102, tar: 0.000461 
l0: 0.000784, l1: 0.000811, l2: 0.000779, l3: 0.000930, l4: 0.001008, l5: 0.001966, l6: 0.002597
[epoch: 193/1000, batch:   736/ 1052, ite: 50680] train loss: 0.006101, tar: 0.000461 
l0: 0.000448, l1: 0.000449, l2: 0.000493, l3: 0.000510, l4: 0.000783, l5: 0.001508, l6: 0.002553
[epoch: 193/1000, batch:   816/ 1052, ite: 50700] train loss: 0.006100, tar: 0.000461 
l0: 0.000201, l1: 0.000195, l2: 0.000230, l3: 0.000257, l4: 0.000456, l5: 0.000967, l6: 0.001290
[epoch: 193/1000, batch:   896/ 1052, ite: 50720] train loss: 0.006098, tar: 0.000461 
l0: 0.000596, l1: 0.000633, l2: 0.000697, l3: 0.000695, l4: 0.000842, l5: 0.001503, l6: 0.003121
[epoch: 193/1000, batch:   976/ 1052, ite: 50740] train loss: 0.006096, tar: 0.000460 
[Epoch 193/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000579, l1: 0.000618, l2: 0.000653, l3: 0.000602, l4: 0.000775, l5: 0.001556, l6: 0.002325
[epoch: 194/1000, batch:     4/ 1052, ite: 50760] train loss: 0.006095, tar: 0.000460 
l0: 0.000319, l1: 0.000333, l2: 0.000345, l3: 0.000346, l4: 0.000639, l5: 0.001010, l6: 0.001417
[epoch: 194/1000, batch:    84/ 1052, ite: 50780] train loss: 0.006094, tar: 0.000460 
l0: 0.000201, l1: 0.000206, l2: 0.000220, l3: 0.000225, l4: 0.000425, l5: 0.000922, l6: 0.001462
[epoch: 194/1000, batch:   164/ 1052, ite: 50800] train loss: 0.006094, tar: 0.000460 
l0: 0.000284, l1: 0.000301, l2: 0.000300, l3: 0.000274, l4: 0.000493, l5: 0.000911, l6: 0.001512
[epoch: 194/1000, batch:   244/ 1052, ite: 50820] train loss: 0.006092, tar: 0.000460 
l0: 0.000273, l1: 0.000266, l2: 0.000257, l3: 0.000316, l4: 0.000452, l5: 0.000823, l6: 0.000970
[epoch: 194/1000, batch:   324/ 1052, ite: 50840] train loss: 0.006092, tar: 0.000460 
l0: 0.000176, l1: 0.000178, l2: 0.000214, l3: 0.000255, l4: 0.000462, l5: 0.001082, l6: 0.001922
[epoch: 194/1000, batch:   404/ 1052, ite: 50860] train loss: 0.006091, tar: 0.000460 
l0: 0.000384, l1: 0.000403, l2: 0.000428, l3: 0.000439, l4: 0.000669, l5: 0.000914, l6: 0.002099
[epoch: 194/1000, batch:   484/ 1052, ite: 50880] train loss: 0.006090, tar: 0.000459 
l0: 0.000412, l1: 0.000432, l2: 0.000507, l3: 0.000543, l4: 0.000730, l5: 0.001207, l6: 0.003236
[epoch: 194/1000, batch:   564/ 1052, ite: 50900] train loss: 0.006090, tar: 0.000459 
l0: 0.000218, l1: 0.000212, l2: 0.000245, l3: 0.000269, l4: 0.000567, l5: 0.000733, l6: 0.001858
[epoch: 194/1000, batch:   644/ 1052, ite: 50920] train loss: 0.006088, tar: 0.000459 
l0: 0.000461, l1: 0.000451, l2: 0.000481, l3: 0.000575, l4: 0.000980, l5: 0.001652, l6: 0.003527
[epoch: 194/1000, batch:   724/ 1052, ite: 50940] train loss: 0.006087, tar: 0.000459 
l0: 0.000315, l1: 0.000310, l2: 0.000383, l3: 0.000426, l4: 0.000761, l5: 0.001447, l6: 0.002027
[epoch: 194/1000, batch:   804/ 1052, ite: 50960] train loss: 0.006088, tar: 0.000459 
l0: 0.000356, l1: 0.000357, l2: 0.000371, l3: 0.000429, l4: 0.000652, l5: 0.001167, l6: 0.001622
[epoch: 194/1000, batch:   884/ 1052, ite: 50980] train loss: 0.006086, tar: 0.000459 
l0: 0.000265, l1: 0.000283, l2: 0.000279, l3: 0.000301, l4: 0.000360, l5: 0.000702, l6: 0.001377
[epoch: 194/1000, batch:   964/ 1052, ite: 51000] train loss: 0.006087, tar: 0.000459 
l0: 0.000355, l1: 0.000368, l2: 0.000359, l3: 0.000424, l4: 0.000564, l5: 0.000913, l6: 0.002166
[epoch: 194/1000, batch:  1044/ 1052, ite: 51020] train loss: 0.006085, tar: 0.000459 
[Epoch 194/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000360, l1: 0.000357, l2: 0.000388, l3: 0.000442, l4: 0.000527, l5: 0.001014, l6: 0.001923
[epoch: 195/1000, batch:    72/ 1052, ite: 51040] train loss: 0.006084, tar: 0.000459 
l0: 0.000464, l1: 0.000456, l2: 0.000507, l3: 0.000552, l4: 0.000795, l5: 0.001199, l6: 0.002385
[epoch: 195/1000, batch:   152/ 1052, ite: 51060] train loss: 0.006085, tar: 0.000459 
l0: 0.000241, l1: 0.000248, l2: 0.000303, l3: 0.000328, l4: 0.000491, l5: 0.000894, l6: 0.001653
[epoch: 195/1000, batch:   232/ 1052, ite: 51080] train loss: 0.006083, tar: 0.000459 
l0: 0.000306, l1: 0.000332, l2: 0.000347, l3: 0.000265, l4: 0.000552, l5: 0.000999, l6: 0.000998
[epoch: 195/1000, batch:   312/ 1052, ite: 51100] train loss: 0.006081, tar: 0.000458 
l0: 0.000416, l1: 0.000434, l2: 0.000494, l3: 0.000499, l4: 0.000774, l5: 0.001761, l6: 0.002438
[epoch: 195/1000, batch:   392/ 1052, ite: 51120] train loss: 0.006079, tar: 0.000458 
l0: 0.000281, l1: 0.000263, l2: 0.000273, l3: 0.000315, l4: 0.000425, l5: 0.000882, l6: 0.001717
[epoch: 195/1000, batch:   472/ 1052, ite: 51140] train loss: 0.006078, tar: 0.000458 
l0: 0.000755, l1: 0.000750, l2: 0.000829, l3: 0.000911, l4: 0.001417, l5: 0.002599, l6: 0.002864
[epoch: 195/1000, batch:   552/ 1052, ite: 51160] train loss: 0.006077, tar: 0.000458 
l0: 0.000375, l1: 0.000391, l2: 0.000412, l3: 0.000439, l4: 0.000598, l5: 0.001300, l6: 0.002016
[epoch: 195/1000, batch:   632/ 1052, ite: 51180] train loss: 0.006080, tar: 0.000458 
l0: 0.000635, l1: 0.000667, l2: 0.000688, l3: 0.000707, l4: 0.000973, l5: 0.002167, l6: 0.002936
[epoch: 195/1000, batch:   712/ 1052, ite: 51200] train loss: 0.006079, tar: 0.000458 
l0: 0.000250, l1: 0.000251, l2: 0.000260, l3: 0.000299, l4: 0.000355, l5: 0.000700, l6: 0.001187
[epoch: 195/1000, batch:   792/ 1052, ite: 51220] train loss: 0.006079, tar: 0.000458 
l0: 0.000367, l1: 0.000380, l2: 0.000408, l3: 0.000396, l4: 0.000475, l5: 0.000898, l6: 0.001715
[epoch: 195/1000, batch:   872/ 1052, ite: 51240] train loss: 0.006079, tar: 0.000458 
l0: 0.000609, l1: 0.000631, l2: 0.000648, l3: 0.000690, l4: 0.001038, l5: 0.002591, l6: 0.003854
[epoch: 195/1000, batch:   952/ 1052, ite: 51260] train loss: 0.006079, tar: 0.000458 
l0: 0.000417, l1: 0.000428, l2: 0.000395, l3: 0.000444, l4: 0.000660, l5: 0.001186, l6: 0.001465
[epoch: 195/1000, batch:  1032/ 1052, ite: 51280] train loss: 0.006078, tar: 0.000458 
[Epoch 195/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000182, l1: 0.000189, l2: 0.000204, l3: 0.000224, l4: 0.000410, l5: 0.000634, l6: 0.001259
[epoch: 196/1000, batch:    60/ 1052, ite: 51300] train loss: 0.006076, tar: 0.000458 
l0: 0.000347, l1: 0.000341, l2: 0.000358, l3: 0.000366, l4: 0.000635, l5: 0.000713, l6: 0.001988
[epoch: 196/1000, batch:   140/ 1052, ite: 51320] train loss: 0.006076, tar: 0.000458 
l0: 0.000243, l1: 0.000250, l2: 0.000312, l3: 0.000334, l4: 0.000611, l5: 0.000925, l6: 0.001243
[epoch: 196/1000, batch:   220/ 1052, ite: 51340] train loss: 0.006074, tar: 0.000458 
l0: 0.000180, l1: 0.000191, l2: 0.000189, l3: 0.000231, l4: 0.000373, l5: 0.000601, l6: 0.000997
[epoch: 196/1000, batch:   300/ 1052, ite: 51360] train loss: 0.006074, tar: 0.000458 
l0: 0.000710, l1: 0.000714, l2: 0.000756, l3: 0.000785, l4: 0.000885, l5: 0.001701, l6: 0.003402
[epoch: 196/1000, batch:   380/ 1052, ite: 51380] train loss: 0.006073, tar: 0.000457 
l0: 0.000430, l1: 0.000477, l2: 0.000440, l3: 0.000380, l4: 0.000480, l5: 0.000945, l6: 0.001411
[epoch: 196/1000, batch:   460/ 1052, ite: 51400] train loss: 0.006073, tar: 0.000457 
l0: 0.000215, l1: 0.000222, l2: 0.000264, l3: 0.000248, l4: 0.000349, l5: 0.000720, l6: 0.001282
[epoch: 196/1000, batch:   540/ 1052, ite: 51420] train loss: 0.006071, tar: 0.000457 
l0: 0.000190, l1: 0.000194, l2: 0.000192, l3: 0.000244, l4: 0.000367, l5: 0.000583, l6: 0.001048
[epoch: 196/1000, batch:   620/ 1052, ite: 51440] train loss: 0.006070, tar: 0.000457 
l0: 0.000407, l1: 0.000388, l2: 0.000412, l3: 0.000512, l4: 0.000768, l5: 0.001100, l6: 0.001809
[epoch: 196/1000, batch:   700/ 1052, ite: 51460] train loss: 0.006071, tar: 0.000457 
l0: 0.000797, l1: 0.000779, l2: 0.000876, l3: 0.001071, l4: 0.001382, l5: 0.002450, l6: 0.004496
[epoch: 196/1000, batch:   780/ 1052, ite: 51480] train loss: 0.006072, tar: 0.000457 
l0: 0.000173, l1: 0.000156, l2: 0.000189, l3: 0.000259, l4: 0.000452, l5: 0.000525, l6: 0.000951
[epoch: 196/1000, batch:   860/ 1052, ite: 51500] train loss: 0.006072, tar: 0.000457 
l0: 0.000571, l1: 0.000559, l2: 0.000594, l3: 0.000692, l4: 0.001106, l5: 0.001719, l6: 0.002389
[epoch: 196/1000, batch:   940/ 1052, ite: 51520] train loss: 0.006073, tar: 0.000457 
l0: 0.000256, l1: 0.000255, l2: 0.000255, l3: 0.000314, l4: 0.000493, l5: 0.000861, l6: 0.002360
[epoch: 196/1000, batch:  1020/ 1052, ite: 51540] train loss: 0.006073, tar: 0.000457 
[Epoch 196/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000481, l1: 0.000482, l2: 0.000527, l3: 0.000582, l4: 0.000692, l5: 0.001399, l6: 0.002742
[epoch: 197/1000, batch:    48/ 1052, ite: 51560] train loss: 0.006073, tar: 0.000457 
l0: 0.000254, l1: 0.000258, l2: 0.000262, l3: 0.000291, l4: 0.000471, l5: 0.000973, l6: 0.001352
[epoch: 197/1000, batch:   128/ 1052, ite: 51580] train loss: 0.006072, tar: 0.000457 
l0: 0.000203, l1: 0.000198, l2: 0.000230, l3: 0.000266, l4: 0.000430, l5: 0.000855, l6: 0.001156
[epoch: 197/1000, batch:   208/ 1052, ite: 51600] train loss: 0.006070, tar: 0.000457 
l0: 0.000408, l1: 0.000422, l2: 0.000470, l3: 0.000489, l4: 0.000680, l5: 0.001637, l6: 0.002963
[epoch: 197/1000, batch:   288/ 1052, ite: 51620] train loss: 0.006070, tar: 0.000457 
l0: 0.000426, l1: 0.000435, l2: 0.000456, l3: 0.000496, l4: 0.000671, l5: 0.001256, l6: 0.002189
[epoch: 197/1000, batch:   368/ 1052, ite: 51640] train loss: 0.006070, tar: 0.000457 
l0: 0.000305, l1: 0.000323, l2: 0.000367, l3: 0.000384, l4: 0.000541, l5: 0.000722, l6: 0.001347
[epoch: 197/1000, batch:   448/ 1052, ite: 51660] train loss: 0.006070, tar: 0.000457 
l0: 0.000427, l1: 0.000454, l2: 0.000449, l3: 0.000505, l4: 0.000736, l5: 0.002082, l6: 0.002213
[epoch: 197/1000, batch:   528/ 1052, ite: 51680] train loss: 0.006067, tar: 0.000456 
l0: 0.000269, l1: 0.000269, l2: 0.000315, l3: 0.000338, l4: 0.000442, l5: 0.000966, l6: 0.001473
[epoch: 197/1000, batch:   608/ 1052, ite: 51700] train loss: 0.006068, tar: 0.000456 
l0: 0.000415, l1: 0.000442, l2: 0.000448, l3: 0.000447, l4: 0.000595, l5: 0.001305, l6: 0.002995
[epoch: 197/1000, batch:   688/ 1052, ite: 51720] train loss: 0.006067, tar: 0.000456 
l0: 0.000434, l1: 0.000433, l2: 0.000484, l3: 0.000510, l4: 0.000832, l5: 0.001453, l6: 0.002397
[epoch: 197/1000, batch:   768/ 1052, ite: 51740] train loss: 0.006066, tar: 0.000456 
l0: 0.000260, l1: 0.000258, l2: 0.000257, l3: 0.000338, l4: 0.000468, l5: 0.000782, l6: 0.000977
[epoch: 197/1000, batch:   848/ 1052, ite: 51760] train loss: 0.006063, tar: 0.000456 
l0: 0.000362, l1: 0.000337, l2: 0.000371, l3: 0.000509, l4: 0.000914, l5: 0.001264, l6: 0.001875
[epoch: 197/1000, batch:   928/ 1052, ite: 51780] train loss: 0.006062, tar: 0.000456 
l0: 0.000981, l1: 0.001011, l2: 0.001072, l3: 0.001006, l4: 0.001178, l5: 0.001830, l6: 0.002822
[epoch: 197/1000, batch:  1008/ 1052, ite: 51800] train loss: 0.006065, tar: 0.000456 
[Epoch 197/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000342, l1: 0.000335, l2: 0.000387, l3: 0.000431, l4: 0.000623, l5: 0.001283, l6: 0.002263
[epoch: 198/1000, batch:    36/ 1052, ite: 51820] train loss: 0.006067, tar: 0.000456 
l0: 0.000415, l1: 0.000434, l2: 0.000482, l3: 0.000559, l4: 0.000761, l5: 0.001183, l6: 0.002475
[epoch: 198/1000, batch:   116/ 1052, ite: 51840] train loss: 0.006067, tar: 0.000456 
l0: 0.000211, l1: 0.000226, l2: 0.000247, l3: 0.000296, l4: 0.000727, l5: 0.001157, l6: 0.001507
[epoch: 198/1000, batch:   196/ 1052, ite: 51860] train loss: 0.006066, tar: 0.000456 
l0: 0.000254, l1: 0.000267, l2: 0.000267, l3: 0.000295, l4: 0.000498, l5: 0.000960, l6: 0.001812
[epoch: 198/1000, batch:   276/ 1052, ite: 51880] train loss: 0.006066, tar: 0.000456 
l0: 0.000561, l1: 0.000606, l2: 0.000697, l3: 0.000580, l4: 0.000786, l5: 0.001575, l6: 0.003051
[epoch: 198/1000, batch:   356/ 1052, ite: 51900] train loss: 0.006065, tar: 0.000456 
l0: 0.000365, l1: 0.000375, l2: 0.000421, l3: 0.000416, l4: 0.000683, l5: 0.001281, l6: 0.002033
[epoch: 198/1000, batch:   436/ 1052, ite: 51920] train loss: 0.006065, tar: 0.000456 
l0: 0.000514, l1: 0.000505, l2: 0.000526, l3: 0.000576, l4: 0.000652, l5: 0.001425, l6: 0.001887
[epoch: 198/1000, batch:   516/ 1052, ite: 51940] train loss: 0.006064, tar: 0.000456 
l0: 0.000282, l1: 0.000274, l2: 0.000321, l3: 0.000349, l4: 0.000489, l5: 0.001051, l6: 0.001711
[epoch: 198/1000, batch:   596/ 1052, ite: 51960] train loss: 0.006064, tar: 0.000456 
l0: 0.000606, l1: 0.000616, l2: 0.000650, l3: 0.000664, l4: 0.000900, l5: 0.001648, l6: 0.002838
[epoch: 198/1000, batch:   676/ 1052, ite: 51980] train loss: 0.006064, tar: 0.000456 
l0: 0.000779, l1: 0.000705, l2: 0.000746, l3: 0.001027, l4: 0.001544, l5: 0.002875, l6: 0.004405
[epoch: 198/1000, batch:   756/ 1052, ite: 52000] train loss: 0.006063, tar: 0.000456 
l0: 0.000840, l1: 0.000880, l2: 0.000940, l3: 0.000879, l4: 0.001312, l5: 0.001849, l6: 0.003757
[epoch: 198/1000, batch:   836/ 1052, ite: 52020] train loss: 0.006063, tar: 0.000456 
l0: 0.000350, l1: 0.000352, l2: 0.000369, l3: 0.000381, l4: 0.000645, l5: 0.001255, l6: 0.001642
[epoch: 198/1000, batch:   916/ 1052, ite: 52040] train loss: 0.006061, tar: 0.000455 
l0: 0.000341, l1: 0.000334, l2: 0.000349, l3: 0.000445, l4: 0.000746, l5: 0.001007, l6: 0.001926
[epoch: 198/1000, batch:   996/ 1052, ite: 52060] train loss: 0.006062, tar: 0.000455 
[Epoch 198/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000706, l1: 0.000746, l2: 0.000715, l3: 0.000735, l4: 0.001339, l5: 0.002137, l6: 0.004347
[epoch: 199/1000, batch:    24/ 1052, ite: 52080] train loss: 0.006062, tar: 0.000455 
l0: 0.000763, l1: 0.000761, l2: 0.000777, l3: 0.000867, l4: 0.001332, l5: 0.001786, l6: 0.002754
[epoch: 199/1000, batch:   104/ 1052, ite: 52100] train loss: 0.006061, tar: 0.000455 
l0: 0.000713, l1: 0.000759, l2: 0.000701, l3: 0.000707, l4: 0.000684, l5: 0.001625, l6: 0.002504
[epoch: 199/1000, batch:   184/ 1052, ite: 52120] train loss: 0.006060, tar: 0.000455 
l0: 0.001380, l1: 0.001402, l2: 0.001528, l3: 0.001656, l4: 0.001661, l5: 0.003040, l6: 0.004859
[epoch: 199/1000, batch:   264/ 1052, ite: 52140] train loss: 0.006060, tar: 0.000455 
l0: 0.000693, l1: 0.000682, l2: 0.000821, l3: 0.000832, l4: 0.001057, l5: 0.001398, l6: 0.002677
[epoch: 199/1000, batch:   344/ 1052, ite: 52160] train loss: 0.006059, tar: 0.000455 
l0: 0.000623, l1: 0.000656, l2: 0.000739, l3: 0.000758, l4: 0.001128, l5: 0.002281, l6: 0.003746
[epoch: 199/1000, batch:   424/ 1052, ite: 52180] train loss: 0.006059, tar: 0.000455 
l0: 0.000473, l1: 0.000473, l2: 0.000478, l3: 0.000557, l4: 0.000860, l5: 0.001728, l6: 0.002291
[epoch: 199/1000, batch:   504/ 1052, ite: 52200] train loss: 0.006058, tar: 0.000455 
l0: 0.000238, l1: 0.000230, l2: 0.000262, l3: 0.000299, l4: 0.000517, l5: 0.000890, l6: 0.001158
[epoch: 199/1000, batch:   584/ 1052, ite: 52220] train loss: 0.006055, tar: 0.000455 
l0: 0.000412, l1: 0.000417, l2: 0.000411, l3: 0.000503, l4: 0.000741, l5: 0.001133, l6: 0.001969
[epoch: 199/1000, batch:   664/ 1052, ite: 52240] train loss: 0.006055, tar: 0.000454 
l0: 0.000635, l1: 0.000668, l2: 0.000684, l3: 0.000605, l4: 0.000752, l5: 0.001493, l6: 0.001618
[epoch: 199/1000, batch:   744/ 1052, ite: 52260] train loss: 0.006055, tar: 0.000454 
l0: 0.000306, l1: 0.000313, l2: 0.000318, l3: 0.000352, l4: 0.000478, l5: 0.000719, l6: 0.001273
[epoch: 199/1000, batch:   824/ 1052, ite: 52280] train loss: 0.006054, tar: 0.000454 
l0: 0.000907, l1: 0.000959, l2: 0.000965, l3: 0.001013, l4: 0.001195, l5: 0.001718, l6: 0.003351
[epoch: 199/1000, batch:   904/ 1052, ite: 52300] train loss: 0.006053, tar: 0.000454 
l0: 0.000891, l1: 0.000989, l2: 0.000997, l3: 0.000915, l4: 0.001074, l5: 0.002156, l6: 0.004513
[epoch: 199/1000, batch:   984/ 1052, ite: 52320] train loss: 0.006053, tar: 0.000454 
[Epoch 199/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000284, l1: 0.000296, l2: 0.000282, l3: 0.000332, l4: 0.000480, l5: 0.000960, l6: 0.001567
[epoch: 200/1000, batch:    12/ 1052, ite: 52340] train loss: 0.006054, tar: 0.000454 
l0: 0.000899, l1: 0.000909, l2: 0.000842, l3: 0.000941, l4: 0.001419, l5: 0.002583, l6: 0.004767
[epoch: 200/1000, batch:    92/ 1052, ite: 52360] train loss: 0.006052, tar: 0.000454 
l0: 0.000220, l1: 0.000237, l2: 0.000236, l3: 0.000293, l4: 0.000469, l5: 0.000917, l6: 0.001317
[epoch: 200/1000, batch:   172/ 1052, ite: 52380] train loss: 0.006053, tar: 0.000454 
l0: 0.000337, l1: 0.000326, l2: 0.000349, l3: 0.000449, l4: 0.000754, l5: 0.001210, l6: 0.002184
[epoch: 200/1000, batch:   252/ 1052, ite: 52400] train loss: 0.006053, tar: 0.000454 
l0: 0.000655, l1: 0.000692, l2: 0.000709, l3: 0.000706, l4: 0.001054, l5: 0.001733, l6: 0.004212
[epoch: 200/1000, batch:   332/ 1052, ite: 52420] train loss: 0.006052, tar: 0.000454 
l0: 0.000434, l1: 0.000435, l2: 0.000472, l3: 0.000521, l4: 0.000633, l5: 0.000920, l6: 0.001372
[epoch: 200/1000, batch:   412/ 1052, ite: 52440] train loss: 0.006053, tar: 0.000454 
l0: 0.000132, l1: 0.000131, l2: 0.000162, l3: 0.000188, l4: 0.000413, l5: 0.000551, l6: 0.001130
[epoch: 200/1000, batch:   492/ 1052, ite: 52460] train loss: 0.006051, tar: 0.000454 
l0: 0.000312, l1: 0.000307, l2: 0.000351, l3: 0.000446, l4: 0.000753, l5: 0.001077, l6: 0.002270
[epoch: 200/1000, batch:   572/ 1052, ite: 52480] train loss: 0.006050, tar: 0.000453 
l0: 0.000435, l1: 0.000422, l2: 0.000505, l3: 0.000584, l4: 0.000719, l5: 0.001965, l6: 0.002957
[epoch: 200/1000, batch:   652/ 1052, ite: 52500] train loss: 0.006049, tar: 0.000453 
l0: 0.000431, l1: 0.000436, l2: 0.000500, l3: 0.000514, l4: 0.000783, l5: 0.001348, l6: 0.002442
[epoch: 200/1000, batch:   732/ 1052, ite: 52520] train loss: 0.006048, tar: 0.000453 
l0: 0.000343, l1: 0.000349, l2: 0.000377, l3: 0.000440, l4: 0.000516, l5: 0.001296, l6: 0.002800
[epoch: 200/1000, batch:   812/ 1052, ite: 52540] train loss: 0.006047, tar: 0.000453 
l0: 0.000264, l1: 0.000257, l2: 0.000306, l3: 0.000364, l4: 0.000693, l5: 0.001150, l6: 0.001966
[epoch: 200/1000, batch:   892/ 1052, ite: 52560] train loss: 0.006048, tar: 0.000453 
l0: 0.000657, l1: 0.000691, l2: 0.000677, l3: 0.000679, l4: 0.000930, l5: 0.001421, l6: 0.002901
[epoch: 200/1000, batch:   972/ 1052, ite: 52580] train loss: 0.006047, tar: 0.000453 
l0: 0.000274, l1: 0.000281, l2: 0.000315, l3: 0.000324, l4: 0.000438, l5: 0.000838, l6: 0.002181
[epoch: 200/1000, batch:  1052/ 1052, ite: 52600] train loss: 0.006046, tar: 0.000453 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_52600.pkl
[Epoch 200/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000340, l1: 0.000350, l2: 0.000403, l3: 0.000467, l4: 0.000725, l5: 0.001717, l6: 0.002637
[epoch: 201/1000, batch:    80/ 1052, ite: 52620] train loss: 0.005753, tar: 0.000399 
l0: 0.000251, l1: 0.000265, l2: 0.000232, l3: 0.000253, l4: 0.000323, l5: 0.000810, l6: 0.000801
[epoch: 201/1000, batch:   160/ 1052, ite: 52640] train loss: 0.005452, tar: 0.000387 
l0: 0.000333, l1: 0.000329, l2: 0.000326, l3: 0.000430, l4: 0.000733, l5: 0.001591, l6: 0.001616
[epoch: 201/1000, batch:   240/ 1052, ite: 52660] train loss: 0.005584, tar: 0.000397 
l0: 0.000459, l1: 0.000459, l2: 0.000459, l3: 0.000532, l4: 0.000701, l5: 0.001301, l6: 0.002618
[epoch: 201/1000, batch:   320/ 1052, ite: 52680] train loss: 0.005507, tar: 0.000387 
l0: 0.000395, l1: 0.000414, l2: 0.000382, l3: 0.000386, l4: 0.000522, l5: 0.001047, l6: 0.001640
[epoch: 201/1000, batch:   400/ 1052, ite: 52700] train loss: 0.005570, tar: 0.000392 
l0: 0.000391, l1: 0.000373, l2: 0.000419, l3: 0.000562, l4: 0.000919, l5: 0.001455, l6: 0.002308
[epoch: 201/1000, batch:   480/ 1052, ite: 52720] train loss: 0.005632, tar: 0.000395 
l0: 0.000450, l1: 0.000439, l2: 0.000507, l3: 0.000625, l4: 0.000946, l5: 0.001606, l6: 0.002216
[epoch: 201/1000, batch:   560/ 1052, ite: 52740] train loss: 0.005661, tar: 0.000395 
l0: 0.000476, l1: 0.000499, l2: 0.000517, l3: 0.000508, l4: 0.000788, l5: 0.001686, l6: 0.001723
[epoch: 201/1000, batch:   640/ 1052, ite: 52760] train loss: 0.005655, tar: 0.000396 
l0: 0.000279, l1: 0.000292, l2: 0.000277, l3: 0.000349, l4: 0.000597, l5: 0.000890, l6: 0.001207
[epoch: 201/1000, batch:   720/ 1052, ite: 52780] train loss: 0.005674, tar: 0.000400 
l0: 0.000417, l1: 0.000454, l2: 0.000439, l3: 0.000510, l4: 0.000855, l5: 0.001795, l6: 0.001552
[epoch: 201/1000, batch:   800/ 1052, ite: 52800] train loss: 0.005763, tar: 0.000409 
l0: 0.000370, l1: 0.000391, l2: 0.000408, l3: 0.000404, l4: 0.000365, l5: 0.001458, l6: 0.001719
[epoch: 201/1000, batch:   880/ 1052, ite: 52820] train loss: 0.005878, tar: 0.000422 
l0: 0.000278, l1: 0.000274, l2: 0.000304, l3: 0.000349, l4: 0.000574, l5: 0.000861, l6: 0.001765
[epoch: 201/1000, batch:   960/ 1052, ite: 52840] train loss: 0.005869, tar: 0.000424 
l0: 0.000363, l1: 0.000370, l2: 0.000362, l3: 0.000378, l4: 0.000735, l5: 0.001157, l6: 0.001793
[epoch: 201/1000, batch:  1040/ 1052, ite: 52860] train loss: 0.005882, tar: 0.000426 
[Epoch 201/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000256, l1: 0.000265, l2: 0.000270, l3: 0.000320, l4: 0.000506, l5: 0.001213, l6: 0.002186
[epoch: 202/1000, batch:    68/ 1052, ite: 52880] train loss: 0.005892, tar: 0.000427 
l0: 0.000279, l1: 0.000287, l2: 0.000290, l3: 0.000323, l4: 0.000368, l5: 0.000685, l6: 0.000827
[epoch: 202/1000, batch:   148/ 1052, ite: 52900] train loss: 0.005832, tar: 0.000423 
l0: 0.000979, l1: 0.001096, l2: 0.000992, l3: 0.000855, l4: 0.001000, l5: 0.001735, l6: 0.002344
[epoch: 202/1000, batch:   228/ 1052, ite: 52920] train loss: 0.005885, tar: 0.000428 
l0: 0.000377, l1: 0.000399, l2: 0.000413, l3: 0.000387, l4: 0.000601, l5: 0.001042, l6: 0.001027
[epoch: 202/1000, batch:   308/ 1052, ite: 52940] train loss: 0.005841, tar: 0.000424 
l0: 0.000410, l1: 0.000436, l2: 0.000407, l3: 0.000494, l4: 0.000669, l5: 0.000913, l6: 0.001899
[epoch: 202/1000, batch:   388/ 1052, ite: 52960] train loss: 0.005806, tar: 0.000420 
l0: 0.000433, l1: 0.000390, l2: 0.000445, l3: 0.000646, l4: 0.000771, l5: 0.000979, l6: 0.002024
[epoch: 202/1000, batch:   468/ 1052, ite: 52980] train loss: 0.005801, tar: 0.000421 
l0: 0.000364, l1: 0.000375, l2: 0.000363, l3: 0.000409, l4: 0.000500, l5: 0.001089, l6: 0.001718
[epoch: 202/1000, batch:   548/ 1052, ite: 53000] train loss: 0.005804, tar: 0.000419 
l0: 0.000773, l1: 0.000801, l2: 0.000907, l3: 0.000755, l4: 0.000983, l5: 0.001341, l6: 0.002335
[epoch: 202/1000, batch:   628/ 1052, ite: 53020] train loss: 0.005791, tar: 0.000418 
l0: 0.000658, l1: 0.000681, l2: 0.000728, l3: 0.000783, l4: 0.001108, l5: 0.002526, l6: 0.004428
[epoch: 202/1000, batch:   708/ 1052, ite: 53040] train loss: 0.005810, tar: 0.000419 
l0: 0.000407, l1: 0.000421, l2: 0.000446, l3: 0.000487, l4: 0.000665, l5: 0.001336, l6: 0.002185
[epoch: 202/1000, batch:   788/ 1052, ite: 53060] train loss: 0.005822, tar: 0.000419 
l0: 0.000388, l1: 0.000375, l2: 0.000461, l3: 0.000496, l4: 0.000862, l5: 0.001480, l6: 0.002560
[epoch: 202/1000, batch:   868/ 1052, ite: 53080] train loss: 0.005815, tar: 0.000417 
l0: 0.001162, l1: 0.001166, l2: 0.001296, l3: 0.001273, l4: 0.001546, l5: 0.002877, l6: 0.003438
[epoch: 202/1000, batch:   948/ 1052, ite: 53100] train loss: 0.005813, tar: 0.000419 
l0: 0.000453, l1: 0.000438, l2: 0.000455, l3: 0.000584, l4: 0.000741, l5: 0.001403, l6: 0.002829
[epoch: 202/1000, batch:  1028/ 1052, ite: 53120] train loss: 0.005824, tar: 0.000419 
[Epoch 202/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000472, l1: 0.000475, l2: 0.000504, l3: 0.000546, l4: 0.000786, l5: 0.001290, l6: 0.002867
[epoch: 203/1000, batch:    56/ 1052, ite: 53140] train loss: 0.005819, tar: 0.000418 
l0: 0.000374, l1: 0.000365, l2: 0.000382, l3: 0.000510, l4: 0.000691, l5: 0.001208, l6: 0.001607
[epoch: 203/1000, batch:   136/ 1052, ite: 53160] train loss: 0.005818, tar: 0.000418 
l0: 0.000184, l1: 0.000197, l2: 0.000220, l3: 0.000197, l4: 0.000285, l5: 0.000575, l6: 0.001078
[epoch: 203/1000, batch:   216/ 1052, ite: 53180] train loss: 0.005802, tar: 0.000418 
l0: 0.000172, l1: 0.000178, l2: 0.000185, l3: 0.000236, l4: 0.000310, l5: 0.001037, l6: 0.001948
[epoch: 203/1000, batch:   296/ 1052, ite: 53200] train loss: 0.005771, tar: 0.000415 
l0: 0.000327, l1: 0.000313, l2: 0.000332, l3: 0.000439, l4: 0.000617, l5: 0.001202, l6: 0.001635
[epoch: 203/1000, batch:   376/ 1052, ite: 53220] train loss: 0.005792, tar: 0.000416 
l0: 0.000420, l1: 0.000418, l2: 0.000450, l3: 0.000464, l4: 0.000532, l5: 0.000809, l6: 0.001207
[epoch: 203/1000, batch:   456/ 1052, ite: 53240] train loss: 0.005808, tar: 0.000417 
l0: 0.001200, l1: 0.001277, l2: 0.001341, l3: 0.001315, l4: 0.001224, l5: 0.001847, l6: 0.002355
[epoch: 203/1000, batch:   536/ 1052, ite: 53260] train loss: 0.005837, tar: 0.000420 
l0: 0.000515, l1: 0.000515, l2: 0.000566, l3: 0.000663, l4: 0.000849, l5: 0.001518, l6: 0.002618
[epoch: 203/1000, batch:   616/ 1052, ite: 53280] train loss: 0.005824, tar: 0.000419 
l0: 0.000740, l1: 0.000781, l2: 0.000751, l3: 0.000769, l4: 0.001232, l5: 0.002048, l6: 0.002594
[epoch: 203/1000, batch:   696/ 1052, ite: 53300] train loss: 0.005804, tar: 0.000418 
l0: 0.000447, l1: 0.000447, l2: 0.000479, l3: 0.000523, l4: 0.000880, l5: 0.001929, l6: 0.002321
[epoch: 203/1000, batch:   776/ 1052, ite: 53320] train loss: 0.005806, tar: 0.000418 
l0: 0.000535, l1: 0.000529, l2: 0.000580, l3: 0.000608, l4: 0.000725, l5: 0.001636, l6: 0.002623
[epoch: 203/1000, batch:   856/ 1052, ite: 53340] train loss: 0.005817, tar: 0.000418 
l0: 0.000719, l1: 0.000755, l2: 0.000793, l3: 0.000864, l4: 0.001353, l5: 0.002428, l6: 0.003983
[epoch: 203/1000, batch:   936/ 1052, ite: 53360] train loss: 0.005797, tar: 0.000417 
l0: 0.000673, l1: 0.000686, l2: 0.000678, l3: 0.000816, l4: 0.001423, l5: 0.001778, l6: 0.003753
[epoch: 203/1000, batch:  1016/ 1052, ite: 53380] train loss: 0.005798, tar: 0.000416 
[Epoch 203/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000378, l1: 0.000347, l2: 0.000450, l3: 0.000615, l4: 0.000835, l5: 0.001290, l6: 0.002127
[epoch: 204/1000, batch:    44/ 1052, ite: 53400] train loss: 0.005796, tar: 0.000415 
l0: 0.000522, l1: 0.000535, l2: 0.000540, l3: 0.000571, l4: 0.000648, l5: 0.000859, l6: 0.002430
[epoch: 204/1000, batch:   124/ 1052, ite: 53420] train loss: 0.005793, tar: 0.000415 
l0: 0.000456, l1: 0.000515, l2: 0.000507, l3: 0.000480, l4: 0.000710, l5: 0.001473, l6: 0.003303
[epoch: 204/1000, batch:   204/ 1052, ite: 53440] train loss: 0.005796, tar: 0.000415 
l0: 0.000479, l1: 0.000485, l2: 0.000502, l3: 0.000557, l4: 0.000723, l5: 0.001274, l6: 0.002781
[epoch: 204/1000, batch:   284/ 1052, ite: 53460] train loss: 0.005802, tar: 0.000415 
l0: 0.000501, l1: 0.000519, l2: 0.000524, l3: 0.000569, l4: 0.000776, l5: 0.001652, l6: 0.002373
[epoch: 204/1000, batch:   364/ 1052, ite: 53480] train loss: 0.005807, tar: 0.000416 
l0: 0.000692, l1: 0.000767, l2: 0.000751, l3: 0.000767, l4: 0.000985, l5: 0.002170, l6: 0.003420
[epoch: 204/1000, batch:   444/ 1052, ite: 53500] train loss: 0.005805, tar: 0.000415 
l0: 0.000712, l1: 0.000694, l2: 0.000731, l3: 0.000746, l4: 0.001033, l5: 0.001724, l6: 0.002468
[epoch: 204/1000, batch:   524/ 1052, ite: 53520] train loss: 0.005803, tar: 0.000415 
l0: 0.000352, l1: 0.000313, l2: 0.000369, l3: 0.000493, l4: 0.000643, l5: 0.001027, l6: 0.001556
[epoch: 204/1000, batch:   604/ 1052, ite: 53540] train loss: 0.005794, tar: 0.000415 
l0: 0.000342, l1: 0.000331, l2: 0.000396, l3: 0.000469, l4: 0.000628, l5: 0.001338, l6: 0.002299
[epoch: 204/1000, batch:   684/ 1052, ite: 53560] train loss: 0.005796, tar: 0.000415 
l0: 0.000267, l1: 0.000279, l2: 0.000260, l3: 0.000328, l4: 0.000441, l5: 0.000742, l6: 0.001539
[epoch: 204/1000, batch:   764/ 1052, ite: 53580] train loss: 0.005797, tar: 0.000415 
l0: 0.000520, l1: 0.000507, l2: 0.000522, l3: 0.000665, l4: 0.000831, l5: 0.001128, l6: 0.002517
[epoch: 204/1000, batch:   844/ 1052, ite: 53600] train loss: 0.005814, tar: 0.000416 
l0: 0.000279, l1: 0.000274, l2: 0.000287, l3: 0.000388, l4: 0.000618, l5: 0.001025, l6: 0.001484
[epoch: 204/1000, batch:   924/ 1052, ite: 53620] train loss: 0.005818, tar: 0.000416 
l0: 0.000228, l1: 0.000233, l2: 0.000246, l3: 0.000280, l4: 0.000392, l5: 0.000972, l6: 0.001449
[epoch: 204/1000, batch:  1004/ 1052, ite: 53640] train loss: 0.005807, tar: 0.000415 
[Epoch 204/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000387, l1: 0.000412, l2: 0.000410, l3: 0.000399, l4: 0.000629, l5: 0.001092, l6: 0.002240
[epoch: 205/1000, batch:    32/ 1052, ite: 53660] train loss: 0.005809, tar: 0.000415 
l0: 0.000452, l1: 0.000459, l2: 0.000476, l3: 0.000522, l4: 0.000593, l5: 0.001248, l6: 0.002180
[epoch: 205/1000, batch:   112/ 1052, ite: 53680] train loss: 0.005813, tar: 0.000415 
l0: 0.000137, l1: 0.000147, l2: 0.000136, l3: 0.000155, l4: 0.000334, l5: 0.000565, l6: 0.001090
[epoch: 205/1000, batch:   192/ 1052, ite: 53700] train loss: 0.005817, tar: 0.000416 
l0: 0.000259, l1: 0.000278, l2: 0.000281, l3: 0.000267, l4: 0.000401, l5: 0.000920, l6: 0.001679
[epoch: 205/1000, batch:   272/ 1052, ite: 53720] train loss: 0.005815, tar: 0.000415 
l0: 0.000320, l1: 0.000335, l2: 0.000319, l3: 0.000338, l4: 0.000545, l5: 0.001537, l6: 0.002048
[epoch: 205/1000, batch:   352/ 1052, ite: 53740] train loss: 0.005817, tar: 0.000415 
l0: 0.000554, l1: 0.000541, l2: 0.000612, l3: 0.000675, l4: 0.000930, l5: 0.001449, l6: 0.001876
[epoch: 205/1000, batch:   432/ 1052, ite: 53760] train loss: 0.005818, tar: 0.000415 
l0: 0.000330, l1: 0.000333, l2: 0.000331, l3: 0.000381, l4: 0.000524, l5: 0.000959, l6: 0.001605
[epoch: 205/1000, batch:   512/ 1052, ite: 53780] train loss: 0.005824, tar: 0.000418 
l0: 0.000370, l1: 0.000407, l2: 0.000372, l3: 0.000406, l4: 0.000572, l5: 0.000804, l6: 0.001617
[epoch: 205/1000, batch:   592/ 1052, ite: 53800] train loss: 0.005814, tar: 0.000417 
l0: 0.000177, l1: 0.000177, l2: 0.000193, l3: 0.000212, l4: 0.000358, l5: 0.000643, l6: 0.001152
[epoch: 205/1000, batch:   672/ 1052, ite: 53820] train loss: 0.005824, tar: 0.000418 
l0: 0.000628, l1: 0.000782, l2: 0.000848, l3: 0.000638, l4: 0.000528, l5: 0.000853, l6: 0.001611
[epoch: 205/1000, batch:   752/ 1052, ite: 53840] train loss: 0.005847, tar: 0.000421 
l0: 0.000595, l1: 0.000634, l2: 0.000676, l3: 0.000660, l4: 0.000946, l5: 0.001042, l6: 0.002328
[epoch: 205/1000, batch:   832/ 1052, ite: 53860] train loss: 0.005863, tar: 0.000423 
l0: 0.000527, l1: 0.000499, l2: 0.000516, l3: 0.000667, l4: 0.000958, l5: 0.001555, l6: 0.003370
[epoch: 205/1000, batch:   912/ 1052, ite: 53880] train loss: 0.005856, tar: 0.000423 
l0: 0.000500, l1: 0.000481, l2: 0.000453, l3: 0.000506, l4: 0.000600, l5: 0.001429, l6: 0.002652
[epoch: 205/1000, batch:   992/ 1052, ite: 53900] train loss: 0.005861, tar: 0.000423 
[Epoch 205/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000439, l1: 0.000440, l2: 0.000446, l3: 0.000509, l4: 0.000715, l5: 0.001256, l6: 0.002199
[epoch: 206/1000, batch:    20/ 1052, ite: 53920] train loss: 0.005864, tar: 0.000423 
l0: 0.000197, l1: 0.000186, l2: 0.000218, l3: 0.000290, l4: 0.000341, l5: 0.000593, l6: 0.001273
[epoch: 206/1000, batch:   100/ 1052, ite: 53940] train loss: 0.005881, tar: 0.000424 
l0: 0.000296, l1: 0.000327, l2: 0.000342, l3: 0.000288, l4: 0.000512, l5: 0.000875, l6: 0.001306
[epoch: 206/1000, batch:   180/ 1052, ite: 53960] train loss: 0.005878, tar: 0.000423 
l0: 0.000221, l1: 0.000216, l2: 0.000234, l3: 0.000292, l4: 0.000629, l5: 0.001251, l6: 0.001654
[epoch: 206/1000, batch:   260/ 1052, ite: 53980] train loss: 0.005881, tar: 0.000424 
l0: 0.000294, l1: 0.000295, l2: 0.000313, l3: 0.000358, l4: 0.000423, l5: 0.001211, l6: 0.002329
[epoch: 206/1000, batch:   340/ 1052, ite: 54000] train loss: 0.005886, tar: 0.000424 
l0: 0.000398, l1: 0.000407, l2: 0.000416, l3: 0.000476, l4: 0.000640, l5: 0.001114, l6: 0.002113
[epoch: 206/1000, batch:   420/ 1052, ite: 54020] train loss: 0.005885, tar: 0.000424 
l0: 0.000839, l1: 0.000893, l2: 0.000896, l3: 0.000895, l4: 0.001046, l5: 0.002279, l6: 0.004938
[epoch: 206/1000, batch:   500/ 1052, ite: 54040] train loss: 0.005880, tar: 0.000424 
l0: 0.000574, l1: 0.000587, l2: 0.000645, l3: 0.000706, l4: 0.001069, l5: 0.001542, l6: 0.002838
[epoch: 206/1000, batch:   580/ 1052, ite: 54060] train loss: 0.005886, tar: 0.000425 
l0: 0.000306, l1: 0.000305, l2: 0.000324, l3: 0.000381, l4: 0.000484, l5: 0.000920, l6: 0.001460
[epoch: 206/1000, batch:   660/ 1052, ite: 54080] train loss: 0.005875, tar: 0.000423 
l0: 0.000422, l1: 0.000422, l2: 0.000494, l3: 0.000567, l4: 0.000696, l5: 0.001164, l6: 0.001997
[epoch: 206/1000, batch:   740/ 1052, ite: 54100] train loss: 0.005880, tar: 0.000424 
l0: 0.000379, l1: 0.000414, l2: 0.000409, l3: 0.000428, l4: 0.000706, l5: 0.001418, l6: 0.002545
[epoch: 206/1000, batch:   820/ 1052, ite: 54120] train loss: 0.005882, tar: 0.000424 
l0: 0.000413, l1: 0.000457, l2: 0.000589, l3: 0.000434, l4: 0.000791, l5: 0.000890, l6: 0.001197
[epoch: 206/1000, batch:   900/ 1052, ite: 54140] train loss: 0.005890, tar: 0.000425 
l0: 0.000249, l1: 0.000259, l2: 0.000253, l3: 0.000260, l4: 0.000523, l5: 0.000772, l6: 0.001757
[epoch: 206/1000, batch:   980/ 1052, ite: 54160] train loss: 0.005884, tar: 0.000425 
[Epoch 206/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000537, l1: 0.000533, l2: 0.000516, l3: 0.000629, l4: 0.000772, l5: 0.001415, l6: 0.002300
[epoch: 207/1000, batch:     8/ 1052, ite: 54180] train loss: 0.005873, tar: 0.000424 
l0: 0.000433, l1: 0.000416, l2: 0.000464, l3: 0.000595, l4: 0.001151, l5: 0.002213, l6: 0.003323
[epoch: 207/1000, batch:    88/ 1052, ite: 54200] train loss: 0.005896, tar: 0.000425 
l0: 0.000287, l1: 0.000287, l2: 0.000325, l3: 0.000406, l4: 0.000578, l5: 0.001223, l6: 0.002328
[epoch: 207/1000, batch:   168/ 1052, ite: 54220] train loss: 0.005896, tar: 0.000425 
l0: 0.000261, l1: 0.000256, l2: 0.000298, l3: 0.000359, l4: 0.000523, l5: 0.001302, l6: 0.001823
[epoch: 207/1000, batch:   248/ 1052, ite: 54240] train loss: 0.005895, tar: 0.000425 
l0: 0.000593, l1: 0.000633, l2: 0.000605, l3: 0.000584, l4: 0.000662, l5: 0.001117, l6: 0.002085
[epoch: 207/1000, batch:   328/ 1052, ite: 54260] train loss: 0.005879, tar: 0.000424 
l0: 0.000287, l1: 0.000288, l2: 0.000281, l3: 0.000327, l4: 0.000466, l5: 0.001139, l6: 0.001699
[epoch: 207/1000, batch:   408/ 1052, ite: 54280] train loss: 0.005877, tar: 0.000423 
l0: 0.000798, l1: 0.000939, l2: 0.000919, l3: 0.000791, l4: 0.001003, l5: 0.002108, l6: 0.004605
[epoch: 207/1000, batch:   488/ 1052, ite: 54300] train loss: 0.005878, tar: 0.000423 
l0: 0.000465, l1: 0.000476, l2: 0.000531, l3: 0.000555, l4: 0.000864, l5: 0.001912, l6: 0.003430
[epoch: 207/1000, batch:   568/ 1052, ite: 54320] train loss: 0.005875, tar: 0.000423 
l0: 0.000202, l1: 0.000202, l2: 0.000226, l3: 0.000225, l4: 0.000373, l5: 0.000643, l6: 0.001187
[epoch: 207/1000, batch:   648/ 1052, ite: 54340] train loss: 0.005863, tar: 0.000422 
l0: 0.000533, l1: 0.000559, l2: 0.000593, l3: 0.000654, l4: 0.000940, l5: 0.001376, l6: 0.002694
[epoch: 207/1000, batch:   728/ 1052, ite: 54360] train loss: 0.005860, tar: 0.000422 
l0: 0.000397, l1: 0.000424, l2: 0.000431, l3: 0.000447, l4: 0.000687, l5: 0.001038, l6: 0.002160
[epoch: 207/1000, batch:   808/ 1052, ite: 54380] train loss: 0.005855, tar: 0.000422 
l0: 0.000364, l1: 0.000384, l2: 0.000369, l3: 0.000381, l4: 0.000531, l5: 0.001069, l6: 0.001906
[epoch: 207/1000, batch:   888/ 1052, ite: 54400] train loss: 0.005853, tar: 0.000422 
l0: 0.000599, l1: 0.000574, l2: 0.000622, l3: 0.000781, l4: 0.001101, l5: 0.001649, l6: 0.002609
[epoch: 207/1000, batch:   968/ 1052, ite: 54420] train loss: 0.005850, tar: 0.000421 
l0: 0.000423, l1: 0.000445, l2: 0.000494, l3: 0.000444, l4: 0.000608, l5: 0.001470, l6: 0.001801
[epoch: 207/1000, batch:  1048/ 1052, ite: 54440] train loss: 0.005865, tar: 0.000422 
[Epoch 207/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000301, l1: 0.000303, l2: 0.000318, l3: 0.000347, l4: 0.000441, l5: 0.000842, l6: 0.001172
[epoch: 208/1000, batch:    76/ 1052, ite: 54460] train loss: 0.005870, tar: 0.000422 
l0: 0.000628, l1: 0.000702, l2: 0.000653, l3: 0.000678, l4: 0.000818, l5: 0.001040, l6: 0.002133
[epoch: 208/1000, batch:   156/ 1052, ite: 54480] train loss: 0.005877, tar: 0.000423 
l0: 0.000371, l1: 0.000394, l2: 0.000381, l3: 0.000389, l4: 0.000713, l5: 0.000959, l6: 0.001910
[epoch: 208/1000, batch:   236/ 1052, ite: 54500] train loss: 0.005888, tar: 0.000423 
l0: 0.001092, l1: 0.001233, l2: 0.001044, l3: 0.000710, l4: 0.001050, l5: 0.001569, l6: 0.002725
[epoch: 208/1000, batch:   316/ 1052, ite: 54520] train loss: 0.005904, tar: 0.000424 
l0: 0.000791, l1: 0.000733, l2: 0.000742, l3: 0.001164, l4: 0.001451, l5: 0.001912, l6: 0.003326
[epoch: 208/1000, batch:   396/ 1052, ite: 54540] train loss: 0.005910, tar: 0.000425 
l0: 0.000328, l1: 0.000351, l2: 0.000315, l3: 0.000411, l4: 0.000747, l5: 0.000988, l6: 0.001798
[epoch: 208/1000, batch:   476/ 1052, ite: 54560] train loss: 0.005906, tar: 0.000425 
l0: 0.000586, l1: 0.000561, l2: 0.000660, l3: 0.000726, l4: 0.000840, l5: 0.001159, l6: 0.001723
[epoch: 208/1000, batch:   556/ 1052, ite: 54580] train loss: 0.005905, tar: 0.000425 
l0: 0.000600, l1: 0.000657, l2: 0.000697, l3: 0.000661, l4: 0.001039, l5: 0.001837, l6: 0.003585
[epoch: 208/1000, batch:   636/ 1052, ite: 54600] train loss: 0.005988, tar: 0.000435 
l0: 0.001369, l1: 0.001463, l2: 0.001501, l3: 0.001776, l4: 0.001552, l5: 0.003698, l6: 0.005740
[epoch: 208/1000, batch:   716/ 1052, ite: 54620] train loss: 0.006024, tar: 0.000440 
l0: 0.000595, l1: 0.000595, l2: 0.000604, l3: 0.000688, l4: 0.000808, l5: 0.001371, l6: 0.002372
[epoch: 208/1000, batch:   796/ 1052, ite: 54640] train loss: 0.006042, tar: 0.000442 
l0: 0.000398, l1: 0.000406, l2: 0.000403, l3: 0.000432, l4: 0.000588, l5: 0.001112, l6: 0.002025
[epoch: 208/1000, batch:   876/ 1052, ite: 54660] train loss: 0.006049, tar: 0.000443 
l0: 0.000652, l1: 0.000638, l2: 0.000680, l3: 0.000882, l4: 0.001065, l5: 0.001417, l6: 0.003213
[epoch: 208/1000, batch:   956/ 1052, ite: 54680] train loss: 0.006049, tar: 0.000443 
l0: 0.000548, l1: 0.000760, l2: 0.000497, l3: 0.000681, l4: 0.000752, l5: 0.001533, l6: 0.002066
[epoch: 208/1000, batch:  1036/ 1052, ite: 54700] train loss: 0.006143, tar: 0.000456 
[Epoch 208/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000456, l1: 0.000451, l2: 0.000504, l3: 0.000551, l4: 0.000898, l5: 0.001210, l6: 0.002365
[epoch: 209/1000, batch:    64/ 1052, ite: 54720] train loss: 0.006163, tar: 0.000457 
l0: 0.000244, l1: 0.000245, l2: 0.000248, l3: 0.000290, l4: 0.000414, l5: 0.000813, l6: 0.001280
[epoch: 209/1000, batch:   144/ 1052, ite: 54740] train loss: 0.006175, tar: 0.000459 
l0: 0.000157, l1: 0.000163, l2: 0.000160, l3: 0.000175, l4: 0.000284, l5: 0.000648, l6: 0.000831
[epoch: 209/1000, batch:   224/ 1052, ite: 54760] train loss: 0.006184, tar: 0.000460 
l0: 0.000605, l1: 0.000605, l2: 0.000603, l3: 0.000675, l4: 0.001234, l5: 0.001863, l6: 0.002492
[epoch: 209/1000, batch:   304/ 1052, ite: 54780] train loss: 0.006188, tar: 0.000461 
l0: 0.000439, l1: 0.000461, l2: 0.000513, l3: 0.000524, l4: 0.000703, l5: 0.001348, l6: 0.001052
[epoch: 209/1000, batch:   384/ 1052, ite: 54800] train loss: 0.006196, tar: 0.000462 
l0: 0.000242, l1: 0.000219, l2: 0.000245, l3: 0.000367, l4: 0.000635, l5: 0.000907, l6: 0.001395
[epoch: 209/1000, batch:   464/ 1052, ite: 54820] train loss: 0.006199, tar: 0.000462 
l0: 0.000228, l1: 0.000247, l2: 0.000327, l3: 0.000261, l4: 0.000403, l5: 0.000775, l6: 0.001166
[epoch: 209/1000, batch:   544/ 1052, ite: 54840] train loss: 0.006208, tar: 0.000463 
l0: 0.000747, l1: 0.000833, l2: 0.000892, l3: 0.000837, l4: 0.001156, l5: 0.001489, l6: 0.002562
[epoch: 209/1000, batch:   624/ 1052, ite: 54860] train loss: 0.006212, tar: 0.000464 
l0: 0.000535, l1: 0.000557, l2: 0.000553, l3: 0.000649, l4: 0.000713, l5: 0.001264, l6: 0.001538
[epoch: 209/1000, batch:   704/ 1052, ite: 54880] train loss: 0.006221, tar: 0.000465 
l0: 0.000368, l1: 0.000387, l2: 0.000344, l3: 0.000375, l4: 0.000516, l5: 0.001110, l6: 0.001933
[epoch: 209/1000, batch:   784/ 1052, ite: 54900] train loss: 0.006221, tar: 0.000465 
l0: 0.000551, l1: 0.000572, l2: 0.000557, l3: 0.000595, l4: 0.001065, l5: 0.002266, l6: 0.002412
[epoch: 209/1000, batch:   864/ 1052, ite: 54920] train loss: 0.006212, tar: 0.000465 
l0: 0.000307, l1: 0.000294, l2: 0.000326, l3: 0.000400, l4: 0.000619, l5: 0.001158, l6: 0.001411
[epoch: 209/1000, batch:   944/ 1052, ite: 54940] train loss: 0.006213, tar: 0.000465 
l0: 0.001098, l1: 0.001221, l2: 0.001199, l3: 0.001088, l4: 0.001368, l5: 0.002385, l6: 0.003887
[epoch: 209/1000, batch:  1024/ 1052, ite: 54960] train loss: 0.006216, tar: 0.000465 
[Epoch 209/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.001102, l1: 0.001082, l2: 0.001188, l3: 0.001252, l4: 0.001534, l5: 0.002247, l6: 0.004087
[epoch: 210/1000, batch:    52/ 1052, ite: 54980] train loss: 0.006217, tar: 0.000465 
l0: 0.000347, l1: 0.000342, l2: 0.000343, l3: 0.000384, l4: 0.000548, l5: 0.000872, l6: 0.001762
[epoch: 210/1000, batch:   132/ 1052, ite: 55000] train loss: 0.006224, tar: 0.000467 
l0: 0.000394, l1: 0.000378, l2: 0.000381, l3: 0.000533, l4: 0.000717, l5: 0.001161, l6: 0.001516
[epoch: 210/1000, batch:   212/ 1052, ite: 55020] train loss: 0.006217, tar: 0.000466 
l0: 0.000267, l1: 0.000263, l2: 0.000311, l3: 0.000347, l4: 0.000509, l5: 0.000771, l6: 0.002143
[epoch: 210/1000, batch:   292/ 1052, ite: 55040] train loss: 0.006218, tar: 0.000466 
l0: 0.000400, l1: 0.000411, l2: 0.000425, l3: 0.000448, l4: 0.000569, l5: 0.000883, l6: 0.001251
[epoch: 210/1000, batch:   372/ 1052, ite: 55060] train loss: 0.006217, tar: 0.000466 
l0: 0.000556, l1: 0.000587, l2: 0.000534, l3: 0.000511, l4: 0.000763, l5: 0.000893, l6: 0.002248
[epoch: 210/1000, batch:   452/ 1052, ite: 55080] train loss: 0.006215, tar: 0.000465 
l0: 0.000554, l1: 0.000574, l2: 0.000559, l3: 0.000559, l4: 0.000759, l5: 0.001538, l6: 0.001742
[epoch: 210/1000, batch:   532/ 1052, ite: 55100] train loss: 0.006212, tar: 0.000465 
l0: 0.000341, l1: 0.000330, l2: 0.000334, l3: 0.000414, l4: 0.000627, l5: 0.001009, l6: 0.001988
[epoch: 210/1000, batch:   612/ 1052, ite: 55120] train loss: 0.006210, tar: 0.000465 
l0: 0.002154, l1: 0.001322, l2: 0.002808, l3: 0.004804, l4: 0.004094, l5: 0.002904, l6: 0.005529
[epoch: 210/1000, batch:   692/ 1052, ite: 55140] train loss: 0.006213, tar: 0.000465 
l0: 0.000282, l1: 0.000298, l2: 0.000268, l3: 0.000305, l4: 0.000528, l5: 0.000817, l6: 0.001140
[epoch: 210/1000, batch:   772/ 1052, ite: 55160] train loss: 0.006211, tar: 0.000465 
l0: 0.000431, l1: 0.000400, l2: 0.000480, l3: 0.000573, l4: 0.000661, l5: 0.001313, l6: 0.002237
[epoch: 210/1000, batch:   852/ 1052, ite: 55180] train loss: 0.006209, tar: 0.000465 
l0: 0.000373, l1: 0.000367, l2: 0.000418, l3: 0.000404, l4: 0.000668, l5: 0.000836, l6: 0.001825
[epoch: 210/1000, batch:   932/ 1052, ite: 55200] train loss: 0.006213, tar: 0.000465 
l0: 0.000688, l1: 0.000707, l2: 0.000682, l3: 0.000810, l4: 0.001006, l5: 0.001174, l6: 0.001840
[epoch: 210/1000, batch:  1012/ 1052, ite: 55220] train loss: 0.006212, tar: 0.000465 
[Epoch 210/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000475, l1: 0.000503, l2: 0.000478, l3: 0.000481, l4: 0.000761, l5: 0.001259, l6: 0.002320
[epoch: 211/1000, batch:    40/ 1052, ite: 55240] train loss: 0.006211, tar: 0.000465 
l0: 0.000358, l1: 0.000394, l2: 0.000389, l3: 0.000421, l4: 0.000547, l5: 0.000889, l6: 0.001395
[epoch: 211/1000, batch:   120/ 1052, ite: 55260] train loss: 0.006207, tar: 0.000464 
l0: 0.000523, l1: 0.000525, l2: 0.000515, l3: 0.000566, l4: 0.000810, l5: 0.001440, l6: 0.002149
[epoch: 211/1000, batch:   200/ 1052, ite: 55280] train loss: 0.006205, tar: 0.000464 
l0: 0.000592, l1: 0.000617, l2: 0.000593, l3: 0.000574, l4: 0.000742, l5: 0.001811, l6: 0.002251
[epoch: 211/1000, batch:   280/ 1052, ite: 55300] train loss: 0.006205, tar: 0.000464 
l0: 0.000264, l1: 0.000264, l2: 0.000265, l3: 0.000314, l4: 0.000422, l5: 0.000817, l6: 0.001345
[epoch: 211/1000, batch:   360/ 1052, ite: 55320] train loss: 0.006198, tar: 0.000463 
l0: 0.000290, l1: 0.000294, l2: 0.000341, l3: 0.000377, l4: 0.000767, l5: 0.001261, l6: 0.001894
[epoch: 211/1000, batch:   440/ 1052, ite: 55340] train loss: 0.006198, tar: 0.000463 
l0: 0.000702, l1: 0.000736, l2: 0.000733, l3: 0.000750, l4: 0.001220, l5: 0.002036, l6: 0.003320
[epoch: 211/1000, batch:   520/ 1052, ite: 55360] train loss: 0.006204, tar: 0.000464 
l0: 0.000399, l1: 0.000404, l2: 0.000397, l3: 0.000437, l4: 0.000622, l5: 0.001245, l6: 0.001453
[epoch: 211/1000, batch:   600/ 1052, ite: 55380] train loss: 0.006201, tar: 0.000463 
l0: 0.000364, l1: 0.000365, l2: 0.000414, l3: 0.000443, l4: 0.000817, l5: 0.001510, l6: 0.003061
[epoch: 211/1000, batch:   680/ 1052, ite: 55400] train loss: 0.006199, tar: 0.000463 
l0: 0.000483, l1: 0.000500, l2: 0.000457, l3: 0.000535, l4: 0.000966, l5: 0.001272, l6: 0.002579
[epoch: 211/1000, batch:   760/ 1052, ite: 55420] train loss: 0.006195, tar: 0.000463 
l0: 0.000974, l1: 0.001021, l2: 0.001017, l3: 0.001042, l4: 0.001154, l5: 0.001753, l6: 0.002894
[epoch: 211/1000, batch:   840/ 1052, ite: 55440] train loss: 0.006194, tar: 0.000462 
l0: 0.000210, l1: 0.000204, l2: 0.000276, l3: 0.000279, l4: 0.000372, l5: 0.000850, l6: 0.001504
[epoch: 211/1000, batch:   920/ 1052, ite: 55460] train loss: 0.006189, tar: 0.000462 
l0: 0.000194, l1: 0.000204, l2: 0.000219, l3: 0.000224, l4: 0.000432, l5: 0.000662, l6: 0.001914
[epoch: 211/1000, batch:  1000/ 1052, ite: 55480] train loss: 0.006184, tar: 0.000461 
[Epoch 211/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000304, l1: 0.000317, l2: 0.000360, l3: 0.000352, l4: 0.000586, l5: 0.001232, l6: 0.002119
[epoch: 212/1000, batch:    28/ 1052, ite: 55500] train loss: 0.006184, tar: 0.000461 
l0: 0.000239, l1: 0.000249, l2: 0.000233, l3: 0.000293, l4: 0.000425, l5: 0.000758, l6: 0.000751
[epoch: 212/1000, batch:   108/ 1052, ite: 55520] train loss: 0.006178, tar: 0.000461 
l0: 0.000551, l1: 0.000594, l2: 0.000548, l3: 0.000546, l4: 0.000626, l5: 0.001107, l6: 0.001557
[epoch: 212/1000, batch:   188/ 1052, ite: 55540] train loss: 0.006176, tar: 0.000460 
l0: 0.000281, l1: 0.000278, l2: 0.000282, l3: 0.000342, l4: 0.000394, l5: 0.000841, l6: 0.000890
[epoch: 212/1000, batch:   268/ 1052, ite: 55560] train loss: 0.006172, tar: 0.000460 
l0: 0.001454, l1: 0.001088, l2: 0.001230, l3: 0.002108, l4: 0.001926, l5: 0.002518, l6: 0.003363
[epoch: 212/1000, batch:   348/ 1052, ite: 55580] train loss: 0.006173, tar: 0.000460 
l0: 0.000546, l1: 0.000533, l2: 0.000628, l3: 0.000830, l4: 0.001115, l5: 0.001981, l6: 0.002773
[epoch: 212/1000, batch:   428/ 1052, ite: 55600] train loss: 0.006172, tar: 0.000460 
l0: 0.000373, l1: 0.000373, l2: 0.000390, l3: 0.000461, l4: 0.000671, l5: 0.000947, l6: 0.002266
[epoch: 212/1000, batch:   508/ 1052, ite: 55620] train loss: 0.006163, tar: 0.000459 
l0: 0.000367, l1: 0.000409, l2: 0.000417, l3: 0.000432, l4: 0.000520, l5: 0.001211, l6: 0.001385
[epoch: 212/1000, batch:   588/ 1052, ite: 55640] train loss: 0.006166, tar: 0.000459 
l0: 0.000474, l1: 0.000503, l2: 0.000479, l3: 0.000517, l4: 0.000652, l5: 0.002009, l6: 0.003557
[epoch: 212/1000, batch:   668/ 1052, ite: 55660] train loss: 0.006164, tar: 0.000459 
l0: 0.000273, l1: 0.000295, l2: 0.000308, l3: 0.000310, l4: 0.000361, l5: 0.001024, l6: 0.001350
[epoch: 212/1000, batch:   748/ 1052, ite: 55680] train loss: 0.006160, tar: 0.000459 
l0: 0.000663, l1: 0.000857, l2: 0.000734, l3: 0.000690, l4: 0.000983, l5: 0.001664, l6: 0.003447
[epoch: 212/1000, batch:   828/ 1052, ite: 55700] train loss: 0.006162, tar: 0.000459 
l0: 0.000270, l1: 0.000241, l2: 0.000280, l3: 0.000395, l4: 0.000528, l5: 0.000901, l6: 0.001695
[epoch: 212/1000, batch:   908/ 1052, ite: 55720] train loss: 0.006158, tar: 0.000458 
l0: 0.000700, l1: 0.000740, l2: 0.000721, l3: 0.000745, l4: 0.001074, l5: 0.002144, l6: 0.002422
[epoch: 212/1000, batch:   988/ 1052, ite: 55740] train loss: 0.006159, tar: 0.000458 
[Epoch 212/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000421, l1: 0.000410, l2: 0.000412, l3: 0.000530, l4: 0.000819, l5: 0.001787, l6: 0.002790
[epoch: 213/1000, batch:    16/ 1052, ite: 55760] train loss: 0.006157, tar: 0.000458 
l0: 0.000393, l1: 0.000378, l2: 0.000411, l3: 0.000499, l4: 0.000429, l5: 0.000925, l6: 0.001536
[epoch: 213/1000, batch:    96/ 1052, ite: 55780] train loss: 0.006156, tar: 0.000457 
l0: 0.000216, l1: 0.000209, l2: 0.000241, l3: 0.000300, l4: 0.000468, l5: 0.000651, l6: 0.001246
[epoch: 213/1000, batch:   176/ 1052, ite: 55800] train loss: 0.006146, tar: 0.000457 
l0: 0.000535, l1: 0.000562, l2: 0.000546, l3: 0.000752, l4: 0.001000, l5: 0.001524, l6: 0.002380
[epoch: 213/1000, batch:   256/ 1052, ite: 55820] train loss: 0.006150, tar: 0.000457 
l0: 0.000435, l1: 0.000424, l2: 0.000434, l3: 0.000579, l4: 0.000721, l5: 0.001111, l6: 0.001944
[epoch: 213/1000, batch:   336/ 1052, ite: 55840] train loss: 0.006144, tar: 0.000456 
l0: 0.000458, l1: 0.000484, l2: 0.000471, l3: 0.000477, l4: 0.000824, l5: 0.001353, l6: 0.003260
[epoch: 213/1000, batch:   416/ 1052, ite: 55860] train loss: 0.006144, tar: 0.000456 
l0: 0.000312, l1: 0.000318, l2: 0.000365, l3: 0.000387, l4: 0.000466, l5: 0.000929, l6: 0.001922
[epoch: 213/1000, batch:   496/ 1052, ite: 55880] train loss: 0.006137, tar: 0.000456 
l0: 0.000285, l1: 0.000308, l2: 0.000315, l3: 0.000320, l4: 0.000613, l5: 0.001085, l6: 0.002202
[epoch: 213/1000, batch:   576/ 1052, ite: 55900] train loss: 0.006138, tar: 0.000456 
l0: 0.000302, l1: 0.000297, l2: 0.000344, l3: 0.000392, l4: 0.000538, l5: 0.001242, l6: 0.001526
[epoch: 213/1000, batch:   656/ 1052, ite: 55920] train loss: 0.006134, tar: 0.000455 
l0: 0.000397, l1: 0.000385, l2: 0.000434, l3: 0.000494, l4: 0.000737, l5: 0.001255, l6: 0.002709
[epoch: 213/1000, batch:   736/ 1052, ite: 55940] train loss: 0.006130, tar: 0.000455 
l0: 0.000421, l1: 0.000449, l2: 0.000469, l3: 0.000505, l4: 0.000675, l5: 0.001003, l6: 0.002147
[epoch: 213/1000, batch:   816/ 1052, ite: 55960] train loss: 0.006127, tar: 0.000455 
l0: 0.000483, l1: 0.000447, l2: 0.000508, l3: 0.000776, l4: 0.001195, l5: 0.001570, l6: 0.002808
[epoch: 213/1000, batch:   896/ 1052, ite: 55980] train loss: 0.006126, tar: 0.000455 
l0: 0.000256, l1: 0.000260, l2: 0.000290, l3: 0.000298, l4: 0.000560, l5: 0.001074, l6: 0.001903
[epoch: 213/1000, batch:   976/ 1052, ite: 56000] train loss: 0.006128, tar: 0.000455 
[Epoch 213/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000500, l1: 0.000540, l2: 0.000539, l3: 0.000522, l4: 0.000922, l5: 0.001214, l6: 0.001883
[epoch: 214/1000, batch:     4/ 1052, ite: 56020] train loss: 0.006128, tar: 0.000454 
l0: 0.000408, l1: 0.000424, l2: 0.000451, l3: 0.000471, l4: 0.000564, l5: 0.001320, l6: 0.001641
[epoch: 214/1000, batch:    84/ 1052, ite: 56040] train loss: 0.006126, tar: 0.000454 
l0: 0.000198, l1: 0.000202, l2: 0.000232, l3: 0.000257, l4: 0.000427, l5: 0.001015, l6: 0.001258
[epoch: 214/1000, batch:   164/ 1052, ite: 56060] train loss: 0.006126, tar: 0.000454 
l0: 0.000353, l1: 0.000344, l2: 0.000385, l3: 0.000478, l4: 0.000577, l5: 0.001247, l6: 0.002617
[epoch: 214/1000, batch:   244/ 1052, ite: 56080] train loss: 0.006124, tar: 0.000453 
l0: 0.000362, l1: 0.000334, l2: 0.000423, l3: 0.000558, l4: 0.000839, l5: 0.001149, l6: 0.002282
[epoch: 214/1000, batch:   324/ 1052, ite: 56100] train loss: 0.006119, tar: 0.000453 
l0: 0.000163, l1: 0.000171, l2: 0.000155, l3: 0.000202, l4: 0.000307, l5: 0.000490, l6: 0.000736
[epoch: 214/1000, batch:   404/ 1052, ite: 56120] train loss: 0.006112, tar: 0.000452 
l0: 0.000572, l1: 0.000576, l2: 0.000600, l3: 0.000704, l4: 0.000986, l5: 0.002137, l6: 0.003459
[epoch: 214/1000, batch:   484/ 1052, ite: 56140] train loss: 0.006108, tar: 0.000452 
l0: 0.000510, l1: 0.000544, l2: 0.000549, l3: 0.000580, l4: 0.000747, l5: 0.001516, l6: 0.002823
[epoch: 214/1000, batch:   564/ 1052, ite: 56160] train loss: 0.006107, tar: 0.000452 
l0: 0.000264, l1: 0.000278, l2: 0.000306, l3: 0.000300, l4: 0.000599, l5: 0.000804, l6: 0.001655
[epoch: 214/1000, batch:   644/ 1052, ite: 56180] train loss: 0.006105, tar: 0.000451 
l0: 0.000597, l1: 0.000596, l2: 0.000665, l3: 0.000716, l4: 0.000799, l5: 0.001644, l6: 0.002626
[epoch: 214/1000, batch:   724/ 1052, ite: 56200] train loss: 0.006104, tar: 0.000451 
l0: 0.000636, l1: 0.000621, l2: 0.000684, l3: 0.000777, l4: 0.001007, l5: 0.001629, l6: 0.002091
[epoch: 214/1000, batch:   804/ 1052, ite: 56220] train loss: 0.006102, tar: 0.000451 
l0: 0.000391, l1: 0.000447, l2: 0.000365, l3: 0.000365, l4: 0.000497, l5: 0.000870, l6: 0.002492
[epoch: 214/1000, batch:   884/ 1052, ite: 56240] train loss: 0.006105, tar: 0.000451 
l0: 0.000351, l1: 0.000342, l2: 0.000361, l3: 0.000468, l4: 0.000539, l5: 0.000895, l6: 0.001939
[epoch: 214/1000, batch:   964/ 1052, ite: 56260] train loss: 0.006100, tar: 0.000450 
l0: 0.000594, l1: 0.000607, l2: 0.000636, l3: 0.000677, l4: 0.000727, l5: 0.001023, l6: 0.002780
[epoch: 214/1000, batch:  1044/ 1052, ite: 56280] train loss: 0.006099, tar: 0.000450 
[Epoch 214/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000244, l1: 0.000245, l2: 0.000246, l3: 0.000295, l4: 0.000396, l5: 0.000790, l6: 0.001301
[epoch: 215/1000, batch:    72/ 1052, ite: 56300] train loss: 0.006096, tar: 0.000450 
l0: 0.000975, l1: 0.001530, l2: 0.001453, l3: 0.000754, l4: 0.001111, l5: 0.002220, l6: 0.003436
[epoch: 215/1000, batch:   152/ 1052, ite: 56320] train loss: 0.006097, tar: 0.000450 
l0: 0.000486, l1: 0.000481, l2: 0.000559, l3: 0.000607, l4: 0.000900, l5: 0.001520, l6: 0.002292
[epoch: 215/1000, batch:   232/ 1052, ite: 56340] train loss: 0.006097, tar: 0.000449 
l0: 0.000367, l1: 0.000359, l2: 0.000424, l3: 0.000427, l4: 0.000495, l5: 0.000688, l6: 0.001154
[epoch: 215/1000, batch:   312/ 1052, ite: 56360] train loss: 0.006097, tar: 0.000449 
l0: 0.000219, l1: 0.000245, l2: 0.000260, l3: 0.000299, l4: 0.000427, l5: 0.000809, l6: 0.001268
[epoch: 215/1000, batch:   392/ 1052, ite: 56380] train loss: 0.006094, tar: 0.000449 
l0: 0.000391, l1: 0.000404, l2: 0.000437, l3: 0.000568, l4: 0.000718, l5: 0.001418, l6: 0.003704
[epoch: 215/1000, batch:   472/ 1052, ite: 56400] train loss: 0.006094, tar: 0.000449 
l0: 0.000432, l1: 0.000453, l2: 0.000450, l3: 0.000401, l4: 0.000565, l5: 0.000846, l6: 0.001507
[epoch: 215/1000, batch:   552/ 1052, ite: 56420] train loss: 0.006093, tar: 0.000449 
l0: 0.000213, l1: 0.000231, l2: 0.000212, l3: 0.000195, l4: 0.000330, l5: 0.000414, l6: 0.000721
[epoch: 215/1000, batch:   632/ 1052, ite: 56440] train loss: 0.006086, tar: 0.000448 
l0: 0.000294, l1: 0.000298, l2: 0.000338, l3: 0.000378, l4: 0.000618, l5: 0.001065, l6: 0.001796
[epoch: 215/1000, batch:   712/ 1052, ite: 56460] train loss: 0.006088, tar: 0.000449 
l0: 0.000354, l1: 0.000365, l2: 0.000363, l3: 0.000416, l4: 0.000607, l5: 0.001192, l6: 0.001933
[epoch: 215/1000, batch:   792/ 1052, ite: 56480] train loss: 0.006086, tar: 0.000448 
l0: 0.000275, l1: 0.000298, l2: 0.000303, l3: 0.000241, l4: 0.000556, l5: 0.000924, l6: 0.001503
[epoch: 215/1000, batch:   872/ 1052, ite: 56500] train loss: 0.006086, tar: 0.000448 
l0: 0.000187, l1: 0.000194, l2: 0.000244, l3: 0.000365, l4: 0.000442, l5: 0.000768, l6: 0.001171
[epoch: 215/1000, batch:   952/ 1052, ite: 56520] train loss: 0.006078, tar: 0.000448 
l0: 0.000333, l1: 0.000332, l2: 0.000412, l3: 0.000389, l4: 0.000574, l5: 0.001123, l6: 0.001834
[epoch: 215/1000, batch:  1032/ 1052, ite: 56540] train loss: 0.006080, tar: 0.000448 
[Epoch 215/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000427, l1: 0.000422, l2: 0.000479, l3: 0.000575, l4: 0.000732, l5: 0.001713, l6: 0.003004
[epoch: 216/1000, batch:    60/ 1052, ite: 56560] train loss: 0.006075, tar: 0.000447 
l0: 0.000441, l1: 0.000445, l2: 0.000478, l3: 0.000530, l4: 0.000651, l5: 0.001464, l6: 0.002252
[epoch: 216/1000, batch:   140/ 1052, ite: 56580] train loss: 0.006069, tar: 0.000447 
l0: 0.000436, l1: 0.000458, l2: 0.000465, l3: 0.000525, l4: 0.000839, l5: 0.001408, l6: 0.003054
[epoch: 216/1000, batch:   220/ 1052, ite: 56600] train loss: 0.006067, tar: 0.000446 
l0: 0.000353, l1: 0.000356, l2: 0.000368, l3: 0.000431, l4: 0.000520, l5: 0.000997, l6: 0.001883
[epoch: 216/1000, batch:   300/ 1052, ite: 56620] train loss: 0.006071, tar: 0.000447 
l0: 0.000221, l1: 0.000226, l2: 0.000242, l3: 0.000279, l4: 0.000378, l5: 0.000643, l6: 0.001609
[epoch: 216/1000, batch:   380/ 1052, ite: 56640] train loss: 0.006070, tar: 0.000447 
l0: 0.000280, l1: 0.000307, l2: 0.000329, l3: 0.000284, l4: 0.000517, l5: 0.000872, l6: 0.001383
[epoch: 216/1000, batch:   460/ 1052, ite: 56660] train loss: 0.006066, tar: 0.000446 
l0: 0.000519, l1: 0.000525, l2: 0.000570, l3: 0.000682, l4: 0.000891, l5: 0.001343, l6: 0.002529
[epoch: 216/1000, batch:   540/ 1052, ite: 56680] train loss: 0.006068, tar: 0.000446 
l0: 0.000398, l1: 0.000408, l2: 0.000443, l3: 0.000530, l4: 0.000874, l5: 0.001628, l6: 0.002983
[epoch: 216/1000, batch:   620/ 1052, ite: 56700] train loss: 0.006067, tar: 0.000446 
l0: 0.000385, l1: 0.000375, l2: 0.000441, l3: 0.000438, l4: 0.000544, l5: 0.001080, l6: 0.001132
[epoch: 216/1000, batch:   700/ 1052, ite: 56720] train loss: 0.006067, tar: 0.000446 
l0: 0.000529, l1: 0.000556, l2: 0.000589, l3: 0.000613, l4: 0.001081, l5: 0.001677, l6: 0.003162
[epoch: 216/1000, batch:   780/ 1052, ite: 56740] train loss: 0.006067, tar: 0.000446 
l0: 0.000373, l1: 0.000399, l2: 0.000395, l3: 0.000392, l4: 0.000605, l5: 0.001395, l6: 0.001631
[epoch: 216/1000, batch:   860/ 1052, ite: 56760] train loss: 0.006065, tar: 0.000445 
l0: 0.000284, l1: 0.000303, l2: 0.000300, l3: 0.000343, l4: 0.000381, l5: 0.000824, l6: 0.001617
[epoch: 216/1000, batch:   940/ 1052, ite: 56780] train loss: 0.006061, tar: 0.000445 
l0: 0.000305, l1: 0.000304, l2: 0.000336, l3: 0.000369, l4: 0.000657, l5: 0.000884, l6: 0.001885
[epoch: 216/1000, batch:  1020/ 1052, ite: 56800] train loss: 0.006061, tar: 0.000445 
[Epoch 216/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000272, l1: 0.000265, l2: 0.000317, l3: 0.000421, l4: 0.000610, l5: 0.000876, l6: 0.001990
[epoch: 217/1000, batch:    48/ 1052, ite: 56820] train loss: 0.006055, tar: 0.000444 
l0: 0.000247, l1: 0.000248, l2: 0.000256, l3: 0.000287, l4: 0.000453, l5: 0.000844, l6: 0.001555
[epoch: 217/1000, batch:   128/ 1052, ite: 56840] train loss: 0.006051, tar: 0.000444 
l0: 0.000421, l1: 0.000418, l2: 0.000418, l3: 0.000589, l4: 0.000854, l5: 0.001199, l6: 0.002196
[epoch: 217/1000, batch:   208/ 1052, ite: 56860] train loss: 0.006052, tar: 0.000444 
l0: 0.000292, l1: 0.000298, l2: 0.000306, l3: 0.000366, l4: 0.000446, l5: 0.001076, l6: 0.002211
[epoch: 217/1000, batch:   288/ 1052, ite: 56880] train loss: 0.006053, tar: 0.000443 
l0: 0.000148, l1: 0.000150, l2: 0.000163, l3: 0.000173, l4: 0.000487, l5: 0.000470, l6: 0.001142
[epoch: 217/1000, batch:   368/ 1052, ite: 56900] train loss: 0.006051, tar: 0.000443 
l0: 0.000260, l1: 0.000273, l2: 0.000270, l3: 0.000280, l4: 0.000413, l5: 0.000819, l6: 0.001708
[epoch: 217/1000, batch:   448/ 1052, ite: 56920] train loss: 0.006046, tar: 0.000443 
l0: 0.000356, l1: 0.000361, l2: 0.000399, l3: 0.000426, l4: 0.000763, l5: 0.000920, l6: 0.002434
[epoch: 217/1000, batch:   528/ 1052, ite: 56940] train loss: 0.006045, tar: 0.000443 
l0: 0.000289, l1: 0.000282, l2: 0.000349, l3: 0.000370, l4: 0.000392, l5: 0.000856, l6: 0.001516
[epoch: 217/1000, batch:   608/ 1052, ite: 56960] train loss: 0.006045, tar: 0.000442 
l0: 0.000453, l1: 0.000472, l2: 0.000508, l3: 0.000489, l4: 0.000672, l5: 0.001148, l6: 0.001403
[epoch: 217/1000, batch:   688/ 1052, ite: 56980] train loss: 0.006042, tar: 0.000442 
l0: 0.000416, l1: 0.000391, l2: 0.000475, l3: 0.000546, l4: 0.000779, l5: 0.001781, l6: 0.002476
[epoch: 217/1000, batch:   768/ 1052, ite: 57000] train loss: 0.006037, tar: 0.000442 
l0: 0.000350, l1: 0.000366, l2: 0.000415, l3: 0.000384, l4: 0.000652, l5: 0.001419, l6: 0.002130
[epoch: 217/1000, batch:   848/ 1052, ite: 57020] train loss: 0.006033, tar: 0.000441 
l0: 0.000553, l1: 0.000497, l2: 0.000571, l3: 0.000809, l4: 0.001227, l5: 0.002306, l6: 0.002590
[epoch: 217/1000, batch:   928/ 1052, ite: 57040] train loss: 0.006034, tar: 0.000441 
l0: 0.000364, l1: 0.000407, l2: 0.000387, l3: 0.000360, l4: 0.000500, l5: 0.000991, l6: 0.001528
[epoch: 217/1000, batch:  1008/ 1052, ite: 57060] train loss: 0.006032, tar: 0.000441 
[Epoch 217/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000240, l1: 0.000241, l2: 0.000252, l3: 0.000325, l4: 0.000530, l5: 0.001086, l6: 0.001670
[epoch: 218/1000, batch:    36/ 1052, ite: 57080] train loss: 0.006033, tar: 0.000441 
l0: 0.000455, l1: 0.000430, l2: 0.000487, l3: 0.000675, l4: 0.000999, l5: 0.001500, l6: 0.002276
[epoch: 218/1000, batch:   116/ 1052, ite: 57100] train loss: 0.006030, tar: 0.000440 
l0: 0.000210, l1: 0.000216, l2: 0.000215, l3: 0.000254, l4: 0.000409, l5: 0.001015, l6: 0.001499
[epoch: 218/1000, batch:   196/ 1052, ite: 57120] train loss: 0.006030, tar: 0.000440 
l0: 0.000238, l1: 0.000222, l2: 0.000246, l3: 0.000300, l4: 0.000499, l5: 0.000915, l6: 0.001312
[epoch: 218/1000, batch:   276/ 1052, ite: 57140] train loss: 0.006029, tar: 0.000440 
l0: 0.000255, l1: 0.000265, l2: 0.000260, l3: 0.000311, l4: 0.000432, l5: 0.000666, l6: 0.001395
[epoch: 218/1000, batch:   356/ 1052, ite: 57160] train loss: 0.006028, tar: 0.000440 
l0: 0.000120, l1: 0.000112, l2: 0.000126, l3: 0.000193, l4: 0.000317, l5: 0.000345, l6: 0.000893
[epoch: 218/1000, batch:   436/ 1052, ite: 57180] train loss: 0.006025, tar: 0.000440 
l0: 0.000362, l1: 0.000365, l2: 0.000423, l3: 0.000429, l4: 0.000511, l5: 0.000831, l6: 0.001306
[epoch: 218/1000, batch:   516/ 1052, ite: 57200] train loss: 0.006026, tar: 0.000440 
l0: 0.000337, l1: 0.000343, l2: 0.000363, l3: 0.000394, l4: 0.000517, l5: 0.001143, l6: 0.001644
[epoch: 218/1000, batch:   596/ 1052, ite: 57220] train loss: 0.006023, tar: 0.000439 
l0: 0.000367, l1: 0.000363, l2: 0.000408, l3: 0.000471, l4: 0.000640, l5: 0.001226, l6: 0.003139
[epoch: 218/1000, batch:   676/ 1052, ite: 57240] train loss: 0.006023, tar: 0.000439 
l0: 0.000272, l1: 0.000282, l2: 0.000314, l3: 0.000330, l4: 0.000539, l5: 0.001243, l6: 0.002139
[epoch: 218/1000, batch:   756/ 1052, ite: 57260] train loss: 0.006022, tar: 0.000439 
l0: 0.000509, l1: 0.000532, l2: 0.000571, l3: 0.000713, l4: 0.000990, l5: 0.002096, l6: 0.003615
[epoch: 218/1000, batch:   836/ 1052, ite: 57280] train loss: 0.006019, tar: 0.000439 
l0: 0.000282, l1: 0.000271, l2: 0.000284, l3: 0.000383, l4: 0.000482, l5: 0.000733, l6: 0.001617
[epoch: 218/1000, batch:   916/ 1052, ite: 57300] train loss: 0.006017, tar: 0.000438 
l0: 0.000380, l1: 0.000421, l2: 0.000462, l3: 0.000408, l4: 0.000658, l5: 0.001166, l6: 0.002317
[epoch: 218/1000, batch:   996/ 1052, ite: 57320] train loss: 0.006015, tar: 0.000438 
[Epoch 218/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000345, l1: 0.000350, l2: 0.000382, l3: 0.000411, l4: 0.000580, l5: 0.000748, l6: 0.001163
[epoch: 219/1000, batch:    24/ 1052, ite: 57340] train loss: 0.006012, tar: 0.000438 
l0: 0.000354, l1: 0.000356, l2: 0.000372, l3: 0.000450, l4: 0.000698, l5: 0.001172, l6: 0.002165
[epoch: 219/1000, batch:   104/ 1052, ite: 57360] train loss: 0.006011, tar: 0.000438 
l0: 0.000373, l1: 0.000376, l2: 0.000402, l3: 0.000455, l4: 0.000558, l5: 0.000917, l6: 0.001575
[epoch: 219/1000, batch:   184/ 1052, ite: 57380] train loss: 0.006008, tar: 0.000437 
l0: 0.000628, l1: 0.000667, l2: 0.000715, l3: 0.000707, l4: 0.001065, l5: 0.002569, l6: 0.005198
[epoch: 219/1000, batch:   264/ 1052, ite: 57400] train loss: 0.006010, tar: 0.000438 
l0: 0.000281, l1: 0.000279, l2: 0.000303, l3: 0.000393, l4: 0.000415, l5: 0.000599, l6: 0.001779
[epoch: 219/1000, batch:   344/ 1052, ite: 57420] train loss: 0.006011, tar: 0.000438 
l0: 0.000465, l1: 0.000483, l2: 0.000505, l3: 0.000482, l4: 0.000753, l5: 0.000898, l6: 0.001784
[epoch: 219/1000, batch:   424/ 1052, ite: 57440] train loss: 0.006010, tar: 0.000437 
l0: 0.000555, l1: 0.000563, l2: 0.000669, l3: 0.000676, l4: 0.000796, l5: 0.001775, l6: 0.002469
[epoch: 219/1000, batch:   504/ 1052, ite: 57460] train loss: 0.006009, tar: 0.000437 
l0: 0.000622, l1: 0.000665, l2: 0.000678, l3: 0.000759, l4: 0.000827, l5: 0.001278, l6: 0.002155
[epoch: 219/1000, batch:   584/ 1052, ite: 57480] train loss: 0.006008, tar: 0.000437 
l0: 0.000178, l1: 0.000198, l2: 0.000195, l3: 0.000193, l4: 0.000356, l5: 0.000666, l6: 0.000790
[epoch: 219/1000, batch:   664/ 1052, ite: 57500] train loss: 0.006004, tar: 0.000437 
l0: 0.000247, l1: 0.000239, l2: 0.000255, l3: 0.000334, l4: 0.000593, l5: 0.000754, l6: 0.001414
[epoch: 219/1000, batch:   744/ 1052, ite: 57520] train loss: 0.006004, tar: 0.000437 
l0: 0.000202, l1: 0.000203, l2: 0.000271, l3: 0.000227, l4: 0.000469, l5: 0.000728, l6: 0.000975
[epoch: 219/1000, batch:   824/ 1052, ite: 57540] train loss: 0.006001, tar: 0.000436 
l0: 0.000342, l1: 0.000344, l2: 0.000420, l3: 0.000449, l4: 0.000756, l5: 0.001350, l6: 0.001507
[epoch: 219/1000, batch:   904/ 1052, ite: 57560] train loss: 0.006000, tar: 0.000436 
l0: 0.000385, l1: 0.000432, l2: 0.000401, l3: 0.000404, l4: 0.000553, l5: 0.000778, l6: 0.001283
[epoch: 219/1000, batch:   984/ 1052, ite: 57580] train loss: 0.005999, tar: 0.000436 
[Epoch 219/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000241, l1: 0.000238, l2: 0.000289, l3: 0.000327, l4: 0.000398, l5: 0.000850, l6: 0.001351
[epoch: 220/1000, batch:    12/ 1052, ite: 57600] train loss: 0.005996, tar: 0.000436 
l0: 0.000337, l1: 0.000408, l2: 0.000349, l3: 0.000312, l4: 0.000579, l5: 0.000982, l6: 0.001697
[epoch: 220/1000, batch:    92/ 1052, ite: 57620] train loss: 0.005996, tar: 0.000436 
l0: 0.000345, l1: 0.000349, l2: 0.000376, l3: 0.000449, l4: 0.000623, l5: 0.001134, l6: 0.002069
[epoch: 220/1000, batch:   172/ 1052, ite: 57640] train loss: 0.005994, tar: 0.000435 
l0: 0.000399, l1: 0.000393, l2: 0.000451, l3: 0.000518, l4: 0.000703, l5: 0.001492, l6: 0.002784
[epoch: 220/1000, batch:   252/ 1052, ite: 57660] train loss: 0.005991, tar: 0.000435 
l0: 0.000249, l1: 0.000264, l2: 0.000276, l3: 0.000263, l4: 0.000389, l5: 0.001061, l6: 0.001391
[epoch: 220/1000, batch:   332/ 1052, ite: 57680] train loss: 0.005988, tar: 0.000435 
l0: 0.000193, l1: 0.000184, l2: 0.000218, l3: 0.000284, l4: 0.000398, l5: 0.000869, l6: 0.001477
[epoch: 220/1000, batch:   412/ 1052, ite: 57700] train loss: 0.005987, tar: 0.000434 
l0: 0.000386, l1: 0.000377, l2: 0.000432, l3: 0.000525, l4: 0.000794, l5: 0.001447, l6: 0.002113
[epoch: 220/1000, batch:   492/ 1052, ite: 57720] train loss: 0.005987, tar: 0.000434 
l0: 0.000313, l1: 0.000315, l2: 0.000334, l3: 0.000397, l4: 0.000447, l5: 0.001423, l6: 0.001701
[epoch: 220/1000, batch:   572/ 1052, ite: 57740] train loss: 0.005986, tar: 0.000434 
l0: 0.000471, l1: 0.000443, l2: 0.000500, l3: 0.000639, l4: 0.001255, l5: 0.001350, l6: 0.002575
[epoch: 220/1000, batch:   652/ 1052, ite: 57760] train loss: 0.005985, tar: 0.000434 
l0: 0.000597, l1: 0.000623, l2: 0.000611, l3: 0.000626, l4: 0.000742, l5: 0.001467, l6: 0.002887
[epoch: 220/1000, batch:   732/ 1052, ite: 57780] train loss: 0.005983, tar: 0.000434 
l0: 0.000286, l1: 0.000283, l2: 0.000329, l3: 0.000347, l4: 0.000528, l5: 0.000892, l6: 0.001100
[epoch: 220/1000, batch:   812/ 1052, ite: 57800] train loss: 0.005983, tar: 0.000434 
l0: 0.000351, l1: 0.000342, l2: 0.000395, l3: 0.000480, l4: 0.000721, l5: 0.000915, l6: 0.002231
[epoch: 220/1000, batch:   892/ 1052, ite: 57820] train loss: 0.005983, tar: 0.000434 
l0: 0.000201, l1: 0.000201, l2: 0.000245, l3: 0.000256, l4: 0.000537, l5: 0.001046, l6: 0.001428
[epoch: 220/1000, batch:   972/ 1052, ite: 57840] train loss: 0.005979, tar: 0.000433 
l0: 0.000753, l1: 0.000798, l2: 0.000792, l3: 0.000825, l4: 0.001335, l5: 0.001668, l6: 0.002693
[epoch: 220/1000, batch:  1052/ 1052, ite: 57860] train loss: 0.005977, tar: 0.000433 
[Epoch 220/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000244, l1: 0.000256, l2: 0.000412, l3: 0.000326, l4: 0.000511, l5: 0.001109, l6: 0.001593
[epoch: 221/1000, batch:    80/ 1052, ite: 57880] train loss: 0.005976, tar: 0.000433 
l0: 0.000223, l1: 0.000207, l2: 0.000251, l3: 0.000353, l4: 0.000674, l5: 0.001119, l6: 0.001374
[epoch: 221/1000, batch:   160/ 1052, ite: 57900] train loss: 0.005976, tar: 0.000433 
l0: 0.000243, l1: 0.000250, l2: 0.000256, l3: 0.000298, l4: 0.000545, l5: 0.000835, l6: 0.001496
[epoch: 221/1000, batch:   240/ 1052, ite: 57920] train loss: 0.005977, tar: 0.000433 
l0: 0.000428, l1: 0.000447, l2: 0.000390, l3: 0.000433, l4: 0.000643, l5: 0.001030, l6: 0.001510
[epoch: 221/1000, batch:   320/ 1052, ite: 57940] train loss: 0.005974, tar: 0.000432 
l0: 0.000366, l1: 0.000365, l2: 0.000382, l3: 0.000385, l4: 0.000524, l5: 0.000841, l6: 0.001610
[epoch: 221/1000, batch:   400/ 1052, ite: 57960] train loss: 0.005973, tar: 0.000432 
l0: 0.000894, l1: 0.001006, l2: 0.000923, l3: 0.000905, l4: 0.001203, l5: 0.001666, l6: 0.003059
[epoch: 221/1000, batch:   480/ 1052, ite: 57980] train loss: 0.005976, tar: 0.000432 
l0: 0.000279, l1: 0.000267, l2: 0.000315, l3: 0.000411, l4: 0.000700, l5: 0.001110, l6: 0.002363
[epoch: 221/1000, batch:   560/ 1052, ite: 58000] train loss: 0.005974, tar: 0.000432 
l0: 0.000158, l1: 0.000158, l2: 0.000157, l3: 0.000210, l4: 0.000441, l5: 0.000503, l6: 0.001432
[epoch: 221/1000, batch:   640/ 1052, ite: 58020] train loss: 0.005970, tar: 0.000432 
l0: 0.000290, l1: 0.000295, l2: 0.000318, l3: 0.000367, l4: 0.000869, l5: 0.001257, l6: 0.002539
[epoch: 221/1000, batch:   720/ 1052, ite: 58040] train loss: 0.005966, tar: 0.000431 
l0: 0.000209, l1: 0.000219, l2: 0.000238, l3: 0.000238, l4: 0.000289, l5: 0.000825, l6: 0.000822
[epoch: 221/1000, batch:   800/ 1052, ite: 58060] train loss: 0.005964, tar: 0.000431 
l0: 0.000256, l1: 0.000269, l2: 0.000285, l3: 0.000283, l4: 0.000482, l5: 0.000987, l6: 0.001428
[epoch: 221/1000, batch:   880/ 1052, ite: 58080] train loss: 0.005961, tar: 0.000431 
l0: 0.000221, l1: 0.000222, l2: 0.000240, l3: 0.000303, l4: 0.000418, l5: 0.000873, l6: 0.001491
[epoch: 221/1000, batch:   960/ 1052, ite: 58100] train loss: 0.005956, tar: 0.000430 
l0: 0.000491, l1: 0.000515, l2: 0.000508, l3: 0.000603, l4: 0.001069, l5: 0.002271, l6: 0.003449
[epoch: 221/1000, batch:  1040/ 1052, ite: 58120] train loss: 0.005957, tar: 0.000430 
[Epoch 221/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000256, l1: 0.000242, l2: 0.000284, l3: 0.000370, l4: 0.000464, l5: 0.000746, l6: 0.001344
[epoch: 222/1000, batch:    68/ 1052, ite: 58140] train loss: 0.005955, tar: 0.000430 
l0: 0.000348, l1: 0.000378, l2: 0.000405, l3: 0.000390, l4: 0.000819, l5: 0.001254, l6: 0.001674
[epoch: 222/1000, batch:   148/ 1052, ite: 58160] train loss: 0.005950, tar: 0.000430 
l0: 0.000295, l1: 0.000287, l2: 0.000330, l3: 0.000446, l4: 0.000655, l5: 0.001491, l6: 0.002155
[epoch: 222/1000, batch:   228/ 1052, ite: 58180] train loss: 0.005952, tar: 0.000430 
l0: 0.000716, l1: 0.000772, l2: 0.000723, l3: 0.000730, l4: 0.000904, l5: 0.001528, l6: 0.003274
[epoch: 222/1000, batch:   308/ 1052, ite: 58200] train loss: 0.005954, tar: 0.000430 
l0: 0.000460, l1: 0.000482, l2: 0.000432, l3: 0.000523, l4: 0.000994, l5: 0.001454, l6: 0.002536
[epoch: 222/1000, batch:   388/ 1052, ite: 58220] train loss: 0.005952, tar: 0.000429 
l0: 0.000180, l1: 0.000173, l2: 0.000199, l3: 0.000240, l4: 0.000410, l5: 0.000710, l6: 0.001071
[epoch: 222/1000, batch:   468/ 1052, ite: 58240] train loss: 0.005952, tar: 0.000429 
l0: 0.000254, l1: 0.000269, l2: 0.000263, l3: 0.000296, l4: 0.000402, l5: 0.000975, l6: 0.001459
[epoch: 222/1000, batch:   548/ 1052, ite: 58260] train loss: 0.005951, tar: 0.000429 
l0: 0.000181, l1: 0.000200, l2: 0.000185, l3: 0.000216, l4: 0.000440, l5: 0.000863, l6: 0.001584
[epoch: 222/1000, batch:   628/ 1052, ite: 58280] train loss: 0.005949, tar: 0.000429 
l0: 0.000285, l1: 0.000279, l2: 0.000323, l3: 0.000359, l4: 0.000534, l5: 0.001102, l6: 0.001824
[epoch: 222/1000, batch:   708/ 1052, ite: 58300] train loss: 0.005945, tar: 0.000428 
l0: 0.000406, l1: 0.000416, l2: 0.000411, l3: 0.000636, l4: 0.000787, l5: 0.000892, l6: 0.001260
[epoch: 222/1000, batch:   788/ 1052, ite: 58320] train loss: 0.005943, tar: 0.000428 
l0: 0.000309, l1: 0.000354, l2: 0.000318, l3: 0.000270, l4: 0.000272, l5: 0.000697, l6: 0.001539
[epoch: 222/1000, batch:   868/ 1052, ite: 58340] train loss: 0.005940, tar: 0.000428 
l0: 0.000196, l1: 0.000205, l2: 0.000220, l3: 0.000263, l4: 0.000286, l5: 0.000520, l6: 0.001222
[epoch: 222/1000, batch:   948/ 1052, ite: 58360] train loss: 0.005942, tar: 0.000428 
l0: 0.000276, l1: 0.000264, l2: 0.000275, l3: 0.000345, l4: 0.000517, l5: 0.001052, l6: 0.001830
[epoch: 222/1000, batch:  1028/ 1052, ite: 58380] train loss: 0.005941, tar: 0.000428 
[Epoch 222/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000261, l1: 0.000261, l2: 0.000301, l3: 0.000336, l4: 0.000597, l5: 0.001012, l6: 0.001615
[epoch: 223/1000, batch:    56/ 1052, ite: 58400] train loss: 0.005938, tar: 0.000428 
l0: 0.000294, l1: 0.000295, l2: 0.000315, l3: 0.000363, l4: 0.000532, l5: 0.001100, l6: 0.001500
[epoch: 223/1000, batch:   136/ 1052, ite: 58420] train loss: 0.005939, tar: 0.000428 
l0: 0.000326, l1: 0.000334, l2: 0.000351, l3: 0.000429, l4: 0.000457, l5: 0.001338, l6: 0.001846
[epoch: 223/1000, batch:   216/ 1052, ite: 58440] train loss: 0.005937, tar: 0.000427 
l0: 0.000227, l1: 0.000230, l2: 0.000260, l3: 0.000261, l4: 0.000510, l5: 0.001081, l6: 0.002017
[epoch: 223/1000, batch:   296/ 1052, ite: 58460] train loss: 0.005934, tar: 0.000427 
l0: 0.000334, l1: 0.000350, l2: 0.000357, l3: 0.000331, l4: 0.000526, l5: 0.001154, l6: 0.001786
[epoch: 223/1000, batch:   376/ 1052, ite: 58480] train loss: 0.005932, tar: 0.000427 
l0: 0.000497, l1: 0.000520, l2: 0.000505, l3: 0.000520, l4: 0.000720, l5: 0.001587, l6: 0.003336
[epoch: 223/1000, batch:   456/ 1052, ite: 58500] train loss: 0.005930, tar: 0.000427 
l0: 0.000255, l1: 0.000258, l2: 0.000270, l3: 0.000264, l4: 0.000516, l5: 0.000918, l6: 0.001737
[epoch: 223/1000, batch:   536/ 1052, ite: 58520] train loss: 0.005927, tar: 0.000426 
l0: 0.000417, l1: 0.000420, l2: 0.000411, l3: 0.000567, l4: 0.000903, l5: 0.001854, l6: 0.003321
[epoch: 223/1000, batch:   616/ 1052, ite: 58540] train loss: 0.005927, tar: 0.000426 
l0: 0.000368, l1: 0.000387, l2: 0.000385, l3: 0.000391, l4: 0.000925, l5: 0.001231, l6: 0.002375
[epoch: 223/1000, batch:   696/ 1052, ite: 58560] train loss: 0.005928, tar: 0.000426 
l0: 0.000515, l1: 0.000533, l2: 0.000590, l3: 0.000550, l4: 0.000639, l5: 0.001094, l6: 0.002349
[epoch: 223/1000, batch:   776/ 1052, ite: 58580] train loss: 0.005927, tar: 0.000426 
l0: 0.000369, l1: 0.000395, l2: 0.000440, l3: 0.000382, l4: 0.000543, l5: 0.001286, l6: 0.001829
[epoch: 223/1000, batch:   856/ 1052, ite: 58600] train loss: 0.005928, tar: 0.000426 
l0: 0.000308, l1: 0.000319, l2: 0.000357, l3: 0.000410, l4: 0.000664, l5: 0.001075, l6: 0.002507
[epoch: 223/1000, batch:   936/ 1052, ite: 58620] train loss: 0.005927, tar: 0.000426 
l0: 0.000327, l1: 0.000340, l2: 0.000398, l3: 0.000405, l4: 0.000576, l5: 0.001166, l6: 0.001558
[epoch: 223/1000, batch:  1016/ 1052, ite: 58640] train loss: 0.005928, tar: 0.000426 
[Epoch 223/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000248, l1: 0.000252, l2: 0.000305, l3: 0.000311, l4: 0.000426, l5: 0.000769, l6: 0.001412
[epoch: 224/1000, batch:    44/ 1052, ite: 58660] train loss: 0.005926, tar: 0.000426 
l0: 0.000485, l1: 0.000481, l2: 0.000562, l3: 0.000731, l4: 0.001107, l5: 0.002019, l6: 0.003516
[epoch: 224/1000, batch:   124/ 1052, ite: 58680] train loss: 0.005927, tar: 0.000426 
l0: 0.000853, l1: 0.000961, l2: 0.001863, l3: 0.000674, l4: 0.000992, l5: 0.001395, l6: 0.002523
[epoch: 224/1000, batch:   204/ 1052, ite: 58700] train loss: 0.005928, tar: 0.000426 
l0: 0.000361, l1: 0.000384, l2: 0.000316, l3: 0.000368, l4: 0.000520, l5: 0.000851, l6: 0.001221
[epoch: 224/1000, batch:   284/ 1052, ite: 58720] train loss: 0.005926, tar: 0.000426 
l0: 0.000633, l1: 0.000637, l2: 0.000656, l3: 0.000752, l4: 0.000978, l5: 0.001871, l6: 0.002622
[epoch: 224/1000, batch:   364/ 1052, ite: 58740] train loss: 0.005930, tar: 0.000426 
l0: 0.000477, l1: 0.000497, l2: 0.000495, l3: 0.000474, l4: 0.000597, l5: 0.000924, l6: 0.001149
[epoch: 224/1000, batch:   444/ 1052, ite: 58760] train loss: 0.005929, tar: 0.000426 
l0: 0.000298, l1: 0.000288, l2: 0.000308, l3: 0.000369, l4: 0.000409, l5: 0.001140, l6: 0.001585
[epoch: 224/1000, batch:   524/ 1052, ite: 58780] train loss: 0.005927, tar: 0.000426 
l0: 0.000269, l1: 0.000283, l2: 0.000269, l3: 0.000299, l4: 0.000407, l5: 0.000572, l6: 0.001013
[epoch: 224/1000, batch:   604/ 1052, ite: 58800] train loss: 0.005926, tar: 0.000426 
l0: 0.000388, l1: 0.000398, l2: 0.000452, l3: 0.000482, l4: 0.000912, l5: 0.002027, l6: 0.003042
[epoch: 224/1000, batch:   684/ 1052, ite: 58820] train loss: 0.005927, tar: 0.000426 
l0: 0.000333, l1: 0.000334, l2: 0.000349, l3: 0.000394, l4: 0.000620, l5: 0.001488, l6: 0.002016
[epoch: 224/1000, batch:   764/ 1052, ite: 58840] train loss: 0.005927, tar: 0.000426 
l0: 0.000555, l1: 0.000615, l2: 0.000650, l3: 0.000615, l4: 0.001008, l5: 0.002092, l6: 0.002782
[epoch: 224/1000, batch:   844/ 1052, ite: 58860] train loss: 0.005927, tar: 0.000426 
l0: 0.000551, l1: 0.000600, l2: 0.000637, l3: 0.000679, l4: 0.000873, l5: 0.002435, l6: 0.002495
[epoch: 224/1000, batch:   924/ 1052, ite: 58880] train loss: 0.005928, tar: 0.000426 
l0: 0.000460, l1: 0.000487, l2: 0.000492, l3: 0.000467, l4: 0.000507, l5: 0.000963, l6: 0.001853
[epoch: 224/1000, batch:  1004/ 1052, ite: 58900] train loss: 0.005929, tar: 0.000426 
[Epoch 224/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000251, l1: 0.000244, l2: 0.000266, l3: 0.000322, l4: 0.000640, l5: 0.000804, l6: 0.001631
[epoch: 225/1000, batch:    32/ 1052, ite: 58920] train loss: 0.005928, tar: 0.000426 
l0: 0.000453, l1: 0.000423, l2: 0.000530, l3: 0.000670, l4: 0.000964, l5: 0.001701, l6: 0.002802
[epoch: 225/1000, batch:   112/ 1052, ite: 58940] train loss: 0.005927, tar: 0.000426 
l0: 0.000341, l1: 0.000304, l2: 0.000329, l3: 0.000440, l4: 0.000604, l5: 0.001017, l6: 0.002478
[epoch: 225/1000, batch:   192/ 1052, ite: 58960] train loss: 0.005926, tar: 0.000426 
l0: 0.000628, l1: 0.000639, l2: 0.000680, l3: 0.000547, l4: 0.000948, l5: 0.001302, l6: 0.002032
[epoch: 225/1000, batch:   272/ 1052, ite: 58980] train loss: 0.005926, tar: 0.000426 
l0: 0.000761, l1: 0.000792, l2: 0.000787, l3: 0.000792, l4: 0.000985, l5: 0.001509, l6: 0.002223
[epoch: 225/1000, batch:   352/ 1052, ite: 59000] train loss: 0.005927, tar: 0.000426 
l0: 0.000208, l1: 0.000211, l2: 0.000216, l3: 0.000256, l4: 0.000400, l5: 0.000779, l6: 0.001171
[epoch: 225/1000, batch:   432/ 1052, ite: 59020] train loss: 0.005926, tar: 0.000426 
l0: 0.000241, l1: 0.000260, l2: 0.000246, l3: 0.000274, l4: 0.000347, l5: 0.000807, l6: 0.000854
[epoch: 225/1000, batch:   512/ 1052, ite: 59040] train loss: 0.005925, tar: 0.000426 
l0: 0.000435, l1: 0.000441, l2: 0.000449, l3: 0.000454, l4: 0.000619, l5: 0.001261, l6: 0.001745
[epoch: 225/1000, batch:   592/ 1052, ite: 59060] train loss: 0.005925, tar: 0.000425 
l0: 0.000546, l1: 0.000588, l2: 0.000636, l3: 0.000571, l4: 0.000754, l5: 0.001520, l6: 0.002948
[epoch: 225/1000, batch:   672/ 1052, ite: 59080] train loss: 0.005924, tar: 0.000425 
l0: 0.000303, l1: 0.000306, l2: 0.000314, l3: 0.000430, l4: 0.000601, l5: 0.001009, l6: 0.001383
[epoch: 225/1000, batch:   752/ 1052, ite: 59100] train loss: 0.005923, tar: 0.000425 
l0: 0.000226, l1: 0.000231, l2: 0.000250, l3: 0.000267, l4: 0.000325, l5: 0.000894, l6: 0.001306
[epoch: 225/1000, batch:   832/ 1052, ite: 59120] train loss: 0.005920, tar: 0.000425 
l0: 0.000504, l1: 0.000505, l2: 0.000565, l3: 0.000708, l4: 0.000863, l5: 0.001778, l6: 0.002278
[epoch: 225/1000, batch:   912/ 1052, ite: 59140] train loss: 0.005921, tar: 0.000425 
l0: 0.000439, l1: 0.000442, l2: 0.000464, l3: 0.000477, l4: 0.000649, l5: 0.000988, l6: 0.001919
[epoch: 225/1000, batch:   992/ 1052, ite: 59160] train loss: 0.005922, tar: 0.000425 
[Epoch 225/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000122, l1: 0.000129, l2: 0.000146, l3: 0.000138, l4: 0.000270, l5: 0.000668, l6: 0.000920
[epoch: 226/1000, batch:    20/ 1052, ite: 59180] train loss: 0.005921, tar: 0.000425 
l0: 0.000146, l1: 0.000146, l2: 0.000178, l3: 0.000226, l4: 0.000358, l5: 0.000982, l6: 0.001166
[epoch: 226/1000, batch:   100/ 1052, ite: 59200] train loss: 0.005920, tar: 0.000425 
l0: 0.000220, l1: 0.000208, l2: 0.000224, l3: 0.000299, l4: 0.000390, l5: 0.000944, l6: 0.001606
[epoch: 226/1000, batch:   180/ 1052, ite: 59220] train loss: 0.005920, tar: 0.000425 
l0: 0.000334, l1: 0.000332, l2: 0.000327, l3: 0.000381, l4: 0.000569, l5: 0.001259, l6: 0.002034
[epoch: 226/1000, batch:   260/ 1052, ite: 59240] train loss: 0.005917, tar: 0.000424 
l0: 0.000447, l1: 0.000464, l2: 0.000459, l3: 0.000468, l4: 0.000749, l5: 0.001088, l6: 0.001906
[epoch: 226/1000, batch:   340/ 1052, ite: 59260] train loss: 0.005919, tar: 0.000424 
l0: 0.000140, l1: 0.000134, l2: 0.000151, l3: 0.000216, l4: 0.000305, l5: 0.000741, l6: 0.001233
[epoch: 226/1000, batch:   420/ 1052, ite: 59280] train loss: 0.005916, tar: 0.000424 
l0: 0.000661, l1: 0.000712, l2: 0.000680, l3: 0.000646, l4: 0.000858, l5: 0.001464, l6: 0.001723
[epoch: 226/1000, batch:   500/ 1052, ite: 59300] train loss: 0.005914, tar: 0.000424 
l0: 0.000296, l1: 0.000305, l2: 0.000354, l3: 0.000467, l4: 0.000674, l5: 0.001415, l6: 0.001719
[epoch: 226/1000, batch:   580/ 1052, ite: 59320] train loss: 0.005914, tar: 0.000424 
l0: 0.000128, l1: 0.000125, l2: 0.000147, l3: 0.000187, l4: 0.000345, l5: 0.000710, l6: 0.000901
[epoch: 226/1000, batch:   660/ 1052, ite: 59340] train loss: 0.005913, tar: 0.000424 
l0: 0.000273, l1: 0.000270, l2: 0.000295, l3: 0.000316, l4: 0.000515, l5: 0.000976, l6: 0.001346
[epoch: 226/1000, batch:   740/ 1052, ite: 59360] train loss: 0.005912, tar: 0.000424 
l0: 0.000424, l1: 0.000443, l2: 0.000460, l3: 0.000470, l4: 0.000546, l5: 0.000949, l6: 0.001509
[epoch: 226/1000, batch:   820/ 1052, ite: 59380] train loss: 0.005912, tar: 0.000424 
l0: 0.000213, l1: 0.000218, l2: 0.000243, l3: 0.000254, l4: 0.000468, l5: 0.001068, l6: 0.001541
[epoch: 226/1000, batch:   900/ 1052, ite: 59400] train loss: 0.005910, tar: 0.000423 
l0: 0.000256, l1: 0.000242, l2: 0.000279, l3: 0.000405, l4: 0.000631, l5: 0.000719, l6: 0.001579
[epoch: 226/1000, batch:   980/ 1052, ite: 59420] train loss: 0.005907, tar: 0.000423 
[Epoch 226/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000365, l1: 0.000354, l2: 0.000393, l3: 0.000421, l4: 0.000566, l5: 0.001181, l6: 0.001569
[epoch: 227/1000, batch:     8/ 1052, ite: 59440] train loss: 0.005907, tar: 0.000423 
l0: 0.000284, l1: 0.000289, l2: 0.000323, l3: 0.000418, l4: 0.000765, l5: 0.001194, l6: 0.001844
[epoch: 227/1000, batch:    88/ 1052, ite: 59460] train loss: 0.005907, tar: 0.000423 
l0: 0.000277, l1: 0.000283, l2: 0.000342, l3: 0.000378, l4: 0.000440, l5: 0.001255, l6: 0.001431
[epoch: 227/1000, batch:   168/ 1052, ite: 59480] train loss: 0.005903, tar: 0.000423 
l0: 0.000206, l1: 0.000235, l2: 0.000234, l3: 0.000272, l4: 0.000616, l5: 0.000875, l6: 0.001418
[epoch: 227/1000, batch:   248/ 1052, ite: 59500] train loss: 0.005902, tar: 0.000422 
l0: 0.000673, l1: 0.000750, l2: 0.000707, l3: 0.000657, l4: 0.000906, l5: 0.001737, l6: 0.003595
[epoch: 227/1000, batch:   328/ 1052, ite: 59520] train loss: 0.005904, tar: 0.000423 
l0: 0.000252, l1: 0.000245, l2: 0.000292, l3: 0.000348, l4: 0.000506, l5: 0.001032, l6: 0.001287
[epoch: 227/1000, batch:   408/ 1052, ite: 59540] train loss: 0.005903, tar: 0.000422 
l0: 0.000466, l1: 0.000502, l2: 0.000552, l3: 0.000432, l4: 0.000640, l5: 0.001358, l6: 0.001742
[epoch: 227/1000, batch:   488/ 1052, ite: 59560] train loss: 0.005904, tar: 0.000422 
l0: 0.000389, l1: 0.000371, l2: 0.000442, l3: 0.000555, l4: 0.000863, l5: 0.001210, l6: 0.001608
[epoch: 227/1000, batch:   568/ 1052, ite: 59580] train loss: 0.005904, tar: 0.000422 
l0: 0.000214, l1: 0.000216, l2: 0.000252, l3: 0.000359, l4: 0.000476, l5: 0.000968, l6: 0.001199
[epoch: 227/1000, batch:   648/ 1052, ite: 59600] train loss: 0.005901, tar: 0.000422 
l0: 0.000246, l1: 0.000246, l2: 0.000297, l3: 0.000379, l4: 0.000668, l5: 0.000944, l6: 0.001520
[epoch: 227/1000, batch:   728/ 1052, ite: 59620] train loss: 0.005901, tar: 0.000422 
l0: 0.000515, l1: 0.000545, l2: 0.000588, l3: 0.000549, l4: 0.000744, l5: 0.001263, l6: 0.002361
[epoch: 227/1000, batch:   808/ 1052, ite: 59640] train loss: 0.005899, tar: 0.000422 
l0: 0.000343, l1: 0.000358, l2: 0.000401, l3: 0.000558, l4: 0.000855, l5: 0.001605, l6: 0.002749
[epoch: 227/1000, batch:   888/ 1052, ite: 59660] train loss: 0.005897, tar: 0.000422 
l0: 0.000314, l1: 0.000312, l2: 0.000340, l3: 0.000369, l4: 0.000611, l5: 0.001083, l6: 0.001815
[epoch: 227/1000, batch:   968/ 1052, ite: 59680] train loss: 0.005897, tar: 0.000422 
l0: 0.000290, l1: 0.000289, l2: 0.000355, l3: 0.000406, l4: 0.000769, l5: 0.001207, l6: 0.002014
[epoch: 227/1000, batch:  1048/ 1052, ite: 59700] train loss: 0.005896, tar: 0.000421 
[Epoch 227/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000560, l1: 0.000556, l2: 0.000581, l3: 0.000705, l4: 0.001021, l5: 0.001473, l6: 0.002536
[epoch: 228/1000, batch:    76/ 1052, ite: 59720] train loss: 0.005893, tar: 0.000421 
l0: 0.000853, l1: 0.000889, l2: 0.000988, l3: 0.001107, l4: 0.001820, l5: 0.003195, l6: 0.004240
[epoch: 228/1000, batch:   156/ 1052, ite: 59740] train loss: 0.005892, tar: 0.000421 
l0: 0.000559, l1: 0.000545, l2: 0.000538, l3: 0.000609, l4: 0.000651, l5: 0.000931, l6: 0.001683
[epoch: 228/1000, batch:   236/ 1052, ite: 59760] train loss: 0.005893, tar: 0.000421 
l0: 0.000261, l1: 0.000264, l2: 0.000282, l3: 0.000322, l4: 0.000474, l5: 0.000842, l6: 0.001255
[epoch: 228/1000, batch:   316/ 1052, ite: 59780] train loss: 0.005893, tar: 0.000421 
l0: 0.000795, l1: 0.000833, l2: 0.000870, l3: 0.000855, l4: 0.000952, l5: 0.001225, l6: 0.002876
[epoch: 228/1000, batch:   396/ 1052, ite: 59800] train loss: 0.005894, tar: 0.000421 
l0: 0.000349, l1: 0.000344, l2: 0.000357, l3: 0.000452, l4: 0.000550, l5: 0.001341, l6: 0.002168
[epoch: 228/1000, batch:   476/ 1052, ite: 59820] train loss: 0.005894, tar: 0.000421 
l0: 0.000415, l1: 0.000425, l2: 0.000448, l3: 0.000482, l4: 0.000814, l5: 0.001155, l6: 0.003013
[epoch: 228/1000, batch:   556/ 1052, ite: 59840] train loss: 0.005893, tar: 0.000421 
l0: 0.000356, l1: 0.000361, l2: 0.000384, l3: 0.000459, l4: 0.000560, l5: 0.001077, l6: 0.002258
[epoch: 228/1000, batch:   636/ 1052, ite: 59860] train loss: 0.005893, tar: 0.000421 
l0: 0.000238, l1: 0.000226, l2: 0.000265, l3: 0.000339, l4: 0.000543, l5: 0.001132, l6: 0.001601
[epoch: 228/1000, batch:   716/ 1052, ite: 59880] train loss: 0.005891, tar: 0.000421 
l0: 0.000251, l1: 0.000259, l2: 0.000269, l3: 0.000266, l4: 0.000328, l5: 0.000854, l6: 0.001617
[epoch: 228/1000, batch:   796/ 1052, ite: 59900] train loss: 0.005889, tar: 0.000420 
l0: 0.000336, l1: 0.000348, l2: 0.000363, l3: 0.000431, l4: 0.000495, l5: 0.001025, l6: 0.001696
[epoch: 228/1000, batch:   876/ 1052, ite: 59920] train loss: 0.005886, tar: 0.000420 
l0: 0.000410, l1: 0.000401, l2: 0.000473, l3: 0.000494, l4: 0.000792, l5: 0.000934, l6: 0.002018
[epoch: 228/1000, batch:   956/ 1052, ite: 59940] train loss: 0.005887, tar: 0.000420 
l0: 0.000339, l1: 0.000340, l2: 0.000346, l3: 0.000397, l4: 0.000563, l5: 0.001259, l6: 0.002333
[epoch: 228/1000, batch:  1036/ 1052, ite: 59960] train loss: 0.005885, tar: 0.000420 
[Epoch 228/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000376, l1: 0.000369, l2: 0.000401, l3: 0.000457, l4: 0.000739, l5: 0.001038, l6: 0.002192
[epoch: 229/1000, batch:    64/ 1052, ite: 59980] train loss: 0.005884, tar: 0.000420 
l0: 0.000351, l1: 0.000365, l2: 0.000421, l3: 0.000446, l4: 0.000757, l5: 0.001360, l6: 0.002626
[epoch: 229/1000, batch:   144/ 1052, ite: 60000] train loss: 0.005884, tar: 0.000419 
l0: 0.000879, l1: 0.000911, l2: 0.001182, l3: 0.000890, l4: 0.001170, l5: 0.001936, l6: 0.003206
[epoch: 229/1000, batch:   224/ 1052, ite: 60020] train loss: 0.005884, tar: 0.000419 
l0: 0.000481, l1: 0.000491, l2: 0.000576, l3: 0.000531, l4: 0.000737, l5: 0.001091, l6: 0.001476
[epoch: 229/1000, batch:   304/ 1052, ite: 60040] train loss: 0.005883, tar: 0.000419 
l0: 0.000198, l1: 0.000199, l2: 0.000213, l3: 0.000284, l4: 0.000677, l5: 0.001178, l6: 0.001344
[epoch: 229/1000, batch:   384/ 1052, ite: 60060] train loss: 0.005881, tar: 0.000419 
l0: 0.000311, l1: 0.000335, l2: 0.000355, l3: 0.000406, l4: 0.000525, l5: 0.000769, l6: 0.001698
[epoch: 229/1000, batch:   464/ 1052, ite: 60080] train loss: 0.005882, tar: 0.000419 
l0: 0.000739, l1: 0.000862, l2: 0.000894, l3: 0.000758, l4: 0.001202, l5: 0.001442, l6: 0.002754
[epoch: 229/1000, batch:   544/ 1052, ite: 60100] train loss: 0.005883, tar: 0.000419 
l0: 0.000301, l1: 0.000302, l2: 0.000325, l3: 0.000390, l4: 0.000660, l5: 0.001296, l6: 0.002395
[epoch: 229/1000, batch:   624/ 1052, ite: 60120] train loss: 0.005883, tar: 0.000419 
l0: 0.000637, l1: 0.000681, l2: 0.000619, l3: 0.000649, l4: 0.000851, l5: 0.001480, l6: 0.003331
[epoch: 229/1000, batch:   704/ 1052, ite: 60140] train loss: 0.005880, tar: 0.000419 
l0: 0.000369, l1: 0.000370, l2: 0.000405, l3: 0.000461, l4: 0.000731, l5: 0.001161, l6: 0.002619
[epoch: 229/1000, batch:   784/ 1052, ite: 60160] train loss: 0.005880, tar: 0.000419 
l0: 0.000675, l1: 0.000677, l2: 0.000745, l3: 0.000759, l4: 0.001051, l5: 0.001973, l6: 0.003445
[epoch: 229/1000, batch:   864/ 1052, ite: 60180] train loss: 0.005878, tar: 0.000418 
l0: 0.000569, l1: 0.000562, l2: 0.000602, l3: 0.000717, l4: 0.000914, l5: 0.001489, l6: 0.002145
[epoch: 229/1000, batch:   944/ 1052, ite: 60200] train loss: 0.005876, tar: 0.000418 
l0: 0.001132, l1: 0.001079, l2: 0.001019, l3: 0.001274, l4: 0.001040, l5: 0.001923, l6: 0.002862
[epoch: 229/1000, batch:  1024/ 1052, ite: 60220] train loss: 0.005877, tar: 0.000418 
[Epoch 229/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000509, l1: 0.000546, l2: 0.000528, l3: 0.000551, l4: 0.000840, l5: 0.001706, l6: 0.003707
[epoch: 230/1000, batch:    52/ 1052, ite: 60240] train loss: 0.005875, tar: 0.000418 
l0: 0.000303, l1: 0.000307, l2: 0.000325, l3: 0.000399, l4: 0.000729, l5: 0.001082, l6: 0.002187
[epoch: 230/1000, batch:   132/ 1052, ite: 60260] train loss: 0.005876, tar: 0.000418 
l0: 0.000216, l1: 0.000223, l2: 0.000217, l3: 0.000241, l4: 0.000373, l5: 0.000650, l6: 0.001167
[epoch: 230/1000, batch:   212/ 1052, ite: 60280] train loss: 0.005876, tar: 0.000418 
l0: 0.000597, l1: 0.000651, l2: 0.000593, l3: 0.000617, l4: 0.000746, l5: 0.001266, l6: 0.003013
[epoch: 230/1000, batch:   292/ 1052, ite: 60300] train loss: 0.005875, tar: 0.000418 
l0: 0.000208, l1: 0.000204, l2: 0.000230, l3: 0.000270, l4: 0.000333, l5: 0.000557, l6: 0.001141
[epoch: 230/1000, batch:   372/ 1052, ite: 60320] train loss: 0.005874, tar: 0.000418 
l0: 0.000202, l1: 0.000203, l2: 0.000218, l3: 0.000267, l4: 0.000451, l5: 0.000571, l6: 0.001442
[epoch: 230/1000, batch:   452/ 1052, ite: 60340] train loss: 0.005872, tar: 0.000418 
l0: 0.000207, l1: 0.000215, l2: 0.000215, l3: 0.000247, l4: 0.000339, l5: 0.000813, l6: 0.001173
[epoch: 230/1000, batch:   532/ 1052, ite: 60360] train loss: 0.005871, tar: 0.000417 
l0: 0.000415, l1: 0.000396, l2: 0.000449, l3: 0.000560, l4: 0.000682, l5: 0.001546, l6: 0.002446
[epoch: 230/1000, batch:   612/ 1052, ite: 60380] train loss: 0.005870, tar: 0.000417 
l0: 0.000531, l1: 0.000560, l2: 0.000609, l3: 0.000569, l4: 0.000832, l5: 0.001480, l6: 0.003528
[epoch: 230/1000, batch:   692/ 1052, ite: 60400] train loss: 0.005869, tar: 0.000417 
l0: 0.000209, l1: 0.000212, l2: 0.000289, l3: 0.000271, l4: 0.000425, l5: 0.000795, l6: 0.001303
[epoch: 230/1000, batch:   772/ 1052, ite: 60420] train loss: 0.005869, tar: 0.000417 
l0: 0.000673, l1: 0.000664, l2: 0.000674, l3: 0.000762, l4: 0.000778, l5: 0.001477, l6: 0.002804
[epoch: 230/1000, batch:   852/ 1052, ite: 60440] train loss: 0.005867, tar: 0.000417 
l0: 0.000283, l1: 0.000289, l2: 0.000291, l3: 0.000345, l4: 0.000462, l5: 0.001092, l6: 0.001989
[epoch: 230/1000, batch:   932/ 1052, ite: 60460] train loss: 0.005868, tar: 0.000417 
l0: 0.000370, l1: 0.000359, l2: 0.000398, l3: 0.000497, l4: 0.000689, l5: 0.000950, l6: 0.001993
[epoch: 230/1000, batch:  1012/ 1052, ite: 60480] train loss: 0.005866, tar: 0.000417 
[Epoch 230/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000417, l1: 0.000413, l2: 0.000453, l3: 0.000608, l4: 0.000845, l5: 0.001742, l6: 0.002324
[epoch: 231/1000, batch:    40/ 1052, ite: 60500] train loss: 0.005865, tar: 0.000416 
l0: 0.000458, l1: 0.000434, l2: 0.000489, l3: 0.000600, l4: 0.000644, l5: 0.000989, l6: 0.001457
[epoch: 231/1000, batch:   120/ 1052, ite: 60520] train loss: 0.005863, tar: 0.000416 
l0: 0.000376, l1: 0.000404, l2: 0.000410, l3: 0.000433, l4: 0.000736, l5: 0.001766, l6: 0.001862
[epoch: 231/1000, batch:   200/ 1052, ite: 60540] train loss: 0.005863, tar: 0.000416 
l0: 0.000197, l1: 0.000191, l2: 0.000227, l3: 0.000304, l4: 0.000464, l5: 0.000719, l6: 0.000930
[epoch: 231/1000, batch:   280/ 1052, ite: 60560] train loss: 0.005861, tar: 0.000416 
l0: 0.000316, l1: 0.000320, l2: 0.000358, l3: 0.000378, l4: 0.000746, l5: 0.001384, l6: 0.002546
[epoch: 231/1000, batch:   360/ 1052, ite: 60580] train loss: 0.005861, tar: 0.000416 
l0: 0.000390, l1: 0.000411, l2: 0.000444, l3: 0.000431, l4: 0.000668, l5: 0.001277, l6: 0.003231
[epoch: 231/1000, batch:   440/ 1052, ite: 60600] train loss: 0.005862, tar: 0.000416 
l0: 0.000275, l1: 0.000299, l2: 0.000310, l3: 0.000297, l4: 0.000458, l5: 0.000672, l6: 0.001685
[epoch: 231/1000, batch:   520/ 1052, ite: 60620] train loss: 0.005860, tar: 0.000416 
l0: 0.000490, l1: 0.000503, l2: 0.000524, l3: 0.000587, l4: 0.000951, l5: 0.002263, l6: 0.004080
[epoch: 231/1000, batch:   600/ 1052, ite: 60640] train loss: 0.005858, tar: 0.000415 
l0: 0.000386, l1: 0.000417, l2: 0.000358, l3: 0.000405, l4: 0.000526, l5: 0.001087, l6: 0.001971
[epoch: 231/1000, batch:   680/ 1052, ite: 60660] train loss: 0.005858, tar: 0.000415 
l0: 0.000400, l1: 0.000415, l2: 0.000386, l3: 0.000360, l4: 0.000422, l5: 0.001038, l6: 0.001397
[epoch: 231/1000, batch:   760/ 1052, ite: 60680] train loss: 0.005858, tar: 0.000415 
l0: 0.000701, l1: 0.000642, l2: 0.000681, l3: 0.000879, l4: 0.001110, l5: 0.001763, l6: 0.002734
[epoch: 231/1000, batch:   840/ 1052, ite: 60700] train loss: 0.005858, tar: 0.000415 
l0: 0.000247, l1: 0.000242, l2: 0.000270, l3: 0.000322, l4: 0.000580, l5: 0.000910, l6: 0.002225
[epoch: 231/1000, batch:   920/ 1052, ite: 60720] train loss: 0.005857, tar: 0.000415 
l0: 0.000431, l1: 0.000426, l2: 0.000463, l3: 0.000517, l4: 0.000660, l5: 0.000882, l6: 0.001819
[epoch: 231/1000, batch:  1000/ 1052, ite: 60740] train loss: 0.005854, tar: 0.000415 
[Epoch 231/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000249, l1: 0.000246, l2: 0.000278, l3: 0.000321, l4: 0.000480, l5: 0.000721, l6: 0.001935
[epoch: 232/1000, batch:    28/ 1052, ite: 60760] train loss: 0.005852, tar: 0.000415 
l0: 0.000360, l1: 0.000360, l2: 0.000397, l3: 0.000433, l4: 0.000680, l5: 0.001138, l6: 0.002022
[epoch: 232/1000, batch:   108/ 1052, ite: 60780] train loss: 0.005852, tar: 0.000415 
l0: 0.000273, l1: 0.000291, l2: 0.000291, l3: 0.000293, l4: 0.000442, l5: 0.000820, l6: 0.001113
[epoch: 232/1000, batch:   188/ 1052, ite: 60800] train loss: 0.005850, tar: 0.000415 
l0: 0.000411, l1: 0.000419, l2: 0.000434, l3: 0.000555, l4: 0.000770, l5: 0.001406, l6: 0.002230
[epoch: 232/1000, batch:   268/ 1052, ite: 60820] train loss: 0.005850, tar: 0.000414 
l0: 0.000253, l1: 0.000253, l2: 0.000282, l3: 0.000334, l4: 0.000438, l5: 0.000902, l6: 0.001358
[epoch: 232/1000, batch:   348/ 1052, ite: 60840] train loss: 0.005848, tar: 0.000414 
l0: 0.000956, l1: 0.000979, l2: 0.001001, l3: 0.001235, l4: 0.001754, l5: 0.003644, l6: 0.005766
[epoch: 232/1000, batch:   428/ 1052, ite: 60860] train loss: 0.005850, tar: 0.000414 
l0: 0.000244, l1: 0.000236, l2: 0.000253, l3: 0.000318, l4: 0.000432, l5: 0.000657, l6: 0.001072
[epoch: 232/1000, batch:   508/ 1052, ite: 60880] train loss: 0.005848, tar: 0.000414 
l0: 0.000308, l1: 0.000305, l2: 0.000333, l3: 0.000359, l4: 0.000769, l5: 0.000963, l6: 0.001875
[epoch: 232/1000, batch:   588/ 1052, ite: 60900] train loss: 0.005846, tar: 0.000414 
l0: 0.000329, l1: 0.000338, l2: 0.000359, l3: 0.000394, l4: 0.000513, l5: 0.001136, l6: 0.001541
[epoch: 232/1000, batch:   668/ 1052, ite: 60920] train loss: 0.005845, tar: 0.000414 
l0: 0.000347, l1: 0.000359, l2: 0.000341, l3: 0.000351, l4: 0.000510, l5: 0.001239, l6: 0.001985
[epoch: 232/1000, batch:   748/ 1052, ite: 60940] train loss: 0.005845, tar: 0.000414 
l0: 0.000847, l1: 0.000870, l2: 0.000871, l3: 0.001057, l4: 0.001543, l5: 0.003674, l6: 0.005752
[epoch: 232/1000, batch:   828/ 1052, ite: 60960] train loss: 0.005844, tar: 0.000413 
l0: 0.000414, l1: 0.000447, l2: 0.000450, l3: 0.000441, l4: 0.000647, l5: 0.001230, l6: 0.002141
[epoch: 232/1000, batch:   908/ 1052, ite: 60980] train loss: 0.005845, tar: 0.000413 
l0: 0.000243, l1: 0.000245, l2: 0.000272, l3: 0.000288, l4: 0.000436, l5: 0.000724, l6: 0.000987
[epoch: 232/1000, batch:   988/ 1052, ite: 61000] train loss: 0.005843, tar: 0.000413 
[Epoch 232/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000277, l1: 0.000293, l2: 0.000320, l3: 0.000346, l4: 0.000549, l5: 0.001065, l6: 0.001873
[epoch: 233/1000, batch:    16/ 1052, ite: 61020] train loss: 0.005842, tar: 0.000413 
l0: 0.000178, l1: 0.000183, l2: 0.000193, l3: 0.000205, l4: 0.000366, l5: 0.000606, l6: 0.001287
[epoch: 233/1000, batch:    96/ 1052, ite: 61040] train loss: 0.005842, tar: 0.000413 
l0: 0.000332, l1: 0.000330, l2: 0.000459, l3: 0.000603, l4: 0.000905, l5: 0.001790, l6: 0.003077
[epoch: 233/1000, batch:   176/ 1052, ite: 61060] train loss: 0.005841, tar: 0.000413 
l0: 0.000202, l1: 0.000200, l2: 0.000237, l3: 0.000279, l4: 0.000378, l5: 0.000926, l6: 0.001710
[epoch: 233/1000, batch:   256/ 1052, ite: 61080] train loss: 0.005841, tar: 0.000413 
l0: 0.000691, l1: 0.000675, l2: 0.000736, l3: 0.000810, l4: 0.001060, l5: 0.001512, l6: 0.002841
[epoch: 233/1000, batch:   336/ 1052, ite: 61100] train loss: 0.005841, tar: 0.000413 
l0: 0.000175, l1: 0.000181, l2: 0.000228, l3: 0.000183, l4: 0.000307, l5: 0.000461, l6: 0.000878
[epoch: 233/1000, batch:   416/ 1052, ite: 61120] train loss: 0.005840, tar: 0.000413 
l0: 0.000328, l1: 0.000325, l2: 0.000393, l3: 0.000387, l4: 0.000492, l5: 0.000606, l6: 0.001139
[epoch: 233/1000, batch:   496/ 1052, ite: 61140] train loss: 0.005839, tar: 0.000413 
l0: 0.000244, l1: 0.000241, l2: 0.000262, l3: 0.000290, l4: 0.000481, l5: 0.001034, l6: 0.001374
[epoch: 233/1000, batch:   576/ 1052, ite: 61160] train loss: 0.005841, tar: 0.000413 
l0: 0.000306, l1: 0.000316, l2: 0.000330, l3: 0.000435, l4: 0.000533, l5: 0.000951, l6: 0.001802
[epoch: 233/1000, batch:   656/ 1052, ite: 61180] train loss: 0.005847, tar: 0.000414 
l0: 0.000672, l1: 0.000707, l2: 0.001013, l3: 0.000674, l4: 0.001076, l5: 0.001639, l6: 0.001923
[epoch: 233/1000, batch:   736/ 1052, ite: 61200] train loss: 0.005861, tar: 0.000416 
l0: 0.000202, l1: 0.000207, l2: 0.000209, l3: 0.000227, l4: 0.000448, l5: 0.000775, l6: 0.001280
[epoch: 233/1000, batch:   816/ 1052, ite: 61220] train loss: 0.005861, tar: 0.000416 
l0: 0.000706, l1: 0.000690, l2: 0.000663, l3: 0.000763, l4: 0.001291, l5: 0.002320, l6: 0.002786
[epoch: 233/1000, batch:   896/ 1052, ite: 61240] train loss: 0.005862, tar: 0.000417 
l0: 0.000409, l1: 0.000414, l2: 0.000421, l3: 0.000492, l4: 0.000675, l5: 0.001301, l6: 0.001581
[epoch: 233/1000, batch:   976/ 1052, ite: 61260] train loss: 0.005866, tar: 0.000417 
[Epoch 233/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000430, l1: 0.000484, l2: 0.000455, l3: 0.000473, l4: 0.000549, l5: 0.001283, l6: 0.001797
[epoch: 234/1000, batch:     4/ 1052, ite: 61280] train loss: 0.005866, tar: 0.000417 
l0: 0.000431, l1: 0.000416, l2: 0.000472, l3: 0.000583, l4: 0.000581, l5: 0.001105, l6: 0.001576
[epoch: 234/1000, batch:    84/ 1052, ite: 61300] train loss: 0.005866, tar: 0.000417 
l0: 0.000593, l1: 0.000566, l2: 0.000714, l3: 0.000815, l4: 0.001163, l5: 0.002262, l6: 0.002939
[epoch: 234/1000, batch:   164/ 1052, ite: 61320] train loss: 0.005867, tar: 0.000417 
l0: 0.000808, l1: 0.000901, l2: 0.000816, l3: 0.000862, l4: 0.001011, l5: 0.001592, l6: 0.002375
[epoch: 234/1000, batch:   244/ 1052, ite: 61340] train loss: 0.005868, tar: 0.000417 
l0: 0.000352, l1: 0.000351, l2: 0.000508, l3: 0.000515, l4: 0.000423, l5: 0.001153, l6: 0.001886
[epoch: 234/1000, batch:   324/ 1052, ite: 61360] train loss: 0.005868, tar: 0.000418 
l0: 0.000556, l1: 0.000567, l2: 0.000580, l3: 0.000637, l4: 0.001116, l5: 0.001547, l6: 0.002288
[epoch: 234/1000, batch:   404/ 1052, ite: 61380] train loss: 0.005870, tar: 0.000418 
l0: 0.000268, l1: 0.000272, l2: 0.000254, l3: 0.000297, l4: 0.000536, l5: 0.001100, l6: 0.001646
[epoch: 234/1000, batch:   484/ 1052, ite: 61400] train loss: 0.005871, tar: 0.000418 
l0: 0.000266, l1: 0.000269, l2: 0.000284, l3: 0.000452, l4: 0.000481, l5: 0.000831, l6: 0.001570
[epoch: 234/1000, batch:   564/ 1052, ite: 61420] train loss: 0.005871, tar: 0.000418 
l0: 0.000359, l1: 0.000351, l2: 0.000382, l3: 0.000476, l4: 0.000751, l5: 0.001051, l6: 0.001899
[epoch: 234/1000, batch:   644/ 1052, ite: 61440] train loss: 0.005870, tar: 0.000418 
l0: 0.000267, l1: 0.000290, l2: 0.000242, l3: 0.000273, l4: 0.000512, l5: 0.001029, l6: 0.001191
[epoch: 234/1000, batch:   724/ 1052, ite: 61460] train loss: 0.005870, tar: 0.000418 
l0: 0.000269, l1: 0.000289, l2: 0.000314, l3: 0.000298, l4: 0.000470, l5: 0.000947, l6: 0.001856
[epoch: 234/1000, batch:   804/ 1052, ite: 61480] train loss: 0.005870, tar: 0.000418 
l0: 0.000229, l1: 0.000234, l2: 0.000232, l3: 0.000298, l4: 0.000490, l5: 0.000810, l6: 0.001468
[epoch: 234/1000, batch:   884/ 1052, ite: 61500] train loss: 0.005869, tar: 0.000418 
l0: 0.000590, l1: 0.000578, l2: 0.000621, l3: 0.000649, l4: 0.000759, l5: 0.001480, l6: 0.003049
[epoch: 234/1000, batch:   964/ 1052, ite: 61520] train loss: 0.005868, tar: 0.000417 
l0: 0.000778, l1: 0.000773, l2: 0.000880, l3: 0.000927, l4: 0.001407, l5: 0.002375, l6: 0.004231
[epoch: 234/1000, batch:  1044/ 1052, ite: 61540] train loss: 0.005865, tar: 0.000417 
[Epoch 234/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000259, l1: 0.000274, l2: 0.000272, l3: 0.000300, l4: 0.000448, l5: 0.001160, l6: 0.001636
[epoch: 235/1000, batch:    72/ 1052, ite: 61560] train loss: 0.005867, tar: 0.000417 
l0: 0.000833, l1: 0.000879, l2: 0.000954, l3: 0.000968, l4: 0.001277, l5: 0.002081, l6: 0.003608
[epoch: 235/1000, batch:   152/ 1052, ite: 61580] train loss: 0.005866, tar: 0.000417 
l0: 0.000347, l1: 0.000355, l2: 0.000401, l3: 0.000386, l4: 0.000739, l5: 0.001254, l6: 0.002301
[epoch: 235/1000, batch:   232/ 1052, ite: 61600] train loss: 0.005866, tar: 0.000417 
l0: 0.000612, l1: 0.000663, l2: 0.000632, l3: 0.000607, l4: 0.000979, l5: 0.001776, l6: 0.003647
[epoch: 235/1000, batch:   312/ 1052, ite: 61620] train loss: 0.005866, tar: 0.000417 
l0: 0.000246, l1: 0.000252, l2: 0.000279, l3: 0.000310, l4: 0.000587, l5: 0.000846, l6: 0.002452
[epoch: 235/1000, batch:   392/ 1052, ite: 61640] train loss: 0.005865, tar: 0.000417 
l0: 0.000925, l1: 0.001007, l2: 0.000921, l3: 0.000933, l4: 0.001671, l5: 0.003713, l6: 0.005937
[epoch: 235/1000, batch:   472/ 1052, ite: 61660] train loss: 0.005864, tar: 0.000417 
l0: 0.000421, l1: 0.000422, l2: 0.000422, l3: 0.000463, l4: 0.000740, l5: 0.001603, l6: 0.002215
[epoch: 235/1000, batch:   552/ 1052, ite: 61680] train loss: 0.005864, tar: 0.000417 
l0: 0.000399, l1: 0.000376, l2: 0.000464, l3: 0.000499, l4: 0.000744, l5: 0.001117, l6: 0.002127
[epoch: 235/1000, batch:   632/ 1052, ite: 61700] train loss: 0.005863, tar: 0.000417 
l0: 0.000436, l1: 0.000492, l2: 0.000487, l3: 0.000453, l4: 0.000611, l5: 0.001406, l6: 0.002611
[epoch: 235/1000, batch:   712/ 1052, ite: 61720] train loss: 0.005862, tar: 0.000417 
l0: 0.000897, l1: 0.000921, l2: 0.000968, l3: 0.000961, l4: 0.001343, l5: 0.002409, l6: 0.003847
[epoch: 235/1000, batch:   792/ 1052, ite: 61740] train loss: 0.005863, tar: 0.000417 
l0: 0.000602, l1: 0.000605, l2: 0.000620, l3: 0.000663, l4: 0.000956, l5: 0.001501, l6: 0.002364
[epoch: 235/1000, batch:   872/ 1052, ite: 61760] train loss: 0.005863, tar: 0.000417 
l0: 0.000242, l1: 0.000245, l2: 0.000220, l3: 0.000282, l4: 0.000527, l5: 0.000829, l6: 0.001896
[epoch: 235/1000, batch:   952/ 1052, ite: 61780] train loss: 0.005860, tar: 0.000416 
l0: 0.000349, l1: 0.000382, l2: 0.000411, l3: 0.000363, l4: 0.000719, l5: 0.001396, l6: 0.001954
[epoch: 235/1000, batch:  1032/ 1052, ite: 61800] train loss: 0.005860, tar: 0.000416 
[Epoch 235/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000399, l1: 0.000377, l2: 0.000374, l3: 0.000502, l4: 0.000704, l5: 0.001056, l6: 0.001737
[epoch: 236/1000, batch:    60/ 1052, ite: 61820] train loss: 0.005859, tar: 0.000416 
l0: 0.000225, l1: 0.000226, l2: 0.000233, l3: 0.000243, l4: 0.000352, l5: 0.000926, l6: 0.001266
[epoch: 236/1000, batch:   140/ 1052, ite: 61840] train loss: 0.005857, tar: 0.000416 
l0: 0.000480, l1: 0.000480, l2: 0.000512, l3: 0.000533, l4: 0.000854, l5: 0.001199, l6: 0.002680
[epoch: 236/1000, batch:   220/ 1052, ite: 61860] train loss: 0.005857, tar: 0.000416 
l0: 0.000369, l1: 0.000363, l2: 0.000422, l3: 0.000444, l4: 0.000763, l5: 0.001427, l6: 0.002166
[epoch: 236/1000, batch:   300/ 1052, ite: 61880] train loss: 0.005855, tar: 0.000416 
l0: 0.000482, l1: 0.000476, l2: 0.000506, l3: 0.000614, l4: 0.000984, l5: 0.002296, l6: 0.003491
[epoch: 236/1000, batch:   380/ 1052, ite: 61900] train loss: 0.005854, tar: 0.000416 
l0: 0.000198, l1: 0.000201, l2: 0.000217, l3: 0.000286, l4: 0.000524, l5: 0.000680, l6: 0.001411
[epoch: 236/1000, batch:   460/ 1052, ite: 61920] train loss: 0.005854, tar: 0.000416 
l0: 0.000286, l1: 0.000282, l2: 0.000317, l3: 0.000344, l4: 0.000615, l5: 0.001291, l6: 0.002019
[epoch: 236/1000, batch:   540/ 1052, ite: 61940] train loss: 0.005852, tar: 0.000415 
l0: 0.000465, l1: 0.000505, l2: 0.000492, l3: 0.000492, l4: 0.000684, l5: 0.001317, l6: 0.003218
[epoch: 236/1000, batch:   620/ 1052, ite: 61960] train loss: 0.005852, tar: 0.000415 
l0: 0.000555, l1: 0.000545, l2: 0.000587, l3: 0.000646, l4: 0.000956, l5: 0.001654, l6: 0.001927
[epoch: 236/1000, batch:   700/ 1052, ite: 61980] train loss: 0.005852, tar: 0.000415 
l0: 0.000438, l1: 0.000434, l2: 0.000466, l3: 0.000510, l4: 0.000837, l5: 0.001515, l6: 0.002186
[epoch: 236/1000, batch:   780/ 1052, ite: 62000] train loss: 0.005853, tar: 0.000415 
l0: 0.000242, l1: 0.000243, l2: 0.000293, l3: 0.000319, l4: 0.000482, l5: 0.000926, l6: 0.001563
[epoch: 236/1000, batch:   860/ 1052, ite: 62020] train loss: 0.005854, tar: 0.000415 
l0: 0.000369, l1: 0.000377, l2: 0.000445, l3: 0.000414, l4: 0.000782, l5: 0.001752, l6: 0.002443
[epoch: 236/1000, batch:   940/ 1052, ite: 62040] train loss: 0.005853, tar: 0.000415 
l0: 0.000336, l1: 0.000321, l2: 0.000355, l3: 0.000434, l4: 0.000700, l5: 0.001468, l6: 0.002385
[epoch: 236/1000, batch:  1020/ 1052, ite: 62060] train loss: 0.005851, tar: 0.000415 
[Epoch 236/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000182, l1: 0.000190, l2: 0.000200, l3: 0.000233, l4: 0.000475, l5: 0.000570, l6: 0.000813
[epoch: 237/1000, batch:    48/ 1052, ite: 62080] train loss: 0.005850, tar: 0.000415 
l0: 0.000416, l1: 0.000417, l2: 0.000445, l3: 0.000557, l4: 0.000637, l5: 0.001751, l6: 0.002768
[epoch: 237/1000, batch:   128/ 1052, ite: 62100] train loss: 0.005851, tar: 0.000415 
l0: 0.000501, l1: 0.000523, l2: 0.000560, l3: 0.000576, l4: 0.000886, l5: 0.001303, l6: 0.002192
[epoch: 237/1000, batch:   208/ 1052, ite: 62120] train loss: 0.005851, tar: 0.000415 
l0: 0.000182, l1: 0.000175, l2: 0.000226, l3: 0.000241, l4: 0.000457, l5: 0.000644, l6: 0.001107
[epoch: 237/1000, batch:   288/ 1052, ite: 62140] train loss: 0.005850, tar: 0.000415 
l0: 0.000200, l1: 0.000195, l2: 0.000218, l3: 0.000258, l4: 0.000451, l5: 0.000882, l6: 0.001122
[epoch: 237/1000, batch:   368/ 1052, ite: 62160] train loss: 0.005852, tar: 0.000415 
l0: 0.000300, l1: 0.000307, l2: 0.000339, l3: 0.000331, l4: 0.000443, l5: 0.000843, l6: 0.001657
[epoch: 237/1000, batch:   448/ 1052, ite: 62180] train loss: 0.005851, tar: 0.000415 
l0: 0.000236, l1: 0.000216, l2: 0.000255, l3: 0.000325, l4: 0.000613, l5: 0.000915, l6: 0.001522
[epoch: 237/1000, batch:   528/ 1052, ite: 62200] train loss: 0.005853, tar: 0.000415 
l0: 0.000291, l1: 0.000263, l2: 0.000301, l3: 0.000450, l4: 0.000494, l5: 0.000939, l6: 0.001171
[epoch: 237/1000, batch:   608/ 1052, ite: 62220] train loss: 0.005853, tar: 0.000415 
l0: 0.000515, l1: 0.000518, l2: 0.000534, l3: 0.000604, l4: 0.000909, l5: 0.001327, l6: 0.002128
[epoch: 237/1000, batch:   688/ 1052, ite: 62240] train loss: 0.005852, tar: 0.000415 
l0: 0.000335, l1: 0.000351, l2: 0.000351, l3: 0.000345, l4: 0.000519, l5: 0.000778, l6: 0.000988
[epoch: 237/1000, batch:   768/ 1052, ite: 62260] train loss: 0.005851, tar: 0.000415 
l0: 0.000243, l1: 0.000243, l2: 0.000268, l3: 0.000291, l4: 0.000443, l5: 0.000869, l6: 0.001264
[epoch: 237/1000, batch:   848/ 1052, ite: 62280] train loss: 0.005848, tar: 0.000414 
l0: 0.000290, l1: 0.000303, l2: 0.000332, l3: 0.000319, l4: 0.000614, l5: 0.000906, l6: 0.001749
[epoch: 237/1000, batch:   928/ 1052, ite: 62300] train loss: 0.005850, tar: 0.000414 
l0: 0.000190, l1: 0.000194, l2: 0.000205, l3: 0.000210, l4: 0.000444, l5: 0.000766, l6: 0.001203
[epoch: 237/1000, batch:  1008/ 1052, ite: 62320] train loss: 0.005850, tar: 0.000414 
[Epoch 237/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000312, l1: 0.000355, l2: 0.000334, l3: 0.000289, l4: 0.000550, l5: 0.000992, l6: 0.001961
[epoch: 238/1000, batch:    36/ 1052, ite: 62340] train loss: 0.005850, tar: 0.000414 
l0: 0.000587, l1: 0.000623, l2: 0.000659, l3: 0.000717, l4: 0.001057, l5: 0.001558, l6: 0.003938
[epoch: 238/1000, batch:   116/ 1052, ite: 62360] train loss: 0.005850, tar: 0.000414 
l0: 0.000471, l1: 0.000468, l2: 0.000523, l3: 0.000581, l4: 0.000844, l5: 0.001491, l6: 0.001878
[epoch: 238/1000, batch:   196/ 1052, ite: 62380] train loss: 0.005848, tar: 0.000414 
l0: 0.000303, l1: 0.000315, l2: 0.000331, l3: 0.000394, l4: 0.000642, l5: 0.000840, l6: 0.001646
[epoch: 238/1000, batch:   276/ 1052, ite: 62400] train loss: 0.005847, tar: 0.000414 
l0: 0.000244, l1: 0.000243, l2: 0.000231, l3: 0.000288, l4: 0.000425, l5: 0.000788, l6: 0.001420
[epoch: 238/1000, batch:   356/ 1052, ite: 62420] train loss: 0.005847, tar: 0.000414 
l0: 0.000271, l1: 0.000285, l2: 0.000293, l3: 0.000289, l4: 0.000464, l5: 0.000847, l6: 0.001675
[epoch: 238/1000, batch:   436/ 1052, ite: 62440] train loss: 0.005846, tar: 0.000414 
l0: 0.000708, l1: 0.000742, l2: 0.000816, l3: 0.000790, l4: 0.001357, l5: 0.002879, l6: 0.005326
[epoch: 238/1000, batch:   516/ 1052, ite: 62460] train loss: 0.005847, tar: 0.000414 
l0: 0.000481, l1: 0.000484, l2: 0.000516, l3: 0.000611, l4: 0.000930, l5: 0.002064, l6: 0.002934
[epoch: 238/1000, batch:   596/ 1052, ite: 62480] train loss: 0.005846, tar: 0.000414 
l0: 0.000508, l1: 0.000524, l2: 0.000530, l3: 0.000599, l4: 0.001044, l5: 0.001492, l6: 0.003177
[epoch: 238/1000, batch:   676/ 1052, ite: 62500] train loss: 0.005847, tar: 0.000414 
l0: 0.000463, l1: 0.000463, l2: 0.000508, l3: 0.000595, l4: 0.000790, l5: 0.001808, l6: 0.002644
[epoch: 238/1000, batch:   756/ 1052, ite: 62520] train loss: 0.005848, tar: 0.000414 
l0: 0.000324, l1: 0.000339, l2: 0.000387, l3: 0.000401, l4: 0.000449, l5: 0.001243, l6: 0.002337
[epoch: 238/1000, batch:   836/ 1052, ite: 62540] train loss: 0.005846, tar: 0.000413 
l0: 0.000281, l1: 0.000287, l2: 0.000271, l3: 0.000316, l4: 0.000508, l5: 0.001243, l6: 0.001999
[epoch: 238/1000, batch:   916/ 1052, ite: 62560] train loss: 0.005845, tar: 0.000413 
l0: 0.000598, l1: 0.000605, l2: 0.000645, l3: 0.000744, l4: 0.001271, l5: 0.002529, l6: 0.005535
[epoch: 238/1000, batch:   996/ 1052, ite: 62580] train loss: 0.005844, tar: 0.000413 
[Epoch 238/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000231, l1: 0.000215, l2: 0.000241, l3: 0.000316, l4: 0.000335, l5: 0.000845, l6: 0.001628
[epoch: 239/1000, batch:    24/ 1052, ite: 62600] train loss: 0.005843, tar: 0.000413 
l0: 0.000357, l1: 0.000349, l2: 0.000406, l3: 0.000477, l4: 0.000735, l5: 0.001043, l6: 0.002408
[epoch: 239/1000, batch:   104/ 1052, ite: 62620] train loss: 0.005842, tar: 0.000413 
l0: 0.000456, l1: 0.000434, l2: 0.000456, l3: 0.000541, l4: 0.000719, l5: 0.001213, l6: 0.002951
[epoch: 239/1000, batch:   184/ 1052, ite: 62640] train loss: 0.005840, tar: 0.000413 
l0: 0.000781, l1: 0.000828, l2: 0.000896, l3: 0.000964, l4: 0.001661, l5: 0.002988, l6: 0.004472
[epoch: 239/1000, batch:   264/ 1052, ite: 62660] train loss: 0.005840, tar: 0.000413 
l0: 0.000256, l1: 0.000250, l2: 0.000269, l3: 0.000341, l4: 0.000555, l5: 0.001098, l6: 0.001225
[epoch: 239/1000, batch:   344/ 1052, ite: 62680] train loss: 0.005839, tar: 0.000413 
l0: 0.000218, l1: 0.000223, l2: 0.000248, l3: 0.000259, l4: 0.000342, l5: 0.000868, l6: 0.001397
[epoch: 239/1000, batch:   424/ 1052, ite: 62700] train loss: 0.005838, tar: 0.000412 
l0: 0.000388, l1: 0.000406, l2: 0.000417, l3: 0.000414, l4: 0.000688, l5: 0.001231, l6: 0.002507
[epoch: 239/1000, batch:   504/ 1052, ite: 62720] train loss: 0.005838, tar: 0.000412 
l0: 0.000475, l1: 0.000496, l2: 0.000530, l3: 0.000548, l4: 0.001082, l5: 0.002147, l6: 0.003790
[epoch: 239/1000, batch:   584/ 1052, ite: 62740] train loss: 0.005835, tar: 0.000412 
l0: 0.000399, l1: 0.000417, l2: 0.000473, l3: 0.000503, l4: 0.000577, l5: 0.001150, l6: 0.002449
[epoch: 239/1000, batch:   664/ 1052, ite: 62760] train loss: 0.005835, tar: 0.000412 
l0: 0.000393, l1: 0.000419, l2: 0.000455, l3: 0.000475, l4: 0.000704, l5: 0.001238, l6: 0.002721
[epoch: 239/1000, batch:   744/ 1052, ite: 62780] train loss: 0.005835, tar: 0.000412 
l0: 0.000356, l1: 0.000346, l2: 0.000343, l3: 0.000412, l4: 0.000598, l5: 0.000988, l6: 0.001733
[epoch: 239/1000, batch:   824/ 1052, ite: 62800] train loss: 0.005835, tar: 0.000412 
l0: 0.000235, l1: 0.000232, l2: 0.000281, l3: 0.000321, l4: 0.000529, l5: 0.001050, l6: 0.001751
[epoch: 239/1000, batch:   904/ 1052, ite: 62820] train loss: 0.005834, tar: 0.000412 
l0: 0.000201, l1: 0.000200, l2: 0.000219, l3: 0.000270, l4: 0.000331, l5: 0.000998, l6: 0.001627
[epoch: 239/1000, batch:   984/ 1052, ite: 62840] train loss: 0.005835, tar: 0.000412 
[Epoch 239/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000686, l1: 0.000708, l2: 0.000714, l3: 0.000958, l4: 0.001480, l5: 0.002443, l6: 0.004931
[epoch: 240/1000, batch:    12/ 1052, ite: 62860] train loss: 0.005833, tar: 0.000412 
l0: 0.000161, l1: 0.000163, l2: 0.000203, l3: 0.000219, l4: 0.000420, l5: 0.000698, l6: 0.000968
[epoch: 240/1000, batch:    92/ 1052, ite: 62880] train loss: 0.005832, tar: 0.000411 
l0: 0.000209, l1: 0.000202, l2: 0.000239, l3: 0.000291, l4: 0.000484, l5: 0.000821, l6: 0.001610
[epoch: 240/1000, batch:   172/ 1052, ite: 62900] train loss: 0.005831, tar: 0.000411 
l0: 0.000399, l1: 0.000401, l2: 0.000427, l3: 0.000492, l4: 0.000744, l5: 0.001167, l6: 0.001752
[epoch: 240/1000, batch:   252/ 1052, ite: 62920] train loss: 0.005831, tar: 0.000411 
l0: 0.000295, l1: 0.000297, l2: 0.000304, l3: 0.000347, l4: 0.000439, l5: 0.000594, l6: 0.001773
[epoch: 240/1000, batch:   332/ 1052, ite: 62940] train loss: 0.005830, tar: 0.000411 
l0: 0.000342, l1: 0.000364, l2: 0.000366, l3: 0.000422, l4: 0.000634, l5: 0.001038, l6: 0.001648
[epoch: 240/1000, batch:   412/ 1052, ite: 62960] train loss: 0.005829, tar: 0.000411 
l0: 0.000241, l1: 0.000251, l2: 0.000269, l3: 0.000271, l4: 0.000396, l5: 0.000813, l6: 0.001309
[epoch: 240/1000, batch:   492/ 1052, ite: 62980] train loss: 0.005828, tar: 0.000411 
l0: 0.000465, l1: 0.000455, l2: 0.000482, l3: 0.000599, l4: 0.000933, l5: 0.001492, l6: 0.002287
[epoch: 240/1000, batch:   572/ 1052, ite: 63000] train loss: 0.005829, tar: 0.000411 
l0: 0.000446, l1: 0.000481, l2: 0.000494, l3: 0.000468, l4: 0.000739, l5: 0.001344, l6: 0.002588
[epoch: 240/1000, batch:   652/ 1052, ite: 63020] train loss: 0.005829, tar: 0.000411 
l0: 0.000261, l1: 0.000259, l2: 0.000333, l3: 0.000334, l4: 0.000538, l5: 0.001035, l6: 0.002911
[epoch: 240/1000, batch:   732/ 1052, ite: 63040] train loss: 0.005828, tar: 0.000411 
l0: 0.000273, l1: 0.000264, l2: 0.000291, l3: 0.000376, l4: 0.000553, l5: 0.001081, l6: 0.001807
[epoch: 240/1000, batch:   812/ 1052, ite: 63060] train loss: 0.005826, tar: 0.000410 
l0: 0.000366, l1: 0.000382, l2: 0.000367, l3: 0.000388, l4: 0.000545, l5: 0.000768, l6: 0.002443
[epoch: 240/1000, batch:   892/ 1052, ite: 63080] train loss: 0.005825, tar: 0.000410 
l0: 0.000381, l1: 0.000382, l2: 0.000426, l3: 0.000539, l4: 0.000743, l5: 0.001785, l6: 0.002247
[epoch: 240/1000, batch:   972/ 1052, ite: 63100] train loss: 0.005824, tar: 0.000410 
l0: 0.000297, l1: 0.000289, l2: 0.000296, l3: 0.000355, l4: 0.000578, l5: 0.000914, l6: 0.001724
[epoch: 240/1000, batch:  1052/ 1052, ite: 63120] train loss: 0.005822, tar: 0.000410 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_63120.pkl
[Epoch 240/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000178, l1: 0.000183, l2: 0.000189, l3: 0.000217, l4: 0.000312, l5: 0.000727, l6: 0.000782
[epoch: 241/1000, batch:    80/ 1052, ite: 63140] train loss: 0.004918, tar: 0.000315 
l0: 0.000286, l1: 0.000294, l2: 0.000293, l3: 0.000326, l4: 0.000617, l5: 0.001056, l6: 0.001406
[epoch: 241/1000, batch:   160/ 1052, ite: 63160] train loss: 0.005320, tar: 0.000332 
l0: 0.000680, l1: 0.000696, l2: 0.000679, l3: 0.000679, l4: 0.000945, l5: 0.001683, l6: 0.003412
[epoch: 241/1000, batch:   240/ 1052, ite: 63180] train loss: 0.005732, tar: 0.000363 
l0: 0.000448, l1: 0.000433, l2: 0.000475, l3: 0.000528, l4: 0.000967, l5: 0.001239, l6: 0.002207
[epoch: 241/1000, batch:   320/ 1052, ite: 63200] train loss: 0.005633, tar: 0.000356 
l0: 0.000327, l1: 0.000330, l2: 0.000314, l3: 0.000333, l4: 0.000452, l5: 0.000693, l6: 0.001372
[epoch: 241/1000, batch:   400/ 1052, ite: 63220] train loss: 0.005507, tar: 0.000346 
l0: 0.000275, l1: 0.000273, l2: 0.000297, l3: 0.000328, l4: 0.000388, l5: 0.000632, l6: 0.001905
[epoch: 241/1000, batch:   480/ 1052, ite: 63240] train loss: 0.005622, tar: 0.000353 
l0: 0.000654, l1: 0.000677, l2: 0.000738, l3: 0.000709, l4: 0.001061, l5: 0.001763, l6: 0.003440
[epoch: 241/1000, batch:   560/ 1052, ite: 63260] train loss: 0.005609, tar: 0.000352 
l0: 0.000271, l1: 0.000286, l2: 0.000285, l3: 0.000331, l4: 0.000617, l5: 0.001271, l6: 0.002246
[epoch: 241/1000, batch:   640/ 1052, ite: 63280] train loss: 0.005570, tar: 0.000350 
l0: 0.000372, l1: 0.000374, l2: 0.000372, l3: 0.000419, l4: 0.000587, l5: 0.001558, l6: 0.002429
[epoch: 241/1000, batch:   720/ 1052, ite: 63300] train loss: 0.005570, tar: 0.000351 
l0: 0.000281, l1: 0.000278, l2: 0.000305, l3: 0.000390, l4: 0.000540, l5: 0.000971, l6: 0.001265
[epoch: 241/1000, batch:   800/ 1052, ite: 63320] train loss: 0.005541, tar: 0.000352 
l0: 0.000166, l1: 0.000161, l2: 0.000194, l3: 0.000234, l4: 0.000335, l5: 0.000723, l6: 0.001165
[epoch: 241/1000, batch:   880/ 1052, ite: 63340] train loss: 0.005441, tar: 0.000345 
l0: 0.000491, l1: 0.000488, l2: 0.000507, l3: 0.000497, l4: 0.000824, l5: 0.001815, l6: 0.002840
[epoch: 241/1000, batch:   960/ 1052, ite: 63360] train loss: 0.005416, tar: 0.000344 
l0: 0.000366, l1: 0.000371, l2: 0.000384, l3: 0.000445, l4: 0.000500, l5: 0.000864, l6: 0.002165
[epoch: 241/1000, batch:  1040/ 1052, ite: 63380] train loss: 0.005448, tar: 0.000346 
[Epoch 241/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000427, l1: 0.000414, l2: 0.000505, l3: 0.000714, l4: 0.001113, l5: 0.001729, l6: 0.003843
[epoch: 242/1000, batch:    68/ 1052, ite: 63400] train loss: 0.005481, tar: 0.000348 
l0: 0.000413, l1: 0.000426, l2: 0.000425, l3: 0.000466, l4: 0.000784, l5: 0.001949, l6: 0.003221
[epoch: 242/1000, batch:   148/ 1052, ite: 63420] train loss: 0.005520, tar: 0.000350 
l0: 0.000278, l1: 0.000270, l2: 0.000308, l3: 0.000450, l4: 0.000798, l5: 0.000948, l6: 0.002026
[epoch: 242/1000, batch:   228/ 1052, ite: 63440] train loss: 0.005509, tar: 0.000349 
l0: 0.000154, l1: 0.000156, l2: 0.000172, l3: 0.000203, l4: 0.000453, l5: 0.000758, l6: 0.001310
[epoch: 242/1000, batch:   308/ 1052, ite: 63460] train loss: 0.005478, tar: 0.000346 
l0: 0.000463, l1: 0.000486, l2: 0.000477, l3: 0.000521, l4: 0.000827, l5: 0.001718, l6: 0.002755
[epoch: 242/1000, batch:   388/ 1052, ite: 63480] train loss: 0.005495, tar: 0.000349 
l0: 0.000543, l1: 0.000611, l2: 0.000650, l3: 0.000601, l4: 0.001185, l5: 0.002310, l6: 0.003106
[epoch: 242/1000, batch:   468/ 1052, ite: 63500] train loss: 0.005500, tar: 0.000348 
l0: 0.000341, l1: 0.000333, l2: 0.000339, l3: 0.000530, l4: 0.000926, l5: 0.001033, l6: 0.002532
[epoch: 242/1000, batch:   548/ 1052, ite: 63520] train loss: 0.005483, tar: 0.000348 
l0: 0.000440, l1: 0.000435, l2: 0.000399, l3: 0.000432, l4: 0.000601, l5: 0.000884, l6: 0.001371
[epoch: 242/1000, batch:   628/ 1052, ite: 63540] train loss: 0.005485, tar: 0.000348 
l0: 0.000329, l1: 0.000381, l2: 0.000359, l3: 0.000356, l4: 0.000414, l5: 0.001363, l6: 0.001946
[epoch: 242/1000, batch:   708/ 1052, ite: 63560] train loss: 0.005476, tar: 0.000348 
l0: 0.000187, l1: 0.000188, l2: 0.000236, l3: 0.000271, l4: 0.000416, l5: 0.001022, l6: 0.001563
[epoch: 242/1000, batch:   788/ 1052, ite: 63580] train loss: 0.005460, tar: 0.000347 
l0: 0.000078, l1: 0.000083, l2: 0.000108, l3: 0.000133, l4: 0.000174, l5: 0.000330, l6: 0.000610
[epoch: 242/1000, batch:   868/ 1052, ite: 63600] train loss: 0.005433, tar: 0.000346 
l0: 0.000279, l1: 0.000283, l2: 0.000345, l3: 0.000487, l4: 0.000771, l5: 0.001999, l6: 0.002698
[epoch: 242/1000, batch:   948/ 1052, ite: 63620] train loss: 0.005438, tar: 0.000345 
l0: 0.000397, l1: 0.000479, l2: 0.000348, l3: 0.000342, l4: 0.000638, l5: 0.001360, l6: 0.002281
[epoch: 242/1000, batch:  1028/ 1052, ite: 63640] train loss: 0.005443, tar: 0.000346 
[Epoch 242/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000251, l1: 0.000250, l2: 0.000256, l3: 0.000303, l4: 0.000443, l5: 0.000846, l6: 0.001311
[epoch: 243/1000, batch:    56/ 1052, ite: 63660] train loss: 0.005435, tar: 0.000346 
l0: 0.000222, l1: 0.000233, l2: 0.000264, l3: 0.000268, l4: 0.000393, l5: 0.000857, l6: 0.001900
[epoch: 243/1000, batch:   136/ 1052, ite: 63680] train loss: 0.005439, tar: 0.000346 
l0: 0.000343, l1: 0.000351, l2: 0.000392, l3: 0.000410, l4: 0.000690, l5: 0.001555, l6: 0.003807
[epoch: 243/1000, batch:   216/ 1052, ite: 63700] train loss: 0.005436, tar: 0.000346 
l0: 0.000501, l1: 0.000521, l2: 0.000475, l3: 0.000569, l4: 0.000942, l5: 0.001406, l6: 0.002543
[epoch: 243/1000, batch:   296/ 1052, ite: 63720] train loss: 0.005418, tar: 0.000344 
l0: 0.000434, l1: 0.000444, l2: 0.000434, l3: 0.000473, l4: 0.000764, l5: 0.001507, l6: 0.001931
[epoch: 243/1000, batch:   376/ 1052, ite: 63740] train loss: 0.005430, tar: 0.000346 
l0: 0.000196, l1: 0.000205, l2: 0.000234, l3: 0.000282, l4: 0.000472, l5: 0.000905, l6: 0.001297
[epoch: 243/1000, batch:   456/ 1052, ite: 63760] train loss: 0.005418, tar: 0.000344 
l0: 0.000366, l1: 0.000370, l2: 0.000395, l3: 0.000441, l4: 0.000707, l5: 0.001334, l6: 0.002459
[epoch: 243/1000, batch:   536/ 1052, ite: 63780] train loss: 0.005424, tar: 0.000344 
l0: 0.000234, l1: 0.000241, l2: 0.000232, l3: 0.000254, l4: 0.000514, l5: 0.000720, l6: 0.001117
[epoch: 243/1000, batch:   616/ 1052, ite: 63800] train loss: 0.005420, tar: 0.000344 
l0: 0.000278, l1: 0.000278, l2: 0.000302, l3: 0.000425, l4: 0.000725, l5: 0.001360, l6: 0.001842
[epoch: 243/1000, batch:   696/ 1052, ite: 63820] train loss: 0.005416, tar: 0.000344 
l0: 0.000486, l1: 0.000507, l2: 0.000550, l3: 0.000571, l4: 0.000874, l5: 0.001683, l6: 0.002693
[epoch: 243/1000, batch:   776/ 1052, ite: 63840] train loss: 0.005411, tar: 0.000343 
l0: 0.000220, l1: 0.000240, l2: 0.000193, l3: 0.000243, l4: 0.000338, l5: 0.000972, l6: 0.002022
[epoch: 243/1000, batch:   856/ 1052, ite: 63860] train loss: 0.005429, tar: 0.000345 
l0: 0.000453, l1: 0.000487, l2: 0.000481, l3: 0.000516, l4: 0.000881, l5: 0.001926, l6: 0.002889
[epoch: 243/1000, batch:   936/ 1052, ite: 63880] train loss: 0.005448, tar: 0.000345 
l0: 0.000127, l1: 0.000138, l2: 0.000141, l3: 0.000172, l4: 0.000224, l5: 0.000589, l6: 0.000930
[epoch: 243/1000, batch:  1016/ 1052, ite: 63900] train loss: 0.005434, tar: 0.000345 
[Epoch 243/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000227, l1: 0.000224, l2: 0.000234, l3: 0.000308, l4: 0.000502, l5: 0.001054, l6: 0.001361
[epoch: 244/1000, batch:    44/ 1052, ite: 63920] train loss: 0.005422, tar: 0.000344 
l0: 0.000446, l1: 0.000453, l2: 0.000480, l3: 0.000544, l4: 0.000878, l5: 0.001572, l6: 0.003432
[epoch: 244/1000, batch:   124/ 1052, ite: 63940] train loss: 0.005429, tar: 0.000345 
l0: 0.000166, l1: 0.000163, l2: 0.000182, l3: 0.000218, l4: 0.000464, l5: 0.000802, l6: 0.001204
[epoch: 244/1000, batch:   204/ 1052, ite: 63960] train loss: 0.005426, tar: 0.000345 
l0: 0.000698, l1: 0.000719, l2: 0.000669, l3: 0.000792, l4: 0.000949, l5: 0.001607, l6: 0.004992
[epoch: 244/1000, batch:   284/ 1052, ite: 63980] train loss: 0.005423, tar: 0.000345 
l0: 0.000342, l1: 0.000344, l2: 0.000363, l3: 0.000431, l4: 0.000711, l5: 0.000936, l6: 0.002331
[epoch: 244/1000, batch:   364/ 1052, ite: 64000] train loss: 0.005415, tar: 0.000344 
l0: 0.000291, l1: 0.000308, l2: 0.000331, l3: 0.000320, l4: 0.000568, l5: 0.001017, l6: 0.001806
[epoch: 244/1000, batch:   444/ 1052, ite: 64020] train loss: 0.005413, tar: 0.000343 
l0: 0.000303, l1: 0.000312, l2: 0.000325, l3: 0.000397, l4: 0.000562, l5: 0.001408, l6: 0.002378
[epoch: 244/1000, batch:   524/ 1052, ite: 64040] train loss: 0.005413, tar: 0.000344 
l0: 0.000139, l1: 0.000142, l2: 0.000196, l3: 0.000217, l4: 0.000315, l5: 0.000650, l6: 0.001139
[epoch: 244/1000, batch:   604/ 1052, ite: 64060] train loss: 0.005427, tar: 0.000344 
l0: 0.000106, l1: 0.000115, l2: 0.000137, l3: 0.000176, l4: 0.000245, l5: 0.000346, l6: 0.000971
[epoch: 244/1000, batch:   684/ 1052, ite: 64080] train loss: 0.005414, tar: 0.000343 
l0: 0.000211, l1: 0.000218, l2: 0.000225, l3: 0.000240, l4: 0.000420, l5: 0.001218, l6: 0.001781
[epoch: 244/1000, batch:   764/ 1052, ite: 64100] train loss: 0.005403, tar: 0.000343 
l0: 0.000251, l1: 0.000276, l2: 0.000260, l3: 0.000258, l4: 0.000367, l5: 0.000633, l6: 0.001304
[epoch: 244/1000, batch:   844/ 1052, ite: 64120] train loss: 0.005404, tar: 0.000342 
l0: 0.000248, l1: 0.000245, l2: 0.000286, l3: 0.000316, l4: 0.000463, l5: 0.001266, l6: 0.001823
[epoch: 244/1000, batch:   924/ 1052, ite: 64140] train loss: 0.005398, tar: 0.000342 
l0: 0.000276, l1: 0.000284, l2: 0.000307, l3: 0.000343, l4: 0.000441, l5: 0.000970, l6: 0.001245
[epoch: 244/1000, batch:  1004/ 1052, ite: 64160] train loss: 0.005412, tar: 0.000342 
[Epoch 244/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000186, l1: 0.000184, l2: 0.000229, l3: 0.000273, l4: 0.000452, l5: 0.000788, l6: 0.001716
[epoch: 245/1000, batch:    32/ 1052, ite: 64180] train loss: 0.005418, tar: 0.000343 
l0: 0.000469, l1: 0.000494, l2: 0.000535, l3: 0.000537, l4: 0.000743, l5: 0.001245, l6: 0.002615
[epoch: 245/1000, batch:   112/ 1052, ite: 64200] train loss: 0.005425, tar: 0.000343 
l0: 0.000214, l1: 0.000200, l2: 0.000212, l3: 0.000288, l4: 0.000450, l5: 0.001027, l6: 0.001692
[epoch: 245/1000, batch:   192/ 1052, ite: 64220] train loss: 0.005414, tar: 0.000342 
l0: 0.000282, l1: 0.000287, l2: 0.000299, l3: 0.000350, l4: 0.000629, l5: 0.000729, l6: 0.001417
[epoch: 245/1000, batch:   272/ 1052, ite: 64240] train loss: 0.005429, tar: 0.000343 
l0: 0.000458, l1: 0.000481, l2: 0.000491, l3: 0.000637, l4: 0.000883, l5: 0.002136, l6: 0.002374
[epoch: 245/1000, batch:   352/ 1052, ite: 64260] train loss: 0.005432, tar: 0.000343 
l0: 0.000722, l1: 0.000708, l2: 0.000746, l3: 0.000862, l4: 0.000934, l5: 0.001718, l6: 0.003453
[epoch: 245/1000, batch:   432/ 1052, ite: 64280] train loss: 0.005436, tar: 0.000343 
l0: 0.000243, l1: 0.000255, l2: 0.000241, l3: 0.000309, l4: 0.000475, l5: 0.000981, l6: 0.001732
[epoch: 245/1000, batch:   512/ 1052, ite: 64300] train loss: 0.005432, tar: 0.000343 
l0: 0.000216, l1: 0.000215, l2: 0.000239, l3: 0.000309, l4: 0.000480, l5: 0.000997, l6: 0.001535
[epoch: 245/1000, batch:   592/ 1052, ite: 64320] train loss: 0.005426, tar: 0.000343 
l0: 0.000750, l1: 0.000775, l2: 0.000717, l3: 0.000833, l4: 0.001165, l5: 0.001923, l6: 0.003684
[epoch: 245/1000, batch:   672/ 1052, ite: 64340] train loss: 0.005426, tar: 0.000343 
l0: 0.000296, l1: 0.000293, l2: 0.000298, l3: 0.000411, l4: 0.000499, l5: 0.000993, l6: 0.001422
[epoch: 245/1000, batch:   752/ 1052, ite: 64360] train loss: 0.005425, tar: 0.000343 
l0: 0.000221, l1: 0.000221, l2: 0.000210, l3: 0.000287, l4: 0.000452, l5: 0.001079, l6: 0.001449
[epoch: 245/1000, batch:   832/ 1052, ite: 64380] train loss: 0.005420, tar: 0.000343 
l0: 0.000430, l1: 0.000447, l2: 0.000464, l3: 0.000516, l4: 0.000910, l5: 0.001962, l6: 0.002441
[epoch: 245/1000, batch:   912/ 1052, ite: 64400] train loss: 0.005417, tar: 0.000343 
l0: 0.000211, l1: 0.000204, l2: 0.000271, l3: 0.000315, l4: 0.000515, l5: 0.001093, l6: 0.001641
[epoch: 245/1000, batch:   992/ 1052, ite: 64420] train loss: 0.005415, tar: 0.000343 
[Epoch 245/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000325, l1: 0.000320, l2: 0.000319, l3: 0.000347, l4: 0.000540, l5: 0.000837, l6: 0.001862
[epoch: 246/1000, batch:    20/ 1052, ite: 64440] train loss: 0.005417, tar: 0.000343 
l0: 0.000592, l1: 0.000591, l2: 0.000654, l3: 0.000783, l4: 0.001222, l5: 0.002311, l6: 0.003090
[epoch: 246/1000, batch:   100/ 1052, ite: 64460] train loss: 0.005423, tar: 0.000343 
l0: 0.000212, l1: 0.000208, l2: 0.000268, l3: 0.000307, l4: 0.000393, l5: 0.000895, l6: 0.001297
[epoch: 246/1000, batch:   180/ 1052, ite: 64480] train loss: 0.005421, tar: 0.000343 
l0: 0.000707, l1: 0.000704, l2: 0.000830, l3: 0.000731, l4: 0.000945, l5: 0.001626, l6: 0.002192
[epoch: 246/1000, batch:   260/ 1052, ite: 64500] train loss: 0.005415, tar: 0.000343 
l0: 0.000435, l1: 0.000427, l2: 0.000506, l3: 0.000546, l4: 0.000686, l5: 0.001084, l6: 0.002069
[epoch: 246/1000, batch:   340/ 1052, ite: 64520] train loss: 0.005430, tar: 0.000344 
l0: 0.000143, l1: 0.000148, l2: 0.000158, l3: 0.000170, l4: 0.000255, l5: 0.000504, l6: 0.000882
[epoch: 246/1000, batch:   420/ 1052, ite: 64540] train loss: 0.005436, tar: 0.000344 
l0: 0.000454, l1: 0.000452, l2: 0.000488, l3: 0.000518, l4: 0.000934, l5: 0.001805, l6: 0.001916
[epoch: 246/1000, batch:   500/ 1052, ite: 64560] train loss: 0.005435, tar: 0.000344 
l0: 0.000187, l1: 0.000184, l2: 0.000231, l3: 0.000280, l4: 0.000407, l5: 0.000645, l6: 0.001492
[epoch: 246/1000, batch:   580/ 1052, ite: 64580] train loss: 0.005438, tar: 0.000344 
l0: 0.000627, l1: 0.000678, l2: 0.000753, l3: 0.000638, l4: 0.000810, l5: 0.001210, l6: 0.002314
[epoch: 246/1000, batch:   660/ 1052, ite: 64600] train loss: 0.005451, tar: 0.000346 
l0: 0.000518, l1: 0.000563, l2: 0.000591, l3: 0.000651, l4: 0.001242, l5: 0.002139, l6: 0.003530
[epoch: 246/1000, batch:   740/ 1052, ite: 64620] train loss: 0.005445, tar: 0.000345 
l0: 0.000417, l1: 0.000443, l2: 0.000485, l3: 0.000440, l4: 0.000671, l5: 0.001436, l6: 0.001746
[epoch: 246/1000, batch:   820/ 1052, ite: 64640] train loss: 0.005446, tar: 0.000346 
l0: 0.000154, l1: 0.000169, l2: 0.000187, l3: 0.000222, l4: 0.000319, l5: 0.000851, l6: 0.000980
[epoch: 246/1000, batch:   900/ 1052, ite: 64660] train loss: 0.005440, tar: 0.000346 
l0: 0.000300, l1: 0.000323, l2: 0.000320, l3: 0.000344, l4: 0.000540, l5: 0.000914, l6: 0.001752
[epoch: 246/1000, batch:   980/ 1052, ite: 64680] train loss: 0.005437, tar: 0.000346 
[Epoch 246/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000108, l1: 0.000104, l2: 0.000134, l3: 0.000149, l4: 0.000228, l5: 0.000530, l6: 0.000981
[epoch: 247/1000, batch:     8/ 1052, ite: 64700] train loss: 0.005432, tar: 0.000345 
l0: 0.000174, l1: 0.000163, l2: 0.000210, l3: 0.000275, l4: 0.000632, l5: 0.001244, l6: 0.001809
[epoch: 247/1000, batch:    88/ 1052, ite: 64720] train loss: 0.005432, tar: 0.000346 
l0: 0.000446, l1: 0.000483, l2: 0.000518, l3: 0.000472, l4: 0.000616, l5: 0.001270, l6: 0.001794
[epoch: 247/1000, batch:   168/ 1052, ite: 64740] train loss: 0.005434, tar: 0.000346 
l0: 0.000226, l1: 0.000211, l2: 0.000251, l3: 0.000332, l4: 0.000544, l5: 0.001347, l6: 0.001426
[epoch: 247/1000, batch:   248/ 1052, ite: 64760] train loss: 0.005428, tar: 0.000345 
l0: 0.000339, l1: 0.000342, l2: 0.000380, l3: 0.000463, l4: 0.000601, l5: 0.001320, l6: 0.001928
[epoch: 247/1000, batch:   328/ 1052, ite: 64780] train loss: 0.005433, tar: 0.000345 
l0: 0.000435, l1: 0.000442, l2: 0.000469, l3: 0.000589, l4: 0.000831, l5: 0.001624, l6: 0.002349
[epoch: 247/1000, batch:   408/ 1052, ite: 64800] train loss: 0.005429, tar: 0.000345 
l0: 0.000179, l1: 0.000182, l2: 0.000241, l3: 0.000254, l4: 0.000521, l5: 0.000919, l6: 0.001098
[epoch: 247/1000, batch:   488/ 1052, ite: 64820] train loss: 0.005434, tar: 0.000345 
l0: 0.000475, l1: 0.000480, l2: 0.000503, l3: 0.000570, l4: 0.000722, l5: 0.001261, l6: 0.002022
[epoch: 247/1000, batch:   568/ 1052, ite: 64840] train loss: 0.005433, tar: 0.000345 
l0: 0.000367, l1: 0.000362, l2: 0.000384, l3: 0.000434, l4: 0.000582, l5: 0.001158, l6: 0.002159
[epoch: 247/1000, batch:   648/ 1052, ite: 64860] train loss: 0.005436, tar: 0.000346 
l0: 0.000415, l1: 0.000430, l2: 0.000454, l3: 0.000500, l4: 0.000700, l5: 0.001227, l6: 0.002710
[epoch: 247/1000, batch:   728/ 1052, ite: 64880] train loss: 0.005438, tar: 0.000346 
l0: 0.000543, l1: 0.000590, l2: 0.000640, l3: 0.000644, l4: 0.001054, l5: 0.001767, l6: 0.003689
[epoch: 247/1000, batch:   808/ 1052, ite: 64900] train loss: 0.005433, tar: 0.000345 
l0: 0.000245, l1: 0.000244, l2: 0.000254, l3: 0.000304, l4: 0.000591, l5: 0.001031, l6: 0.001269
[epoch: 247/1000, batch:   888/ 1052, ite: 64920] train loss: 0.005438, tar: 0.000346 
l0: 0.000252, l1: 0.000269, l2: 0.000307, l3: 0.000397, l4: 0.000698, l5: 0.001170, l6: 0.001904
[epoch: 247/1000, batch:   968/ 1052, ite: 64940] train loss: 0.005436, tar: 0.000346 
l0: 0.000297, l1: 0.000313, l2: 0.000289, l3: 0.000336, l4: 0.000376, l5: 0.000840, l6: 0.001477
[epoch: 247/1000, batch:  1048/ 1052, ite: 64960] train loss: 0.005436, tar: 0.000346 
[Epoch 247/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000316, l1: 0.000303, l2: 0.000308, l3: 0.000384, l4: 0.000545, l5: 0.000788, l6: 0.001361
[epoch: 248/1000, batch:    76/ 1052, ite: 64980] train loss: 0.005430, tar: 0.000346 
l0: 0.000419, l1: 0.000439, l2: 0.000437, l3: 0.000438, l4: 0.000567, l5: 0.001176, l6: 0.001761
[epoch: 248/1000, batch:   156/ 1052, ite: 65000] train loss: 0.005433, tar: 0.000347 
l0: 0.000293, l1: 0.000312, l2: 0.000293, l3: 0.000310, l4: 0.000473, l5: 0.000984, l6: 0.001014
[epoch: 248/1000, batch:   236/ 1052, ite: 65020] train loss: 0.005438, tar: 0.000347 
l0: 0.000366, l1: 0.000349, l2: 0.000374, l3: 0.000512, l4: 0.000598, l5: 0.001473, l6: 0.001999
[epoch: 248/1000, batch:   316/ 1052, ite: 65040] train loss: 0.005455, tar: 0.000349 
l0: 0.000460, l1: 0.000502, l2: 0.000502, l3: 0.000459, l4: 0.000642, l5: 0.001067, l6: 0.002264
[epoch: 248/1000, batch:   396/ 1052, ite: 65060] train loss: 0.005459, tar: 0.000349 
l0: 0.000199, l1: 0.000206, l2: 0.000227, l3: 0.000298, l4: 0.000363, l5: 0.000748, l6: 0.001076
[epoch: 248/1000, batch:   476/ 1052, ite: 65080] train loss: 0.005472, tar: 0.000350 
l0: 0.000206, l1: 0.000238, l2: 0.000250, l3: 0.000233, l4: 0.000593, l5: 0.001016, l6: 0.001256
[epoch: 248/1000, batch:   556/ 1052, ite: 65100] train loss: 0.005477, tar: 0.000351 
l0: 0.000321, l1: 0.000316, l2: 0.000261, l3: 0.000545, l4: 0.000475, l5: 0.000903, l6: 0.001345
[epoch: 248/1000, batch:   636/ 1052, ite: 65120] train loss: 0.005473, tar: 0.000350 
l0: 0.000251, l1: 0.000269, l2: 0.000267, l3: 0.000278, l4: 0.000364, l5: 0.000543, l6: 0.001143
[epoch: 248/1000, batch:   716/ 1052, ite: 65140] train loss: 0.005476, tar: 0.000351 
l0: 0.000542, l1: 0.000544, l2: 0.000544, l3: 0.000615, l4: 0.000958, l5: 0.001553, l6: 0.003284
[epoch: 248/1000, batch:   796/ 1052, ite: 65160] train loss: 0.005476, tar: 0.000351 
l0: 0.000267, l1: 0.000263, l2: 0.000284, l3: 0.000391, l4: 0.000523, l5: 0.000824, l6: 0.001809
[epoch: 248/1000, batch:   876/ 1052, ite: 65180] train loss: 0.005471, tar: 0.000351 
l0: 0.000429, l1: 0.000457, l2: 0.000470, l3: 0.000488, l4: 0.000627, l5: 0.001267, l6: 0.001652
[epoch: 248/1000, batch:   956/ 1052, ite: 65200] train loss: 0.005472, tar: 0.000351 
l0: 0.000290, l1: 0.000274, l2: 0.000331, l3: 0.000434, l4: 0.000656, l5: 0.000718, l6: 0.001488
[epoch: 248/1000, batch:  1036/ 1052, ite: 65220] train loss: 0.005473, tar: 0.000351 
[Epoch 248/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000362, l1: 0.000377, l2: 0.000427, l3: 0.000490, l4: 0.000651, l5: 0.001507, l6: 0.003029
[epoch: 249/1000, batch:    64/ 1052, ite: 65240] train loss: 0.005474, tar: 0.000351 
l0: 0.000222, l1: 0.000222, l2: 0.000233, l3: 0.000306, l4: 0.000497, l5: 0.000834, l6: 0.001479
[epoch: 249/1000, batch:   144/ 1052, ite: 65260] train loss: 0.005479, tar: 0.000351 
l0: 0.000420, l1: 0.000420, l2: 0.000443, l3: 0.000544, l4: 0.000636, l5: 0.001449, l6: 0.002973
[epoch: 249/1000, batch:   224/ 1052, ite: 65280] train loss: 0.005485, tar: 0.000351 
l0: 0.000219, l1: 0.000218, l2: 0.000246, l3: 0.000284, l4: 0.000566, l5: 0.001118, l6: 0.001365
[epoch: 249/1000, batch:   304/ 1052, ite: 65300] train loss: 0.005484, tar: 0.000351 
l0: 0.000248, l1: 0.000250, l2: 0.000272, l3: 0.000356, l4: 0.000434, l5: 0.000845, l6: 0.000965
[epoch: 249/1000, batch:   384/ 1052, ite: 65320] train loss: 0.005486, tar: 0.000351 
l0: 0.000424, l1: 0.000436, l2: 0.000483, l3: 0.000451, l4: 0.000712, l5: 0.001077, l6: 0.002368
[epoch: 249/1000, batch:   464/ 1052, ite: 65340] train loss: 0.005487, tar: 0.000351 
l0: 0.000237, l1: 0.000230, l2: 0.000269, l3: 0.000312, l4: 0.000385, l5: 0.000764, l6: 0.001324
[epoch: 249/1000, batch:   544/ 1052, ite: 65360] train loss: 0.005487, tar: 0.000351 
l0: 0.000498, l1: 0.000509, l2: 0.000548, l3: 0.000539, l4: 0.000589, l5: 0.000995, l6: 0.002073
[epoch: 249/1000, batch:   624/ 1052, ite: 65380] train loss: 0.005490, tar: 0.000352 
l0: 0.000216, l1: 0.000224, l2: 0.000228, l3: 0.000239, l4: 0.000370, l5: 0.000961, l6: 0.001718
[epoch: 249/1000, batch:   704/ 1052, ite: 65400] train loss: 0.005483, tar: 0.000351 
l0: 0.000432, l1: 0.000431, l2: 0.000441, l3: 0.000540, l4: 0.000875, l5: 0.001216, l6: 0.002553
[epoch: 249/1000, batch:   784/ 1052, ite: 65420] train loss: 0.005483, tar: 0.000351 
l0: 0.000230, l1: 0.000220, l2: 0.000255, l3: 0.000275, l4: 0.000616, l5: 0.000786, l6: 0.001429
[epoch: 249/1000, batch:   864/ 1052, ite: 65440] train loss: 0.005475, tar: 0.000351 
l0: 0.000473, l1: 0.000509, l2: 0.000508, l3: 0.000515, l4: 0.000875, l5: 0.001920, l6: 0.003660
[epoch: 249/1000, batch:   944/ 1052, ite: 65460] train loss: 0.005471, tar: 0.000350 
l0: 0.000225, l1: 0.000254, l2: 0.000249, l3: 0.000228, l4: 0.000483, l5: 0.000975, l6: 0.001048
[epoch: 249/1000, batch:  1024/ 1052, ite: 65480] train loss: 0.005466, tar: 0.000350 
[Epoch 249/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000788, l1: 0.000783, l2: 0.000812, l3: 0.001098, l4: 0.001339, l5: 0.001922, l6: 0.003290
[epoch: 250/1000, batch:    52/ 1052, ite: 65500] train loss: 0.005476, tar: 0.000351 
l0: 0.000351, l1: 0.000358, l2: 0.000375, l3: 0.000411, l4: 0.000595, l5: 0.001636, l6: 0.002599
[epoch: 250/1000, batch:   132/ 1052, ite: 65520] train loss: 0.005477, tar: 0.000351 
l0: 0.000340, l1: 0.000261, l2: 0.000362, l3: 0.000520, l4: 0.000667, l5: 0.000857, l6: 0.001191
[epoch: 250/1000, batch:   212/ 1052, ite: 65540] train loss: 0.005474, tar: 0.000351 
l0: 0.000577, l1: 0.000587, l2: 0.000579, l3: 0.000625, l4: 0.000986, l5: 0.001716, l6: 0.003283
[epoch: 250/1000, batch:   292/ 1052, ite: 65560] train loss: 0.005482, tar: 0.000351 
l0: 0.000171, l1: 0.000175, l2: 0.000190, l3: 0.000239, l4: 0.000357, l5: 0.000781, l6: 0.001351
[epoch: 250/1000, batch:   372/ 1052, ite: 65580] train loss: 0.005485, tar: 0.000351 
l0: 0.000392, l1: 0.000388, l2: 0.000441, l3: 0.000624, l4: 0.000795, l5: 0.001252, l6: 0.002620
[epoch: 250/1000, batch:   452/ 1052, ite: 65600] train loss: 0.005480, tar: 0.000351 
l0: 0.000197, l1: 0.000200, l2: 0.000236, l3: 0.000244, l4: 0.000465, l5: 0.001083, l6: 0.001574
[epoch: 250/1000, batch:   532/ 1052, ite: 65620] train loss: 0.005473, tar: 0.000350 
l0: 0.000159, l1: 0.000156, l2: 0.000183, l3: 0.000221, l4: 0.000319, l5: 0.000578, l6: 0.001169
[epoch: 250/1000, batch:   612/ 1052, ite: 65640] train loss: 0.005466, tar: 0.000350 
l0: 0.000278, l1: 0.000280, l2: 0.000327, l3: 0.000368, l4: 0.000601, l5: 0.001242, l6: 0.002109
[epoch: 250/1000, batch:   692/ 1052, ite: 65660] train loss: 0.005464, tar: 0.000350 
l0: 0.000306, l1: 0.000308, l2: 0.000312, l3: 0.000369, l4: 0.000493, l5: 0.000831, l6: 0.001439
[epoch: 250/1000, batch:   772/ 1052, ite: 65680] train loss: 0.005459, tar: 0.000349 
l0: 0.000396, l1: 0.000380, l2: 0.000474, l3: 0.000689, l4: 0.000965, l5: 0.002172, l6: 0.002968
[epoch: 250/1000, batch:   852/ 1052, ite: 65700] train loss: 0.005463, tar: 0.000350 
l0: 0.000323, l1: 0.000343, l2: 0.000358, l3: 0.000338, l4: 0.000676, l5: 0.001333, l6: 0.001583
[epoch: 250/1000, batch:   932/ 1052, ite: 65720] train loss: 0.005460, tar: 0.000349 
l0: 0.000418, l1: 0.000430, l2: 0.000476, l3: 0.000599, l4: 0.000996, l5: 0.001850, l6: 0.002640
[epoch: 250/1000, batch:  1012/ 1052, ite: 65740] train loss: 0.005465, tar: 0.000349 
[Epoch 250/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000244, l1: 0.000256, l2: 0.000268, l3: 0.000286, l4: 0.000516, l5: 0.000922, l6: 0.001654
[epoch: 251/1000, batch:    40/ 1052, ite: 65760] train loss: 0.005460, tar: 0.000349 
l0: 0.000350, l1: 0.000358, l2: 0.000404, l3: 0.000456, l4: 0.000543, l5: 0.001229, l6: 0.001962
[epoch: 251/1000, batch:   120/ 1052, ite: 65780] train loss: 0.005463, tar: 0.000349 
l0: 0.000251, l1: 0.000272, l2: 0.000282, l3: 0.000263, l4: 0.000509, l5: 0.000759, l6: 0.001959
[epoch: 251/1000, batch:   200/ 1052, ite: 65800] train loss: 0.005463, tar: 0.000349 
l0: 0.000205, l1: 0.000209, l2: 0.000245, l3: 0.000307, l4: 0.000466, l5: 0.000611, l6: 0.001646
[epoch: 251/1000, batch:   280/ 1052, ite: 65820] train loss: 0.005462, tar: 0.000349 
l0: 0.000350, l1: 0.000362, l2: 0.000379, l3: 0.000346, l4: 0.000733, l5: 0.001014, l6: 0.001785
[epoch: 251/1000, batch:   360/ 1052, ite: 65840] train loss: 0.005457, tar: 0.000348 
l0: 0.000333, l1: 0.000313, l2: 0.000336, l3: 0.000406, l4: 0.000641, l5: 0.001074, l6: 0.001716
[epoch: 251/1000, batch:   440/ 1052, ite: 65860] train loss: 0.005455, tar: 0.000348 
l0: 0.000273, l1: 0.000278, l2: 0.000314, l3: 0.000387, l4: 0.000671, l5: 0.001418, l6: 0.002481
[epoch: 251/1000, batch:   520/ 1052, ite: 65880] train loss: 0.005456, tar: 0.000348 
l0: 0.000205, l1: 0.000202, l2: 0.000223, l3: 0.000243, l4: 0.000344, l5: 0.000813, l6: 0.001558
[epoch: 251/1000, batch:   600/ 1052, ite: 65900] train loss: 0.005454, tar: 0.000348 
l0: 0.000157, l1: 0.000171, l2: 0.000184, l3: 0.000205, l4: 0.000353, l5: 0.000882, l6: 0.001424
[epoch: 251/1000, batch:   680/ 1052, ite: 65920] train loss: 0.005451, tar: 0.000348 
l0: 0.000218, l1: 0.000216, l2: 0.000243, l3: 0.000324, l4: 0.000517, l5: 0.000883, l6: 0.002082
[epoch: 251/1000, batch:   760/ 1052, ite: 65940] train loss: 0.005445, tar: 0.000347 
l0: 0.000197, l1: 0.000209, l2: 0.000199, l3: 0.000250, l4: 0.000357, l5: 0.000579, l6: 0.001644
[epoch: 251/1000, batch:   840/ 1052, ite: 65960] train loss: 0.005445, tar: 0.000347 
l0: 0.000263, l1: 0.000260, l2: 0.000283, l3: 0.000330, l4: 0.000584, l5: 0.001234, l6: 0.001664
[epoch: 251/1000, batch:   920/ 1052, ite: 65980] train loss: 0.005443, tar: 0.000347 
l0: 0.000498, l1: 0.000487, l2: 0.000583, l3: 0.000620, l4: 0.001258, l5: 0.001567, l6: 0.002257
[epoch: 251/1000, batch:  1000/ 1052, ite: 66000] train loss: 0.005449, tar: 0.000347 
[Epoch 251/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000195, l1: 0.000195, l2: 0.000236, l3: 0.000267, l4: 0.000565, l5: 0.000730, l6: 0.001520
[epoch: 252/1000, batch:    28/ 1052, ite: 66020] train loss: 0.005446, tar: 0.000348 
l0: 0.000444, l1: 0.000448, l2: 0.000521, l3: 0.000607, l4: 0.000905, l5: 0.001398, l6: 0.002464
[epoch: 252/1000, batch:   108/ 1052, ite: 66040] train loss: 0.005444, tar: 0.000347 
l0: 0.000401, l1: 0.000425, l2: 0.000461, l3: 0.000440, l4: 0.000792, l5: 0.001500, l6: 0.002730
[epoch: 252/1000, batch:   188/ 1052, ite: 66060] train loss: 0.005442, tar: 0.000347 
l0: 0.000271, l1: 0.000291, l2: 0.000300, l3: 0.000267, l4: 0.000482, l5: 0.000887, l6: 0.001332
[epoch: 252/1000, batch:   268/ 1052, ite: 66080] train loss: 0.005441, tar: 0.000347 
l0: 0.000503, l1: 0.000520, l2: 0.000525, l3: 0.000512, l4: 0.000686, l5: 0.000951, l6: 0.001737
[epoch: 252/1000, batch:   348/ 1052, ite: 66100] train loss: 0.005441, tar: 0.000347 
l0: 0.000335, l1: 0.000335, l2: 0.000351, l3: 0.000418, l4: 0.000570, l5: 0.001259, l6: 0.001587
[epoch: 252/1000, batch:   428/ 1052, ite: 66120] train loss: 0.005442, tar: 0.000347 
l0: 0.000432, l1: 0.000433, l2: 0.000446, l3: 0.000501, l4: 0.000890, l5: 0.001605, l6: 0.002894
[epoch: 252/1000, batch:   508/ 1052, ite: 66140] train loss: 0.005441, tar: 0.000347 
l0: 0.000360, l1: 0.000366, l2: 0.000371, l3: 0.000415, l4: 0.000559, l5: 0.000971, l6: 0.002386
[epoch: 252/1000, batch:   588/ 1052, ite: 66160] train loss: 0.005441, tar: 0.000347 
l0: 0.000273, l1: 0.000276, l2: 0.000296, l3: 0.000329, l4: 0.000562, l5: 0.000839, l6: 0.001510
[epoch: 252/1000, batch:   668/ 1052, ite: 66180] train loss: 0.005445, tar: 0.000347 
l0: 0.000149, l1: 0.000164, l2: 0.000202, l3: 0.000195, l4: 0.000359, l5: 0.000644, l6: 0.000613
[epoch: 252/1000, batch:   748/ 1052, ite: 66200] train loss: 0.005448, tar: 0.000347 
l0: 0.000788, l1: 0.000722, l2: 0.000624, l3: 0.000911, l4: 0.000977, l5: 0.001695, l6: 0.002327
[epoch: 252/1000, batch:   828/ 1052, ite: 66220] train loss: 0.005451, tar: 0.000347 
l0: 0.000443, l1: 0.000475, l2: 0.000471, l3: 0.000455, l4: 0.000670, l5: 0.001259, l6: 0.002316
[epoch: 252/1000, batch:   908/ 1052, ite: 66240] train loss: 0.005450, tar: 0.000347 
l0: 0.000212, l1: 0.000213, l2: 0.000248, l3: 0.000267, l4: 0.000359, l5: 0.000706, l6: 0.000942
[epoch: 252/1000, batch:   988/ 1052, ite: 66260] train loss: 0.005448, tar: 0.000347 
[Epoch 252/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000507, l1: 0.000516, l2: 0.000590, l3: 0.000560, l4: 0.000731, l5: 0.001216, l6: 0.002458
[epoch: 253/1000, batch:    16/ 1052, ite: 66280] train loss: 0.005446, tar: 0.000347 
l0: 0.000447, l1: 0.000475, l2: 0.000446, l3: 0.000404, l4: 0.000566, l5: 0.000823, l6: 0.001557
[epoch: 253/1000, batch:    96/ 1052, ite: 66300] train loss: 0.005442, tar: 0.000346 
l0: 0.000487, l1: 0.000479, l2: 0.000547, l3: 0.000659, l4: 0.000905, l5: 0.001386, l6: 0.002459
[epoch: 253/1000, batch:   176/ 1052, ite: 66320] train loss: 0.005438, tar: 0.000346 
l0: 0.000251, l1: 0.000266, l2: 0.000231, l3: 0.000287, l4: 0.000566, l5: 0.000983, l6: 0.001787
[epoch: 253/1000, batch:   256/ 1052, ite: 66340] train loss: 0.005440, tar: 0.000346 
l0: 0.000540, l1: 0.000542, l2: 0.000572, l3: 0.000646, l4: 0.000834, l5: 0.001457, l6: 0.002892
[epoch: 253/1000, batch:   336/ 1052, ite: 66360] train loss: 0.005443, tar: 0.000347 
l0: 0.000463, l1: 0.000440, l2: 0.000522, l3: 0.000645, l4: 0.000923, l5: 0.001789, l6: 0.002818
[epoch: 253/1000, batch:   416/ 1052, ite: 66380] train loss: 0.005445, tar: 0.000347 
l0: 0.000306, l1: 0.000312, l2: 0.000300, l3: 0.000354, l4: 0.000436, l5: 0.001026, l6: 0.001492
[epoch: 253/1000, batch:   496/ 1052, ite: 66400] train loss: 0.005442, tar: 0.000347 
l0: 0.000262, l1: 0.000275, l2: 0.000293, l3: 0.000326, l4: 0.000621, l5: 0.001230, l6: 0.001356
[epoch: 253/1000, batch:   576/ 1052, ite: 66420] train loss: 0.005443, tar: 0.000347 
l0: 0.000530, l1: 0.000551, l2: 0.000589, l3: 0.000690, l4: 0.000787, l5: 0.001528, l6: 0.002311
[epoch: 253/1000, batch:   656/ 1052, ite: 66440] train loss: 0.005442, tar: 0.000347 
l0: 0.000183, l1: 0.000190, l2: 0.000193, l3: 0.000223, l4: 0.000252, l5: 0.000760, l6: 0.001213
[epoch: 253/1000, batch:   736/ 1052, ite: 66460] train loss: 0.005440, tar: 0.000346 
l0: 0.000283, l1: 0.000280, l2: 0.000306, l3: 0.000378, l4: 0.000498, l5: 0.000935, l6: 0.001508
[epoch: 253/1000, batch:   816/ 1052, ite: 66480] train loss: 0.005441, tar: 0.000346 
l0: 0.000330, l1: 0.000339, l2: 0.000345, l3: 0.000429, l4: 0.000787, l5: 0.001717, l6: 0.002762
[epoch: 253/1000, batch:   896/ 1052, ite: 66500] train loss: 0.005439, tar: 0.000346 
l0: 0.000196, l1: 0.000194, l2: 0.000210, l3: 0.000249, l4: 0.000506, l5: 0.000891, l6: 0.000980
[epoch: 253/1000, batch:   976/ 1052, ite: 66520] train loss: 0.005441, tar: 0.000346 
[Epoch 253/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000162, l1: 0.000168, l2: 0.000179, l3: 0.000184, l4: 0.000284, l5: 0.000797, l6: 0.001169
[epoch: 254/1000, batch:     4/ 1052, ite: 66540] train loss: 0.005446, tar: 0.000346 
l0: 0.000243, l1: 0.000254, l2: 0.000247, l3: 0.000257, l4: 0.000390, l5: 0.000890, l6: 0.001146
[epoch: 254/1000, batch:    84/ 1052, ite: 66560] train loss: 0.005448, tar: 0.000346 
l0: 0.000701, l1: 0.000726, l2: 0.000790, l3: 0.000854, l4: 0.001222, l5: 0.002132, l6: 0.004604
[epoch: 254/1000, batch:   164/ 1052, ite: 66580] train loss: 0.005449, tar: 0.000346 
l0: 0.000344, l1: 0.000374, l2: 0.000370, l3: 0.000392, l4: 0.000678, l5: 0.000916, l6: 0.002755
[epoch: 254/1000, batch:   244/ 1052, ite: 66600] train loss: 0.005448, tar: 0.000346 
l0: 0.000232, l1: 0.000238, l2: 0.000232, l3: 0.000263, l4: 0.000438, l5: 0.000969, l6: 0.001055
[epoch: 254/1000, batch:   324/ 1052, ite: 66620] train loss: 0.005446, tar: 0.000346 
l0: 0.000425, l1: 0.000442, l2: 0.000533, l3: 0.000500, l4: 0.000694, l5: 0.001293, l6: 0.003238
[epoch: 254/1000, batch:   404/ 1052, ite: 66640] train loss: 0.005444, tar: 0.000346 
l0: 0.000481, l1: 0.000487, l2: 0.000519, l3: 0.000593, l4: 0.000862, l5: 0.002127, l6: 0.002967
[epoch: 254/1000, batch:   484/ 1052, ite: 66660] train loss: 0.005445, tar: 0.000346 
l0: 0.000354, l1: 0.000372, l2: 0.000360, l3: 0.000404, l4: 0.000676, l5: 0.001528, l6: 0.003355
[epoch: 254/1000, batch:   564/ 1052, ite: 66680] train loss: 0.005441, tar: 0.000346 
l0: 0.000143, l1: 0.000142, l2: 0.000175, l3: 0.000261, l4: 0.000379, l5: 0.000536, l6: 0.001214
[epoch: 254/1000, batch:   644/ 1052, ite: 66700] train loss: 0.005443, tar: 0.000346 
l0: 0.000354, l1: 0.000360, l2: 0.000374, l3: 0.000419, l4: 0.000516, l5: 0.001174, l6: 0.002498
[epoch: 254/1000, batch:   724/ 1052, ite: 66720] train loss: 0.005443, tar: 0.000346 
l0: 0.000340, l1: 0.000372, l2: 0.000392, l3: 0.000396, l4: 0.000609, l5: 0.001232, l6: 0.002503
[epoch: 254/1000, batch:   804/ 1052, ite: 66740] train loss: 0.005441, tar: 0.000345 
l0: 0.000284, l1: 0.000278, l2: 0.000300, l3: 0.000442, l4: 0.000551, l5: 0.000941, l6: 0.002118
[epoch: 254/1000, batch:   884/ 1052, ite: 66760] train loss: 0.005440, tar: 0.000345 
l0: 0.000237, l1: 0.000228, l2: 0.000225, l3: 0.000314, l4: 0.000483, l5: 0.000586, l6: 0.001136
[epoch: 254/1000, batch:   964/ 1052, ite: 66780] train loss: 0.005437, tar: 0.000345 
l0: 0.000580, l1: 0.000586, l2: 0.000636, l3: 0.000688, l4: 0.000928, l5: 0.003041, l6: 0.003312
[epoch: 254/1000, batch:  1044/ 1052, ite: 66800] train loss: 0.005437, tar: 0.000345 
[Epoch 254/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000307, l1: 0.000316, l2: 0.000331, l3: 0.000396, l4: 0.000463, l5: 0.001096, l6: 0.002324
[epoch: 255/1000, batch:    72/ 1052, ite: 66820] train loss: 0.005435, tar: 0.000345 
l0: 0.000278, l1: 0.000268, l2: 0.000351, l3: 0.000400, l4: 0.000627, l5: 0.000957, l6: 0.002566
[epoch: 255/1000, batch:   152/ 1052, ite: 66840] train loss: 0.005439, tar: 0.000345 
l0: 0.000323, l1: 0.000320, l2: 0.000344, l3: 0.000399, l4: 0.000861, l5: 0.001548, l6: 0.002326
[epoch: 255/1000, batch:   232/ 1052, ite: 66860] train loss: 0.005437, tar: 0.000345 
l0: 0.000394, l1: 0.000422, l2: 0.000413, l3: 0.000499, l4: 0.000823, l5: 0.002077, l6: 0.003718
[epoch: 255/1000, batch:   312/ 1052, ite: 66880] train loss: 0.005444, tar: 0.000345 
l0: 0.000297, l1: 0.000321, l2: 0.000333, l3: 0.000319, l4: 0.000624, l5: 0.001059, l6: 0.001642
[epoch: 255/1000, batch:   392/ 1052, ite: 66900] train loss: 0.005446, tar: 0.000346 
l0: 0.000305, l1: 0.000313, l2: 0.000318, l3: 0.000364, l4: 0.000582, l5: 0.001353, l6: 0.001898
[epoch: 255/1000, batch:   472/ 1052, ite: 66920] train loss: 0.005447, tar: 0.000346 
l0: 0.000273, l1: 0.000271, l2: 0.000325, l3: 0.000345, l4: 0.000573, l5: 0.001284, l6: 0.001569
[epoch: 255/1000, batch:   552/ 1052, ite: 66940] train loss: 0.005446, tar: 0.000346 
l0: 0.000510, l1: 0.000554, l2: 0.000528, l3: 0.000602, l4: 0.000939, l5: 0.002099, l6: 0.003009
[epoch: 255/1000, batch:   632/ 1052, ite: 66960] train loss: 0.005449, tar: 0.000346 
l0: 0.000854, l1: 0.000867, l2: 0.000896, l3: 0.000946, l4: 0.001115, l5: 0.001617, l6: 0.002937
[epoch: 255/1000, batch:   712/ 1052, ite: 66980] train loss: 0.005452, tar: 0.000347 
l0: 0.000804, l1: 0.000806, l2: 0.000806, l3: 0.000989, l4: 0.001010, l5: 0.001716, l6: 0.003641
[epoch: 255/1000, batch:   792/ 1052, ite: 67000] train loss: 0.005454, tar: 0.000347 
l0: 0.000341, l1: 0.000378, l2: 0.000326, l3: 0.000405, l4: 0.000595, l5: 0.000781, l6: 0.001319
[epoch: 255/1000, batch:   872/ 1052, ite: 67020] train loss: 0.005454, tar: 0.000348 
l0: 0.000405, l1: 0.000423, l2: 0.000429, l3: 0.000454, l4: 0.000667, l5: 0.001277, l6: 0.002868
[epoch: 255/1000, batch:   952/ 1052, ite: 67040] train loss: 0.005457, tar: 0.000348 
l0: 0.000252, l1: 0.000245, l2: 0.000282, l3: 0.000351, l4: 0.000518, l5: 0.001003, l6: 0.001080
[epoch: 255/1000, batch:  1032/ 1052, ite: 67060] train loss: 0.005455, tar: 0.000348 
[Epoch 255/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000214, l1: 0.000213, l2: 0.000226, l3: 0.000326, l4: 0.000426, l5: 0.000418, l6: 0.001253
[epoch: 256/1000, batch:    60/ 1052, ite: 67080] train loss: 0.005459, tar: 0.000349 
l0: 0.000389, l1: 0.000394, l2: 0.000386, l3: 0.000443, l4: 0.000651, l5: 0.001224, l6: 0.001937
[epoch: 256/1000, batch:   140/ 1052, ite: 67100] train loss: 0.005456, tar: 0.000348 
l0: 0.000503, l1: 0.000504, l2: 0.000552, l3: 0.000674, l4: 0.001001, l5: 0.002062, l6: 0.004019
[epoch: 256/1000, batch:   220/ 1052, ite: 67120] train loss: 0.005460, tar: 0.000348 
l0: 0.000191, l1: 0.000189, l2: 0.000192, l3: 0.000264, l4: 0.000312, l5: 0.000741, l6: 0.000922
[epoch: 256/1000, batch:   300/ 1052, ite: 67140] train loss: 0.005458, tar: 0.000348 
l0: 0.000342, l1: 0.000348, l2: 0.000346, l3: 0.000400, l4: 0.000775, l5: 0.001369, l6: 0.002753
[epoch: 256/1000, batch:   380/ 1052, ite: 67160] train loss: 0.005458, tar: 0.000348 
l0: 0.000278, l1: 0.000269, l2: 0.000318, l3: 0.000409, l4: 0.000601, l5: 0.001107, l6: 0.001465
[epoch: 256/1000, batch:   460/ 1052, ite: 67180] train loss: 0.005457, tar: 0.000348 
l0: 0.000200, l1: 0.000200, l2: 0.000248, l3: 0.000272, l4: 0.000466, l5: 0.000815, l6: 0.001310
[epoch: 256/1000, batch:   540/ 1052, ite: 67200] train loss: 0.005458, tar: 0.000348 
l0: 0.000241, l1: 0.000252, l2: 0.000244, l3: 0.000277, l4: 0.000468, l5: 0.000866, l6: 0.001192
[epoch: 256/1000, batch:   620/ 1052, ite: 67220] train loss: 0.005456, tar: 0.000348 
l0: 0.000346, l1: 0.000372, l2: 0.000403, l3: 0.000388, l4: 0.000759, l5: 0.001247, l6: 0.003552
[epoch: 256/1000, batch:   700/ 1052, ite: 67240] train loss: 0.005455, tar: 0.000348 
l0: 0.000327, l1: 0.000313, l2: 0.000392, l3: 0.000555, l4: 0.000702, l5: 0.001427, l6: 0.001754
[epoch: 256/1000, batch:   780/ 1052, ite: 67260] train loss: 0.005458, tar: 0.000348 
l0: 0.000254, l1: 0.000260, l2: 0.000297, l3: 0.000318, l4: 0.000484, l5: 0.000897, l6: 0.001558
[epoch: 256/1000, batch:   860/ 1052, ite: 67280] train loss: 0.005455, tar: 0.000348 
l0: 0.000494, l1: 0.000493, l2: 0.000553, l3: 0.000620, l4: 0.001038, l5: 0.001237, l6: 0.002186
[epoch: 256/1000, batch:   940/ 1052, ite: 67300] train loss: 0.005458, tar: 0.000349 
l0: 0.000924, l1: 0.000913, l2: 0.001074, l3: 0.001274, l4: 0.001357, l5: 0.002946, l6: 0.004758
[epoch: 256/1000, batch:  1020/ 1052, ite: 67320] train loss: 0.005461, tar: 0.000349 
[Epoch 256/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000208, l1: 0.000214, l2: 0.000223, l3: 0.000235, l4: 0.000461, l5: 0.000534, l6: 0.001149
[epoch: 257/1000, batch:    48/ 1052, ite: 67340] train loss: 0.005460, tar: 0.000349 
l0: 0.000359, l1: 0.000351, l2: 0.000367, l3: 0.000469, l4: 0.000696, l5: 0.000925, l6: 0.001902
[epoch: 257/1000, batch:   128/ 1052, ite: 67360] train loss: 0.005458, tar: 0.000349 
l0: 0.000393, l1: 0.000381, l2: 0.000418, l3: 0.000536, l4: 0.000789, l5: 0.001588, l6: 0.002734
[epoch: 257/1000, batch:   208/ 1052, ite: 67380] train loss: 0.005455, tar: 0.000349 
l0: 0.000247, l1: 0.000263, l2: 0.000263, l3: 0.000282, l4: 0.000457, l5: 0.000644, l6: 0.001863
[epoch: 257/1000, batch:   288/ 1052, ite: 67400] train loss: 0.005453, tar: 0.000348 
l0: 0.000364, l1: 0.000386, l2: 0.000417, l3: 0.000467, l4: 0.000757, l5: 0.001664, l6: 0.003058
[epoch: 257/1000, batch:   368/ 1052, ite: 67420] train loss: 0.005453, tar: 0.000348 
l0: 0.000218, l1: 0.000217, l2: 0.000240, l3: 0.000276, l4: 0.000488, l5: 0.000938, l6: 0.001613
[epoch: 257/1000, batch:   448/ 1052, ite: 67440] train loss: 0.005452, tar: 0.000348 
l0: 0.000330, l1: 0.000346, l2: 0.000339, l3: 0.000348, l4: 0.000607, l5: 0.001518, l6: 0.002544
[epoch: 257/1000, batch:   528/ 1052, ite: 67460] train loss: 0.005456, tar: 0.000348 
l0: 0.000463, l1: 0.000463, l2: 0.000427, l3: 0.000556, l4: 0.000754, l5: 0.001208, l6: 0.001417
[epoch: 257/1000, batch:   608/ 1052, ite: 67480] train loss: 0.005457, tar: 0.000348 
l0: 0.000258, l1: 0.000270, l2: 0.000251, l3: 0.000273, l4: 0.000488, l5: 0.001065, l6: 0.001609
[epoch: 257/1000, batch:   688/ 1052, ite: 67500] train loss: 0.005454, tar: 0.000348 
l0: 0.000197, l1: 0.000202, l2: 0.000183, l3: 0.000244, l4: 0.000460, l5: 0.000687, l6: 0.001692
[epoch: 257/1000, batch:   768/ 1052, ite: 67520] train loss: 0.005455, tar: 0.000348 
l0: 0.000413, l1: 0.000436, l2: 0.000447, l3: 0.000461, l4: 0.000809, l5: 0.001404, l6: 0.002315
[epoch: 257/1000, batch:   848/ 1052, ite: 67540] train loss: 0.005456, tar: 0.000348 
l0: 0.000596, l1: 0.000608, l2: 0.000603, l3: 0.000743, l4: 0.001077, l5: 0.001448, l6: 0.003415
[epoch: 257/1000, batch:   928/ 1052, ite: 67560] train loss: 0.005457, tar: 0.000348 
l0: 0.000466, l1: 0.000500, l2: 0.000469, l3: 0.000550, l4: 0.000845, l5: 0.001537, l6: 0.003312
[epoch: 257/1000, batch:  1008/ 1052, ite: 67580] train loss: 0.005456, tar: 0.000348 
[Epoch 257/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000244, l1: 0.000257, l2: 0.000244, l3: 0.000256, l4: 0.000480, l5: 0.000942, l6: 0.001862
[epoch: 258/1000, batch:    36/ 1052, ite: 67600] train loss: 0.005455, tar: 0.000348 
l0: 0.000339, l1: 0.000342, l2: 0.000340, l3: 0.000401, l4: 0.000607, l5: 0.001249, l6: 0.001312
[epoch: 258/1000, batch:   116/ 1052, ite: 67620] train loss: 0.005457, tar: 0.000348 
l0: 0.000386, l1: 0.000398, l2: 0.000414, l3: 0.000460, l4: 0.000741, l5: 0.001489, l6: 0.002517
[epoch: 258/1000, batch:   196/ 1052, ite: 67640] train loss: 0.005461, tar: 0.000349 
l0: 0.000450, l1: 0.000448, l2: 0.000506, l3: 0.000578, l4: 0.000967, l5: 0.001513, l6: 0.003726
[epoch: 258/1000, batch:   276/ 1052, ite: 67660] train loss: 0.005462, tar: 0.000349 
l0: 0.000205, l1: 0.000204, l2: 0.000227, l3: 0.000282, l4: 0.000458, l5: 0.000734, l6: 0.000955
[epoch: 258/1000, batch:   356/ 1052, ite: 67680] train loss: 0.005460, tar: 0.000349 
l0: 0.000328, l1: 0.000337, l2: 0.000332, l3: 0.000401, l4: 0.000715, l5: 0.001179, l6: 0.002317
[epoch: 258/1000, batch:   436/ 1052, ite: 67700] train loss: 0.005459, tar: 0.000348 
l0: 0.000481, l1: 0.000478, l2: 0.000499, l3: 0.000556, l4: 0.000842, l5: 0.001422, l6: 0.003051
[epoch: 258/1000, batch:   516/ 1052, ite: 67720] train loss: 0.005458, tar: 0.000348 
l0: 0.000245, l1: 0.000251, l2: 0.000261, l3: 0.000277, l4: 0.000427, l5: 0.000862, l6: 0.001380
[epoch: 258/1000, batch:   596/ 1052, ite: 67740] train loss: 0.005460, tar: 0.000348 
l0: 0.000936, l1: 0.000970, l2: 0.001000, l3: 0.001141, l4: 0.001864, l5: 0.003297, l6: 0.006106
[epoch: 258/1000, batch:   676/ 1052, ite: 67760] train loss: 0.005460, tar: 0.000348 
l0: 0.000187, l1: 0.000211, l2: 0.000207, l3: 0.000227, l4: 0.000458, l5: 0.000805, l6: 0.000989
[epoch: 258/1000, batch:   756/ 1052, ite: 67780] train loss: 0.005462, tar: 0.000348 
l0: 0.000238, l1: 0.000240, l2: 0.000277, l3: 0.000327, l4: 0.000501, l5: 0.001023, l6: 0.002387
[epoch: 258/1000, batch:   836/ 1052, ite: 67800] train loss: 0.005461, tar: 0.000348 
l0: 0.000213, l1: 0.000230, l2: 0.000253, l3: 0.000253, l4: 0.000364, l5: 0.000864, l6: 0.001980
[epoch: 258/1000, batch:   916/ 1052, ite: 67820] train loss: 0.005460, tar: 0.000348 
l0: 0.000295, l1: 0.000330, l2: 0.000336, l3: 0.000331, l4: 0.000602, l5: 0.001101, l6: 0.002600
[epoch: 258/1000, batch:   996/ 1052, ite: 67840] train loss: 0.005460, tar: 0.000348 
[Epoch 258/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000307, l1: 0.000321, l2: 0.000350, l3: 0.000379, l4: 0.000779, l5: 0.001680, l6: 0.002972
[epoch: 259/1000, batch:    24/ 1052, ite: 67860] train loss: 0.005456, tar: 0.000348 
l0: 0.000607, l1: 0.000633, l2: 0.000606, l3: 0.000670, l4: 0.000926, l5: 0.001325, l6: 0.002861
[epoch: 259/1000, batch:   104/ 1052, ite: 67880] train loss: 0.005457, tar: 0.000348 
l0: 0.000565, l1: 0.000560, l2: 0.000580, l3: 0.000774, l4: 0.001238, l5: 0.002434, l6: 0.003605
[epoch: 259/1000, batch:   184/ 1052, ite: 67900] train loss: 0.005456, tar: 0.000348 
l0: 0.000184, l1: 0.000195, l2: 0.000182, l3: 0.000216, l4: 0.000421, l5: 0.000543, l6: 0.001700
[epoch: 259/1000, batch:   264/ 1052, ite: 67920] train loss: 0.005457, tar: 0.000348 
l0: 0.000422, l1: 0.000438, l2: 0.000433, l3: 0.000485, l4: 0.000765, l5: 0.001497, l6: 0.003094
[epoch: 259/1000, batch:   344/ 1052, ite: 67940] train loss: 0.005456, tar: 0.000348 
l0: 0.000262, l1: 0.000288, l2: 0.000297, l3: 0.000352, l4: 0.000426, l5: 0.000899, l6: 0.002017
[epoch: 259/1000, batch:   424/ 1052, ite: 67960] train loss: 0.005456, tar: 0.000348 
l0: 0.000268, l1: 0.000264, l2: 0.000323, l3: 0.000407, l4: 0.000627, l5: 0.001502, l6: 0.002356
[epoch: 259/1000, batch:   504/ 1052, ite: 67980] train loss: 0.005453, tar: 0.000347 
l0: 0.000178, l1: 0.000182, l2: 0.000221, l3: 0.000288, l4: 0.000424, l5: 0.001088, l6: 0.001271
[epoch: 259/1000, batch:   584/ 1052, ite: 68000] train loss: 0.005451, tar: 0.000347 
l0: 0.000200, l1: 0.000196, l2: 0.000222, l3: 0.000268, l4: 0.000482, l5: 0.000889, l6: 0.001025
[epoch: 259/1000, batch:   664/ 1052, ite: 68020] train loss: 0.005449, tar: 0.000347 
l0: 0.000259, l1: 0.000268, l2: 0.000270, l3: 0.000302, l4: 0.000589, l5: 0.000796, l6: 0.001847
[epoch: 259/1000, batch:   744/ 1052, ite: 68040] train loss: 0.005447, tar: 0.000347 
l0: 0.000259, l1: 0.000256, l2: 0.000304, l3: 0.000315, l4: 0.000617, l5: 0.001021, l6: 0.001956
[epoch: 259/1000, batch:   824/ 1052, ite: 68060] train loss: 0.005449, tar: 0.000347 
l0: 0.000277, l1: 0.000225, l2: 0.000308, l3: 0.000426, l4: 0.000691, l5: 0.001398, l6: 0.001768
[epoch: 259/1000, batch:   904/ 1052, ite: 68080] train loss: 0.005449, tar: 0.000347 
l0: 0.000552, l1: 0.000588, l2: 0.000590, l3: 0.000537, l4: 0.000786, l5: 0.001127, l6: 0.002879
[epoch: 259/1000, batch:   984/ 1052, ite: 68100] train loss: 0.005446, tar: 0.000347 
[Epoch 259/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000269, l1: 0.000278, l2: 0.000309, l3: 0.000320, l4: 0.000458, l5: 0.000849, l6: 0.001552
[epoch: 260/1000, batch:    12/ 1052, ite: 68120] train loss: 0.005447, tar: 0.000347 
l0: 0.000264, l1: 0.000285, l2: 0.000273, l3: 0.000262, l4: 0.000574, l5: 0.001147, l6: 0.001837
[epoch: 260/1000, batch:    92/ 1052, ite: 68140] train loss: 0.005446, tar: 0.000347 
l0: 0.000287, l1: 0.000307, l2: 0.000321, l3: 0.000337, l4: 0.000579, l5: 0.000753, l6: 0.002002
[epoch: 260/1000, batch:   172/ 1052, ite: 68160] train loss: 0.005446, tar: 0.000346 
l0: 0.000172, l1: 0.000167, l2: 0.000228, l3: 0.000256, l4: 0.000437, l5: 0.000799, l6: 0.001259
[epoch: 260/1000, batch:   252/ 1052, ite: 68180] train loss: 0.005442, tar: 0.000346 
l0: 0.000322, l1: 0.000325, l2: 0.000336, l3: 0.000360, l4: 0.000633, l5: 0.001410, l6: 0.002457
[epoch: 260/1000, batch:   332/ 1052, ite: 68200] train loss: 0.005445, tar: 0.000346 
l0: 0.000332, l1: 0.000324, l2: 0.000405, l3: 0.000509, l4: 0.000742, l5: 0.001278, l6: 0.002211
[epoch: 260/1000, batch:   412/ 1052, ite: 68220] train loss: 0.005444, tar: 0.000346 
l0: 0.000192, l1: 0.000193, l2: 0.000219, l3: 0.000257, l4: 0.000420, l5: 0.001043, l6: 0.001192
[epoch: 260/1000, batch:   492/ 1052, ite: 68240] train loss: 0.005441, tar: 0.000346 
l0: 0.000723, l1: 0.000734, l2: 0.000795, l3: 0.000749, l4: 0.001093, l5: 0.002693, l6: 0.002916
[epoch: 260/1000, batch:   572/ 1052, ite: 68260] train loss: 0.005440, tar: 0.000346 
l0: 0.000328, l1: 0.000324, l2: 0.000354, l3: 0.000411, l4: 0.000532, l5: 0.000803, l6: 0.001902
[epoch: 260/1000, batch:   652/ 1052, ite: 68280] train loss: 0.005440, tar: 0.000346 
l0: 0.000441, l1: 0.000450, l2: 0.000436, l3: 0.000539, l4: 0.000906, l5: 0.002010, l6: 0.002516
[epoch: 260/1000, batch:   732/ 1052, ite: 68300] train loss: 0.005439, tar: 0.000346 
l0: 0.000556, l1: 0.000563, l2: 0.000564, l3: 0.000674, l4: 0.000795, l5: 0.001880, l6: 0.003755
[epoch: 260/1000, batch:   812/ 1052, ite: 68320] train loss: 0.005437, tar: 0.000345 
l0: 0.000261, l1: 0.000254, l2: 0.000322, l3: 0.000407, l4: 0.000558, l5: 0.001130, l6: 0.001240
[epoch: 260/1000, batch:   892/ 1052, ite: 68340] train loss: 0.005438, tar: 0.000345 
l0: 0.000384, l1: 0.000376, l2: 0.000446, l3: 0.000517, l4: 0.000877, l5: 0.001222, l6: 0.002699
[epoch: 260/1000, batch:   972/ 1052, ite: 68360] train loss: 0.005438, tar: 0.000345 
l0: 0.000209, l1: 0.000213, l2: 0.000231, l3: 0.000313, l4: 0.000576, l5: 0.000797, l6: 0.001548
[epoch: 260/1000, batch:  1052/ 1052, ite: 68380] train loss: 0.005439, tar: 0.000346 
[Epoch 260/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000190, l1: 0.000202, l2: 0.000199, l3: 0.000247, l4: 0.000552, l5: 0.000898, l6: 0.001591
[epoch: 261/1000, batch:    80/ 1052, ite: 68400] train loss: 0.005439, tar: 0.000345 
l0: 0.000318, l1: 0.000307, l2: 0.000348, l3: 0.000423, l4: 0.000704, l5: 0.001266, l6: 0.001598
[epoch: 261/1000, batch:   160/ 1052, ite: 68420] train loss: 0.005441, tar: 0.000345 
l0: 0.000326, l1: 0.000321, l2: 0.000329, l3: 0.000408, l4: 0.000544, l5: 0.001137, l6: 0.002030
[epoch: 261/1000, batch:   240/ 1052, ite: 68440] train loss: 0.005443, tar: 0.000345 
l0: 0.000246, l1: 0.000253, l2: 0.000240, l3: 0.000269, l4: 0.000524, l5: 0.000999, l6: 0.001112
[epoch: 261/1000, batch:   320/ 1052, ite: 68460] train loss: 0.005441, tar: 0.000345 
l0: 0.000486, l1: 0.000493, l2: 0.000521, l3: 0.000595, l4: 0.001115, l5: 0.001461, l6: 0.002901
[epoch: 261/1000, batch:   400/ 1052, ite: 68480] train loss: 0.005441, tar: 0.000345 
l0: 0.000392, l1: 0.000386, l2: 0.000469, l3: 0.000465, l4: 0.000496, l5: 0.000942, l6: 0.001179
[epoch: 261/1000, batch:   480/ 1052, ite: 68500] train loss: 0.005440, tar: 0.000345 
l0: 0.000184, l1: 0.000184, l2: 0.000225, l3: 0.000251, l4: 0.000528, l5: 0.001045, l6: 0.001697
[epoch: 261/1000, batch:   560/ 1052, ite: 68520] train loss: 0.005438, tar: 0.000345 
l0: 0.000140, l1: 0.000138, l2: 0.000187, l3: 0.000213, l4: 0.000479, l5: 0.000838, l6: 0.000952
[epoch: 261/1000, batch:   640/ 1052, ite: 68540] train loss: 0.005436, tar: 0.000345 
l0: 0.000232, l1: 0.000229, l2: 0.000248, l3: 0.000336, l4: 0.000394, l5: 0.000978, l6: 0.001616
[epoch: 261/1000, batch:   720/ 1052, ite: 68560] train loss: 0.005438, tar: 0.000345 
l0: 0.000238, l1: 0.000232, l2: 0.000270, l3: 0.000320, l4: 0.000437, l5: 0.000894, l6: 0.001150
[epoch: 261/1000, batch:   800/ 1052, ite: 68580] train loss: 0.005436, tar: 0.000345 
l0: 0.000233, l1: 0.000242, l2: 0.000238, l3: 0.000279, l4: 0.000440, l5: 0.000872, l6: 0.000998
[epoch: 261/1000, batch:   880/ 1052, ite: 68600] train loss: 0.005435, tar: 0.000345 
l0: 0.000195, l1: 0.000198, l2: 0.000199, l3: 0.000229, l4: 0.000426, l5: 0.000665, l6: 0.001407
[epoch: 261/1000, batch:   960/ 1052, ite: 68620] train loss: 0.005434, tar: 0.000345 
l0: 0.000526, l1: 0.000582, l2: 0.000591, l3: 0.000577, l4: 0.001310, l5: 0.001666, l6: 0.003905
[epoch: 261/1000, batch:  1040/ 1052, ite: 68640] train loss: 0.005437, tar: 0.000345 
[Epoch 261/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000294, l1: 0.000319, l2: 0.000315, l3: 0.000301, l4: 0.000519, l5: 0.001052, l6: 0.001607
[epoch: 262/1000, batch:    68/ 1052, ite: 68660] train loss: 0.005438, tar: 0.000345 
l0: 0.000342, l1: 0.000342, l2: 0.000359, l3: 0.000423, l4: 0.000664, l5: 0.001229, l6: 0.001934
[epoch: 262/1000, batch:   148/ 1052, ite: 68680] train loss: 0.005437, tar: 0.000344 
l0: 0.000268, l1: 0.000268, l2: 0.000316, l3: 0.000352, l4: 0.000456, l5: 0.000611, l6: 0.001480
[epoch: 262/1000, batch:   228/ 1052, ite: 68700] train loss: 0.005437, tar: 0.000344 
l0: 0.000453, l1: 0.000461, l2: 0.000465, l3: 0.000513, l4: 0.000979, l5: 0.001187, l6: 0.002984
[epoch: 262/1000, batch:   308/ 1052, ite: 68720] train loss: 0.005437, tar: 0.000344 
l0: 0.000317, l1: 0.000311, l2: 0.000350, l3: 0.000379, l4: 0.000545, l5: 0.000926, l6: 0.002015
[epoch: 262/1000, batch:   388/ 1052, ite: 68740] train loss: 0.005436, tar: 0.000344 
l0: 0.000329, l1: 0.000326, l2: 0.000360, l3: 0.000453, l4: 0.000714, l5: 0.001110, l6: 0.001859
[epoch: 262/1000, batch:   468/ 1052, ite: 68760] train loss: 0.005433, tar: 0.000344 
l0: 0.000305, l1: 0.000300, l2: 0.000325, l3: 0.000387, l4: 0.000663, l5: 0.001255, l6: 0.001486
[epoch: 262/1000, batch:   548/ 1052, ite: 68780] train loss: 0.005436, tar: 0.000344 
l0: 0.000609, l1: 0.000628, l2: 0.000627, l3: 0.000726, l4: 0.001128, l5: 0.002620, l6: 0.004426
[epoch: 262/1000, batch:   628/ 1052, ite: 68800] train loss: 0.005437, tar: 0.000344 
l0: 0.000270, l1: 0.000290, l2: 0.000290, l3: 0.000331, l4: 0.000540, l5: 0.001094, l6: 0.001676
[epoch: 262/1000, batch:   708/ 1052, ite: 68820] train loss: 0.005435, tar: 0.000344 
l0: 0.000182, l1: 0.000176, l2: 0.000208, l3: 0.000262, l4: 0.000385, l5: 0.000888, l6: 0.001178
[epoch: 262/1000, batch:   788/ 1052, ite: 68840] train loss: 0.005433, tar: 0.000344 
l0: 0.000292, l1: 0.000298, l2: 0.000312, l3: 0.000344, l4: 0.000516, l5: 0.001121, l6: 0.002118
[epoch: 262/1000, batch:   868/ 1052, ite: 68860] train loss: 0.005431, tar: 0.000343 
l0: 0.000338, l1: 0.000341, l2: 0.000414, l3: 0.000465, l4: 0.000665, l5: 0.001572, l6: 0.002571
[epoch: 262/1000, batch:   948/ 1052, ite: 68880] train loss: 0.005431, tar: 0.000343 
l0: 0.000247, l1: 0.000243, l2: 0.000253, l3: 0.000308, l4: 0.000457, l5: 0.000810, l6: 0.001207
[epoch: 262/1000, batch:  1028/ 1052, ite: 68900] train loss: 0.005428, tar: 0.000343 
[Epoch 262/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000228, l1: 0.000218, l2: 0.000278, l3: 0.000399, l4: 0.000587, l5: 0.001107, l6: 0.002831
[epoch: 263/1000, batch:    56/ 1052, ite: 68920] train loss: 0.005428, tar: 0.000343 
l0: 0.000390, l1: 0.000387, l2: 0.000418, l3: 0.000502, l4: 0.000629, l5: 0.001164, l6: 0.001604
[epoch: 263/1000, batch:   136/ 1052, ite: 68940] train loss: 0.005427, tar: 0.000343 
l0: 0.000291, l1: 0.000316, l2: 0.000295, l3: 0.000277, l4: 0.000608, l5: 0.000628, l6: 0.001692
[epoch: 263/1000, batch:   216/ 1052, ite: 68960] train loss: 0.005427, tar: 0.000343 
l0: 0.000443, l1: 0.000454, l2: 0.000535, l3: 0.000646, l4: 0.000930, l5: 0.001706, l6: 0.002018
[epoch: 263/1000, batch:   296/ 1052, ite: 68980] train loss: 0.005426, tar: 0.000343 
l0: 0.000241, l1: 0.000246, l2: 0.000278, l3: 0.000298, l4: 0.000553, l5: 0.000934, l6: 0.002057
[epoch: 263/1000, batch:   376/ 1052, ite: 69000] train loss: 0.005425, tar: 0.000343 
l0: 0.000154, l1: 0.000155, l2: 0.000176, l3: 0.000238, l4: 0.000399, l5: 0.001044, l6: 0.001278
[epoch: 263/1000, batch:   456/ 1052, ite: 69020] train loss: 0.005424, tar: 0.000343 
l0: 0.000417, l1: 0.000439, l2: 0.000461, l3: 0.000422, l4: 0.000701, l5: 0.001591, l6: 0.001862
[epoch: 263/1000, batch:   536/ 1052, ite: 69040] train loss: 0.005425, tar: 0.000343 
l0: 0.000361, l1: 0.000371, l2: 0.000411, l3: 0.000437, l4: 0.000755, l5: 0.001353, l6: 0.001937
[epoch: 263/1000, batch:   616/ 1052, ite: 69060] train loss: 0.005425, tar: 0.000342 
l0: 0.000391, l1: 0.000416, l2: 0.000417, l3: 0.000380, l4: 0.000620, l5: 0.001283, l6: 0.001309
[epoch: 263/1000, batch:   696/ 1052, ite: 69080] train loss: 0.005424, tar: 0.000342 
l0: 0.000378, l1: 0.000377, l2: 0.000412, l3: 0.000530, l4: 0.000676, l5: 0.001465, l6: 0.002656
[epoch: 263/1000, batch:   776/ 1052, ite: 69100] train loss: 0.005425, tar: 0.000342 
l0: 0.000418, l1: 0.000440, l2: 0.000464, l3: 0.000446, l4: 0.000643, l5: 0.001032, l6: 0.002562
[epoch: 263/1000, batch:   856/ 1052, ite: 69120] train loss: 0.005425, tar: 0.000342 
l0: 0.000302, l1: 0.000308, l2: 0.000316, l3: 0.000455, l4: 0.000776, l5: 0.001295, l6: 0.002083
[epoch: 263/1000, batch:   936/ 1052, ite: 69140] train loss: 0.005425, tar: 0.000342 
l0: 0.000202, l1: 0.000202, l2: 0.000219, l3: 0.000285, l4: 0.000445, l5: 0.001132, l6: 0.001943
[epoch: 263/1000, batch:  1016/ 1052, ite: 69160] train loss: 0.005426, tar: 0.000342 
[Epoch 263/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000301, l1: 0.000302, l2: 0.000368, l3: 0.000438, l4: 0.000755, l5: 0.001237, l6: 0.001666
[epoch: 264/1000, batch:    44/ 1052, ite: 69180] train loss: 0.005423, tar: 0.000342 
l0: 0.000181, l1: 0.000182, l2: 0.000194, l3: 0.000213, l4: 0.000383, l5: 0.000877, l6: 0.001081
[epoch: 264/1000, batch:   124/ 1052, ite: 69200] train loss: 0.005421, tar: 0.000342 
l0: 0.000360, l1: 0.000352, l2: 0.000521, l3: 0.000633, l4: 0.001146, l5: 0.002024, l6: 0.002789
[epoch: 264/1000, batch:   204/ 1052, ite: 69220] train loss: 0.005420, tar: 0.000342 
l0: 0.000347, l1: 0.000353, l2: 0.000352, l3: 0.000433, l4: 0.000694, l5: 0.001340, l6: 0.001619
[epoch: 264/1000, batch:   284/ 1052, ite: 69240] train loss: 0.005419, tar: 0.000342 
l0: 0.000427, l1: 0.000428, l2: 0.000461, l3: 0.000561, l4: 0.000978, l5: 0.002183, l6: 0.004388
[epoch: 264/1000, batch:   364/ 1052, ite: 69260] train loss: 0.005419, tar: 0.000342 
l0: 0.000295, l1: 0.000317, l2: 0.000325, l3: 0.000316, l4: 0.000490, l5: 0.001091, l6: 0.001866
[epoch: 264/1000, batch:   444/ 1052, ite: 69280] train loss: 0.005419, tar: 0.000341 
l0: 0.000221, l1: 0.000222, l2: 0.000232, l3: 0.000301, l4: 0.000470, l5: 0.000928, l6: 0.001730
[epoch: 264/1000, batch:   524/ 1052, ite: 69300] train loss: 0.005420, tar: 0.000341 
l0: 0.000174, l1: 0.000177, l2: 0.000208, l3: 0.000260, l4: 0.000574, l5: 0.001039, l6: 0.001232
[epoch: 264/1000, batch:   604/ 1052, ite: 69320] train loss: 0.005421, tar: 0.000341 
l0: 0.000158, l1: 0.000158, l2: 0.000164, l3: 0.000190, l4: 0.000379, l5: 0.000483, l6: 0.000942
[epoch: 264/1000, batch:   684/ 1052, ite: 69340] train loss: 0.005421, tar: 0.000341 
l0: 0.000268, l1: 0.000280, l2: 0.000312, l3: 0.000374, l4: 0.000619, l5: 0.001259, l6: 0.001420
[epoch: 264/1000, batch:   764/ 1052, ite: 69360] train loss: 0.005420, tar: 0.000341 
l0: 0.000208, l1: 0.000221, l2: 0.000230, l3: 0.000267, l4: 0.000471, l5: 0.001014, l6: 0.002386
[epoch: 264/1000, batch:   844/ 1052, ite: 69380] train loss: 0.005417, tar: 0.000341 
l0: 0.000197, l1: 0.000198, l2: 0.000213, l3: 0.000280, l4: 0.000508, l5: 0.001193, l6: 0.001689
[epoch: 264/1000, batch:   924/ 1052, ite: 69400] train loss: 0.005416, tar: 0.000341 
l0: 0.000340, l1: 0.000345, l2: 0.000345, l3: 0.000432, l4: 0.000694, l5: 0.001213, l6: 0.002069
[epoch: 264/1000, batch:  1004/ 1052, ite: 69420] train loss: 0.005414, tar: 0.000341 
[Epoch 264/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000468, l1: 0.000475, l2: 0.000531, l3: 0.000563, l4: 0.000798, l5: 0.001302, l6: 0.002382
[epoch: 265/1000, batch:    32/ 1052, ite: 69440] train loss: 0.005415, tar: 0.000341 
l0: 0.000286, l1: 0.000283, l2: 0.000360, l3: 0.000507, l4: 0.000669, l5: 0.001361, l6: 0.002587
[epoch: 265/1000, batch:   112/ 1052, ite: 69460] train loss: 0.005414, tar: 0.000341 
l0: 0.000212, l1: 0.000227, l2: 0.000246, l3: 0.000366, l4: 0.000596, l5: 0.000890, l6: 0.002044
[epoch: 265/1000, batch:   192/ 1052, ite: 69480] train loss: 0.005413, tar: 0.000341 
l0: 0.000373, l1: 0.000384, l2: 0.000403, l3: 0.000458, l4: 0.000777, l5: 0.001273, l6: 0.001968
[epoch: 265/1000, batch:   272/ 1052, ite: 69500] train loss: 0.005415, tar: 0.000341 
l0: 0.000214, l1: 0.000194, l2: 0.000253, l3: 0.000418, l4: 0.000610, l5: 0.000954, l6: 0.001539
[epoch: 265/1000, batch:   352/ 1052, ite: 69520] train loss: 0.005415, tar: 0.000341 
l0: 0.000295, l1: 0.000309, l2: 0.000287, l3: 0.000364, l4: 0.000748, l5: 0.001349, l6: 0.002096
[epoch: 265/1000, batch:   432/ 1052, ite: 69540] train loss: 0.005414, tar: 0.000340 
l0: 0.000460, l1: 0.000473, l2: 0.000512, l3: 0.000514, l4: 0.000678, l5: 0.001426, l6: 0.002661
[epoch: 265/1000, batch:   512/ 1052, ite: 69560] train loss: 0.005413, tar: 0.000340 
l0: 0.000403, l1: 0.000395, l2: 0.000416, l3: 0.000560, l4: 0.000932, l5: 0.001492, l6: 0.001734
[epoch: 265/1000, batch:   592/ 1052, ite: 69580] train loss: 0.005412, tar: 0.000340 
l0: 0.000300, l1: 0.000316, l2: 0.000312, l3: 0.000329, l4: 0.000501, l5: 0.001009, l6: 0.001843
[epoch: 265/1000, batch:   672/ 1052, ite: 69600] train loss: 0.005412, tar: 0.000340 
l0: 0.000268, l1: 0.000281, l2: 0.000326, l3: 0.000363, l4: 0.000738, l5: 0.001020, l6: 0.002190
[epoch: 265/1000, batch:   752/ 1052, ite: 69620] train loss: 0.005413, tar: 0.000340 
l0: 0.000208, l1: 0.000211, l2: 0.000248, l3: 0.000304, l4: 0.000664, l5: 0.001002, l6: 0.001525
[epoch: 265/1000, batch:   832/ 1052, ite: 69640] train loss: 0.005413, tar: 0.000340 
l0: 0.000329, l1: 0.000325, l2: 0.000401, l3: 0.000451, l4: 0.000798, l5: 0.001395, l6: 0.003232
[epoch: 265/1000, batch:   912/ 1052, ite: 69660] train loss: 0.005412, tar: 0.000340 
l0: 0.000304, l1: 0.000306, l2: 0.000293, l3: 0.000343, l4: 0.000502, l5: 0.000831, l6: 0.001463
[epoch: 265/1000, batch:   992/ 1052, ite: 69680] train loss: 0.005411, tar: 0.000340 
[Epoch 265/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000377, l1: 0.000391, l2: 0.000385, l3: 0.000382, l4: 0.000616, l5: 0.001007, l6: 0.002187
[epoch: 266/1000, batch:    20/ 1052, ite: 69700] train loss: 0.005409, tar: 0.000340 
l0: 0.000341, l1: 0.000322, l2: 0.000371, l3: 0.000485, l4: 0.000787, l5: 0.001182, l6: 0.001744
[epoch: 266/1000, batch:   100/ 1052, ite: 69720] train loss: 0.005410, tar: 0.000340 
l0: 0.000185, l1: 0.000236, l2: 0.000210, l3: 0.000173, l4: 0.000386, l5: 0.000723, l6: 0.001206
[epoch: 266/1000, batch:   180/ 1052, ite: 69740] train loss: 0.005409, tar: 0.000340 
l0: 0.000163, l1: 0.000168, l2: 0.000217, l3: 0.000266, l4: 0.000561, l5: 0.001078, l6: 0.001697
[epoch: 266/1000, batch:   260/ 1052, ite: 69760] train loss: 0.005408, tar: 0.000340 
l0: 0.000560, l1: 0.000580, l2: 0.000639, l3: 0.000651, l4: 0.001120, l5: 0.002123, l6: 0.005315
[epoch: 266/1000, batch:   340/ 1052, ite: 69780] train loss: 0.005408, tar: 0.000340 
l0: 0.000340, l1: 0.000349, l2: 0.000343, l3: 0.000417, l4: 0.000844, l5: 0.001629, l6: 0.002796
[epoch: 266/1000, batch:   420/ 1052, ite: 69800] train loss: 0.005409, tar: 0.000340 
l0: 0.000212, l1: 0.000215, l2: 0.000261, l3: 0.000264, l4: 0.000423, l5: 0.001171, l6: 0.001500
[epoch: 266/1000, batch:   500/ 1052, ite: 69820] train loss: 0.005408, tar: 0.000339 
l0: 0.000139, l1: 0.000132, l2: 0.000150, l3: 0.000226, l4: 0.000292, l5: 0.000626, l6: 0.001113
[epoch: 266/1000, batch:   580/ 1052, ite: 69840] train loss: 0.005409, tar: 0.000339 
l0: 0.000617, l1: 0.000640, l2: 0.000670, l3: 0.000795, l4: 0.001307, l5: 0.001560, l6: 0.003924
[epoch: 266/1000, batch:   660/ 1052, ite: 69860] train loss: 0.005411, tar: 0.000340 
l0: 0.000264, l1: 0.000284, l2: 0.000289, l3: 0.000284, l4: 0.000508, l5: 0.000846, l6: 0.001736
[epoch: 266/1000, batch:   740/ 1052, ite: 69880] train loss: 0.005410, tar: 0.000339 
l0: 0.000336, l1: 0.000351, l2: 0.000355, l3: 0.000406, l4: 0.000436, l5: 0.000992, l6: 0.001554
[epoch: 266/1000, batch:   820/ 1052, ite: 69900] train loss: 0.005408, tar: 0.000339 
l0: 0.000211, l1: 0.000205, l2: 0.000206, l3: 0.000338, l4: 0.000523, l5: 0.000915, l6: 0.001774
[epoch: 266/1000, batch:   900/ 1052, ite: 69920] train loss: 0.005407, tar: 0.000339 
l0: 0.000283, l1: 0.000282, l2: 0.000337, l3: 0.000499, l4: 0.000873, l5: 0.001751, l6: 0.001794
[epoch: 266/1000, batch:   980/ 1052, ite: 69940] train loss: 0.005406, tar: 0.000339 
[Epoch 266/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000282, l1: 0.000280, l2: 0.000302, l3: 0.000418, l4: 0.000778, l5: 0.001396, l6: 0.002180
[epoch: 267/1000, batch:     8/ 1052, ite: 69960] train loss: 0.005406, tar: 0.000339 
l0: 0.000194, l1: 0.000204, l2: 0.000221, l3: 0.000241, l4: 0.000482, l5: 0.000797, l6: 0.001274
[epoch: 267/1000, batch:    88/ 1052, ite: 69980] train loss: 0.005404, tar: 0.000339 
l0: 0.000298, l1: 0.000290, l2: 0.000336, l3: 0.000446, l4: 0.000781, l5: 0.001298, l6: 0.001810
[epoch: 267/1000, batch:   168/ 1052, ite: 70000] train loss: 0.005404, tar: 0.000339 
l0: 0.000170, l1: 0.000168, l2: 0.000173, l3: 0.000218, l4: 0.000299, l5: 0.000773, l6: 0.001185
[epoch: 267/1000, batch:   248/ 1052, ite: 70020] train loss: 0.005405, tar: 0.000339 
l0: 0.000248, l1: 0.000258, l2: 0.000272, l3: 0.000270, l4: 0.000459, l5: 0.001280, l6: 0.001420
[epoch: 267/1000, batch:   328/ 1052, ite: 70040] train loss: 0.005404, tar: 0.000339 
l0: 0.000533, l1: 0.000517, l2: 0.000578, l3: 0.000725, l4: 0.000845, l5: 0.001335, l6: 0.002609
[epoch: 267/1000, batch:   408/ 1052, ite: 70060] train loss: 0.005406, tar: 0.000339 
l0: 0.000441, l1: 0.000437, l2: 0.000460, l3: 0.000499, l4: 0.000802, l5: 0.001245, l6: 0.002826
[epoch: 267/1000, batch:   488/ 1052, ite: 70080] train loss: 0.005405, tar: 0.000339 
l0: 0.000332, l1: 0.000325, l2: 0.000343, l3: 0.000447, l4: 0.000600, l5: 0.000943, l6: 0.002041
[epoch: 267/1000, batch:   568/ 1052, ite: 70100] train loss: 0.005406, tar: 0.000339 
l0: 0.000378, l1: 0.000367, l2: 0.000470, l3: 0.000789, l4: 0.001289, l5: 0.002215, l6: 0.004190
[epoch: 267/1000, batch:   648/ 1052, ite: 70120] train loss: 0.005405, tar: 0.000339 
l0: 0.000296, l1: 0.000291, l2: 0.000330, l3: 0.000418, l4: 0.000532, l5: 0.000985, l6: 0.001910
[epoch: 267/1000, batch:   728/ 1052, ite: 70140] train loss: 0.005402, tar: 0.000338 
l0: 0.000776, l1: 0.000813, l2: 0.000736, l3: 0.000781, l4: 0.001036, l5: 0.002214, l6: 0.004135
[epoch: 267/1000, batch:   808/ 1052, ite: 70160] train loss: 0.005401, tar: 0.000338 
l0: 0.000158, l1: 0.000156, l2: 0.000183, l3: 0.000197, l4: 0.000305, l5: 0.000729, l6: 0.000730
[epoch: 267/1000, batch:   888/ 1052, ite: 70180] train loss: 0.005400, tar: 0.000338 
l0: 0.000436, l1: 0.000425, l2: 0.000513, l3: 0.000553, l4: 0.000719, l5: 0.001242, l6: 0.001863
[epoch: 267/1000, batch:   968/ 1052, ite: 70200] train loss: 0.005401, tar: 0.000338 
l0: 0.000341, l1: 0.000347, l2: 0.000377, l3: 0.000474, l4: 0.000553, l5: 0.001524, l6: 0.002726
[epoch: 267/1000, batch:  1048/ 1052, ite: 70220] train loss: 0.005401, tar: 0.000338 
[Epoch 267/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000189, l1: 0.000182, l2: 0.000227, l3: 0.000274, l4: 0.000477, l5: 0.001196, l6: 0.001717
[epoch: 268/1000, batch:    76/ 1052, ite: 70240] train loss: 0.005401, tar: 0.000338 
l0: 0.000402, l1: 0.000402, l2: 0.000377, l3: 0.000476, l4: 0.000706, l5: 0.001304, l6: 0.002636
[epoch: 268/1000, batch:   156/ 1052, ite: 70260] train loss: 0.005401, tar: 0.000338 
l0: 0.000229, l1: 0.000224, l2: 0.000257, l3: 0.000337, l4: 0.000631, l5: 0.000753, l6: 0.002099
[epoch: 268/1000, batch:   236/ 1052, ite: 70280] train loss: 0.005400, tar: 0.000338 
l0: 0.000728, l1: 0.000743, l2: 0.000858, l3: 0.000990, l4: 0.001404, l5: 0.002260, l6: 0.004329
[epoch: 268/1000, batch:   316/ 1052, ite: 70300] train loss: 0.005402, tar: 0.000338 
l0: 0.000546, l1: 0.000548, l2: 0.000539, l3: 0.000745, l4: 0.000999, l5: 0.001754, l6: 0.002855
[epoch: 268/1000, batch:   396/ 1052, ite: 70320] train loss: 0.005403, tar: 0.000338 
l0: 0.000237, l1: 0.000236, l2: 0.000258, l3: 0.000358, l4: 0.000513, l5: 0.000932, l6: 0.001909
[epoch: 268/1000, batch:   476/ 1052, ite: 70340] train loss: 0.005403, tar: 0.000338 
l0: 0.000196, l1: 0.000192, l2: 0.000204, l3: 0.000292, l4: 0.000473, l5: 0.000835, l6: 0.001375
[epoch: 268/1000, batch:   556/ 1052, ite: 70360] train loss: 0.005402, tar: 0.000338 
l0: 0.000482, l1: 0.000459, l2: 0.000532, l3: 0.000729, l4: 0.000931, l5: 0.001741, l6: 0.003074
[epoch: 268/1000, batch:   636/ 1052, ite: 70380] train loss: 0.005400, tar: 0.000338 
l0: 0.000261, l1: 0.000266, l2: 0.000267, l3: 0.000272, l4: 0.000449, l5: 0.000808, l6: 0.001435
[epoch: 268/1000, batch:   716/ 1052, ite: 70400] train loss: 0.005399, tar: 0.000338 
l0: 0.000258, l1: 0.000241, l2: 0.000274, l3: 0.000336, l4: 0.000503, l5: 0.000930, l6: 0.001400
[epoch: 268/1000, batch:   796/ 1052, ite: 70420] train loss: 0.005400, tar: 0.000338 
l0: 0.000207, l1: 0.000200, l2: 0.000259, l3: 0.000362, l4: 0.000648, l5: 0.000782, l6: 0.001569
[epoch: 268/1000, batch:   876/ 1052, ite: 70440] train loss: 0.005401, tar: 0.000338 
l0: 0.000226, l1: 0.000247, l2: 0.000265, l3: 0.000241, l4: 0.000520, l5: 0.000781, l6: 0.001434
[epoch: 268/1000, batch:   956/ 1052, ite: 70460] train loss: 0.005399, tar: 0.000338 
l0: 0.000329, l1: 0.000339, l2: 0.000390, l3: 0.000389, l4: 0.000720, l5: 0.001202, l6: 0.001639
[epoch: 268/1000, batch:  1036/ 1052, ite: 70480] train loss: 0.005397, tar: 0.000337 
[Epoch 268/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000489, l1: 0.000480, l2: 0.000496, l3: 0.000601, l4: 0.000983, l5: 0.001842, l6: 0.001929
[epoch: 269/1000, batch:    64/ 1052, ite: 70500] train loss: 0.005399, tar: 0.000338 
l0: 0.000325, l1: 0.000336, l2: 0.000361, l3: 0.000306, l4: 0.000421, l5: 0.000727, l6: 0.001126
[epoch: 269/1000, batch:   144/ 1052, ite: 70520] train loss: 0.005398, tar: 0.000337 
l0: 0.000149, l1: 0.000148, l2: 0.000179, l3: 0.000217, l4: 0.000547, l5: 0.000970, l6: 0.002132
[epoch: 269/1000, batch:   224/ 1052, ite: 70540] train loss: 0.005398, tar: 0.000337 
l0: 0.000391, l1: 0.000388, l2: 0.000470, l3: 0.000541, l4: 0.000746, l5: 0.001596, l6: 0.002278
[epoch: 269/1000, batch:   304/ 1052, ite: 70560] train loss: 0.005397, tar: 0.000337 
l0: 0.000228, l1: 0.000229, l2: 0.000249, l3: 0.000293, l4: 0.000534, l5: 0.000989, l6: 0.001369
[epoch: 269/1000, batch:   384/ 1052, ite: 70580] train loss: 0.005398, tar: 0.000337 
l0: 0.000308, l1: 0.000303, l2: 0.000369, l3: 0.000495, l4: 0.000593, l5: 0.001055, l6: 0.002177
[epoch: 269/1000, batch:   464/ 1052, ite: 70600] train loss: 0.005399, tar: 0.000337 
l0: 0.000375, l1: 0.000371, l2: 0.000397, l3: 0.000526, l4: 0.000799, l5: 0.001347, l6: 0.002977
[epoch: 269/1000, batch:   544/ 1052, ite: 70620] train loss: 0.005399, tar: 0.000337 
l0: 0.000173, l1: 0.000182, l2: 0.000193, l3: 0.000189, l4: 0.000401, l5: 0.000499, l6: 0.001208
[epoch: 269/1000, batch:   624/ 1052, ite: 70640] train loss: 0.005396, tar: 0.000337 
l0: 0.000505, l1: 0.000502, l2: 0.000535, l3: 0.000586, l4: 0.000885, l5: 0.001379, l6: 0.004174
[epoch: 269/1000, batch:   704/ 1052, ite: 70660] train loss: 0.005395, tar: 0.000337 
l0: 0.000214, l1: 0.000216, l2: 0.000204, l3: 0.000274, l4: 0.000400, l5: 0.000629, l6: 0.001236
[epoch: 269/1000, batch:   784/ 1052, ite: 70680] train loss: 0.005394, tar: 0.000337 
l0: 0.000367, l1: 0.000376, l2: 0.000387, l3: 0.000441, l4: 0.000851, l5: 0.001248, l6: 0.002087
[epoch: 269/1000, batch:   864/ 1052, ite: 70700] train loss: 0.005393, tar: 0.000337 
l0: 0.000189, l1: 0.000189, l2: 0.000199, l3: 0.000248, l4: 0.000412, l5: 0.001184, l6: 0.001356
[epoch: 269/1000, batch:   944/ 1052, ite: 70720] train loss: 0.005393, tar: 0.000337 
l0: 0.000206, l1: 0.000209, l2: 0.000220, l3: 0.000240, l4: 0.000499, l5: 0.001199, l6: 0.001092
[epoch: 269/1000, batch:  1024/ 1052, ite: 70740] train loss: 0.005392, tar: 0.000337 
[Epoch 269/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000526, l1: 0.000549, l2: 0.000562, l3: 0.000633, l4: 0.000975, l5: 0.002309, l6: 0.004658
[epoch: 270/1000, batch:    52/ 1052, ite: 70760] train loss: 0.005392, tar: 0.000337 
l0: 0.000102, l1: 0.000099, l2: 0.000115, l3: 0.000201, l4: 0.000300, l5: 0.000652, l6: 0.000985
[epoch: 270/1000, batch:   132/ 1052, ite: 70780] train loss: 0.005391, tar: 0.000337 
l0: 0.000180, l1: 0.000176, l2: 0.000195, l3: 0.000255, l4: 0.000310, l5: 0.000695, l6: 0.000996
[epoch: 270/1000, batch:   212/ 1052, ite: 70800] train loss: 0.005389, tar: 0.000336 
l0: 0.000216, l1: 0.000223, l2: 0.000246, l3: 0.000260, l4: 0.000647, l5: 0.000913, l6: 0.001202
[epoch: 270/1000, batch:   292/ 1052, ite: 70820] train loss: 0.005386, tar: 0.000336 
l0: 0.000258, l1: 0.000252, l2: 0.000308, l3: 0.000341, l4: 0.000541, l5: 0.000941, l6: 0.002293
[epoch: 270/1000, batch:   372/ 1052, ite: 70840] train loss: 0.005385, tar: 0.000336 
l0: 0.000341, l1: 0.000347, l2: 0.000382, l3: 0.000392, l4: 0.000466, l5: 0.001019, l6: 0.002080
[epoch: 270/1000, batch:   452/ 1052, ite: 70860] train loss: 0.005386, tar: 0.000336 
l0: 0.000521, l1: 0.000531, l2: 0.000539, l3: 0.000566, l4: 0.000857, l5: 0.001401, l6: 0.004101
[epoch: 270/1000, batch:   532/ 1052, ite: 70880] train loss: 0.005383, tar: 0.000336 
l0: 0.000236, l1: 0.000241, l2: 0.000236, l3: 0.000345, l4: 0.000545, l5: 0.000750, l6: 0.001271
[epoch: 270/1000, batch:   612/ 1052, ite: 70900] train loss: 0.005380, tar: 0.000336 
l0: 0.000315, l1: 0.000307, l2: 0.000372, l3: 0.000471, l4: 0.001058, l5: 0.002358, l6: 0.002705
[epoch: 270/1000, batch:   692/ 1052, ite: 70920] train loss: 0.005380, tar: 0.000335 
l0: 0.000345, l1: 0.000375, l2: 0.000406, l3: 0.000432, l4: 0.000812, l5: 0.001281, l6: 0.002104
[epoch: 270/1000, batch:   772/ 1052, ite: 70940] train loss: 0.005379, tar: 0.000335 
l0: 0.000593, l1: 0.000615, l2: 0.000729, l3: 0.000694, l4: 0.000857, l5: 0.001863, l6: 0.002817
[epoch: 270/1000, batch:   852/ 1052, ite: 70960] train loss: 0.005380, tar: 0.000335 
l0: 0.000359, l1: 0.000382, l2: 0.000463, l3: 0.000457, l4: 0.000830, l5: 0.001411, l6: 0.003496
[epoch: 270/1000, batch:   932/ 1052, ite: 70980] train loss: 0.005382, tar: 0.000336 
l0: 0.000587, l1: 0.000624, l2: 0.000661, l3: 0.000622, l4: 0.000874, l5: 0.002449, l6: 0.003838
[epoch: 270/1000, batch:  1012/ 1052, ite: 71000] train loss: 0.005385, tar: 0.000336 
[Epoch 270/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000168, l1: 0.000258, l2: 0.000278, l3: 0.000242, l4: 0.000453, l5: 0.001079, l6: 0.001368
[epoch: 271/1000, batch:    40/ 1052, ite: 71020] train loss: 0.005385, tar: 0.000336 
l0: 0.000192, l1: 0.000193, l2: 0.000229, l3: 0.000294, l4: 0.000402, l5: 0.000947, l6: 0.001591
[epoch: 271/1000, batch:   120/ 1052, ite: 71040] train loss: 0.005385, tar: 0.000336 
l0: 0.000178, l1: 0.000174, l2: 0.000215, l3: 0.000311, l4: 0.000431, l5: 0.000793, l6: 0.001848
[epoch: 271/1000, batch:   200/ 1052, ite: 71060] train loss: 0.005384, tar: 0.000336 
l0: 0.000270, l1: 0.000268, l2: 0.000287, l3: 0.000321, l4: 0.000491, l5: 0.000960, l6: 0.001530
[epoch: 271/1000, batch:   280/ 1052, ite: 71080] train loss: 0.005385, tar: 0.000336 
l0: 0.000239, l1: 0.000236, l2: 0.000302, l3: 0.000290, l4: 0.000340, l5: 0.000791, l6: 0.001380
[epoch: 271/1000, batch:   360/ 1052, ite: 71100] train loss: 0.005385, tar: 0.000336 
l0: 0.000228, l1: 0.000245, l2: 0.000220, l3: 0.000270, l4: 0.000622, l5: 0.001463, l6: 0.001885
[epoch: 271/1000, batch:   440/ 1052, ite: 71120] train loss: 0.005385, tar: 0.000336 
l0: 0.000262, l1: 0.000249, l2: 0.000318, l3: 0.000494, l4: 0.000842, l5: 0.001278, l6: 0.001712
[epoch: 271/1000, batch:   520/ 1052, ite: 71140] train loss: 0.005385, tar: 0.000336 
l0: 0.000167, l1: 0.000175, l2: 0.000176, l3: 0.000206, l4: 0.000303, l5: 0.000907, l6: 0.001032
[epoch: 271/1000, batch:   600/ 1052, ite: 71160] train loss: 0.005385, tar: 0.000336 
l0: 0.000415, l1: 0.000427, l2: 0.000392, l3: 0.000418, l4: 0.000714, l5: 0.001052, l6: 0.002126
[epoch: 271/1000, batch:   680/ 1052, ite: 71180] train loss: 0.005385, tar: 0.000336 
l0: 0.000285, l1: 0.000307, l2: 0.000321, l3: 0.000327, l4: 0.000502, l5: 0.001445, l6: 0.002289
[epoch: 271/1000, batch:   760/ 1052, ite: 71200] train loss: 0.005386, tar: 0.000336 
l0: 0.000371, l1: 0.000356, l2: 0.000434, l3: 0.000476, l4: 0.000740, l5: 0.001569, l6: 0.002780
[epoch: 271/1000, batch:   840/ 1052, ite: 71220] train loss: 0.005386, tar: 0.000336 
l0: 0.000336, l1: 0.000325, l2: 0.000328, l3: 0.000385, l4: 0.000559, l5: 0.000658, l6: 0.001077
[epoch: 271/1000, batch:   920/ 1052, ite: 71240] train loss: 0.005386, tar: 0.000336 
l0: 0.000208, l1: 0.000226, l2: 0.000236, l3: 0.000304, l4: 0.000467, l5: 0.001123, l6: 0.001255
[epoch: 271/1000, batch:  1000/ 1052, ite: 71260] train loss: 0.005384, tar: 0.000335 
[Epoch 271/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000448, l1: 0.000447, l2: 0.000532, l3: 0.000510, l4: 0.000867, l5: 0.001082, l6: 0.002538
[epoch: 272/1000, batch:    28/ 1052, ite: 71280] train loss: 0.005382, tar: 0.000335 
l0: 0.000551, l1: 0.000546, l2: 0.000598, l3: 0.000696, l4: 0.001082, l5: 0.001312, l6: 0.003275
[epoch: 272/1000, batch:   108/ 1052, ite: 71300] train loss: 0.005382, tar: 0.000335 
l0: 0.000192, l1: 0.000193, l2: 0.000233, l3: 0.000272, l4: 0.000399, l5: 0.001090, l6: 0.001709
[epoch: 272/1000, batch:   188/ 1052, ite: 71320] train loss: 0.005385, tar: 0.000335 
l0: 0.000203, l1: 0.000202, l2: 0.000212, l3: 0.000280, l4: 0.000357, l5: 0.001170, l6: 0.001929
[epoch: 272/1000, batch:   268/ 1052, ite: 71340] train loss: 0.005384, tar: 0.000335 
l0: 0.000219, l1: 0.000233, l2: 0.000254, l3: 0.000331, l4: 0.000510, l5: 0.000985, l6: 0.001698
[epoch: 272/1000, batch:   348/ 1052, ite: 71360] train loss: 0.005384, tar: 0.000335 
l0: 0.000431, l1: 0.000435, l2: 0.000444, l3: 0.000495, l4: 0.000554, l5: 0.000964, l6: 0.002041
[epoch: 272/1000, batch:   428/ 1052, ite: 71380] train loss: 0.005383, tar: 0.000335 
l0: 0.000115, l1: 0.000111, l2: 0.000105, l3: 0.000097, l4: 0.000248, l5: 0.000375, l6: 0.000549
[epoch: 272/1000, batch:   508/ 1052, ite: 71400] train loss: 0.005381, tar: 0.000335 
l0: 0.000199, l1: 0.000207, l2: 0.000228, l3: 0.000219, l4: 0.000401, l5: 0.000796, l6: 0.000990
[epoch: 272/1000, batch:   588/ 1052, ite: 71420] train loss: 0.005380, tar: 0.000335 
l0: 0.000177, l1: 0.000177, l2: 0.000204, l3: 0.000214, l4: 0.000371, l5: 0.000829, l6: 0.001488
[epoch: 272/1000, batch:   668/ 1052, ite: 71440] train loss: 0.005379, tar: 0.000335 
l0: 0.000440, l1: 0.000467, l2: 0.000528, l3: 0.000627, l4: 0.001076, l5: 0.001942, l6: 0.003044
[epoch: 272/1000, batch:   748/ 1052, ite: 71460] train loss: 0.005380, tar: 0.000335 
l0: 0.000741, l1: 0.000749, l2: 0.000798, l3: 0.000889, l4: 0.001026, l5: 0.001612, l6: 0.002961
[epoch: 272/1000, batch:   828/ 1052, ite: 71480] train loss: 0.005381, tar: 0.000335 
l0: 0.000467, l1: 0.000475, l2: 0.000478, l3: 0.000511, l4: 0.000696, l5: 0.001325, l6: 0.002337
[epoch: 272/1000, batch:   908/ 1052, ite: 71500] train loss: 0.005381, tar: 0.000335 
l0: 0.000374, l1: 0.000381, l2: 0.000407, l3: 0.000518, l4: 0.000884, l5: 0.001952, l6: 0.002907
[epoch: 272/1000, batch:   988/ 1052, ite: 71520] train loss: 0.005381, tar: 0.000335 
[Epoch 272/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000492, l1: 0.000495, l2: 0.000510, l3: 0.000634, l4: 0.000934, l5: 0.002535, l6: 0.005124
[epoch: 273/1000, batch:    16/ 1052, ite: 71540] train loss: 0.005382, tar: 0.000335 
l0: 0.000313, l1: 0.000304, l2: 0.000388, l3: 0.000462, l4: 0.000740, l5: 0.000993, l6: 0.001477
[epoch: 273/1000, batch:    96/ 1052, ite: 71560] train loss: 0.005381, tar: 0.000335 
l0: 0.000249, l1: 0.000252, l2: 0.000263, l3: 0.000317, l4: 0.000565, l5: 0.000908, l6: 0.001486
[epoch: 273/1000, batch:   176/ 1052, ite: 71580] train loss: 0.005381, tar: 0.000335 
l0: 0.000557, l1: 0.000561, l2: 0.000617, l3: 0.000774, l4: 0.001059, l5: 0.001948, l6: 0.004275
[epoch: 273/1000, batch:   256/ 1052, ite: 71600] train loss: 0.005380, tar: 0.000335 
l0: 0.000463, l1: 0.000475, l2: 0.000479, l3: 0.000490, l4: 0.000640, l5: 0.001182, l6: 0.002338
[epoch: 273/1000, batch:   336/ 1052, ite: 71620] train loss: 0.005381, tar: 0.000335 
l0: 0.000953, l1: 0.000909, l2: 0.000928, l3: 0.001161, l4: 0.001530, l5: 0.002209, l6: 0.003341
[epoch: 273/1000, batch:   416/ 1052, ite: 71640] train loss: 0.005382, tar: 0.000335 
l0: 0.000354, l1: 0.000366, l2: 0.000370, l3: 0.000468, l4: 0.000898, l5: 0.001002, l6: 0.001244
[epoch: 273/1000, batch:   496/ 1052, ite: 71660] train loss: 0.005383, tar: 0.000335 
l0: 0.000323, l1: 0.000306, l2: 0.000326, l3: 0.000425, l4: 0.000710, l5: 0.001377, l6: 0.001393
[epoch: 273/1000, batch:   576/ 1052, ite: 71680] train loss: 0.005385, tar: 0.000335 
l0: 0.000662, l1: 0.000668, l2: 0.000699, l3: 0.000812, l4: 0.001028, l5: 0.001612, l6: 0.002708
[epoch: 273/1000, batch:   656/ 1052, ite: 71700] train loss: 0.005385, tar: 0.000335 
l0: 0.000217, l1: 0.000210, l2: 0.000235, l3: 0.000438, l4: 0.000930, l5: 0.000748, l6: 0.001710
[epoch: 273/1000, batch:   736/ 1052, ite: 71720] train loss: 0.005385, tar: 0.000335 
l0: 0.000421, l1: 0.000420, l2: 0.000528, l3: 0.000643, l4: 0.001078, l5: 0.002006, l6: 0.003201
[epoch: 273/1000, batch:   816/ 1052, ite: 71740] train loss: 0.005385, tar: 0.000335 
l0: 0.000176, l1: 0.000179, l2: 0.000184, l3: 0.000214, l4: 0.000342, l5: 0.000832, l6: 0.000884
[epoch: 273/1000, batch:   896/ 1052, ite: 71760] train loss: 0.005384, tar: 0.000335 
l0: 0.000194, l1: 0.000194, l2: 0.000200, l3: 0.000232, l4: 0.000336, l5: 0.000836, l6: 0.001823
[epoch: 273/1000, batch:   976/ 1052, ite: 71780] train loss: 0.005385, tar: 0.000335 
[Epoch 273/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000234, l1: 0.000249, l2: 0.000245, l3: 0.000259, l4: 0.000412, l5: 0.000855, l6: 0.002620
[epoch: 274/1000, batch:     4/ 1052, ite: 71800] train loss: 0.005385, tar: 0.000335 
l0: 0.000470, l1: 0.000461, l2: 0.000515, l3: 0.000635, l4: 0.001248, l5: 0.001902, l6: 0.003189
[epoch: 274/1000, batch:    84/ 1052, ite: 71820] train loss: 0.005386, tar: 0.000335 
l0: 0.000404, l1: 0.000418, l2: 0.000448, l3: 0.000487, l4: 0.000622, l5: 0.001731, l6: 0.002315
[epoch: 274/1000, batch:   164/ 1052, ite: 71840] train loss: 0.005386, tar: 0.000335 
l0: 0.000235, l1: 0.000242, l2: 0.000265, l3: 0.000275, l4: 0.000437, l5: 0.001468, l6: 0.001857
[epoch: 274/1000, batch:   244/ 1052, ite: 71860] train loss: 0.005386, tar: 0.000335 
l0: 0.000274, l1: 0.000270, l2: 0.000298, l3: 0.000421, l4: 0.000544, l5: 0.001092, l6: 0.001889
[epoch: 274/1000, batch:   324/ 1052, ite: 71880] train loss: 0.005387, tar: 0.000335 
l0: 0.000432, l1: 0.000465, l2: 0.000541, l3: 0.000505, l4: 0.000619, l5: 0.001047, l6: 0.002125
[epoch: 274/1000, batch:   404/ 1052, ite: 71900] train loss: 0.005387, tar: 0.000335 
l0: 0.000320, l1: 0.000336, l2: 0.000321, l3: 0.000340, l4: 0.000466, l5: 0.000873, l6: 0.001522
[epoch: 274/1000, batch:   484/ 1052, ite: 71920] train loss: 0.005387, tar: 0.000335 
l0: 0.000346, l1: 0.000345, l2: 0.000361, l3: 0.000389, l4: 0.000648, l5: 0.001135, l6: 0.001800
[epoch: 274/1000, batch:   564/ 1052, ite: 71940] train loss: 0.005386, tar: 0.000335 
l0: 0.000366, l1: 0.000402, l2: 0.000352, l3: 0.000404, l4: 0.000833, l5: 0.001864, l6: 0.002823
[epoch: 274/1000, batch:   644/ 1052, ite: 71960] train loss: 0.005386, tar: 0.000335 
l0: 0.001770, l1: 0.001642, l2: 0.002150, l3: 0.001901, l4: 0.001882, l5: 0.003904, l6: 0.005803
[epoch: 274/1000, batch:   724/ 1052, ite: 71980] train loss: 0.005389, tar: 0.000335 
l0: 0.000275, l1: 0.000282, l2: 0.000308, l3: 0.000342, l4: 0.000818, l5: 0.001372, l6: 0.002283
[epoch: 274/1000, batch:   804/ 1052, ite: 72000] train loss: 0.005389, tar: 0.000335 
l0: 0.000479, l1: 0.000453, l2: 0.000588, l3: 0.000791, l4: 0.001101, l5: 0.002382, l6: 0.004189
[epoch: 274/1000, batch:   884/ 1052, ite: 72020] train loss: 0.005391, tar: 0.000336 
l0: 0.000438, l1: 0.000437, l2: 0.000486, l3: 0.000619, l4: 0.001305, l5: 0.001677, l6: 0.003882
[epoch: 274/1000, batch:   964/ 1052, ite: 72040] train loss: 0.005391, tar: 0.000336 
l0: 0.000345, l1: 0.000378, l2: 0.000341, l3: 0.000374, l4: 0.000575, l5: 0.000923, l6: 0.002016
[epoch: 274/1000, batch:  1044/ 1052, ite: 72060] train loss: 0.005389, tar: 0.000335 
[Epoch 274/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000393, l1: 0.000384, l2: 0.000391, l3: 0.000509, l4: 0.000772, l5: 0.001605, l6: 0.002710
[epoch: 275/1000, batch:    72/ 1052, ite: 72080] train loss: 0.005390, tar: 0.000335 
l0: 0.000318, l1: 0.000329, l2: 0.000334, l3: 0.000389, l4: 0.000623, l5: 0.001390, l6: 0.001858
[epoch: 275/1000, batch:   152/ 1052, ite: 72100] train loss: 0.005389, tar: 0.000335 
l0: 0.000267, l1: 0.000285, l2: 0.000294, l3: 0.000275, l4: 0.000428, l5: 0.000818, l6: 0.001570
[epoch: 275/1000, batch:   232/ 1052, ite: 72120] train loss: 0.005387, tar: 0.000335 
l0: 0.000257, l1: 0.000262, l2: 0.000279, l3: 0.000346, l4: 0.000423, l5: 0.001065, l6: 0.001664
[epoch: 275/1000, batch:   312/ 1052, ite: 72140] train loss: 0.005389, tar: 0.000335 
l0: 0.000292, l1: 0.000301, l2: 0.000284, l3: 0.000326, l4: 0.000468, l5: 0.001254, l6: 0.001830
[epoch: 275/1000, batch:   392/ 1052, ite: 72160] train loss: 0.005388, tar: 0.000335 
l0: 0.000258, l1: 0.000252, l2: 0.000299, l3: 0.000344, l4: 0.000666, l5: 0.001203, l6: 0.001743
[epoch: 275/1000, batch:   472/ 1052, ite: 72180] train loss: 0.005389, tar: 0.000335 
l0: 0.000195, l1: 0.000181, l2: 0.000221, l3: 0.000358, l4: 0.000448, l5: 0.001036, l6: 0.001576
[epoch: 275/1000, batch:   552/ 1052, ite: 72200] train loss: 0.005388, tar: 0.000335 
l0: 0.000355, l1: 0.000374, l2: 0.000377, l3: 0.000454, l4: 0.000514, l5: 0.001370, l6: 0.002198
[epoch: 275/1000, batch:   632/ 1052, ite: 72220] train loss: 0.005388, tar: 0.000335 
l0: 0.000364, l1: 0.000380, l2: 0.000367, l3: 0.000454, l4: 0.000687, l5: 0.001151, l6: 0.002320
[epoch: 275/1000, batch:   712/ 1052, ite: 72240] train loss: 0.005387, tar: 0.000335 
l0: 0.000252, l1: 0.000246, l2: 0.000339, l3: 0.000508, l4: 0.000713, l5: 0.001356, l6: 0.001970
[epoch: 275/1000, batch:   792/ 1052, ite: 72260] train loss: 0.005386, tar: 0.000335 
l0: 0.000215, l1: 0.000220, l2: 0.000243, l3: 0.000325, l4: 0.000559, l5: 0.000952, l6: 0.001478
[epoch: 275/1000, batch:   872/ 1052, ite: 72280] train loss: 0.005385, tar: 0.000335 
l0: 0.000235, l1: 0.000229, l2: 0.000268, l3: 0.000311, l4: 0.000385, l5: 0.000863, l6: 0.001664
[epoch: 275/1000, batch:   952/ 1052, ite: 72300] train loss: 0.005385, tar: 0.000335 
l0: 0.000412, l1: 0.000421, l2: 0.000415, l3: 0.000480, l4: 0.000687, l5: 0.001693, l6: 0.003322
[epoch: 275/1000, batch:  1032/ 1052, ite: 72320] train loss: 0.005384, tar: 0.000334 
[Epoch 275/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000580, l1: 0.000610, l2: 0.000616, l3: 0.000654, l4: 0.001073, l5: 0.002531, l6: 0.002921
[epoch: 276/1000, batch:    60/ 1052, ite: 72340] train loss: 0.005385, tar: 0.000334 
l0: 0.000240, l1: 0.000238, l2: 0.000257, l3: 0.000334, l4: 0.000487, l5: 0.001113, l6: 0.001792
[epoch: 276/1000, batch:   140/ 1052, ite: 72360] train loss: 0.005382, tar: 0.000334 
l0: 0.000211, l1: 0.000213, l2: 0.000223, l3: 0.000262, l4: 0.000541, l5: 0.000635, l6: 0.001271
[epoch: 276/1000, batch:   220/ 1052, ite: 72380] train loss: 0.005383, tar: 0.000334 
l0: 0.000275, l1: 0.000286, l2: 0.000291, l3: 0.000346, l4: 0.000581, l5: 0.001021, l6: 0.001973
[epoch: 276/1000, batch:   300/ 1052, ite: 72400] train loss: 0.005383, tar: 0.000334 
l0: 0.000488, l1: 0.000482, l2: 0.000513, l3: 0.000597, l4: 0.000813, l5: 0.001480, l6: 0.003254
[epoch: 276/1000, batch:   380/ 1052, ite: 72420] train loss: 0.005382, tar: 0.000334 
l0: 0.000211, l1: 0.000213, l2: 0.000256, l3: 0.000340, l4: 0.000527, l5: 0.000814, l6: 0.001607
[epoch: 276/1000, batch:   460/ 1052, ite: 72440] train loss: 0.005381, tar: 0.000334 
l0: 0.000247, l1: 0.000246, l2: 0.000255, l3: 0.000319, l4: 0.000522, l5: 0.000761, l6: 0.000909
[epoch: 276/1000, batch:   540/ 1052, ite: 72460] train loss: 0.005381, tar: 0.000334 
l0: 0.000403, l1: 0.000406, l2: 0.000444, l3: 0.000462, l4: 0.000811, l5: 0.002395, l6: 0.003690
[epoch: 276/1000, batch:   620/ 1052, ite: 72480] train loss: 0.005382, tar: 0.000334 
l0: 0.000275, l1: 0.000283, l2: 0.000303, l3: 0.000361, l4: 0.000768, l5: 0.001112, l6: 0.002438
[epoch: 276/1000, batch:   700/ 1052, ite: 72500] train loss: 0.005381, tar: 0.000334 
l0: 0.000444, l1: 0.000449, l2: 0.000522, l3: 0.000553, l4: 0.000874, l5: 0.001601, l6: 0.003099
[epoch: 276/1000, batch:   780/ 1052, ite: 72520] train loss: 0.005380, tar: 0.000334 
l0: 0.000102, l1: 0.000097, l2: 0.000123, l3: 0.000172, l4: 0.000294, l5: 0.000407, l6: 0.001003
[epoch: 276/1000, batch:   860/ 1052, ite: 72540] train loss: 0.005379, tar: 0.000334 
l0: 0.000582, l1: 0.000571, l2: 0.000626, l3: 0.000935, l4: 0.001664, l5: 0.002017, l6: 0.003819
[epoch: 276/1000, batch:   940/ 1052, ite: 72560] train loss: 0.005378, tar: 0.000333 
l0: 0.000438, l1: 0.000452, l2: 0.000432, l3: 0.000411, l4: 0.000593, l5: 0.000830, l6: 0.001584
[epoch: 276/1000, batch:  1020/ 1052, ite: 72580] train loss: 0.005378, tar: 0.000333 
[Epoch 276/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000424, l1: 0.000430, l2: 0.000424, l3: 0.000625, l4: 0.000874, l5: 0.001012, l6: 0.002582
[epoch: 277/1000, batch:    48/ 1052, ite: 72600] train loss: 0.005377, tar: 0.000333 
l0: 0.000259, l1: 0.000286, l2: 0.000257, l3: 0.000346, l4: 0.000645, l5: 0.001267, l6: 0.003020
[epoch: 277/1000, batch:   128/ 1052, ite: 72620] train loss: 0.005378, tar: 0.000333 
l0: 0.000237, l1: 0.000230, l2: 0.000298, l3: 0.000447, l4: 0.000616, l5: 0.000759, l6: 0.001678
[epoch: 277/1000, batch:   208/ 1052, ite: 72640] train loss: 0.005381, tar: 0.000333 
l0: 0.000409, l1: 0.000429, l2: 0.000453, l3: 0.000485, l4: 0.000682, l5: 0.001128, l6: 0.002406
[epoch: 277/1000, batch:   288/ 1052, ite: 72660] train loss: 0.005380, tar: 0.000333 
l0: 0.000252, l1: 0.000252, l2: 0.000272, l3: 0.000351, l4: 0.000476, l5: 0.001186, l6: 0.001855
[epoch: 277/1000, batch:   368/ 1052, ite: 72680] train loss: 0.005380, tar: 0.000333 
l0: 0.000304, l1: 0.000303, l2: 0.000308, l3: 0.000436, l4: 0.000739, l5: 0.001332, l6: 0.002014
[epoch: 277/1000, batch:   448/ 1052, ite: 72700] train loss: 0.005378, tar: 0.000333 
l0: 0.000263, l1: 0.000270, l2: 0.000286, l3: 0.000287, l4: 0.000507, l5: 0.000881, l6: 0.001142
[epoch: 277/1000, batch:   528/ 1052, ite: 72720] train loss: 0.005376, tar: 0.000333 
l0: 0.000514, l1: 0.000522, l2: 0.000541, l3: 0.000622, l4: 0.000812, l5: 0.001693, l6: 0.003256
[epoch: 277/1000, batch:   608/ 1052, ite: 72740] train loss: 0.005376, tar: 0.000333 
l0: 0.000472, l1: 0.000502, l2: 0.000505, l3: 0.000538, l4: 0.000931, l5: 0.001759, l6: 0.003652
[epoch: 277/1000, batch:   688/ 1052, ite: 72760] train loss: 0.005375, tar: 0.000333 
l0: 0.000324, l1: 0.000315, l2: 0.000346, l3: 0.000452, l4: 0.000557, l5: 0.001091, l6: 0.001902
[epoch: 277/1000, batch:   768/ 1052, ite: 72780] train loss: 0.005374, tar: 0.000333 
l0: 0.000346, l1: 0.000346, l2: 0.000383, l3: 0.000501, l4: 0.000808, l5: 0.001542, l6: 0.003063
[epoch: 277/1000, batch:   848/ 1052, ite: 72800] train loss: 0.005373, tar: 0.000333 
l0: 0.000231, l1: 0.000229, l2: 0.000270, l3: 0.000315, l4: 0.000561, l5: 0.001079, l6: 0.001934
[epoch: 277/1000, batch:   928/ 1052, ite: 72820] train loss: 0.005372, tar: 0.000332 
l0: 0.000195, l1: 0.000198, l2: 0.000203, l3: 0.000266, l4: 0.000547, l5: 0.000864, l6: 0.001411
[epoch: 277/1000, batch:  1008/ 1052, ite: 72840] train loss: 0.005373, tar: 0.000332 
[Epoch 277/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000370, l1: 0.000377, l2: 0.000409, l3: 0.000470, l4: 0.000820, l5: 0.001885, l6: 0.002194
[epoch: 278/1000, batch:    36/ 1052, ite: 72860] train loss: 0.005372, tar: 0.000332 
l0: 0.000267, l1: 0.000253, l2: 0.000312, l3: 0.000459, l4: 0.000655, l5: 0.001081, l6: 0.001392
[epoch: 278/1000, batch:   116/ 1052, ite: 72880] train loss: 0.005371, tar: 0.000332 
l0: 0.000309, l1: 0.000307, l2: 0.000382, l3: 0.000397, l4: 0.000597, l5: 0.001027, l6: 0.002442
[epoch: 278/1000, batch:   196/ 1052, ite: 72900] train loss: 0.005370, tar: 0.000332 
l0: 0.000179, l1: 0.000181, l2: 0.000200, l3: 0.000254, l4: 0.000465, l5: 0.001037, l6: 0.001105
[epoch: 278/1000, batch:   276/ 1052, ite: 72920] train loss: 0.005370, tar: 0.000332 
l0: 0.000138, l1: 0.000139, l2: 0.000154, l3: 0.000186, l4: 0.000278, l5: 0.000751, l6: 0.001091
[epoch: 278/1000, batch:   356/ 1052, ite: 72940] train loss: 0.005369, tar: 0.000332 
l0: 0.000168, l1: 0.000162, l2: 0.000184, l3: 0.000212, l4: 0.000394, l5: 0.000623, l6: 0.000732
[epoch: 278/1000, batch:   436/ 1052, ite: 72960] train loss: 0.005369, tar: 0.000332 
l0: 0.000297, l1: 0.000302, l2: 0.000318, l3: 0.000417, l4: 0.000597, l5: 0.001735, l6: 0.002818
[epoch: 278/1000, batch:   516/ 1052, ite: 72980] train loss: 0.005368, tar: 0.000332 
l0: 0.000117, l1: 0.000117, l2: 0.000147, l3: 0.000173, l4: 0.000422, l5: 0.000845, l6: 0.001447
[epoch: 278/1000, batch:   596/ 1052, ite: 73000] train loss: 0.005368, tar: 0.000332 
l0: 0.000165, l1: 0.000158, l2: 0.000214, l3: 0.000240, l4: 0.000413, l5: 0.000683, l6: 0.001144
[epoch: 278/1000, batch:   676/ 1052, ite: 73020] train loss: 0.005367, tar: 0.000331 
l0: 0.000328, l1: 0.000326, l2: 0.000462, l3: 0.000461, l4: 0.000587, l5: 0.001065, l6: 0.001798
[epoch: 278/1000, batch:   756/ 1052, ite: 73040] train loss: 0.005366, tar: 0.000331 
l0: 0.000265, l1: 0.000255, l2: 0.000330, l3: 0.000410, l4: 0.000749, l5: 0.001114, l6: 0.002189
[epoch: 278/1000, batch:   836/ 1052, ite: 73060] train loss: 0.005367, tar: 0.000331 
l0: 0.000504, l1: 0.000540, l2: 0.000548, l3: 0.000574, l4: 0.000782, l5: 0.001726, l6: 0.003091
[epoch: 278/1000, batch:   916/ 1052, ite: 73080] train loss: 0.005368, tar: 0.000331 
l0: 0.000229, l1: 0.000222, l2: 0.000225, l3: 0.000338, l4: 0.000663, l5: 0.000724, l6: 0.001102
[epoch: 278/1000, batch:   996/ 1052, ite: 73100] train loss: 0.005368, tar: 0.000331 
[Epoch 278/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000321, l1: 0.000342, l2: 0.000311, l3: 0.000303, l4: 0.000624, l5: 0.000652, l6: 0.001851
[epoch: 279/1000, batch:    24/ 1052, ite: 73120] train loss: 0.005366, tar: 0.000331 
l0: 0.000373, l1: 0.000374, l2: 0.000448, l3: 0.000499, l4: 0.000751, l5: 0.001573, l6: 0.001982
[epoch: 279/1000, batch:   104/ 1052, ite: 73140] train loss: 0.005365, tar: 0.000331 
l0: 0.000325, l1: 0.000323, l2: 0.000341, l3: 0.000442, l4: 0.000695, l5: 0.001525, l6: 0.002113
[epoch: 279/1000, batch:   184/ 1052, ite: 73160] train loss: 0.005365, tar: 0.000331 
l0: 0.000392, l1: 0.000407, l2: 0.000414, l3: 0.000521, l4: 0.000674, l5: 0.001169, l6: 0.002214
[epoch: 279/1000, batch:   264/ 1052, ite: 73180] train loss: 0.005365, tar: 0.000331 
l0: 0.000403, l1: 0.000410, l2: 0.000470, l3: 0.000419, l4: 0.000541, l5: 0.001205, l6: 0.001878
[epoch: 279/1000, batch:   344/ 1052, ite: 73200] train loss: 0.005365, tar: 0.000331 
l0: 0.000330, l1: 0.000346, l2: 0.000390, l3: 0.000538, l4: 0.000793, l5: 0.001718, l6: 0.003217
[epoch: 279/1000, batch:   424/ 1052, ite: 73220] train loss: 0.005364, tar: 0.000331 
l0: 0.000240, l1: 0.000238, l2: 0.000260, l3: 0.000332, l4: 0.000504, l5: 0.000585, l6: 0.001366
[epoch: 279/1000, batch:   504/ 1052, ite: 73240] train loss: 0.005362, tar: 0.000331 
l0: 0.000313, l1: 0.000306, l2: 0.000357, l3: 0.000503, l4: 0.000842, l5: 0.001439, l6: 0.002257
[epoch: 279/1000, batch:   584/ 1052, ite: 73260] train loss: 0.005361, tar: 0.000331 
l0: 0.000153, l1: 0.000152, l2: 0.000195, l3: 0.000235, l4: 0.000476, l5: 0.000914, l6: 0.001164
[epoch: 279/1000, batch:   664/ 1052, ite: 73280] train loss: 0.005362, tar: 0.000330 
l0: 0.000260, l1: 0.000259, l2: 0.000296, l3: 0.000373, l4: 0.000638, l5: 0.001243, l6: 0.002338
[epoch: 279/1000, batch:   744/ 1052, ite: 73300] train loss: 0.005361, tar: 0.000330 
l0: 0.000214, l1: 0.000212, l2: 0.000266, l3: 0.000300, l4: 0.000431, l5: 0.001164, l6: 0.001642
[epoch: 279/1000, batch:   824/ 1052, ite: 73320] train loss: 0.005360, tar: 0.000330 
l0: 0.000238, l1: 0.000243, l2: 0.000262, l3: 0.000295, l4: 0.000345, l5: 0.001201, l6: 0.001840
[epoch: 279/1000, batch:   904/ 1052, ite: 73340] train loss: 0.005361, tar: 0.000330 
l0: 0.000218, l1: 0.000226, l2: 0.000270, l3: 0.000271, l4: 0.000570, l5: 0.001002, l6: 0.001508
[epoch: 279/1000, batch:   984/ 1052, ite: 73360] train loss: 0.005360, tar: 0.000330 
[Epoch 279/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000330, l1: 0.000352, l2: 0.000327, l3: 0.000349, l4: 0.000469, l5: 0.000812, l6: 0.002045
[epoch: 280/1000, batch:    12/ 1052, ite: 73380] train loss: 0.005359, tar: 0.000330 
l0: 0.000334, l1: 0.000354, l2: 0.000362, l3: 0.000360, l4: 0.000578, l5: 0.000799, l6: 0.001645
[epoch: 280/1000, batch:    92/ 1052, ite: 73400] train loss: 0.005359, tar: 0.000330 
l0: 0.000333, l1: 0.000328, l2: 0.000345, l3: 0.000455, l4: 0.000806, l5: 0.001050, l6: 0.002186
[epoch: 280/1000, batch:   172/ 1052, ite: 73420] train loss: 0.005359, tar: 0.000330 
l0: 0.000300, l1: 0.000302, l2: 0.000345, l3: 0.000401, l4: 0.000554, l5: 0.001125, l6: 0.001633
[epoch: 280/1000, batch:   252/ 1052, ite: 73440] train loss: 0.005359, tar: 0.000330 
l0: 0.000221, l1: 0.000224, l2: 0.000247, l3: 0.000327, l4: 0.000521, l5: 0.001109, l6: 0.001873
[epoch: 280/1000, batch:   332/ 1052, ite: 73460] train loss: 0.005358, tar: 0.000330 
l0: 0.000171, l1: 0.000165, l2: 0.000213, l3: 0.000273, l4: 0.000355, l5: 0.000770, l6: 0.001285
[epoch: 280/1000, batch:   412/ 1052, ite: 73480] train loss: 0.005357, tar: 0.000330 
l0: 0.000302, l1: 0.000312, l2: 0.000378, l3: 0.000491, l4: 0.000781, l5: 0.001834, l6: 0.002701
[epoch: 280/1000, batch:   492/ 1052, ite: 73500] train loss: 0.005356, tar: 0.000330 
l0: 0.000211, l1: 0.000222, l2: 0.000230, l3: 0.000228, l4: 0.000384, l5: 0.000897, l6: 0.001569
[epoch: 280/1000, batch:   572/ 1052, ite: 73520] train loss: 0.005355, tar: 0.000330 
l0: 0.000324, l1: 0.000346, l2: 0.000372, l3: 0.000407, l4: 0.000690, l5: 0.001550, l6: 0.002749
[epoch: 280/1000, batch:   652/ 1052, ite: 73540] train loss: 0.005355, tar: 0.000329 
l0: 0.000336, l1: 0.000345, l2: 0.000357, l3: 0.000384, l4: 0.000548, l5: 0.000895, l6: 0.001935
[epoch: 280/1000, batch:   732/ 1052, ite: 73560] train loss: 0.005354, tar: 0.000329 
l0: 0.000340, l1: 0.000364, l2: 0.000371, l3: 0.000375, l4: 0.000633, l5: 0.001472, l6: 0.003323
[epoch: 280/1000, batch:   812/ 1052, ite: 73580] train loss: 0.005354, tar: 0.000329 
l0: 0.000229, l1: 0.000214, l2: 0.000251, l3: 0.000336, l4: 0.000595, l5: 0.000970, l6: 0.001228
[epoch: 280/1000, batch:   892/ 1052, ite: 73600] train loss: 0.005353, tar: 0.000329 
l0: 0.000294, l1: 0.000300, l2: 0.000318, l3: 0.000392, l4: 0.000689, l5: 0.001217, l6: 0.002587
[epoch: 280/1000, batch:   972/ 1052, ite: 73620] train loss: 0.005353, tar: 0.000329 
l0: 0.000159, l1: 0.000152, l2: 0.000169, l3: 0.000217, l4: 0.000414, l5: 0.000612, l6: 0.001303
[epoch: 280/1000, batch:  1052/ 1052, ite: 73640] train loss: 0.005352, tar: 0.000329 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_73640.pkl
[Epoch 280/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000206, l1: 0.000204, l2: 0.000227, l3: 0.000280, l4: 0.000405, l5: 0.000840, l6: 0.001752
[epoch: 281/1000, batch:    80/ 1052, ite: 73660] train loss: 0.005484, tar: 0.000298 
l0: 0.000494, l1: 0.000520, l2: 0.000557, l3: 0.000610, l4: 0.000862, l5: 0.001312, l6: 0.004120
[epoch: 281/1000, batch:   160/ 1052, ite: 73680] train loss: 0.005203, tar: 0.000285 
l0: 0.000534, l1: 0.000563, l2: 0.000530, l3: 0.000565, l4: 0.000806, l5: 0.001874, l6: 0.003510
[epoch: 281/1000, batch:   240/ 1052, ite: 73700] train loss: 0.005293, tar: 0.000287 
l0: 0.000232, l1: 0.000233, l2: 0.000263, l3: 0.000400, l4: 0.000649, l5: 0.001179, l6: 0.001929
[epoch: 281/1000, batch:   320/ 1052, ite: 73720] train loss: 0.005253, tar: 0.000287 
l0: 0.000357, l1: 0.000355, l2: 0.000391, l3: 0.000455, l4: 0.000559, l5: 0.001232, l6: 0.002402
[epoch: 281/1000, batch:   400/ 1052, ite: 73740] train loss: 0.005238, tar: 0.000290 
l0: 0.000270, l1: 0.000274, l2: 0.000308, l3: 0.000325, l4: 0.000424, l5: 0.000832, l6: 0.002524
[epoch: 281/1000, batch:   480/ 1052, ite: 73760] train loss: 0.005072, tar: 0.000283 
l0: 0.000311, l1: 0.000309, l2: 0.000409, l3: 0.000488, l4: 0.000880, l5: 0.001165, l6: 0.001935
[epoch: 281/1000, batch:   560/ 1052, ite: 73780] train loss: 0.005159, tar: 0.000287 
l0: 0.000309, l1: 0.000309, l2: 0.000342, l3: 0.000401, l4: 0.000521, l5: 0.001103, l6: 0.001694
[epoch: 281/1000, batch:   640/ 1052, ite: 73800] train loss: 0.005129, tar: 0.000288 
l0: 0.000255, l1: 0.000261, l2: 0.000281, l3: 0.000330, l4: 0.000435, l5: 0.000801, l6: 0.001769
[epoch: 281/1000, batch:   720/ 1052, ite: 73820] train loss: 0.005131, tar: 0.000288 
l0: 0.000576, l1: 0.000609, l2: 0.000632, l3: 0.000522, l4: 0.000563, l5: 0.001029, l6: 0.002596
[epoch: 281/1000, batch:   800/ 1052, ite: 73840] train loss: 0.005143, tar: 0.000290 
l0: 0.000273, l1: 0.000283, l2: 0.000283, l3: 0.000317, l4: 0.000468, l5: 0.001135, l6: 0.001383
[epoch: 281/1000, batch:   880/ 1052, ite: 73860] train loss: 0.005134, tar: 0.000290 
l0: 0.000287, l1: 0.000294, l2: 0.000324, l3: 0.000419, l4: 0.000368, l5: 0.001139, l6: 0.001553
[epoch: 281/1000, batch:   960/ 1052, ite: 73880] train loss: 0.005133, tar: 0.000289 
l0: 0.000187, l1: 0.000192, l2: 0.000226, l3: 0.000262, l4: 0.000388, l5: 0.001034, l6: 0.001693
[epoch: 281/1000, batch:  1040/ 1052, ite: 73900] train loss: 0.005080, tar: 0.000284 
[Epoch 281/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000426, l1: 0.000438, l2: 0.000484, l3: 0.000606, l4: 0.001226, l5: 0.001753, l6: 0.003598
[epoch: 282/1000, batch:    68/ 1052, ite: 73920] train loss: 0.005029, tar: 0.000281 
l0: 0.000350, l1: 0.000363, l2: 0.000307, l3: 0.000309, l4: 0.000288, l5: 0.000429, l6: 0.000906
[epoch: 282/1000, batch:   148/ 1052, ite: 73940] train loss: 0.005041, tar: 0.000284 
l0: 0.000675, l1: 0.000669, l2: 0.000704, l3: 0.000777, l4: 0.001442, l5: 0.002486, l6: 0.003431
[epoch: 282/1000, batch:   228/ 1052, ite: 73960] train loss: 0.005070, tar: 0.000287 
l0: 0.000338, l1: 0.000334, l2: 0.000367, l3: 0.000478, l4: 0.000640, l5: 0.001117, l6: 0.001833
[epoch: 282/1000, batch:   308/ 1052, ite: 73980] train loss: 0.005070, tar: 0.000287 
l0: 0.000224, l1: 0.000260, l2: 0.000240, l3: 0.000260, l4: 0.000297, l5: 0.000927, l6: 0.000987
[epoch: 282/1000, batch:   388/ 1052, ite: 74000] train loss: 0.005033, tar: 0.000284 
l0: 0.000452, l1: 0.000451, l2: 0.000463, l3: 0.000589, l4: 0.000847, l5: 0.001736, l6: 0.003208
[epoch: 282/1000, batch:   468/ 1052, ite: 74020] train loss: 0.005053, tar: 0.000285 
l0: 0.000290, l1: 0.000294, l2: 0.000305, l3: 0.000447, l4: 0.000580, l5: 0.001157, l6: 0.001961
[epoch: 282/1000, batch:   548/ 1052, ite: 74040] train loss: 0.005074, tar: 0.000286 
l0: 0.000163, l1: 0.000168, l2: 0.000195, l3: 0.000207, l4: 0.000481, l5: 0.001063, l6: 0.001794
[epoch: 282/1000, batch:   628/ 1052, ite: 74060] train loss: 0.005106, tar: 0.000289 
l0: 0.000302, l1: 0.000310, l2: 0.000336, l3: 0.000367, l4: 0.000519, l5: 0.001274, l6: 0.001632
[epoch: 282/1000, batch:   708/ 1052, ite: 74080] train loss: 0.005096, tar: 0.000289 
l0: 0.000396, l1: 0.000394, l2: 0.000483, l3: 0.000523, l4: 0.001184, l5: 0.002001, l6: 0.002876
[epoch: 282/1000, batch:   788/ 1052, ite: 74100] train loss: 0.005086, tar: 0.000289 
l0: 0.000167, l1: 0.000164, l2: 0.000212, l3: 0.000268, l4: 0.000488, l5: 0.000522, l6: 0.001775
[epoch: 282/1000, batch:   868/ 1052, ite: 74120] train loss: 0.005083, tar: 0.000288 
l0: 0.000178, l1: 0.000176, l2: 0.000192, l3: 0.000286, l4: 0.000369, l5: 0.000963, l6: 0.001944
[epoch: 282/1000, batch:   948/ 1052, ite: 74140] train loss: 0.005100, tar: 0.000289 
l0: 0.000226, l1: 0.000239, l2: 0.000239, l3: 0.000299, l4: 0.000450, l5: 0.001102, l6: 0.001708
[epoch: 282/1000, batch:  1028/ 1052, ite: 74160] train loss: 0.005091, tar: 0.000288 
[Epoch 282/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000363, l1: 0.000340, l2: 0.000408, l3: 0.000460, l4: 0.000646, l5: 0.001388, l6: 0.001999
[epoch: 283/1000, batch:    56/ 1052, ite: 74180] train loss: 0.005117, tar: 0.000289 
l0: 0.000204, l1: 0.000204, l2: 0.000279, l3: 0.000363, l4: 0.000472, l5: 0.001351, l6: 0.001795
[epoch: 283/1000, batch:   136/ 1052, ite: 74200] train loss: 0.005126, tar: 0.000290 
l0: 0.000147, l1: 0.000136, l2: 0.000183, l3: 0.000272, l4: 0.000422, l5: 0.000923, l6: 0.001366
[epoch: 283/1000, batch:   216/ 1052, ite: 74220] train loss: 0.005111, tar: 0.000288 
l0: 0.000202, l1: 0.000198, l2: 0.000256, l3: 0.000306, l4: 0.000723, l5: 0.001520, l6: 0.002153
[epoch: 283/1000, batch:   296/ 1052, ite: 74240] train loss: 0.005102, tar: 0.000288 
l0: 0.000180, l1: 0.000177, l2: 0.000215, l3: 0.000245, l4: 0.000402, l5: 0.000854, l6: 0.001885
[epoch: 283/1000, batch:   376/ 1052, ite: 74260] train loss: 0.005112, tar: 0.000289 
l0: 0.000346, l1: 0.000348, l2: 0.000356, l3: 0.000430, l4: 0.000636, l5: 0.000951, l6: 0.001556
[epoch: 283/1000, batch:   456/ 1052, ite: 74280] train loss: 0.005099, tar: 0.000289 
l0: 0.000319, l1: 0.000329, l2: 0.000336, l3: 0.000351, l4: 0.000479, l5: 0.001154, l6: 0.001858
[epoch: 283/1000, batch:   536/ 1052, ite: 74300] train loss: 0.005098, tar: 0.000289 
l0: 0.000549, l1: 0.000562, l2: 0.000596, l3: 0.000593, l4: 0.000954, l5: 0.001581, l6: 0.002629
[epoch: 283/1000, batch:   616/ 1052, ite: 74320] train loss: 0.005096, tar: 0.000290 
l0: 0.000429, l1: 0.000416, l2: 0.000552, l3: 0.000610, l4: 0.001381, l5: 0.001950, l6: 0.003602
[epoch: 283/1000, batch:   696/ 1052, ite: 74340] train loss: 0.005127, tar: 0.000291 
l0: 0.000271, l1: 0.000274, l2: 0.000292, l3: 0.000347, l4: 0.000499, l5: 0.000946, l6: 0.002085
[epoch: 283/1000, batch:   776/ 1052, ite: 74360] train loss: 0.005112, tar: 0.000291 
l0: 0.000442, l1: 0.000448, l2: 0.000495, l3: 0.000523, l4: 0.000918, l5: 0.001783, l6: 0.002416
[epoch: 283/1000, batch:   856/ 1052, ite: 74380] train loss: 0.005100, tar: 0.000290 
l0: 0.000211, l1: 0.000222, l2: 0.000214, l3: 0.000252, l4: 0.000379, l5: 0.000946, l6: 0.001072
[epoch: 283/1000, batch:   936/ 1052, ite: 74400] train loss: 0.005088, tar: 0.000289 
l0: 0.000213, l1: 0.000218, l2: 0.000246, l3: 0.000309, l4: 0.000510, l5: 0.000912, l6: 0.002744
[epoch: 283/1000, batch:  1016/ 1052, ite: 74420] train loss: 0.005107, tar: 0.000290 
[Epoch 283/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000389, l1: 0.000409, l2: 0.000387, l3: 0.000473, l4: 0.001092, l5: 0.001943, l6: 0.003065
[epoch: 284/1000, batch:    44/ 1052, ite: 74440] train loss: 0.005112, tar: 0.000290 
l0: 0.000170, l1: 0.000166, l2: 0.000188, l3: 0.000217, l4: 0.000281, l5: 0.000709, l6: 0.001347
[epoch: 284/1000, batch:   124/ 1052, ite: 74460] train loss: 0.005100, tar: 0.000289 
l0: 0.000149, l1: 0.000147, l2: 0.000162, l3: 0.000208, l4: 0.000405, l5: 0.000900, l6: 0.001201
[epoch: 284/1000, batch:   204/ 1052, ite: 74480] train loss: 0.005090, tar: 0.000288 
l0: 0.000659, l1: 0.000671, l2: 0.000684, l3: 0.000824, l4: 0.001290, l5: 0.002393, l6: 0.003771
[epoch: 284/1000, batch:   284/ 1052, ite: 74500] train loss: 0.005077, tar: 0.000287 
l0: 0.000319, l1: 0.000324, l2: 0.000355, l3: 0.000369, l4: 0.000636, l5: 0.001124, l6: 0.001221
[epoch: 284/1000, batch:   364/ 1052, ite: 74520] train loss: 0.005085, tar: 0.000288 
l0: 0.000336, l1: 0.000339, l2: 0.000352, l3: 0.000414, l4: 0.000640, l5: 0.001483, l6: 0.002757
[epoch: 284/1000, batch:   444/ 1052, ite: 74540] train loss: 0.005083, tar: 0.000287 
l0: 0.000166, l1: 0.000166, l2: 0.000190, l3: 0.000272, l4: 0.000406, l5: 0.000832, l6: 0.001177
[epoch: 284/1000, batch:   524/ 1052, ite: 74560] train loss: 0.005089, tar: 0.000288 
l0: 0.000497, l1: 0.000529, l2: 0.000543, l3: 0.000504, l4: 0.000818, l5: 0.001484, l6: 0.002544
[epoch: 284/1000, batch:   604/ 1052, ite: 74580] train loss: 0.005106, tar: 0.000290 
l0: 0.000290, l1: 0.000287, l2: 0.000322, l3: 0.000447, l4: 0.000771, l5: 0.001354, l6: 0.002692
[epoch: 284/1000, batch:   684/ 1052, ite: 74600] train loss: 0.005110, tar: 0.000290 
l0: 0.000287, l1: 0.000295, l2: 0.000315, l3: 0.000338, l4: 0.000570, l5: 0.001296, l6: 0.002250
[epoch: 284/1000, batch:   764/ 1052, ite: 74620] train loss: 0.005117, tar: 0.000290 
l0: 0.000357, l1: 0.000360, l2: 0.000385, l3: 0.000393, l4: 0.000520, l5: 0.001024, l6: 0.001675
[epoch: 284/1000, batch:   844/ 1052, ite: 74640] train loss: 0.005109, tar: 0.000290 
l0: 0.000306, l1: 0.000306, l2: 0.000323, l3: 0.000392, l4: 0.000415, l5: 0.001218, l6: 0.001541
[epoch: 284/1000, batch:   924/ 1052, ite: 74660] train loss: 0.005118, tar: 0.000290 
l0: 0.000490, l1: 0.000503, l2: 0.000512, l3: 0.000570, l4: 0.000892, l5: 0.001654, l6: 0.001812
[epoch: 284/1000, batch:  1004/ 1052, ite: 74680] train loss: 0.005112, tar: 0.000290 
[Epoch 284/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000214, l1: 0.000214, l2: 0.000278, l3: 0.000359, l4: 0.000578, l5: 0.001458, l6: 0.002322
[epoch: 285/1000, batch:    32/ 1052, ite: 74700] train loss: 0.005113, tar: 0.000290 
l0: 0.000518, l1: 0.000521, l2: 0.000584, l3: 0.000648, l4: 0.000936, l5: 0.001511, l6: 0.002554
[epoch: 285/1000, batch:   112/ 1052, ite: 74720] train loss: 0.005111, tar: 0.000290 
l0: 0.000281, l1: 0.000282, l2: 0.000294, l3: 0.000367, l4: 0.000728, l5: 0.001077, l6: 0.002551
[epoch: 285/1000, batch:   192/ 1052, ite: 74740] train loss: 0.005121, tar: 0.000291 
l0: 0.000197, l1: 0.000195, l2: 0.000217, l3: 0.000256, l4: 0.000435, l5: 0.000733, l6: 0.001200
[epoch: 285/1000, batch:   272/ 1052, ite: 74760] train loss: 0.005126, tar: 0.000290 
l0: 0.000392, l1: 0.000395, l2: 0.000481, l3: 0.000582, l4: 0.000893, l5: 0.002029, l6: 0.003091
[epoch: 285/1000, batch:   352/ 1052, ite: 74780] train loss: 0.005119, tar: 0.000290 
l0: 0.000120, l1: 0.000133, l2: 0.000137, l3: 0.000143, l4: 0.000383, l5: 0.000424, l6: 0.001073
[epoch: 285/1000, batch:   432/ 1052, ite: 74800] train loss: 0.005124, tar: 0.000290 
l0: 0.000164, l1: 0.000171, l2: 0.000177, l3: 0.000185, l4: 0.000210, l5: 0.000711, l6: 0.001014
[epoch: 285/1000, batch:   512/ 1052, ite: 74820] train loss: 0.005116, tar: 0.000289 
l0: 0.000294, l1: 0.000304, l2: 0.000323, l3: 0.000319, l4: 0.000489, l5: 0.001015, l6: 0.002127
[epoch: 285/1000, batch:   592/ 1052, ite: 74840] train loss: 0.005117, tar: 0.000289 
l0: 0.000305, l1: 0.000319, l2: 0.000291, l3: 0.000421, l4: 0.000710, l5: 0.000945, l6: 0.001418
[epoch: 285/1000, batch:   672/ 1052, ite: 74860] train loss: 0.005118, tar: 0.000290 
l0: 0.000171, l1: 0.000172, l2: 0.000199, l3: 0.000296, l4: 0.000484, l5: 0.000832, l6: 0.001879
[epoch: 285/1000, batch:   752/ 1052, ite: 74880] train loss: 0.005131, tar: 0.000290 
l0: 0.000210, l1: 0.000211, l2: 0.000236, l3: 0.000322, l4: 0.000648, l5: 0.000582, l6: 0.001570
[epoch: 285/1000, batch:   832/ 1052, ite: 74900] train loss: 0.005134, tar: 0.000290 
l0: 0.000235, l1: 0.000243, l2: 0.000244, l3: 0.000310, l4: 0.000461, l5: 0.000929, l6: 0.001688
[epoch: 285/1000, batch:   912/ 1052, ite: 74920] train loss: 0.005136, tar: 0.000290 
l0: 0.000166, l1: 0.000162, l2: 0.000199, l3: 0.000280, l4: 0.000519, l5: 0.000775, l6: 0.001533
[epoch: 285/1000, batch:   992/ 1052, ite: 74940] train loss: 0.005127, tar: 0.000289 
[Epoch 285/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000493, l1: 0.000510, l2: 0.000648, l3: 0.000706, l4: 0.001132, l5: 0.001693, l6: 0.004663
[epoch: 286/1000, batch:    20/ 1052, ite: 74960] train loss: 0.005123, tar: 0.000289 
l0: 0.000116, l1: 0.000115, l2: 0.000141, l3: 0.000181, l4: 0.000272, l5: 0.000511, l6: 0.000751
[epoch: 286/1000, batch:   100/ 1052, ite: 74980] train loss: 0.005124, tar: 0.000289 
l0: 0.000752, l1: 0.000767, l2: 0.000815, l3: 0.001020, l4: 0.001645, l5: 0.002849, l6: 0.006266
[epoch: 286/1000, batch:   180/ 1052, ite: 75000] train loss: 0.005125, tar: 0.000289 
l0: 0.000478, l1: 0.000488, l2: 0.000528, l3: 0.000542, l4: 0.001029, l5: 0.002056, l6: 0.003922
[epoch: 286/1000, batch:   260/ 1052, ite: 75020] train loss: 0.005117, tar: 0.000288 
l0: 0.000328, l1: 0.000341, l2: 0.000344, l3: 0.000381, l4: 0.000676, l5: 0.001119, l6: 0.002358
[epoch: 286/1000, batch:   340/ 1052, ite: 75040] train loss: 0.005114, tar: 0.000288 
l0: 0.000177, l1: 0.000183, l2: 0.000216, l3: 0.000227, l4: 0.000512, l5: 0.000976, l6: 0.001741
[epoch: 286/1000, batch:   420/ 1052, ite: 75060] train loss: 0.005107, tar: 0.000287 
l0: 0.000297, l1: 0.000301, l2: 0.000309, l3: 0.000368, l4: 0.000578, l5: 0.001432, l6: 0.002689
[epoch: 286/1000, batch:   500/ 1052, ite: 75080] train loss: 0.005106, tar: 0.000287 
l0: 0.000187, l1: 0.000187, l2: 0.000220, l3: 0.000236, l4: 0.000367, l5: 0.000668, l6: 0.001097
[epoch: 286/1000, batch:   580/ 1052, ite: 75100] train loss: 0.005101, tar: 0.000287 
l0: 0.000194, l1: 0.000206, l2: 0.000189, l3: 0.000207, l4: 0.000401, l5: 0.000836, l6: 0.001733
[epoch: 286/1000, batch:   660/ 1052, ite: 75120] train loss: 0.005099, tar: 0.000287 
l0: 0.000211, l1: 0.000209, l2: 0.000245, l3: 0.000326, l4: 0.000565, l5: 0.001002, l6: 0.001597
[epoch: 286/1000, batch:   740/ 1052, ite: 75140] train loss: 0.005105, tar: 0.000287 
l0: 0.000181, l1: 0.000189, l2: 0.000179, l3: 0.000211, l4: 0.000359, l5: 0.000856, l6: 0.001104
[epoch: 286/1000, batch:   820/ 1052, ite: 75160] train loss: 0.005102, tar: 0.000287 
l0: 0.000160, l1: 0.000167, l2: 0.000191, l3: 0.000203, l4: 0.000497, l5: 0.000931, l6: 0.002127
[epoch: 286/1000, batch:   900/ 1052, ite: 75180] train loss: 0.005103, tar: 0.000287 
l0: 0.000249, l1: 0.000258, l2: 0.000291, l3: 0.000292, l4: 0.000499, l5: 0.000696, l6: 0.001497
[epoch: 286/1000, batch:   980/ 1052, ite: 75200] train loss: 0.005105, tar: 0.000287 
[Epoch 286/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000568, l1: 0.000610, l2: 0.000582, l3: 0.000674, l4: 0.001306, l5: 0.002202, l6: 0.005072
[epoch: 287/1000, batch:     8/ 1052, ite: 75220] train loss: 0.005114, tar: 0.000287 
l0: 0.000364, l1: 0.000355, l2: 0.000427, l3: 0.000588, l4: 0.001112, l5: 0.001786, l6: 0.002423
[epoch: 287/1000, batch:    88/ 1052, ite: 75240] train loss: 0.005111, tar: 0.000287 
l0: 0.000133, l1: 0.000126, l2: 0.000181, l3: 0.000244, l4: 0.000407, l5: 0.001174, l6: 0.001474
[epoch: 287/1000, batch:   168/ 1052, ite: 75260] train loss: 0.005110, tar: 0.000287 
l0: 0.000183, l1: 0.000189, l2: 0.000182, l3: 0.000226, l4: 0.000309, l5: 0.000419, l6: 0.001035
[epoch: 287/1000, batch:   248/ 1052, ite: 75280] train loss: 0.005117, tar: 0.000287 
l0: 0.000360, l1: 0.000366, l2: 0.000421, l3: 0.000525, l4: 0.000801, l5: 0.001462, l6: 0.001773
[epoch: 287/1000, batch:   328/ 1052, ite: 75300] train loss: 0.005113, tar: 0.000287 
l0: 0.000185, l1: 0.000191, l2: 0.000163, l3: 0.000303, l4: 0.000454, l5: 0.000761, l6: 0.001239
[epoch: 287/1000, batch:   408/ 1052, ite: 75320] train loss: 0.005109, tar: 0.000286 
l0: 0.000252, l1: 0.000262, l2: 0.000310, l3: 0.000354, l4: 0.000487, l5: 0.001057, l6: 0.001792
[epoch: 287/1000, batch:   488/ 1052, ite: 75340] train loss: 0.005113, tar: 0.000286 
l0: 0.000235, l1: 0.000242, l2: 0.000290, l3: 0.000257, l4: 0.000499, l5: 0.000755, l6: 0.001539
[epoch: 287/1000, batch:   568/ 1052, ite: 75360] train loss: 0.005106, tar: 0.000286 
l0: 0.000311, l1: 0.000312, l2: 0.000352, l3: 0.000427, l4: 0.000765, l5: 0.001491, l6: 0.002228
[epoch: 287/1000, batch:   648/ 1052, ite: 75380] train loss: 0.005103, tar: 0.000286 
l0: 0.000170, l1: 0.000182, l2: 0.000190, l3: 0.000190, l4: 0.000576, l5: 0.000817, l6: 0.001171
[epoch: 287/1000, batch:   728/ 1052, ite: 75400] train loss: 0.005103, tar: 0.000286 
l0: 0.000381, l1: 0.000366, l2: 0.000501, l3: 0.000632, l4: 0.000853, l5: 0.001347, l6: 0.002516
[epoch: 287/1000, batch:   808/ 1052, ite: 75420] train loss: 0.005102, tar: 0.000286 
l0: 0.000400, l1: 0.000416, l2: 0.000436, l3: 0.000423, l4: 0.000668, l5: 0.001349, l6: 0.002365
[epoch: 287/1000, batch:   888/ 1052, ite: 75440] train loss: 0.005104, tar: 0.000286 
l0: 0.000265, l1: 0.000277, l2: 0.000298, l3: 0.000294, l4: 0.000547, l5: 0.001097, l6: 0.002025
[epoch: 287/1000, batch:   968/ 1052, ite: 75460] train loss: 0.005110, tar: 0.000286 
l0: 0.000235, l1: 0.000239, l2: 0.000251, l3: 0.000293, l4: 0.000518, l5: 0.000828, l6: 0.001033
[epoch: 287/1000, batch:  1048/ 1052, ite: 75480] train loss: 0.005106, tar: 0.000286 
[Epoch 287/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000187, l1: 0.000179, l2: 0.000200, l3: 0.000240, l4: 0.000524, l5: 0.000896, l6: 0.000991
[epoch: 288/1000, batch:    76/ 1052, ite: 75500] train loss: 0.005100, tar: 0.000286 
l0: 0.000174, l1: 0.000170, l2: 0.000200, l3: 0.000262, l4: 0.000388, l5: 0.000786, l6: 0.001741
[epoch: 288/1000, batch:   156/ 1052, ite: 75520] train loss: 0.005098, tar: 0.000286 
l0: 0.000379, l1: 0.000384, l2: 0.000384, l3: 0.000502, l4: 0.000793, l5: 0.001298, l6: 0.002170
[epoch: 288/1000, batch:   236/ 1052, ite: 75540] train loss: 0.005100, tar: 0.000285 
l0: 0.000252, l1: 0.000264, l2: 0.000302, l3: 0.000357, l4: 0.000722, l5: 0.001240, l6: 0.001973
[epoch: 288/1000, batch:   316/ 1052, ite: 75560] train loss: 0.005100, tar: 0.000286 
l0: 0.000226, l1: 0.000231, l2: 0.000292, l3: 0.000312, l4: 0.000674, l5: 0.001060, l6: 0.002622
[epoch: 288/1000, batch:   396/ 1052, ite: 75580] train loss: 0.005102, tar: 0.000285 
l0: 0.000413, l1: 0.000441, l2: 0.000431, l3: 0.000452, l4: 0.000632, l5: 0.001206, l6: 0.002318
[epoch: 288/1000, batch:   476/ 1052, ite: 75600] train loss: 0.005100, tar: 0.000285 
l0: 0.000127, l1: 0.000132, l2: 0.000153, l3: 0.000212, l4: 0.000280, l5: 0.000617, l6: 0.000891
[epoch: 288/1000, batch:   556/ 1052, ite: 75620] train loss: 0.005103, tar: 0.000286 
l0: 0.000163, l1: 0.000162, l2: 0.000182, l3: 0.000234, l4: 0.000300, l5: 0.000841, l6: 0.001152
[epoch: 288/1000, batch:   636/ 1052, ite: 75640] train loss: 0.005098, tar: 0.000286 
l0: 0.000321, l1: 0.000317, l2: 0.000337, l3: 0.000431, l4: 0.000597, l5: 0.001032, l6: 0.001975
[epoch: 288/1000, batch:   716/ 1052, ite: 75660] train loss: 0.005096, tar: 0.000285 
l0: 0.000182, l1: 0.000190, l2: 0.000184, l3: 0.000204, l4: 0.000393, l5: 0.000748, l6: 0.001361
[epoch: 288/1000, batch:   796/ 1052, ite: 75680] train loss: 0.005097, tar: 0.000285 
l0: 0.000203, l1: 0.000202, l2: 0.000198, l3: 0.000283, l4: 0.000452, l5: 0.000850, l6: 0.001570
[epoch: 288/1000, batch:   876/ 1052, ite: 75700] train loss: 0.005103, tar: 0.000285 
l0: 0.000292, l1: 0.000309, l2: 0.000322, l3: 0.000408, l4: 0.000686, l5: 0.000923, l6: 0.002695
[epoch: 288/1000, batch:   956/ 1052, ite: 75720] train loss: 0.005099, tar: 0.000285 
l0: 0.000154, l1: 0.000155, l2: 0.000168, l3: 0.000241, l4: 0.000524, l5: 0.000827, l6: 0.000994
[epoch: 288/1000, batch:  1036/ 1052, ite: 75740] train loss: 0.005098, tar: 0.000285 
[Epoch 288/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000407, l1: 0.000415, l2: 0.000409, l3: 0.000487, l4: 0.000739, l5: 0.001790, l6: 0.002508
[epoch: 289/1000, batch:    64/ 1052, ite: 75760] train loss: 0.005105, tar: 0.000286 
l0: 0.000203, l1: 0.000202, l2: 0.000235, l3: 0.000296, l4: 0.000594, l5: 0.001486, l6: 0.002268
[epoch: 289/1000, batch:   144/ 1052, ite: 75780] train loss: 0.005103, tar: 0.000285 
l0: 0.000359, l1: 0.000357, l2: 0.000365, l3: 0.000439, l4: 0.000738, l5: 0.001326, l6: 0.003102
[epoch: 289/1000, batch:   224/ 1052, ite: 75800] train loss: 0.005097, tar: 0.000285 
l0: 0.000197, l1: 0.000194, l2: 0.000236, l3: 0.000319, l4: 0.000662, l5: 0.000928, l6: 0.001542
[epoch: 289/1000, batch:   304/ 1052, ite: 75820] train loss: 0.005098, tar: 0.000285 
l0: 0.000310, l1: 0.000311, l2: 0.000338, l3: 0.000461, l4: 0.000812, l5: 0.001362, l6: 0.003420
[epoch: 289/1000, batch:   384/ 1052, ite: 75840] train loss: 0.005104, tar: 0.000285 
l0: 0.000168, l1: 0.000173, l2: 0.000182, l3: 0.000188, l4: 0.000225, l5: 0.000528, l6: 0.000960
[epoch: 289/1000, batch:   464/ 1052, ite: 75860] train loss: 0.005102, tar: 0.000285 
l0: 0.000292, l1: 0.000315, l2: 0.000338, l3: 0.000354, l4: 0.000634, l5: 0.001407, l6: 0.002328
[epoch: 289/1000, batch:   544/ 1052, ite: 75880] train loss: 0.005101, tar: 0.000285 
l0: 0.000263, l1: 0.000261, l2: 0.000285, l3: 0.000403, l4: 0.000650, l5: 0.001066, l6: 0.001198
[epoch: 289/1000, batch:   624/ 1052, ite: 75900] train loss: 0.005098, tar: 0.000285 
l0: 0.000335, l1: 0.000355, l2: 0.000392, l3: 0.000446, l4: 0.000785, l5: 0.001267, l6: 0.002476
[epoch: 289/1000, batch:   704/ 1052, ite: 75920] train loss: 0.005097, tar: 0.000285 
l0: 0.000234, l1: 0.000233, l2: 0.000228, l3: 0.000284, l4: 0.000472, l5: 0.001015, l6: 0.001122
[epoch: 289/1000, batch:   784/ 1052, ite: 75940] train loss: 0.005097, tar: 0.000285 
l0: 0.000278, l1: 0.000267, l2: 0.000377, l3: 0.000477, l4: 0.000887, l5: 0.001515, l6: 0.001451
[epoch: 289/1000, batch:   864/ 1052, ite: 75960] train loss: 0.005094, tar: 0.000284 
l0: 0.000325, l1: 0.000327, l2: 0.000442, l3: 0.000425, l4: 0.000757, l5: 0.001507, l6: 0.002430
[epoch: 289/1000, batch:   944/ 1052, ite: 75980] train loss: 0.005091, tar: 0.000284 
l0: 0.000243, l1: 0.000261, l2: 0.000281, l3: 0.000313, l4: 0.000496, l5: 0.000996, l6: 0.001583
[epoch: 289/1000, batch:  1024/ 1052, ite: 76000] train loss: 0.005093, tar: 0.000285 
[Epoch 289/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000171, l1: 0.000181, l2: 0.000199, l3: 0.000214, l4: 0.000393, l5: 0.000767, l6: 0.001763
[epoch: 290/1000, batch:    52/ 1052, ite: 76020] train loss: 0.005093, tar: 0.000285 
l0: 0.000185, l1: 0.000180, l2: 0.000247, l3: 0.000315, l4: 0.000538, l5: 0.001320, l6: 0.001887
[epoch: 290/1000, batch:   132/ 1052, ite: 76040] train loss: 0.005092, tar: 0.000284 
l0: 0.000325, l1: 0.000326, l2: 0.000386, l3: 0.000471, l4: 0.000834, l5: 0.001256, l6: 0.003008
[epoch: 290/1000, batch:   212/ 1052, ite: 76060] train loss: 0.005091, tar: 0.000284 
l0: 0.000434, l1: 0.000453, l2: 0.000472, l3: 0.000508, l4: 0.000819, l5: 0.001369, l6: 0.003002
[epoch: 290/1000, batch:   292/ 1052, ite: 76080] train loss: 0.005089, tar: 0.000284 
l0: 0.000172, l1: 0.000185, l2: 0.000193, l3: 0.000232, l4: 0.000413, l5: 0.000672, l6: 0.001366
[epoch: 290/1000, batch:   372/ 1052, ite: 76100] train loss: 0.005088, tar: 0.000284 
l0: 0.000215, l1: 0.000220, l2: 0.000224, l3: 0.000287, l4: 0.000474, l5: 0.001211, l6: 0.001132
[epoch: 290/1000, batch:   452/ 1052, ite: 76120] train loss: 0.005083, tar: 0.000283 
l0: 0.000203, l1: 0.000206, l2: 0.000225, l3: 0.000317, l4: 0.000549, l5: 0.001011, l6: 0.001579
[epoch: 290/1000, batch:   532/ 1052, ite: 76140] train loss: 0.005084, tar: 0.000283 
l0: 0.000287, l1: 0.000293, l2: 0.000353, l3: 0.000415, l4: 0.000814, l5: 0.001556, l6: 0.002225
[epoch: 290/1000, batch:   612/ 1052, ite: 76160] train loss: 0.005083, tar: 0.000283 
l0: 0.000344, l1: 0.000358, l2: 0.000363, l3: 0.000402, l4: 0.000743, l5: 0.001376, l6: 0.001838
[epoch: 290/1000, batch:   692/ 1052, ite: 76180] train loss: 0.005088, tar: 0.000283 
l0: 0.000157, l1: 0.000151, l2: 0.000184, l3: 0.000298, l4: 0.000522, l5: 0.000823, l6: 0.001718
[epoch: 290/1000, batch:   772/ 1052, ite: 76200] train loss: 0.005094, tar: 0.000284 
l0: 0.000115, l1: 0.000120, l2: 0.000151, l3: 0.000182, l4: 0.000394, l5: 0.000656, l6: 0.001117
[epoch: 290/1000, batch:   852/ 1052, ite: 76220] train loss: 0.005090, tar: 0.000283 
l0: 0.000433, l1: 0.000463, l2: 0.000480, l3: 0.000665, l4: 0.001159, l5: 0.002329, l6: 0.003114
[epoch: 290/1000, batch:   932/ 1052, ite: 76240] train loss: 0.005090, tar: 0.000283 
l0: 0.000296, l1: 0.000311, l2: 0.000336, l3: 0.000348, l4: 0.000578, l5: 0.001071, l6: 0.001558
[epoch: 290/1000, batch:  1012/ 1052, ite: 76260] train loss: 0.005090, tar: 0.000283 
[Epoch 290/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000167, l1: 0.000168, l2: 0.000206, l3: 0.000258, l4: 0.000484, l5: 0.000638, l6: 0.001102
[epoch: 291/1000, batch:    40/ 1052, ite: 76280] train loss: 0.005088, tar: 0.000283 
l0: 0.000185, l1: 0.000191, l2: 0.000205, l3: 0.000225, l4: 0.000642, l5: 0.000893, l6: 0.001793
[epoch: 291/1000, batch:   120/ 1052, ite: 76300] train loss: 0.005090, tar: 0.000283 
l0: 0.000186, l1: 0.000184, l2: 0.000195, l3: 0.000293, l4: 0.000475, l5: 0.001372, l6: 0.001702
[epoch: 291/1000, batch:   200/ 1052, ite: 76320] train loss: 0.005087, tar: 0.000283 
l0: 0.000110, l1: 0.000105, l2: 0.000158, l3: 0.000186, l4: 0.000455, l5: 0.000451, l6: 0.001192
[epoch: 291/1000, batch:   280/ 1052, ite: 76340] train loss: 0.005084, tar: 0.000283 
l0: 0.000136, l1: 0.000133, l2: 0.000139, l3: 0.000201, l4: 0.000288, l5: 0.000872, l6: 0.000980
[epoch: 291/1000, batch:   360/ 1052, ite: 76360] train loss: 0.005081, tar: 0.000283 
l0: 0.000224, l1: 0.000229, l2: 0.000243, l3: 0.000277, l4: 0.000499, l5: 0.000717, l6: 0.001555
[epoch: 291/1000, batch:   440/ 1052, ite: 76380] train loss: 0.005086, tar: 0.000283 
l0: 0.000543, l1: 0.000551, l2: 0.000581, l3: 0.000584, l4: 0.000833, l5: 0.001415, l6: 0.002990
[epoch: 291/1000, batch:   520/ 1052, ite: 76400] train loss: 0.005083, tar: 0.000283 
l0: 0.000166, l1: 0.000166, l2: 0.000217, l3: 0.000236, l4: 0.000466, l5: 0.000814, l6: 0.001147
[epoch: 291/1000, batch:   600/ 1052, ite: 76420] train loss: 0.005079, tar: 0.000283 
l0: 0.000136, l1: 0.000133, l2: 0.000161, l3: 0.000208, l4: 0.000387, l5: 0.001009, l6: 0.001412
[epoch: 291/1000, batch:   680/ 1052, ite: 76440] train loss: 0.005082, tar: 0.000283 
l0: 0.000395, l1: 0.000408, l2: 0.000387, l3: 0.000436, l4: 0.000637, l5: 0.001118, l6: 0.002613
[epoch: 291/1000, batch:   760/ 1052, ite: 76460] train loss: 0.005089, tar: 0.000283 
l0: 0.000173, l1: 0.000177, l2: 0.000208, l3: 0.000224, l4: 0.000390, l5: 0.000678, l6: 0.000982
[epoch: 291/1000, batch:   840/ 1052, ite: 76480] train loss: 0.005091, tar: 0.000283 
l0: 0.000177, l1: 0.000174, l2: 0.000200, l3: 0.000299, l4: 0.000641, l5: 0.000999, l6: 0.001578
[epoch: 291/1000, batch:   920/ 1052, ite: 76500] train loss: 0.005092, tar: 0.000283 
l0: 0.000428, l1: 0.000425, l2: 0.000471, l3: 0.000572, l4: 0.000759, l5: 0.001710, l6: 0.003058
[epoch: 291/1000, batch:  1000/ 1052, ite: 76520] train loss: 0.005093, tar: 0.000283 
[Epoch 291/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000134, l1: 0.000137, l2: 0.000154, l3: 0.000173, l4: 0.000289, l5: 0.000875, l6: 0.001095
[epoch: 292/1000, batch:    28/ 1052, ite: 76540] train loss: 0.005091, tar: 0.000283 
l0: 0.000234, l1: 0.000231, l2: 0.000259, l3: 0.000370, l4: 0.000716, l5: 0.001535, l6: 0.001857
[epoch: 292/1000, batch:   108/ 1052, ite: 76560] train loss: 0.005090, tar: 0.000283 
l0: 0.000290, l1: 0.000295, l2: 0.000326, l3: 0.000384, l4: 0.000777, l5: 0.001118, l6: 0.001953
[epoch: 292/1000, batch:   188/ 1052, ite: 76580] train loss: 0.005091, tar: 0.000283 
l0: 0.000277, l1: 0.000284, l2: 0.000325, l3: 0.000379, l4: 0.000674, l5: 0.001727, l6: 0.001804
[epoch: 292/1000, batch:   268/ 1052, ite: 76600] train loss: 0.005095, tar: 0.000284 
l0: 0.000302, l1: 0.000306, l2: 0.000311, l3: 0.000361, l4: 0.000508, l5: 0.000987, l6: 0.002323
[epoch: 292/1000, batch:   348/ 1052, ite: 76620] train loss: 0.005097, tar: 0.000284 
l0: 0.000230, l1: 0.000235, l2: 0.000242, l3: 0.000294, l4: 0.000547, l5: 0.001116, l6: 0.001748
[epoch: 292/1000, batch:   428/ 1052, ite: 76640] train loss: 0.005100, tar: 0.000284 
l0: 0.000482, l1: 0.000479, l2: 0.000511, l3: 0.000551, l4: 0.000874, l5: 0.001893, l6: 0.002926
[epoch: 292/1000, batch:   508/ 1052, ite: 76660] train loss: 0.005105, tar: 0.000284 
l0: 0.000344, l1: 0.000356, l2: 0.000389, l3: 0.000415, l4: 0.000628, l5: 0.001538, l6: 0.003368
[epoch: 292/1000, batch:   588/ 1052, ite: 76680] train loss: 0.005105, tar: 0.000284 
l0: 0.000301, l1: 0.000307, l2: 0.000325, l3: 0.000359, l4: 0.000561, l5: 0.000701, l6: 0.001736
[epoch: 292/1000, batch:   668/ 1052, ite: 76700] train loss: 0.005102, tar: 0.000284 
l0: 0.000113, l1: 0.000113, l2: 0.000146, l3: 0.000160, l4: 0.000364, l5: 0.000839, l6: 0.001418
[epoch: 292/1000, batch:   748/ 1052, ite: 76720] train loss: 0.005100, tar: 0.000284 
l0: 0.000356, l1: 0.000369, l2: 0.000390, l3: 0.000420, l4: 0.000603, l5: 0.001143, l6: 0.002168
[epoch: 292/1000, batch:   828/ 1052, ite: 76740] train loss: 0.005101, tar: 0.000284 
l0: 0.000318, l1: 0.000330, l2: 0.000334, l3: 0.000429, l4: 0.000604, l5: 0.001380, l6: 0.002785
[epoch: 292/1000, batch:   908/ 1052, ite: 76760] train loss: 0.005101, tar: 0.000284 
l0: 0.000452, l1: 0.000479, l2: 0.000490, l3: 0.000559, l4: 0.000873, l5: 0.001139, l6: 0.002402
[epoch: 292/1000, batch:   988/ 1052, ite: 76780] train loss: 0.005099, tar: 0.000284 
[Epoch 292/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000208, l1: 0.000199, l2: 0.000241, l3: 0.000356, l4: 0.000694, l5: 0.001486, l6: 0.001592
[epoch: 293/1000, batch:    16/ 1052, ite: 76800] train loss: 0.005097, tar: 0.000284 
l0: 0.000619, l1: 0.000634, l2: 0.000707, l3: 0.000803, l4: 0.000885, l5: 0.001521, l6: 0.003560
[epoch: 293/1000, batch:    96/ 1052, ite: 76820] train loss: 0.005099, tar: 0.000284 
l0: 0.000266, l1: 0.000278, l2: 0.000269, l3: 0.000294, l4: 0.000451, l5: 0.000910, l6: 0.001839
[epoch: 293/1000, batch:   176/ 1052, ite: 76840] train loss: 0.005098, tar: 0.000283 
l0: 0.000336, l1: 0.000351, l2: 0.000375, l3: 0.000467, l4: 0.000739, l5: 0.001046, l6: 0.001767
[epoch: 293/1000, batch:   256/ 1052, ite: 76860] train loss: 0.005097, tar: 0.000283 
l0: 0.000236, l1: 0.000245, l2: 0.000269, l3: 0.000300, l4: 0.000366, l5: 0.000901, l6: 0.002304
[epoch: 293/1000, batch:   336/ 1052, ite: 76880] train loss: 0.005099, tar: 0.000283 
l0: 0.000308, l1: 0.000316, l2: 0.000358, l3: 0.000375, l4: 0.000680, l5: 0.001440, l6: 0.001751
[epoch: 293/1000, batch:   416/ 1052, ite: 76900] train loss: 0.005101, tar: 0.000284 
l0: 0.000179, l1: 0.000180, l2: 0.000222, l3: 0.000303, l4: 0.000497, l5: 0.000966, l6: 0.001538
[epoch: 293/1000, batch:   496/ 1052, ite: 76920] train loss: 0.005100, tar: 0.000283 
l0: 0.000195, l1: 0.000199, l2: 0.000210, l3: 0.000270, l4: 0.000378, l5: 0.000849, l6: 0.001783
[epoch: 293/1000, batch:   576/ 1052, ite: 76940] train loss: 0.005101, tar: 0.000284 
l0: 0.000355, l1: 0.000354, l2: 0.000443, l3: 0.000454, l4: 0.000593, l5: 0.001098, l6: 0.001704
[epoch: 293/1000, batch:   656/ 1052, ite: 76960] train loss: 0.005104, tar: 0.000284 
l0: 0.000180, l1: 0.000195, l2: 0.000211, l3: 0.000239, l4: 0.000464, l5: 0.000977, l6: 0.001548
[epoch: 293/1000, batch:   736/ 1052, ite: 76980] train loss: 0.005101, tar: 0.000283 
l0: 0.000268, l1: 0.000277, l2: 0.000262, l3: 0.000323, l4: 0.000471, l5: 0.000731, l6: 0.000925
[epoch: 293/1000, batch:   816/ 1052, ite: 77000] train loss: 0.005101, tar: 0.000283 
l0: 0.000171, l1: 0.000176, l2: 0.000211, l3: 0.000234, l4: 0.000414, l5: 0.000690, l6: 0.001132
[epoch: 293/1000, batch:   896/ 1052, ite: 77020] train loss: 0.005099, tar: 0.000283 
l0: 0.000177, l1: 0.000180, l2: 0.000210, l3: 0.000286, l4: 0.000477, l5: 0.001372, l6: 0.002407
[epoch: 293/1000, batch:   976/ 1052, ite: 77040] train loss: 0.005095, tar: 0.000283 
[Epoch 293/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000258, l1: 0.000260, l2: 0.000286, l3: 0.000322, l4: 0.000511, l5: 0.001019, l6: 0.002063
[epoch: 294/1000, batch:     4/ 1052, ite: 77060] train loss: 0.005093, tar: 0.000283 
l0: 0.000309, l1: 0.000316, l2: 0.000327, l3: 0.000446, l4: 0.000789, l5: 0.001539, l6: 0.002712
[epoch: 294/1000, batch:    84/ 1052, ite: 77080] train loss: 0.005095, tar: 0.000283 
l0: 0.000236, l1: 0.000232, l2: 0.000276, l3: 0.000325, l4: 0.000617, l5: 0.001087, l6: 0.001616
[epoch: 294/1000, batch:   164/ 1052, ite: 77100] train loss: 0.005094, tar: 0.000283 
l0: 0.000141, l1: 0.000141, l2: 0.000188, l3: 0.000219, l4: 0.000356, l5: 0.000790, l6: 0.001606
[epoch: 294/1000, batch:   244/ 1052, ite: 77120] train loss: 0.005089, tar: 0.000282 
l0: 0.000615, l1: 0.000638, l2: 0.000573, l3: 0.000657, l4: 0.001133, l5: 0.001995, l6: 0.003059
[epoch: 294/1000, batch:   324/ 1052, ite: 77140] train loss: 0.005088, tar: 0.000282 
l0: 0.000183, l1: 0.000186, l2: 0.000233, l3: 0.000248, l4: 0.000458, l5: 0.000791, l6: 0.001465
[epoch: 294/1000, batch:   404/ 1052, ite: 77160] train loss: 0.005087, tar: 0.000282 
l0: 0.000444, l1: 0.000453, l2: 0.000467, l3: 0.000456, l4: 0.000519, l5: 0.001318, l6: 0.002499
[epoch: 294/1000, batch:   484/ 1052, ite: 77180] train loss: 0.005088, tar: 0.000282 
l0: 0.000381, l1: 0.000378, l2: 0.000451, l3: 0.000669, l4: 0.001510, l5: 0.002137, l6: 0.004004
[epoch: 294/1000, batch:   564/ 1052, ite: 77200] train loss: 0.005087, tar: 0.000282 
l0: 0.000187, l1: 0.000193, l2: 0.000235, l3: 0.000225, l4: 0.000445, l5: 0.001056, l6: 0.001283
[epoch: 294/1000, batch:   644/ 1052, ite: 77220] train loss: 0.005089, tar: 0.000282 
l0: 0.000292, l1: 0.000293, l2: 0.000346, l3: 0.000428, l4: 0.000756, l5: 0.001204, l6: 0.002186
[epoch: 294/1000, batch:   724/ 1052, ite: 77240] train loss: 0.005088, tar: 0.000282 
l0: 0.000207, l1: 0.000197, l2: 0.000238, l3: 0.000333, l4: 0.000412, l5: 0.000636, l6: 0.001060
[epoch: 294/1000, batch:   804/ 1052, ite: 77260] train loss: 0.005087, tar: 0.000282 
l0: 0.000347, l1: 0.000362, l2: 0.000349, l3: 0.000382, l4: 0.000725, l5: 0.001340, l6: 0.002241
[epoch: 294/1000, batch:   884/ 1052, ite: 77280] train loss: 0.005088, tar: 0.000282 
l0: 0.000263, l1: 0.000265, l2: 0.000276, l3: 0.000360, l4: 0.000606, l5: 0.001090, l6: 0.001895
[epoch: 294/1000, batch:   964/ 1052, ite: 77300] train loss: 0.005089, tar: 0.000282 
l0: 0.000207, l1: 0.000208, l2: 0.000241, l3: 0.000285, l4: 0.000399, l5: 0.000571, l6: 0.001521
[epoch: 294/1000, batch:  1044/ 1052, ite: 77320] train loss: 0.005092, tar: 0.000282 
[Epoch 294/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000321, l1: 0.000318, l2: 0.000322, l3: 0.000441, l4: 0.000644, l5: 0.001239, l6: 0.001616
[epoch: 295/1000, batch:    72/ 1052, ite: 77340] train loss: 0.005094, tar: 0.000282 
l0: 0.000225, l1: 0.000224, l2: 0.000242, l3: 0.000347, l4: 0.000487, l5: 0.000966, l6: 0.001764
[epoch: 295/1000, batch:   152/ 1052, ite: 77360] train loss: 0.005094, tar: 0.000282 
l0: 0.000283, l1: 0.000283, l2: 0.000301, l3: 0.000362, l4: 0.000634, l5: 0.001133, l6: 0.002020
[epoch: 295/1000, batch:   232/ 1052, ite: 77380] train loss: 0.005093, tar: 0.000282 
l0: 0.000143, l1: 0.000138, l2: 0.000181, l3: 0.000231, l4: 0.000391, l5: 0.000997, l6: 0.001353
[epoch: 295/1000, batch:   312/ 1052, ite: 77400] train loss: 0.005092, tar: 0.000282 
l0: 0.000191, l1: 0.000193, l2: 0.000225, l3: 0.000312, l4: 0.000588, l5: 0.000911, l6: 0.002386
[epoch: 295/1000, batch:   392/ 1052, ite: 77420] train loss: 0.005090, tar: 0.000282 
l0: 0.000184, l1: 0.000175, l2: 0.000184, l3: 0.000304, l4: 0.000584, l5: 0.000714, l6: 0.001291
[epoch: 295/1000, batch:   472/ 1052, ite: 77440] train loss: 0.005092, tar: 0.000282 
l0: 0.000133, l1: 0.000136, l2: 0.000148, l3: 0.000191, l4: 0.000316, l5: 0.000782, l6: 0.001239
[epoch: 295/1000, batch:   552/ 1052, ite: 77460] train loss: 0.005092, tar: 0.000282 
l0: 0.000169, l1: 0.000179, l2: 0.000200, l3: 0.000296, l4: 0.000640, l5: 0.000805, l6: 0.001566
[epoch: 295/1000, batch:   632/ 1052, ite: 77480] train loss: 0.005091, tar: 0.000282 
l0: 0.000984, l1: 0.000978, l2: 0.001254, l3: 0.000883, l4: 0.000731, l5: 0.001383, l6: 0.002556
[epoch: 295/1000, batch:   712/ 1052, ite: 77500] train loss: 0.005094, tar: 0.000282 
l0: 0.000331, l1: 0.000336, l2: 0.000352, l3: 0.000463, l4: 0.000734, l5: 0.001086, l6: 0.002150
[epoch: 295/1000, batch:   792/ 1052, ite: 77520] train loss: 0.005092, tar: 0.000282 
l0: 0.000201, l1: 0.000202, l2: 0.000315, l3: 0.000287, l4: 0.000593, l5: 0.000966, l6: 0.001233
[epoch: 295/1000, batch:   872/ 1052, ite: 77540] train loss: 0.005090, tar: 0.000282 
l0: 0.000141, l1: 0.000140, l2: 0.000174, l3: 0.000196, l4: 0.000466, l5: 0.000873, l6: 0.001916
[epoch: 295/1000, batch:   952/ 1052, ite: 77560] train loss: 0.005088, tar: 0.000282 
l0: 0.000251, l1: 0.000243, l2: 0.000277, l3: 0.000371, l4: 0.000628, l5: 0.000695, l6: 0.001451
[epoch: 295/1000, batch:  1032/ 1052, ite: 77580] train loss: 0.005088, tar: 0.000282 
[Epoch 295/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000200, l1: 0.000212, l2: 0.000239, l3: 0.000297, l4: 0.000503, l5: 0.001006, l6: 0.001463
[epoch: 296/1000, batch:    60/ 1052, ite: 77600] train loss: 0.005086, tar: 0.000281 
l0: 0.000427, l1: 0.000454, l2: 0.000475, l3: 0.000506, l4: 0.000837, l5: 0.001693, l6: 0.002382
[epoch: 296/1000, batch:   140/ 1052, ite: 77620] train loss: 0.005086, tar: 0.000282 
l0: 0.000269, l1: 0.000272, l2: 0.000278, l3: 0.000343, l4: 0.000647, l5: 0.001160, l6: 0.002017
[epoch: 296/1000, batch:   220/ 1052, ite: 77640] train loss: 0.005085, tar: 0.000281 
l0: 0.000163, l1: 0.000165, l2: 0.000164, l3: 0.000229, l4: 0.000376, l5: 0.000705, l6: 0.001488
[epoch: 296/1000, batch:   300/ 1052, ite: 77660] train loss: 0.005083, tar: 0.000281 
l0: 0.000542, l1: 0.000551, l2: 0.000580, l3: 0.000642, l4: 0.001064, l5: 0.001669, l6: 0.004738
[epoch: 296/1000, batch:   380/ 1052, ite: 77680] train loss: 0.005085, tar: 0.000281 
l0: 0.000371, l1: 0.000352, l2: 0.000453, l3: 0.000566, l4: 0.001052, l5: 0.001509, l6: 0.003020
[epoch: 296/1000, batch:   460/ 1052, ite: 77700] train loss: 0.005085, tar: 0.000281 
l0: 0.000333, l1: 0.000332, l2: 0.000387, l3: 0.000427, l4: 0.000560, l5: 0.001515, l6: 0.002461
[epoch: 296/1000, batch:   540/ 1052, ite: 77720] train loss: 0.005084, tar: 0.000281 
l0: 0.000271, l1: 0.000276, l2: 0.000301, l3: 0.000337, l4: 0.000496, l5: 0.000936, l6: 0.002156
[epoch: 296/1000, batch:   620/ 1052, ite: 77740] train loss: 0.005084, tar: 0.000281 
l0: 0.000111, l1: 0.000115, l2: 0.000144, l3: 0.000220, l4: 0.000515, l5: 0.000867, l6: 0.001395
[epoch: 296/1000, batch:   700/ 1052, ite: 77760] train loss: 0.005086, tar: 0.000281 
l0: 0.000183, l1: 0.000178, l2: 0.000264, l3: 0.000347, l4: 0.000674, l5: 0.001503, l6: 0.002512
[epoch: 296/1000, batch:   780/ 1052, ite: 77780] train loss: 0.005088, tar: 0.000281 
l0: 0.000462, l1: 0.000459, l2: 0.000528, l3: 0.000637, l4: 0.001172, l5: 0.002341, l6: 0.004422
[epoch: 296/1000, batch:   860/ 1052, ite: 77800] train loss: 0.005087, tar: 0.000281 
l0: 0.000386, l1: 0.000383, l2: 0.000397, l3: 0.000504, l4: 0.000706, l5: 0.000931, l6: 0.001805
[epoch: 296/1000, batch:   940/ 1052, ite: 77820] train loss: 0.005087, tar: 0.000281 
l0: 0.000185, l1: 0.000183, l2: 0.000205, l3: 0.000364, l4: 0.000560, l5: 0.000980, l6: 0.002280
[epoch: 296/1000, batch:  1020/ 1052, ite: 77840] train loss: 0.005086, tar: 0.000281 
[Epoch 296/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000198, l1: 0.000200, l2: 0.000212, l3: 0.000271, l4: 0.000551, l5: 0.001301, l6: 0.002317
[epoch: 297/1000, batch:    48/ 1052, ite: 77860] train loss: 0.005087, tar: 0.000281 
l0: 0.000172, l1: 0.000195, l2: 0.000205, l3: 0.000236, l4: 0.000316, l5: 0.000838, l6: 0.000936
[epoch: 297/1000, batch:   128/ 1052, ite: 77880] train loss: 0.005088, tar: 0.000281 
l0: 0.000177, l1: 0.000175, l2: 0.000208, l3: 0.000300, l4: 0.000539, l5: 0.001229, l6: 0.001665
[epoch: 297/1000, batch:   208/ 1052, ite: 77900] train loss: 0.005086, tar: 0.000281 
l0: 0.000124, l1: 0.000123, l2: 0.000149, l3: 0.000172, l4: 0.000370, l5: 0.000657, l6: 0.000937
[epoch: 297/1000, batch:   288/ 1052, ite: 77920] train loss: 0.005085, tar: 0.000280 
l0: 0.000278, l1: 0.000294, l2: 0.000285, l3: 0.000291, l4: 0.000498, l5: 0.001573, l6: 0.002560
[epoch: 297/1000, batch:   368/ 1052, ite: 77940] train loss: 0.005084, tar: 0.000280 
l0: 0.000361, l1: 0.000363, l2: 0.000426, l3: 0.000500, l4: 0.000900, l5: 0.001895, l6: 0.003983
[epoch: 297/1000, batch:   448/ 1052, ite: 77960] train loss: 0.005085, tar: 0.000280 
l0: 0.000221, l1: 0.000229, l2: 0.000278, l3: 0.000310, l4: 0.000632, l5: 0.001859, l6: 0.002480
[epoch: 297/1000, batch:   528/ 1052, ite: 77980] train loss: 0.005083, tar: 0.000280 
l0: 0.000471, l1: 0.000490, l2: 0.000476, l3: 0.000651, l4: 0.001106, l5: 0.002022, l6: 0.003425
[epoch: 297/1000, batch:   608/ 1052, ite: 78000] train loss: 0.005084, tar: 0.000280 
l0: 0.000456, l1: 0.000450, l2: 0.000497, l3: 0.000615, l4: 0.000911, l5: 0.001677, l6: 0.002041
[epoch: 297/1000, batch:   688/ 1052, ite: 78020] train loss: 0.005083, tar: 0.000280 
l0: 0.000326, l1: 0.000327, l2: 0.000419, l3: 0.000472, l4: 0.000723, l5: 0.001512, l6: 0.002712
[epoch: 297/1000, batch:   768/ 1052, ite: 78040] train loss: 0.005081, tar: 0.000280 
l0: 0.000186, l1: 0.000179, l2: 0.000243, l3: 0.000295, l4: 0.000678, l5: 0.000893, l6: 0.001864
[epoch: 297/1000, batch:   848/ 1052, ite: 78060] train loss: 0.005082, tar: 0.000280 
l0: 0.000217, l1: 0.000222, l2: 0.000239, l3: 0.000239, l4: 0.000320, l5: 0.000923, l6: 0.001169
[epoch: 297/1000, batch:   928/ 1052, ite: 78080] train loss: 0.005081, tar: 0.000280 
l0: 0.000208, l1: 0.000206, l2: 0.000263, l3: 0.000308, l4: 0.000503, l5: 0.001101, l6: 0.001617
[epoch: 297/1000, batch:  1008/ 1052, ite: 78100] train loss: 0.005077, tar: 0.000279 
[Epoch 297/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000251, l1: 0.000261, l2: 0.000305, l3: 0.000345, l4: 0.000714, l5: 0.001673, l6: 0.002597
[epoch: 298/1000, batch:    36/ 1052, ite: 78120] train loss: 0.005076, tar: 0.000279 
l0: 0.000235, l1: 0.000242, l2: 0.000273, l3: 0.000357, l4: 0.000546, l5: 0.001429, l6: 0.002481
[epoch: 298/1000, batch:   116/ 1052, ite: 78140] train loss: 0.005074, tar: 0.000279 
l0: 0.000306, l1: 0.000305, l2: 0.000314, l3: 0.000371, l4: 0.000560, l5: 0.000866, l6: 0.002131
[epoch: 298/1000, batch:   196/ 1052, ite: 78160] train loss: 0.005072, tar: 0.000279 
l0: 0.000659, l1: 0.000685, l2: 0.000711, l3: 0.000688, l4: 0.000904, l5: 0.001903, l6: 0.003055
[epoch: 298/1000, batch:   276/ 1052, ite: 78180] train loss: 0.005073, tar: 0.000279 
l0: 0.000212, l1: 0.000214, l2: 0.000234, l3: 0.000286, l4: 0.000440, l5: 0.000679, l6: 0.001333
[epoch: 298/1000, batch:   356/ 1052, ite: 78200] train loss: 0.005072, tar: 0.000279 
l0: 0.000164, l1: 0.000169, l2: 0.000179, l3: 0.000255, l4: 0.000349, l5: 0.001636, l6: 0.001948
[epoch: 298/1000, batch:   436/ 1052, ite: 78220] train loss: 0.005076, tar: 0.000279 
l0: 0.000337, l1: 0.000340, l2: 0.000344, l3: 0.000481, l4: 0.000858, l5: 0.001354, l6: 0.002519
[epoch: 298/1000, batch:   516/ 1052, ite: 78240] train loss: 0.005075, tar: 0.000279 
l0: 0.000377, l1: 0.000374, l2: 0.000440, l3: 0.000472, l4: 0.000731, l5: 0.001385, l6: 0.002284
[epoch: 298/1000, batch:   596/ 1052, ite: 78260] train loss: 0.005075, tar: 0.000279 
l0: 0.000194, l1: 0.000205, l2: 0.000184, l3: 0.000251, l4: 0.000427, l5: 0.000794, l6: 0.002375
[epoch: 298/1000, batch:   676/ 1052, ite: 78280] train loss: 0.005074, tar: 0.000279 
l0: 0.000374, l1: 0.000370, l2: 0.000451, l3: 0.000540, l4: 0.000942, l5: 0.001292, l6: 0.002128
[epoch: 298/1000, batch:   756/ 1052, ite: 78300] train loss: 0.005072, tar: 0.000279 
l0: 0.000291, l1: 0.000304, l2: 0.000346, l3: 0.000458, l4: 0.000843, l5: 0.001321, l6: 0.002222
[epoch: 298/1000, batch:   836/ 1052, ite: 78320] train loss: 0.005074, tar: 0.000279 
l0: 0.000133, l1: 0.000135, l2: 0.000166, l3: 0.000226, l4: 0.000339, l5: 0.000822, l6: 0.001383
[epoch: 298/1000, batch:   916/ 1052, ite: 78340] train loss: 0.005074, tar: 0.000279 
l0: 0.000247, l1: 0.000266, l2: 0.000284, l3: 0.000309, l4: 0.000578, l5: 0.001499, l6: 0.002866
[epoch: 298/1000, batch:   996/ 1052, ite: 78360] train loss: 0.005075, tar: 0.000279 
[Epoch 298/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000185, l1: 0.000182, l2: 0.000191, l3: 0.000252, l4: 0.000523, l5: 0.000654, l6: 0.001213
[epoch: 299/1000, batch:    24/ 1052, ite: 78380] train loss: 0.005073, tar: 0.000279 
l0: 0.000334, l1: 0.000334, l2: 0.000358, l3: 0.000447, l4: 0.000933, l5: 0.001300, l6: 0.002611
[epoch: 299/1000, batch:   104/ 1052, ite: 78400] train loss: 0.005077, tar: 0.000279 
l0: 0.000131, l1: 0.000134, l2: 0.000131, l3: 0.000150, l4: 0.000217, l5: 0.000486, l6: 0.000788
[epoch: 299/1000, batch:   184/ 1052, ite: 78420] train loss: 0.005076, tar: 0.000279 
l0: 0.000174, l1: 0.000182, l2: 0.000210, l3: 0.000211, l4: 0.000365, l5: 0.000839, l6: 0.001701
[epoch: 299/1000, batch:   264/ 1052, ite: 78440] train loss: 0.005077, tar: 0.000279 
l0: 0.000349, l1: 0.000351, l2: 0.000366, l3: 0.000432, l4: 0.000684, l5: 0.001360, l6: 0.002096
[epoch: 299/1000, batch:   344/ 1052, ite: 78460] train loss: 0.005078, tar: 0.000279 
l0: 0.000150, l1: 0.000142, l2: 0.000207, l3: 0.000277, l4: 0.000545, l5: 0.000836, l6: 0.000986
[epoch: 299/1000, batch:   424/ 1052, ite: 78480] train loss: 0.005075, tar: 0.000279 
l0: 0.000688, l1: 0.000720, l2: 0.000734, l3: 0.000886, l4: 0.001384, l5: 0.002287, l6: 0.004437
[epoch: 299/1000, batch:   504/ 1052, ite: 78500] train loss: 0.005076, tar: 0.000279 
l0: 0.000192, l1: 0.000192, l2: 0.000241, l3: 0.000259, l4: 0.000465, l5: 0.000679, l6: 0.001909
[epoch: 299/1000, batch:   584/ 1052, ite: 78520] train loss: 0.005078, tar: 0.000279 
l0: 0.000292, l1: 0.000297, l2: 0.000302, l3: 0.000350, l4: 0.000556, l5: 0.001200, l6: 0.002249
[epoch: 299/1000, batch:   664/ 1052, ite: 78540] train loss: 0.005077, tar: 0.000279 
l0: 0.000286, l1: 0.000295, l2: 0.000317, l3: 0.000357, l4: 0.000735, l5: 0.001036, l6: 0.002399
[epoch: 299/1000, batch:   744/ 1052, ite: 78560] train loss: 0.005076, tar: 0.000279 
l0: 0.000279, l1: 0.000278, l2: 0.000286, l3: 0.000397, l4: 0.000623, l5: 0.001153, l6: 0.002423
[epoch: 299/1000, batch:   824/ 1052, ite: 78580] train loss: 0.005076, tar: 0.000279 
l0: 0.000215, l1: 0.000220, l2: 0.000236, l3: 0.000354, l4: 0.000661, l5: 0.001244, l6: 0.002001
[epoch: 299/1000, batch:   904/ 1052, ite: 78600] train loss: 0.005077, tar: 0.000279 
l0: 0.000160, l1: 0.000158, l2: 0.000190, l3: 0.000259, l4: 0.000299, l5: 0.000621, l6: 0.001009
[epoch: 299/1000, batch:   984/ 1052, ite: 78620] train loss: 0.005076, tar: 0.000279 
[Epoch 299/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000143, l1: 0.000159, l2: 0.000153, l3: 0.000229, l4: 0.000441, l5: 0.000908, l6: 0.001827
[epoch: 300/1000, batch:    12/ 1052, ite: 78640] train loss: 0.005074, tar: 0.000278 
l0: 0.000373, l1: 0.000375, l2: 0.000518, l3: 0.000568, l4: 0.000848, l5: 0.001823, l6: 0.003934
[epoch: 300/1000, batch:    92/ 1052, ite: 78660] train loss: 0.005076, tar: 0.000278 
l0: 0.000298, l1: 0.000314, l2: 0.000324, l3: 0.000398, l4: 0.000728, l5: 0.001473, l6: 0.002525
[epoch: 300/1000, batch:   172/ 1052, ite: 78680] train loss: 0.005074, tar: 0.000278 
l0: 0.000213, l1: 0.000221, l2: 0.000262, l3: 0.000297, l4: 0.000563, l5: 0.001250, l6: 0.001328
[epoch: 300/1000, batch:   252/ 1052, ite: 78700] train loss: 0.005073, tar: 0.000278 
l0: 0.000170, l1: 0.000180, l2: 0.000189, l3: 0.000238, l4: 0.000403, l5: 0.000925, l6: 0.001838
[epoch: 300/1000, batch:   332/ 1052, ite: 78720] train loss: 0.005074, tar: 0.000278 
l0: 0.000345, l1: 0.000360, l2: 0.000397, l3: 0.000443, l4: 0.000493, l5: 0.000891, l6: 0.001578
[epoch: 300/1000, batch:   412/ 1052, ite: 78740] train loss: 0.005074, tar: 0.000278 
l0: 0.000299, l1: 0.000300, l2: 0.000304, l3: 0.000398, l4: 0.000759, l5: 0.001609, l6: 0.001994
[epoch: 300/1000, batch:   492/ 1052, ite: 78760] train loss: 0.005077, tar: 0.000278 
l0: 0.000356, l1: 0.000374, l2: 0.000335, l3: 0.000411, l4: 0.000632, l5: 0.000783, l6: 0.001999
[epoch: 300/1000, batch:   572/ 1052, ite: 78780] train loss: 0.005077, tar: 0.000278 
l0: 0.000229, l1: 0.000238, l2: 0.000233, l3: 0.000281, l4: 0.000373, l5: 0.000763, l6: 0.001195
[epoch: 300/1000, batch:   652/ 1052, ite: 78800] train loss: 0.005073, tar: 0.000278 
l0: 0.000190, l1: 0.000193, l2: 0.000225, l3: 0.000293, l4: 0.000611, l5: 0.000749, l6: 0.001902
[epoch: 300/1000, batch:   732/ 1052, ite: 78820] train loss: 0.005073, tar: 0.000278 
l0: 0.000266, l1: 0.000266, l2: 0.000306, l3: 0.000302, l4: 0.000493, l5: 0.001225, l6: 0.001326
[epoch: 300/1000, batch:   812/ 1052, ite: 78840] train loss: 0.005072, tar: 0.000278 
l0: 0.000252, l1: 0.000249, l2: 0.000282, l3: 0.000365, l4: 0.000598, l5: 0.000906, l6: 0.001010
[epoch: 300/1000, batch:   892/ 1052, ite: 78860] train loss: 0.005075, tar: 0.000278 
l0: 0.000332, l1: 0.000335, l2: 0.000383, l3: 0.000411, l4: 0.000712, l5: 0.001046, l6: 0.001891
[epoch: 300/1000, batch:   972/ 1052, ite: 78880] train loss: 0.005076, tar: 0.000278 
l0: 0.000262, l1: 0.000257, l2: 0.000312, l3: 0.000379, l4: 0.000686, l5: 0.001239, l6: 0.001982
[epoch: 300/1000, batch:  1052/ 1052, ite: 78900] train loss: 0.005077, tar: 0.000278 
[Epoch 300/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000184, l1: 0.000194, l2: 0.000188, l3: 0.000244, l4: 0.000374, l5: 0.001092, l6: 0.001539
[epoch: 301/1000, batch:    80/ 1052, ite: 78920] train loss: 0.005075, tar: 0.000278 
l0: 0.000135, l1: 0.000134, l2: 0.000158, l3: 0.000220, l4: 0.000285, l5: 0.000939, l6: 0.001233
[epoch: 301/1000, batch:   160/ 1052, ite: 78940] train loss: 0.005073, tar: 0.000278 
l0: 0.000163, l1: 0.000163, l2: 0.000202, l3: 0.000268, l4: 0.000386, l5: 0.000748, l6: 0.001460
[epoch: 301/1000, batch:   240/ 1052, ite: 78960] train loss: 0.005072, tar: 0.000278 
l0: 0.000082, l1: 0.000079, l2: 0.000114, l3: 0.000154, l4: 0.000496, l5: 0.001058, l6: 0.001458
[epoch: 301/1000, batch:   320/ 1052, ite: 78980] train loss: 0.005071, tar: 0.000278 
l0: 0.000133, l1: 0.000129, l2: 0.000131, l3: 0.000200, l4: 0.000209, l5: 0.000572, l6: 0.001013
[epoch: 301/1000, batch:   400/ 1052, ite: 79000] train loss: 0.005070, tar: 0.000277 
l0: 0.000224, l1: 0.000236, l2: 0.000260, l3: 0.000278, l4: 0.000503, l5: 0.000911, l6: 0.001474
[epoch: 301/1000, batch:   480/ 1052, ite: 79020] train loss: 0.005069, tar: 0.000277 
l0: 0.000119, l1: 0.000113, l2: 0.000165, l3: 0.000287, l4: 0.000395, l5: 0.000918, l6: 0.001652
[epoch: 301/1000, batch:   560/ 1052, ite: 79040] train loss: 0.005069, tar: 0.000277 
l0: 0.000216, l1: 0.000211, l2: 0.000234, l3: 0.000289, l4: 0.000307, l5: 0.000959, l6: 0.001445
[epoch: 301/1000, batch:   640/ 1052, ite: 79060] train loss: 0.005069, tar: 0.000277 
l0: 0.000342, l1: 0.000335, l2: 0.000379, l3: 0.000497, l4: 0.000872, l5: 0.001059, l6: 0.002379
[epoch: 301/1000, batch:   720/ 1052, ite: 79080] train loss: 0.005068, tar: 0.000277 
l0: 0.000401, l1: 0.000428, l2: 0.000405, l3: 0.000436, l4: 0.000559, l5: 0.001014, l6: 0.002061
[epoch: 301/1000, batch:   800/ 1052, ite: 79100] train loss: 0.005068, tar: 0.000277 
l0: 0.000314, l1: 0.000326, l2: 0.000370, l3: 0.000381, l4: 0.000738, l5: 0.001213, l6: 0.001916
[epoch: 301/1000, batch:   880/ 1052, ite: 79120] train loss: 0.005070, tar: 0.000277 
l0: 0.000239, l1: 0.000243, l2: 0.000270, l3: 0.000352, l4: 0.000552, l5: 0.000790, l6: 0.001447
[epoch: 301/1000, batch:   960/ 1052, ite: 79140] train loss: 0.005072, tar: 0.000277 
l0: 0.000192, l1: 0.000193, l2: 0.000212, l3: 0.000227, l4: 0.000491, l5: 0.000694, l6: 0.001415
[epoch: 301/1000, batch:  1040/ 1052, ite: 79160] train loss: 0.005071, tar: 0.000277 
[Epoch 301/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000429, l1: 0.000443, l2: 0.000444, l3: 0.000618, l4: 0.001104, l5: 0.001622, l6: 0.003199
[epoch: 302/1000, batch:    68/ 1052, ite: 79180] train loss: 0.005072, tar: 0.000277 
l0: 0.000247, l1: 0.000254, l2: 0.000258, l3: 0.000296, l4: 0.000581, l5: 0.001143, l6: 0.001786
[epoch: 302/1000, batch:   148/ 1052, ite: 79200] train loss: 0.005072, tar: 0.000277 
l0: 0.000152, l1: 0.000152, l2: 0.000187, l3: 0.000236, l4: 0.000454, l5: 0.001133, l6: 0.001608
[epoch: 302/1000, batch:   228/ 1052, ite: 79220] train loss: 0.005071, tar: 0.000277 
l0: 0.000262, l1: 0.000272, l2: 0.000305, l3: 0.000307, l4: 0.000636, l5: 0.001160, l6: 0.001493
[epoch: 302/1000, batch:   308/ 1052, ite: 79240] train loss: 0.005073, tar: 0.000277 
l0: 0.000121, l1: 0.000120, l2: 0.000147, l3: 0.000177, l4: 0.000302, l5: 0.000584, l6: 0.001730
[epoch: 302/1000, batch:   388/ 1052, ite: 79260] train loss: 0.005073, tar: 0.000277 
l0: 0.000128, l1: 0.000128, l2: 0.000141, l3: 0.000171, l4: 0.000270, l5: 0.000989, l6: 0.001433
[epoch: 302/1000, batch:   468/ 1052, ite: 79280] train loss: 0.005075, tar: 0.000277 
l0: 0.000289, l1: 0.000291, l2: 0.000340, l3: 0.000422, l4: 0.000734, l5: 0.001348, l6: 0.002710
[epoch: 302/1000, batch:   548/ 1052, ite: 79300] train loss: 0.005074, tar: 0.000277 
l0: 0.000258, l1: 0.000259, l2: 0.000257, l3: 0.000332, l4: 0.000600, l5: 0.000596, l6: 0.002529
[epoch: 302/1000, batch:   628/ 1052, ite: 79320] train loss: 0.005074, tar: 0.000277 
l0: 0.000242, l1: 0.000252, l2: 0.000298, l3: 0.000321, l4: 0.000477, l5: 0.000792, l6: 0.001686
[epoch: 302/1000, batch:   708/ 1052, ite: 79340] train loss: 0.005073, tar: 0.000277 
l0: 0.000238, l1: 0.000244, l2: 0.000264, l3: 0.000287, l4: 0.000661, l5: 0.001119, l6: 0.002039
[epoch: 302/1000, batch:   788/ 1052, ite: 79360] train loss: 0.005071, tar: 0.000277 
l0: 0.000089, l1: 0.000090, l2: 0.000127, l3: 0.000170, l4: 0.000406, l5: 0.000579, l6: 0.001407
[epoch: 302/1000, batch:   868/ 1052, ite: 79380] train loss: 0.005072, tar: 0.000277 
l0: 0.000208, l1: 0.000205, l2: 0.000256, l3: 0.000372, l4: 0.000532, l5: 0.001134, l6: 0.001456
[epoch: 302/1000, batch:   948/ 1052, ite: 79400] train loss: 0.005070, tar: 0.000277 
l0: 0.000165, l1: 0.000179, l2: 0.000165, l3: 0.000207, l4: 0.000454, l5: 0.000952, l6: 0.001450
[epoch: 302/1000, batch:  1028/ 1052, ite: 79420] train loss: 0.005069, tar: 0.000276 
[Epoch 302/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000383, l1: 0.000376, l2: 0.000438, l3: 0.000424, l4: 0.000668, l5: 0.001129, l6: 0.002148
[epoch: 303/1000, batch:    56/ 1052, ite: 79440] train loss: 0.005066, tar: 0.000276 
l0: 0.000257, l1: 0.000260, l2: 0.000283, l3: 0.000358, l4: 0.000708, l5: 0.001795, l6: 0.002861
[epoch: 303/1000, batch:   136/ 1052, ite: 79460] train loss: 0.005065, tar: 0.000276 
l0: 0.000197, l1: 0.000213, l2: 0.000205, l3: 0.000210, l4: 0.000434, l5: 0.000690, l6: 0.001898
[epoch: 303/1000, batch:   216/ 1052, ite: 79480] train loss: 0.005064, tar: 0.000276 
l0: 0.000268, l1: 0.000262, l2: 0.000289, l3: 0.000516, l4: 0.000722, l5: 0.001414, l6: 0.002318
[epoch: 303/1000, batch:   296/ 1052, ite: 79500] train loss: 0.005062, tar: 0.000276 
l0: 0.000123, l1: 0.000126, l2: 0.000175, l3: 0.000246, l4: 0.000635, l5: 0.001067, l6: 0.002025
[epoch: 303/1000, batch:   376/ 1052, ite: 79520] train loss: 0.005063, tar: 0.000276 
l0: 0.000216, l1: 0.000227, l2: 0.000217, l3: 0.000273, l4: 0.000633, l5: 0.001215, l6: 0.002393
[epoch: 303/1000, batch:   456/ 1052, ite: 79540] train loss: 0.005061, tar: 0.000276 
l0: 0.000313, l1: 0.000307, l2: 0.000281, l3: 0.000394, l4: 0.000477, l5: 0.001184, l6: 0.001465
[epoch: 303/1000, batch:   536/ 1052, ite: 79560] train loss: 0.005063, tar: 0.000276 
l0: 0.000241, l1: 0.000240, l2: 0.000296, l3: 0.000372, l4: 0.000523, l5: 0.001312, l6: 0.002032
[epoch: 303/1000, batch:   616/ 1052, ite: 79580] train loss: 0.005063, tar: 0.000275 
l0: 0.000223, l1: 0.000229, l2: 0.000272, l3: 0.000307, l4: 0.000529, l5: 0.001169, l6: 0.002009
[epoch: 303/1000, batch:   696/ 1052, ite: 79600] train loss: 0.005061, tar: 0.000275 
l0: 0.000362, l1: 0.000364, l2: 0.000402, l3: 0.000513, l4: 0.000761, l5: 0.001393, l6: 0.002853
[epoch: 303/1000, batch:   776/ 1052, ite: 79620] train loss: 0.005062, tar: 0.000275 
l0: 0.000145, l1: 0.000150, l2: 0.000150, l3: 0.000194, l4: 0.000417, l5: 0.000699, l6: 0.001210
[epoch: 303/1000, batch:   856/ 1052, ite: 79640] train loss: 0.005064, tar: 0.000275 
l0: 0.000134, l1: 0.000131, l2: 0.000165, l3: 0.000250, l4: 0.000297, l5: 0.000679, l6: 0.001085
[epoch: 303/1000, batch:   936/ 1052, ite: 79660] train loss: 0.005063, tar: 0.000275 
l0: 0.000137, l1: 0.000144, l2: 0.000158, l3: 0.000199, l4: 0.000298, l5: 0.000537, l6: 0.001044
[epoch: 303/1000, batch:  1016/ 1052, ite: 79680] train loss: 0.005061, tar: 0.000275 
[Epoch 303/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000110, l1: 0.000112, l2: 0.000174, l3: 0.000246, l4: 0.000522, l5: 0.000918, l6: 0.001732
[epoch: 304/1000, batch:    44/ 1052, ite: 79700] train loss: 0.005063, tar: 0.000275 
l0: 0.000184, l1: 0.000192, l2: 0.000206, l3: 0.000263, l4: 0.000440, l5: 0.000947, l6: 0.001935
[epoch: 304/1000, batch:   124/ 1052, ite: 79720] train loss: 0.005064, tar: 0.000275 
l0: 0.000255, l1: 0.000262, l2: 0.000286, l3: 0.000324, l4: 0.000582, l5: 0.000933, l6: 0.001096
[epoch: 304/1000, batch:   204/ 1052, ite: 79740] train loss: 0.005066, tar: 0.000276 
l0: 0.000342, l1: 0.000353, l2: 0.000373, l3: 0.000363, l4: 0.000472, l5: 0.000935, l6: 0.001725
[epoch: 304/1000, batch:   284/ 1052, ite: 79760] train loss: 0.005068, tar: 0.000276 
l0: 0.000319, l1: 0.000312, l2: 0.000352, l3: 0.000518, l4: 0.000554, l5: 0.000901, l6: 0.001631
[epoch: 304/1000, batch:   364/ 1052, ite: 79780] train loss: 0.005071, tar: 0.000276 
l0: 0.000535, l1: 0.000720, l2: 0.000558, l3: 0.000625, l4: 0.001185, l5: 0.001527, l6: 0.002807
[epoch: 304/1000, batch:   444/ 1052, ite: 79800] train loss: 0.005088, tar: 0.000279 
l0: 0.000311, l1: 0.000323, l2: 0.000272, l3: 0.000287, l4: 0.000483, l5: 0.000885, l6: 0.001196
[epoch: 304/1000, batch:   524/ 1052, ite: 79820] train loss: 0.005090, tar: 0.000279 
l0: 0.000231, l1: 0.000227, l2: 0.000275, l3: 0.000323, l4: 0.000396, l5: 0.000874, l6: 0.001386
[epoch: 304/1000, batch:   604/ 1052, ite: 79840] train loss: 0.005092, tar: 0.000280 
l0: 0.000574, l1: 0.000594, l2: 0.000625, l3: 0.000696, l4: 0.000868, l5: 0.001924, l6: 0.004495
[epoch: 304/1000, batch:   684/ 1052, ite: 79860] train loss: 0.005094, tar: 0.000280 
l0: 0.000491, l1: 0.000432, l2: 0.000458, l3: 0.001552, l4: 0.001388, l5: 0.002393, l6: 0.003378
[epoch: 304/1000, batch:   764/ 1052, ite: 79880] train loss: 0.005104, tar: 0.000281 
l0: 0.001445, l1: 0.001509, l2: 0.001520, l3: 0.002392, l4: 0.001798, l5: 0.003362, l6: 0.004024
[epoch: 304/1000, batch:   844/ 1052, ite: 79900] train loss: 0.005109, tar: 0.000282 
l0: 0.000243, l1: 0.000243, l2: 0.000262, l3: 0.000369, l4: 0.000598, l5: 0.001035, l6: 0.002272
[epoch: 304/1000, batch:   924/ 1052, ite: 79920] train loss: 0.005110, tar: 0.000282 
l0: 0.000290, l1: 0.000297, l2: 0.000323, l3: 0.000348, l4: 0.000491, l5: 0.001113, l6: 0.002075
[epoch: 304/1000, batch:  1004/ 1052, ite: 79940] train loss: 0.005111, tar: 0.000282 
[Epoch 304/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000281, l1: 0.000295, l2: 0.000312, l3: 0.000404, l4: 0.000556, l5: 0.001008, l6: 0.001629
[epoch: 305/1000, batch:    32/ 1052, ite: 79960] train loss: 0.005115, tar: 0.000283 
l0: 0.000641, l1: 0.000642, l2: 0.000841, l3: 0.000659, l4: 0.000777, l5: 0.001790, l6: 0.002731
[epoch: 305/1000, batch:   112/ 1052, ite: 79980] train loss: 0.005116, tar: 0.000283 
l0: 0.000362, l1: 0.000365, l2: 0.000434, l3: 0.000477, l4: 0.000641, l5: 0.001584, l6: 0.001661
[epoch: 305/1000, batch:   192/ 1052, ite: 80000] train loss: 0.005119, tar: 0.000283 
l0: 0.000239, l1: 0.000242, l2: 0.000243, l3: 0.000270, l4: 0.000355, l5: 0.000640, l6: 0.001736
[epoch: 305/1000, batch:   272/ 1052, ite: 80020] train loss: 0.005121, tar: 0.000284 
l0: 0.000397, l1: 0.000377, l2: 0.000445, l3: 0.000601, l4: 0.000998, l5: 0.001384, l6: 0.002646
[epoch: 305/1000, batch:   352/ 1052, ite: 80040] train loss: 0.005120, tar: 0.000284 
l0: 0.000286, l1: 0.000289, l2: 0.000281, l3: 0.000296, l4: 0.000440, l5: 0.000984, l6: 0.001612
[epoch: 305/1000, batch:   432/ 1052, ite: 80060] train loss: 0.005123, tar: 0.000284 
l0: 0.000257, l1: 0.000250, l2: 0.000260, l3: 0.000350, l4: 0.000491, l5: 0.000917, l6: 0.001795
[epoch: 305/1000, batch:   512/ 1052, ite: 80080] train loss: 0.005125, tar: 0.000284 
l0: 0.000097, l1: 0.000097, l2: 0.000115, l3: 0.000165, l4: 0.000229, l5: 0.000800, l6: 0.001028
[epoch: 305/1000, batch:   592/ 1052, ite: 80100] train loss: 0.005123, tar: 0.000284 
l0: 0.000427, l1: 0.000439, l2: 0.000451, l3: 0.000545, l4: 0.000734, l5: 0.002118, l6: 0.003898
[epoch: 305/1000, batch:   672/ 1052, ite: 80120] train loss: 0.005124, tar: 0.000284 
l0: 0.000232, l1: 0.000247, l2: 0.000247, l3: 0.000249, l4: 0.000441, l5: 0.000548, l6: 0.001088
[epoch: 305/1000, batch:   752/ 1052, ite: 80140] train loss: 0.005126, tar: 0.000284 
l0: 0.000402, l1: 0.000408, l2: 0.000488, l3: 0.000567, l4: 0.000576, l5: 0.001365, l6: 0.002209
[epoch: 305/1000, batch:   832/ 1052, ite: 80160] train loss: 0.005128, tar: 0.000285 
l0: 0.000212, l1: 0.000218, l2: 0.000215, l3: 0.000268, l4: 0.000488, l5: 0.000586, l6: 0.001443
[epoch: 305/1000, batch:   912/ 1052, ite: 80180] train loss: 0.005128, tar: 0.000285 
l0: 0.000561, l1: 0.000566, l2: 0.000569, l3: 0.000640, l4: 0.000774, l5: 0.001354, l6: 0.003166
[epoch: 305/1000, batch:   992/ 1052, ite: 80200] train loss: 0.005129, tar: 0.000285 
[Epoch 305/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000267, l1: 0.000265, l2: 0.000298, l3: 0.000327, l4: 0.000557, l5: 0.001019, l6: 0.002197
[epoch: 306/1000, batch:    20/ 1052, ite: 80220] train loss: 0.005126, tar: 0.000285 
l0: 0.000186, l1: 0.000190, l2: 0.000194, l3: 0.000246, l4: 0.000199, l5: 0.000785, l6: 0.001103
[epoch: 306/1000, batch:   100/ 1052, ite: 80240] train loss: 0.005126, tar: 0.000284 
l0: 0.000294, l1: 0.000283, l2: 0.000327, l3: 0.000406, l4: 0.000872, l5: 0.002053, l6: 0.003015
[epoch: 306/1000, batch:   180/ 1052, ite: 80260] train loss: 0.005127, tar: 0.000285 
l0: 0.000571, l1: 0.000556, l2: 0.000636, l3: 0.000897, l4: 0.001361, l5: 0.002033, l6: 0.004547
[epoch: 306/1000, batch:   260/ 1052, ite: 80280] train loss: 0.005125, tar: 0.000284 
l0: 0.000165, l1: 0.000156, l2: 0.000168, l3: 0.000261, l4: 0.000512, l5: 0.000598, l6: 0.001319
[epoch: 306/1000, batch:   340/ 1052, ite: 80300] train loss: 0.005125, tar: 0.000284 
l0: 0.000173, l1: 0.000174, l2: 0.000193, l3: 0.000261, l4: 0.000582, l5: 0.000858, l6: 0.001655
[epoch: 306/1000, batch:   420/ 1052, ite: 80320] train loss: 0.005125, tar: 0.000284 
l0: 0.000292, l1: 0.000296, l2: 0.000359, l3: 0.000388, l4: 0.000763, l5: 0.001274, l6: 0.002188
[epoch: 306/1000, batch:   500/ 1052, ite: 80340] train loss: 0.005126, tar: 0.000284 
l0: 0.000222, l1: 0.000252, l2: 0.000220, l3: 0.000242, l4: 0.000514, l5: 0.000933, l6: 0.001771
[epoch: 306/1000, batch:   580/ 1052, ite: 80360] train loss: 0.005124, tar: 0.000284 
l0: 0.000330, l1: 0.000306, l2: 0.000380, l3: 0.000556, l4: 0.000905, l5: 0.001984, l6: 0.003150
[epoch: 306/1000, batch:   660/ 1052, ite: 80380] train loss: 0.005127, tar: 0.000284 
l0: 0.000335, l1: 0.000347, l2: 0.000397, l3: 0.000489, l4: 0.000673, l5: 0.001277, l6: 0.003025
[epoch: 306/1000, batch:   740/ 1052, ite: 80400] train loss: 0.005126, tar: 0.000284 
l0: 0.000386, l1: 0.000386, l2: 0.000451, l3: 0.000699, l4: 0.001170, l5: 0.002578, l6: 0.004234
[epoch: 306/1000, batch:   820/ 1052, ite: 80420] train loss: 0.005129, tar: 0.000284 
l0: 0.000112, l1: 0.000109, l2: 0.000145, l3: 0.000211, l4: 0.000279, l5: 0.000614, l6: 0.000840
[epoch: 306/1000, batch:   900/ 1052, ite: 80440] train loss: 0.005128, tar: 0.000284 
l0: 0.000216, l1: 0.000232, l2: 0.000219, l3: 0.000255, l4: 0.000437, l5: 0.000691, l6: 0.001078
[epoch: 306/1000, batch:   980/ 1052, ite: 80460] train loss: 0.005127, tar: 0.000284 
[Epoch 306/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000325, l1: 0.000341, l2: 0.000338, l3: 0.000374, l4: 0.000689, l5: 0.001276, l6: 0.002961
[epoch: 307/1000, batch:     8/ 1052, ite: 80480] train loss: 0.005128, tar: 0.000284 
l0: 0.000180, l1: 0.000161, l2: 0.000221, l3: 0.000352, l4: 0.000559, l5: 0.000957, l6: 0.001413
[epoch: 307/1000, batch:    88/ 1052, ite: 80500] train loss: 0.005129, tar: 0.000284 
l0: 0.000316, l1: 0.000316, l2: 0.000369, l3: 0.000459, l4: 0.000857, l5: 0.002025, l6: 0.002270
[epoch: 307/1000, batch:   168/ 1052, ite: 80520] train loss: 0.005127, tar: 0.000284 
l0: 0.000198, l1: 0.000194, l2: 0.000251, l3: 0.000292, l4: 0.000543, l5: 0.000832, l6: 0.001107
[epoch: 307/1000, batch:   248/ 1052, ite: 80540] train loss: 0.005125, tar: 0.000284 
l0: 0.000324, l1: 0.000310, l2: 0.000374, l3: 0.000523, l4: 0.000971, l5: 0.002001, l6: 0.002637
[epoch: 307/1000, batch:   328/ 1052, ite: 80560] train loss: 0.005126, tar: 0.000284 
l0: 0.000298, l1: 0.000304, l2: 0.000341, l3: 0.000376, l4: 0.000707, l5: 0.001551, l6: 0.002005
[epoch: 307/1000, batch:   408/ 1052, ite: 80580] train loss: 0.005125, tar: 0.000284 
l0: 0.000402, l1: 0.000408, l2: 0.000463, l3: 0.000462, l4: 0.000577, l5: 0.001668, l6: 0.002819
[epoch: 307/1000, batch:   488/ 1052, ite: 80600] train loss: 0.005124, tar: 0.000284 
l0: 0.000171, l1: 0.000176, l2: 0.000178, l3: 0.000217, l4: 0.000591, l5: 0.001288, l6: 0.001505
[epoch: 307/1000, batch:   568/ 1052, ite: 80620] train loss: 0.005125, tar: 0.000284 
l0: 0.000193, l1: 0.000186, l2: 0.000217, l3: 0.000306, l4: 0.000466, l5: 0.000995, l6: 0.001068
[epoch: 307/1000, batch:   648/ 1052, ite: 80640] train loss: 0.005125, tar: 0.000284 
l0: 0.000143, l1: 0.000139, l2: 0.000184, l3: 0.000255, l4: 0.000516, l5: 0.000853, l6: 0.001966
[epoch: 307/1000, batch:   728/ 1052, ite: 80660] train loss: 0.005123, tar: 0.000284 
l0: 0.000162, l1: 0.000156, l2: 0.000204, l3: 0.000234, l4: 0.000441, l5: 0.000751, l6: 0.000981
[epoch: 307/1000, batch:   808/ 1052, ite: 80680] train loss: 0.005123, tar: 0.000284 
l0: 0.000148, l1: 0.000145, l2: 0.000155, l3: 0.000179, l4: 0.000392, l5: 0.000563, l6: 0.001185
[epoch: 307/1000, batch:   888/ 1052, ite: 80700] train loss: 0.005122, tar: 0.000284 
l0: 0.000194, l1: 0.000198, l2: 0.000182, l3: 0.000228, l4: 0.000356, l5: 0.000800, l6: 0.001160
[epoch: 307/1000, batch:   968/ 1052, ite: 80720] train loss: 0.005123, tar: 0.000284 
l0: 0.000320, l1: 0.000322, l2: 0.000340, l3: 0.000490, l4: 0.000861, l5: 0.001179, l6: 0.001849
[epoch: 307/1000, batch:  1048/ 1052, ite: 80740] train loss: 0.005122, tar: 0.000283 
[Epoch 307/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000192, l1: 0.000195, l2: 0.000215, l3: 0.000286, l4: 0.000571, l5: 0.000927, l6: 0.001669
[epoch: 308/1000, batch:    76/ 1052, ite: 80760] train loss: 0.005120, tar: 0.000283 
l0: 0.000291, l1: 0.000292, l2: 0.000358, l3: 0.000480, l4: 0.000975, l5: 0.002223, l6: 0.003565
[epoch: 308/1000, batch:   156/ 1052, ite: 80780] train loss: 0.005121, tar: 0.000283 
l0: 0.000150, l1: 0.000150, l2: 0.000232, l3: 0.000288, l4: 0.000547, l5: 0.001028, l6: 0.001352
[epoch: 308/1000, batch:   236/ 1052, ite: 80800] train loss: 0.005120, tar: 0.000283 
l0: 0.000187, l1: 0.000184, l2: 0.000206, l3: 0.000237, l4: 0.000386, l5: 0.001027, l6: 0.001223
[epoch: 308/1000, batch:   316/ 1052, ite: 80820] train loss: 0.005118, tar: 0.000283 
l0: 0.000149, l1: 0.000147, l2: 0.000178, l3: 0.000225, l4: 0.000370, l5: 0.001126, l6: 0.001408
[epoch: 308/1000, batch:   396/ 1052, ite: 80840] train loss: 0.005116, tar: 0.000283 
l0: 0.000200, l1: 0.000224, l2: 0.000183, l3: 0.000231, l4: 0.000431, l5: 0.001056, l6: 0.001081
[epoch: 308/1000, batch:   476/ 1052, ite: 80860] train loss: 0.005116, tar: 0.000283 
l0: 0.000435, l1: 0.000459, l2: 0.000494, l3: 0.000492, l4: 0.000724, l5: 0.001829, l6: 0.003913
[epoch: 308/1000, batch:   556/ 1052, ite: 80880] train loss: 0.005116, tar: 0.000283 
l0: 0.000339, l1: 0.000344, l2: 0.000364, l3: 0.000374, l4: 0.000590, l5: 0.000917, l6: 0.001050
[epoch: 308/1000, batch:   636/ 1052, ite: 80900] train loss: 0.005115, tar: 0.000283 
l0: 0.000194, l1: 0.000197, l2: 0.000203, l3: 0.000220, l4: 0.000305, l5: 0.001103, l6: 0.001294
[epoch: 308/1000, batch:   716/ 1052, ite: 80920] train loss: 0.005114, tar: 0.000282 
l0: 0.000202, l1: 0.000196, l2: 0.000240, l3: 0.000306, l4: 0.000496, l5: 0.001137, l6: 0.002156
[epoch: 308/1000, batch:   796/ 1052, ite: 80940] train loss: 0.005113, tar: 0.000282 
l0: 0.000338, l1: 0.000346, l2: 0.000389, l3: 0.000477, l4: 0.000968, l5: 0.002164, l6: 0.003413
[epoch: 308/1000, batch:   876/ 1052, ite: 80960] train loss: 0.005115, tar: 0.000282 
l0: 0.000173, l1: 0.000173, l2: 0.000194, l3: 0.000234, l4: 0.000417, l5: 0.000948, l6: 0.002178
[epoch: 308/1000, batch:   956/ 1052, ite: 80980] train loss: 0.005117, tar: 0.000283 
l0: 0.000197, l1: 0.000193, l2: 0.000237, l3: 0.000304, l4: 0.000511, l5: 0.000935, l6: 0.001606
[epoch: 308/1000, batch:  1036/ 1052, ite: 81000] train loss: 0.005116, tar: 0.000282 
[Epoch 308/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000281, l1: 0.000297, l2: 0.000313, l3: 0.000445, l4: 0.000700, l5: 0.001124, l6: 0.002073
[epoch: 309/1000, batch:    64/ 1052, ite: 81020] train loss: 0.005116, tar: 0.000282 
l0: 0.000198, l1: 0.000200, l2: 0.000211, l3: 0.000274, l4: 0.000469, l5: 0.000934, l6: 0.001334
[epoch: 309/1000, batch:   144/ 1052, ite: 81040] train loss: 0.005116, tar: 0.000282 
l0: 0.000181, l1: 0.000183, l2: 0.000206, l3: 0.000239, l4: 0.000544, l5: 0.000693, l6: 0.001789
[epoch: 309/1000, batch:   224/ 1052, ite: 81060] train loss: 0.005115, tar: 0.000282 
l0: 0.000331, l1: 0.000319, l2: 0.000373, l3: 0.000499, l4: 0.000818, l5: 0.001152, l6: 0.002264
[epoch: 309/1000, batch:   304/ 1052, ite: 81080] train loss: 0.005113, tar: 0.000282 
l0: 0.000172, l1: 0.000166, l2: 0.000222, l3: 0.000303, l4: 0.000506, l5: 0.000942, l6: 0.001375
[epoch: 309/1000, batch:   384/ 1052, ite: 81100] train loss: 0.005114, tar: 0.000282 
l0: 0.000293, l1: 0.000296, l2: 0.000333, l3: 0.000315, l4: 0.000687, l5: 0.001262, l6: 0.002114
[epoch: 309/1000, batch:   464/ 1052, ite: 81120] train loss: 0.005112, tar: 0.000282 
l0: 0.000202, l1: 0.000216, l2: 0.000244, l3: 0.000296, l4: 0.000432, l5: 0.000759, l6: 0.001393
[epoch: 309/1000, batch:   544/ 1052, ite: 81140] train loss: 0.005112, tar: 0.000282 
l0: 0.000287, l1: 0.000291, l2: 0.000354, l3: 0.000485, l4: 0.000816, l5: 0.001336, l6: 0.002209
[epoch: 309/1000, batch:   624/ 1052, ite: 81160] train loss: 0.005111, tar: 0.000282 
l0: 0.000101, l1: 0.000096, l2: 0.000120, l3: 0.000196, l4: 0.000377, l5: 0.000860, l6: 0.001086
[epoch: 309/1000, batch:   704/ 1052, ite: 81180] train loss: 0.005110, tar: 0.000282 
l0: 0.000561, l1: 0.000556, l2: 0.000620, l3: 0.000747, l4: 0.001136, l5: 0.002148, l6: 0.002636
[epoch: 309/1000, batch:   784/ 1052, ite: 81200] train loss: 0.005110, tar: 0.000282 
l0: 0.000129, l1: 0.000136, l2: 0.000138, l3: 0.000212, l4: 0.000328, l5: 0.000987, l6: 0.001248
[epoch: 309/1000, batch:   864/ 1052, ite: 81220] train loss: 0.005108, tar: 0.000281 
l0: 0.000620, l1: 0.000603, l2: 0.000673, l3: 0.000696, l4: 0.000790, l5: 0.001359, l6: 0.001453
[epoch: 309/1000, batch:   944/ 1052, ite: 81240] train loss: 0.005109, tar: 0.000281 
l0: 0.000306, l1: 0.000297, l2: 0.000355, l3: 0.000423, l4: 0.000591, l5: 0.001867, l6: 0.002706
[epoch: 309/1000, batch:  1024/ 1052, ite: 81260] train loss: 0.005109, tar: 0.000281 
[Epoch 309/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000437, l1: 0.000455, l2: 0.000504, l3: 0.000564, l4: 0.001316, l5: 0.002572, l6: 0.003871
[epoch: 310/1000, batch:    52/ 1052, ite: 81280] train loss: 0.005109, tar: 0.000281 
l0: 0.000195, l1: 0.000205, l2: 0.000186, l3: 0.000246, l4: 0.000434, l5: 0.000877, l6: 0.001256
[epoch: 310/1000, batch:   132/ 1052, ite: 81300] train loss: 0.005108, tar: 0.000281 
l0: 0.000260, l1: 0.000261, l2: 0.000299, l3: 0.000427, l4: 0.001014, l5: 0.001610, l6: 0.002248
[epoch: 310/1000, batch:   212/ 1052, ite: 81320] train loss: 0.005108, tar: 0.000281 
l0: 0.000417, l1: 0.000418, l2: 0.000496, l3: 0.000609, l4: 0.001269, l5: 0.002530, l6: 0.004708
[epoch: 310/1000, batch:   292/ 1052, ite: 81340] train loss: 0.005108, tar: 0.000281 
l0: 0.000278, l1: 0.000294, l2: 0.000285, l3: 0.000381, l4: 0.000730, l5: 0.001389, l6: 0.003470
[epoch: 310/1000, batch:   372/ 1052, ite: 81360] train loss: 0.005108, tar: 0.000281 
l0: 0.000186, l1: 0.000186, l2: 0.000219, l3: 0.000275, l4: 0.000535, l5: 0.001110, l6: 0.002108
[epoch: 310/1000, batch:   452/ 1052, ite: 81380] train loss: 0.005107, tar: 0.000281 
l0: 0.000125, l1: 0.000130, l2: 0.000161, l3: 0.000176, l4: 0.000335, l5: 0.000923, l6: 0.001818
[epoch: 310/1000, batch:   532/ 1052, ite: 81400] train loss: 0.005105, tar: 0.000280 
l0: 0.000256, l1: 0.000270, l2: 0.000324, l3: 0.000379, l4: 0.000735, l5: 0.001826, l6: 0.002085
[epoch: 310/1000, batch:   612/ 1052, ite: 81420] train loss: 0.005103, tar: 0.000280 
l0: 0.000301, l1: 0.000294, l2: 0.000348, l3: 0.000380, l4: 0.000472, l5: 0.000905, l6: 0.001603
[epoch: 310/1000, batch:   692/ 1052, ite: 81440] train loss: 0.005103, tar: 0.000280 
l0: 0.000122, l1: 0.000124, l2: 0.000153, l3: 0.000191, l4: 0.000347, l5: 0.000748, l6: 0.001263
[epoch: 310/1000, batch:   772/ 1052, ite: 81460] train loss: 0.005102, tar: 0.000280 
l0: 0.000261, l1: 0.000302, l2: 0.000314, l3: 0.000290, l4: 0.000561, l5: 0.001272, l6: 0.003151
[epoch: 310/1000, batch:   852/ 1052, ite: 81480] train loss: 0.005102, tar: 0.000280 
l0: 0.000116, l1: 0.000118, l2: 0.000130, l3: 0.000152, l4: 0.000312, l5: 0.000683, l6: 0.000995
[epoch: 310/1000, batch:   932/ 1052, ite: 81500] train loss: 0.005104, tar: 0.000280 
l0: 0.000135, l1: 0.000137, l2: 0.000154, l3: 0.000202, l4: 0.000369, l5: 0.000816, l6: 0.001360
[epoch: 310/1000, batch:  1012/ 1052, ite: 81520] train loss: 0.005106, tar: 0.000281 
[Epoch 310/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000128, l1: 0.000135, l2: 0.000160, l3: 0.000197, l4: 0.000419, l5: 0.000762, l6: 0.001040
[epoch: 311/1000, batch:    40/ 1052, ite: 81540] train loss: 0.005106, tar: 0.000281 
l0: 0.000282, l1: 0.000287, l2: 0.000301, l3: 0.000338, l4: 0.000577, l5: 0.001447, l6: 0.001995
[epoch: 311/1000, batch:   120/ 1052, ite: 81560] train loss: 0.005106, tar: 0.000281 
l0: 0.000218, l1: 0.000219, l2: 0.000227, l3: 0.000246, l4: 0.000401, l5: 0.000921, l6: 0.001187
[epoch: 311/1000, batch:   200/ 1052, ite: 81580] train loss: 0.005107, tar: 0.000281 
l0: 0.000314, l1: 0.000319, l2: 0.000334, l3: 0.000412, l4: 0.000661, l5: 0.001996, l6: 0.003110
[epoch: 311/1000, batch:   280/ 1052, ite: 81600] train loss: 0.005108, tar: 0.000281 
l0: 0.000252, l1: 0.000253, l2: 0.000280, l3: 0.000338, l4: 0.000606, l5: 0.001088, l6: 0.001879
[epoch: 311/1000, batch:   360/ 1052, ite: 81620] train loss: 0.005107, tar: 0.000280 
l0: 0.000227, l1: 0.000226, l2: 0.000243, l3: 0.000350, l4: 0.000367, l5: 0.000872, l6: 0.001999
[epoch: 311/1000, batch:   440/ 1052, ite: 81640] train loss: 0.005106, tar: 0.000280 
l0: 0.000108, l1: 0.000101, l2: 0.000147, l3: 0.000221, l4: 0.000415, l5: 0.000759, l6: 0.001329
[epoch: 311/1000, batch:   520/ 1052, ite: 81660] train loss: 0.005105, tar: 0.000280 
l0: 0.000250, l1: 0.000247, l2: 0.000313, l3: 0.000408, l4: 0.000956, l5: 0.001605, l6: 0.002215
[epoch: 311/1000, batch:   600/ 1052, ite: 81680] train loss: 0.005104, tar: 0.000280 
l0: 0.000157, l1: 0.000162, l2: 0.000182, l3: 0.000233, l4: 0.000435, l5: 0.001110, l6: 0.001355
[epoch: 311/1000, batch:   680/ 1052, ite: 81700] train loss: 0.005103, tar: 0.000280 
l0: 0.000140, l1: 0.000152, l2: 0.000149, l3: 0.000150, l4: 0.000302, l5: 0.000802, l6: 0.000964
[epoch: 311/1000, batch:   760/ 1052, ite: 81720] train loss: 0.005103, tar: 0.000280 
l0: 0.000153, l1: 0.000162, l2: 0.000170, l3: 0.000225, l4: 0.000553, l5: 0.000684, l6: 0.001728
[epoch: 311/1000, batch:   840/ 1052, ite: 81740] train loss: 0.005104, tar: 0.000280 
l0: 0.000140, l1: 0.000136, l2: 0.000145, l3: 0.000181, l4: 0.000576, l5: 0.000856, l6: 0.001698
[epoch: 311/1000, batch:   920/ 1052, ite: 81760] train loss: 0.005104, tar: 0.000280 
l0: 0.000299, l1: 0.000308, l2: 0.000330, l3: 0.000379, l4: 0.000594, l5: 0.001187, l6: 0.003008
[epoch: 311/1000, batch:  1000/ 1052, ite: 81780] train loss: 0.005103, tar: 0.000280 
[Epoch 311/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000353, l1: 0.000361, l2: 0.000373, l3: 0.000427, l4: 0.000540, l5: 0.001606, l6: 0.001784
[epoch: 312/1000, batch:    28/ 1052, ite: 81800] train loss: 0.005102, tar: 0.000280 
l0: 0.000330, l1: 0.000338, l2: 0.000391, l3: 0.000493, l4: 0.000845, l5: 0.002554, l6: 0.002468
[epoch: 312/1000, batch:   108/ 1052, ite: 81820] train loss: 0.005101, tar: 0.000280 
l0: 0.000193, l1: 0.000193, l2: 0.000195, l3: 0.000283, l4: 0.000677, l5: 0.001068, l6: 0.001545
[epoch: 312/1000, batch:   188/ 1052, ite: 81840] train loss: 0.005100, tar: 0.000280 
l0: 0.000156, l1: 0.000155, l2: 0.000218, l3: 0.000210, l4: 0.000523, l5: 0.000614, l6: 0.001251
[epoch: 312/1000, batch:   268/ 1052, ite: 81860] train loss: 0.005101, tar: 0.000280 
l0: 0.000145, l1: 0.000150, l2: 0.000210, l3: 0.000198, l4: 0.000351, l5: 0.000666, l6: 0.001457
[epoch: 312/1000, batch:   348/ 1052, ite: 81880] train loss: 0.005100, tar: 0.000279 
l0: 0.000159, l1: 0.000155, l2: 0.000195, l3: 0.000274, l4: 0.000443, l5: 0.000949, l6: 0.001147
[epoch: 312/1000, batch:   428/ 1052, ite: 81900] train loss: 0.005100, tar: 0.000279 
l0: 0.000312, l1: 0.000307, l2: 0.000342, l3: 0.000455, l4: 0.000673, l5: 0.002011, l6: 0.002878
[epoch: 312/1000, batch:   508/ 1052, ite: 81920] train loss: 0.005099, tar: 0.000279 
l0: 0.000094, l1: 0.000097, l2: 0.000123, l3: 0.000178, l4: 0.000327, l5: 0.000717, l6: 0.001072
[epoch: 312/1000, batch:   588/ 1052, ite: 81940] train loss: 0.005100, tar: 0.000279 
l0: 0.000095, l1: 0.000098, l2: 0.000150, l3: 0.000218, l4: 0.000453, l5: 0.001189, l6: 0.001625
[epoch: 312/1000, batch:   668/ 1052, ite: 81960] train loss: 0.005099, tar: 0.000279 
l0: 0.000219, l1: 0.000226, l2: 0.000237, l3: 0.000286, l4: 0.000392, l5: 0.000931, l6: 0.001837
[epoch: 312/1000, batch:   748/ 1052, ite: 81980] train loss: 0.005099, tar: 0.000279 
l0: 0.000170, l1: 0.000173, l2: 0.000194, l3: 0.000246, l4: 0.000575, l5: 0.001281, l6: 0.002584
[epoch: 312/1000, batch:   828/ 1052, ite: 82000] train loss: 0.005098, tar: 0.000279 
l0: 0.000200, l1: 0.000226, l2: 0.000184, l3: 0.000219, l4: 0.000505, l5: 0.000680, l6: 0.001421
[epoch: 312/1000, batch:   908/ 1052, ite: 82020] train loss: 0.005098, tar: 0.000279 
l0: 0.000103, l1: 0.000104, l2: 0.000132, l3: 0.000183, l4: 0.000266, l5: 0.000602, l6: 0.001066
[epoch: 312/1000, batch:   988/ 1052, ite: 82040] train loss: 0.005098, tar: 0.000279 
[Epoch 312/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000630, l1: 0.000673, l2: 0.000719, l3: 0.000783, l4: 0.001139, l5: 0.001686, l6: 0.004285
[epoch: 313/1000, batch:    16/ 1052, ite: 82060] train loss: 0.005098, tar: 0.000279 
l0: 0.000190, l1: 0.000200, l2: 0.000219, l3: 0.000294, l4: 0.000567, l5: 0.000853, l6: 0.001921
[epoch: 313/1000, batch:    96/ 1052, ite: 82080] train loss: 0.005097, tar: 0.000279 
l0: 0.000170, l1: 0.000170, l2: 0.000208, l3: 0.000252, l4: 0.000470, l5: 0.000976, l6: 0.002090
[epoch: 313/1000, batch:   176/ 1052, ite: 82100] train loss: 0.005096, tar: 0.000279 
l0: 0.000167, l1: 0.000164, l2: 0.000191, l3: 0.000238, l4: 0.000398, l5: 0.000621, l6: 0.001309
[epoch: 313/1000, batch:   256/ 1052, ite: 82120] train loss: 0.005094, tar: 0.000278 
l0: 0.000435, l1: 0.000442, l2: 0.000486, l3: 0.000582, l4: 0.000875, l5: 0.001450, l6: 0.005303
[epoch: 313/1000, batch:   336/ 1052, ite: 82140] train loss: 0.005095, tar: 0.000278 
l0: 0.000147, l1: 0.000148, l2: 0.000175, l3: 0.000198, l4: 0.000399, l5: 0.000932, l6: 0.001341
[epoch: 313/1000, batch:   416/ 1052, ite: 82160] train loss: 0.005094, tar: 0.000278 
l0: 0.000330, l1: 0.000320, l2: 0.000324, l3: 0.000541, l4: 0.000893, l5: 0.001827, l6: 0.002580
[epoch: 313/1000, batch:   496/ 1052, ite: 82180] train loss: 0.005093, tar: 0.000278 
l0: 0.000124, l1: 0.000123, l2: 0.000158, l3: 0.000200, l4: 0.000402, l5: 0.000781, l6: 0.001371
[epoch: 313/1000, batch:   576/ 1052, ite: 82200] train loss: 0.005094, tar: 0.000278 
l0: 0.000155, l1: 0.000153, l2: 0.000176, l3: 0.000203, l4: 0.000427, l5: 0.000804, l6: 0.001139
[epoch: 313/1000, batch:   656/ 1052, ite: 82220] train loss: 0.005094, tar: 0.000278 
l0: 0.000081, l1: 0.000083, l2: 0.000098, l3: 0.000154, l4: 0.000329, l5: 0.000828, l6: 0.000939
[epoch: 313/1000, batch:   736/ 1052, ite: 82240] train loss: 0.005095, tar: 0.000278 
l0: 0.000151, l1: 0.000151, l2: 0.000190, l3: 0.000220, l4: 0.000359, l5: 0.000918, l6: 0.001266
[epoch: 313/1000, batch:   816/ 1052, ite: 82260] train loss: 0.005094, tar: 0.000278 
l0: 0.000172, l1: 0.000168, l2: 0.000198, l3: 0.000233, l4: 0.000353, l5: 0.000495, l6: 0.001136
[epoch: 313/1000, batch:   896/ 1052, ite: 82280] train loss: 0.005094, tar: 0.000278 
l0: 0.000241, l1: 0.000239, l2: 0.000297, l3: 0.000356, l4: 0.000989, l5: 0.001231, l6: 0.002222
[epoch: 313/1000, batch:   976/ 1052, ite: 82300] train loss: 0.005094, tar: 0.000278 
[Epoch 313/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000155, l1: 0.000170, l2: 0.000201, l3: 0.000238, l4: 0.000550, l5: 0.001067, l6: 0.001817
[epoch: 314/1000, batch:     4/ 1052, ite: 82320] train loss: 0.005094, tar: 0.000278 
l0: 0.000330, l1: 0.000330, l2: 0.000372, l3: 0.000398, l4: 0.000722, l5: 0.001496, l6: 0.002062
[epoch: 314/1000, batch:    84/ 1052, ite: 82340] train loss: 0.005095, tar: 0.000278 
l0: 0.000251, l1: 0.000255, l2: 0.000267, l3: 0.000283, l4: 0.000637, l5: 0.001483, l6: 0.002733
[epoch: 314/1000, batch:   164/ 1052, ite: 82360] train loss: 0.005093, tar: 0.000278 
l0: 0.000252, l1: 0.000250, l2: 0.000280, l3: 0.000325, l4: 0.000548, l5: 0.001043, l6: 0.002827
[epoch: 314/1000, batch:   244/ 1052, ite: 82380] train loss: 0.005093, tar: 0.000277 
l0: 0.000312, l1: 0.000323, l2: 0.000320, l3: 0.000281, l4: 0.000425, l5: 0.000779, l6: 0.001008
[epoch: 314/1000, batch:   324/ 1052, ite: 82400] train loss: 0.005092, tar: 0.000277 
l0: 0.000344, l1: 0.000347, l2: 0.000402, l3: 0.000471, l4: 0.000682, l5: 0.001441, l6: 0.002252
[epoch: 314/1000, batch:   404/ 1052, ite: 82420] train loss: 0.005092, tar: 0.000277 
l0: 0.000065, l1: 0.000064, l2: 0.000094, l3: 0.000118, l4: 0.000236, l5: 0.000499, l6: 0.000897
[epoch: 314/1000, batch:   484/ 1052, ite: 82440] train loss: 0.005091, tar: 0.000277 
l0: 0.000245, l1: 0.000240, l2: 0.000325, l3: 0.000415, l4: 0.000652, l5: 0.001683, l6: 0.002099
[epoch: 314/1000, batch:   564/ 1052, ite: 82460] train loss: 0.005091, tar: 0.000277 
l0: 0.000193, l1: 0.000200, l2: 0.000227, l3: 0.000255, l4: 0.000429, l5: 0.000630, l6: 0.001775
[epoch: 314/1000, batch:   644/ 1052, ite: 82480] train loss: 0.005090, tar: 0.000277 
l0: 0.000245, l1: 0.000256, l2: 0.000266, l3: 0.000352, l4: 0.000467, l5: 0.000993, l6: 0.002639
[epoch: 314/1000, batch:   724/ 1052, ite: 82500] train loss: 0.005090, tar: 0.000277 
l0: 0.000144, l1: 0.000142, l2: 0.000179, l3: 0.000284, l4: 0.000651, l5: 0.001031, l6: 0.001358
[epoch: 314/1000, batch:   804/ 1052, ite: 82520] train loss: 0.005089, tar: 0.000277 
l0: 0.000220, l1: 0.000208, l2: 0.000226, l3: 0.000333, l4: 0.000534, l5: 0.000752, l6: 0.001595
[epoch: 314/1000, batch:   884/ 1052, ite: 82540] train loss: 0.005090, tar: 0.000277 
l0: 0.000134, l1: 0.000132, l2: 0.000156, l3: 0.000257, l4: 0.000432, l5: 0.001051, l6: 0.001650
[epoch: 314/1000, batch:   964/ 1052, ite: 82560] train loss: 0.005088, tar: 0.000277 
l0: 0.000250, l1: 0.000273, l2: 0.000251, l3: 0.000277, l4: 0.000595, l5: 0.001194, l6: 0.002101
[epoch: 314/1000, batch:  1044/ 1052, ite: 82580] train loss: 0.005087, tar: 0.000277 
[Epoch 314/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000345, l1: 0.000341, l2: 0.000373, l3: 0.000445, l4: 0.000853, l5: 0.001085, l6: 0.002602
[epoch: 315/1000, batch:    72/ 1052, ite: 82600] train loss: 0.005088, tar: 0.000277 
l0: 0.000213, l1: 0.000216, l2: 0.000257, l3: 0.000271, l4: 0.000491, l5: 0.000727, l6: 0.001694
[epoch: 315/1000, batch:   152/ 1052, ite: 82620] train loss: 0.005089, tar: 0.000277 
l0: 0.000262, l1: 0.000273, l2: 0.000305, l3: 0.000355, l4: 0.000679, l5: 0.000916, l6: 0.002341
[epoch: 315/1000, batch:   232/ 1052, ite: 82640] train loss: 0.005087, tar: 0.000276 
l0: 0.000108, l1: 0.000111, l2: 0.000106, l3: 0.000141, l4: 0.000365, l5: 0.000759, l6: 0.001292
[epoch: 315/1000, batch:   312/ 1052, ite: 82660] train loss: 0.005085, tar: 0.000276 
l0: 0.000379, l1: 0.000382, l2: 0.000437, l3: 0.000476, l4: 0.000860, l5: 0.001271, l6: 0.002518
[epoch: 315/1000, batch:   392/ 1052, ite: 82680] train loss: 0.005085, tar: 0.000276 
l0: 0.000388, l1: 0.000392, l2: 0.000462, l3: 0.000519, l4: 0.001065, l5: 0.001608, l6: 0.003507
[epoch: 315/1000, batch:   472/ 1052, ite: 82700] train loss: 0.005085, tar: 0.000276 
l0: 0.000244, l1: 0.000235, l2: 0.000255, l3: 0.000482, l4: 0.001010, l5: 0.001401, l6: 0.002808
[epoch: 315/1000, batch:   552/ 1052, ite: 82720] train loss: 0.005084, tar: 0.000276 
l0: 0.000274, l1: 0.000275, l2: 0.000310, l3: 0.000339, l4: 0.000621, l5: 0.000987, l6: 0.001411
[epoch: 315/1000, batch:   632/ 1052, ite: 82740] train loss: 0.005082, tar: 0.000276 
l0: 0.000159, l1: 0.000156, l2: 0.000171, l3: 0.000210, l4: 0.000399, l5: 0.000780, l6: 0.001166
[epoch: 315/1000, batch:   712/ 1052, ite: 82760] train loss: 0.005083, tar: 0.000276 
l0: 0.000119, l1: 0.000119, l2: 0.000142, l3: 0.000185, l4: 0.000330, l5: 0.000911, l6: 0.001204
[epoch: 315/1000, batch:   792/ 1052, ite: 82780] train loss: 0.005083, tar: 0.000276 
l0: 0.000230, l1: 0.000247, l2: 0.000230, l3: 0.000257, l4: 0.000513, l5: 0.001028, l6: 0.001825
[epoch: 315/1000, batch:   872/ 1052, ite: 82800] train loss: 0.005082, tar: 0.000276 
l0: 0.000199, l1: 0.000195, l2: 0.000230, l3: 0.000311, l4: 0.000760, l5: 0.001594, l6: 0.002086
[epoch: 315/1000, batch:   952/ 1052, ite: 82820] train loss: 0.005083, tar: 0.000276 
l0: 0.000193, l1: 0.000201, l2: 0.000212, l3: 0.000267, l4: 0.000576, l5: 0.001127, l6: 0.001914
[epoch: 315/1000, batch:  1032/ 1052, ite: 82840] train loss: 0.005082, tar: 0.000276 
[Epoch 315/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000140, l1: 0.000135, l2: 0.000191, l3: 0.000260, l4: 0.000431, l5: 0.000886, l6: 0.001730
[epoch: 316/1000, batch:    60/ 1052, ite: 82860] train loss: 0.005082, tar: 0.000276 
l0: 0.000216, l1: 0.000234, l2: 0.000239, l3: 0.000308, l4: 0.000698, l5: 0.001693, l6: 0.002914
[epoch: 316/1000, batch:   140/ 1052, ite: 82880] train loss: 0.005083, tar: 0.000276 
l0: 0.000151, l1: 0.000150, l2: 0.000187, l3: 0.000284, l4: 0.000464, l5: 0.001026, l6: 0.001460
[epoch: 316/1000, batch:   220/ 1052, ite: 82900] train loss: 0.005082, tar: 0.000275 
l0: 0.000113, l1: 0.000117, l2: 0.000116, l3: 0.000206, l4: 0.000378, l5: 0.000752, l6: 0.000959
[epoch: 316/1000, batch:   300/ 1052, ite: 82920] train loss: 0.005081, tar: 0.000275 
l0: 0.000168, l1: 0.000169, l2: 0.000196, l3: 0.000352, l4: 0.000635, l5: 0.001283, l6: 0.001851
[epoch: 316/1000, batch:   380/ 1052, ite: 82940] train loss: 0.005082, tar: 0.000275 
l0: 0.000424, l1: 0.000437, l2: 0.000430, l3: 0.000439, l4: 0.000571, l5: 0.001068, l6: 0.002507
[epoch: 316/1000, batch:   460/ 1052, ite: 82960] train loss: 0.005081, tar: 0.000275 
l0: 0.000388, l1: 0.000420, l2: 0.000443, l3: 0.000496, l4: 0.000788, l5: 0.001495, l6: 0.002788
[epoch: 316/1000, batch:   540/ 1052, ite: 82980] train loss: 0.005082, tar: 0.000275 
l0: 0.000105, l1: 0.000110, l2: 0.000129, l3: 0.000210, l4: 0.000336, l5: 0.000938, l6: 0.001542
[epoch: 316/1000, batch:   620/ 1052, ite: 83000] train loss: 0.005083, tar: 0.000275 
l0: 0.000309, l1: 0.000314, l2: 0.000330, l3: 0.000468, l4: 0.000688, l5: 0.001121, l6: 0.002145
[epoch: 316/1000, batch:   700/ 1052, ite: 83020] train loss: 0.005082, tar: 0.000275 
l0: 0.000153, l1: 0.000152, l2: 0.000192, l3: 0.000291, l4: 0.000549, l5: 0.000954, l6: 0.001758
[epoch: 316/1000, batch:   780/ 1052, ite: 83040] train loss: 0.005080, tar: 0.000275 
l0: 0.000212, l1: 0.000206, l2: 0.000268, l3: 0.000350, l4: 0.000673, l5: 0.001222, l6: 0.002298
[epoch: 316/1000, batch:   860/ 1052, ite: 83060] train loss: 0.005081, tar: 0.000275 
l0: 0.000235, l1: 0.000232, l2: 0.000303, l3: 0.000351, l4: 0.000490, l5: 0.001088, l6: 0.002243
[epoch: 316/1000, batch:   940/ 1052, ite: 83080] train loss: 0.005080, tar: 0.000275 
l0: 0.000166, l1: 0.000171, l2: 0.000199, l3: 0.000254, l4: 0.000708, l5: 0.001124, l6: 0.001686
[epoch: 316/1000, batch:  1020/ 1052, ite: 83100] train loss: 0.005079, tar: 0.000275 
[Epoch 316/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000401, l1: 0.000414, l2: 0.000486, l3: 0.000585, l4: 0.000790, l5: 0.001862, l6: 0.003897
[epoch: 317/1000, batch:    48/ 1052, ite: 83120] train loss: 0.005079, tar: 0.000275 
l0: 0.000162, l1: 0.000168, l2: 0.000167, l3: 0.000275, l4: 0.000357, l5: 0.000730, l6: 0.001095
[epoch: 317/1000, batch:   128/ 1052, ite: 83140] train loss: 0.005078, tar: 0.000275 
l0: 0.000129, l1: 0.000139, l2: 0.000168, l3: 0.000189, l4: 0.000505, l5: 0.001140, l6: 0.001757
[epoch: 317/1000, batch:   208/ 1052, ite: 83160] train loss: 0.005077, tar: 0.000274 
l0: 0.000222, l1: 0.000216, l2: 0.000280, l3: 0.000511, l4: 0.000750, l5: 0.001463, l6: 0.002372
[epoch: 317/1000, batch:   288/ 1052, ite: 83180] train loss: 0.005077, tar: 0.000274 
l0: 0.000339, l1: 0.000344, l2: 0.000430, l3: 0.000404, l4: 0.000967, l5: 0.002008, l6: 0.003287
[epoch: 317/1000, batch:   368/ 1052, ite: 83200] train loss: 0.005077, tar: 0.000274 
l0: 0.000205, l1: 0.000203, l2: 0.000248, l3: 0.000288, l4: 0.000414, l5: 0.001026, l6: 0.001940
[epoch: 317/1000, batch:   448/ 1052, ite: 83220] train loss: 0.005077, tar: 0.000274 
l0: 0.000231, l1: 0.000230, l2: 0.000240, l3: 0.000329, l4: 0.000601, l5: 0.001229, l6: 0.001666
[epoch: 317/1000, batch:   528/ 1052, ite: 83240] train loss: 0.005078, tar: 0.000274 
l0: 0.000117, l1: 0.000109, l2: 0.000134, l3: 0.000189, l4: 0.000409, l5: 0.000625, l6: 0.001003
[epoch: 317/1000, batch:   608/ 1052, ite: 83260] train loss: 0.005076, tar: 0.000274 
l0: 0.000493, l1: 0.000515, l2: 0.000555, l3: 0.000580, l4: 0.000828, l5: 0.002172, l6: 0.003548
[epoch: 317/1000, batch:   688/ 1052, ite: 83280] train loss: 0.005075, tar: 0.000274 
l0: 0.000154, l1: 0.000146, l2: 0.000180, l3: 0.000347, l4: 0.000608, l5: 0.001268, l6: 0.002423
[epoch: 317/1000, batch:   768/ 1052, ite: 83300] train loss: 0.005075, tar: 0.000274 
l0: 0.000205, l1: 0.000209, l2: 0.000229, l3: 0.000323, l4: 0.000550, l5: 0.000659, l6: 0.001138
[epoch: 317/1000, batch:   848/ 1052, ite: 83320] train loss: 0.005074, tar: 0.000274 
l0: 0.000255, l1: 0.000253, l2: 0.000294, l3: 0.000411, l4: 0.000706, l5: 0.001594, l6: 0.003245
[epoch: 317/1000, batch:   928/ 1052, ite: 83340] train loss: 0.005074, tar: 0.000274 
l0: 0.000179, l1: 0.000187, l2: 0.000189, l3: 0.000193, l4: 0.000483, l5: 0.000866, l6: 0.001619
[epoch: 317/1000, batch:  1008/ 1052, ite: 83360] train loss: 0.005072, tar: 0.000274 
[Epoch 317/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000141, l1: 0.000138, l2: 0.000172, l3: 0.000228, l4: 0.000432, l5: 0.000567, l6: 0.001163
[epoch: 318/1000, batch:    36/ 1052, ite: 83380] train loss: 0.005072, tar: 0.000274 
l0: 0.000786, l1: 0.000772, l2: 0.000917, l3: 0.000953, l4: 0.001024, l5: 0.001893, l6: 0.001923
[epoch: 318/1000, batch:   116/ 1052, ite: 83400] train loss: 0.005070, tar: 0.000273 
l0: 0.000106, l1: 0.000104, l2: 0.000133, l3: 0.000159, l4: 0.000324, l5: 0.000716, l6: 0.000671
[epoch: 318/1000, batch:   196/ 1052, ite: 83420] train loss: 0.005070, tar: 0.000273 
l0: 0.000325, l1: 0.000331, l2: 0.000354, l3: 0.000410, l4: 0.000617, l5: 0.001167, l6: 0.002235
[epoch: 318/1000, batch:   276/ 1052, ite: 83440] train loss: 0.005069, tar: 0.000273 
l0: 0.000280, l1: 0.000295, l2: 0.000266, l3: 0.000311, l4: 0.000592, l5: 0.001228, l6: 0.001804
[epoch: 318/1000, batch:   356/ 1052, ite: 83460] train loss: 0.005069, tar: 0.000273 
l0: 0.000486, l1: 0.000480, l2: 0.000498, l3: 0.000570, l4: 0.000655, l5: 0.001089, l6: 0.002415
[epoch: 318/1000, batch:   436/ 1052, ite: 83480] train loss: 0.005069, tar: 0.000273 
l0: 0.000279, l1: 0.000279, l2: 0.000360, l3: 0.000363, l4: 0.000572, l5: 0.001149, l6: 0.002323
[epoch: 318/1000, batch:   516/ 1052, ite: 83500] train loss: 0.005068, tar: 0.000273 
l0: 0.000179, l1: 0.000187, l2: 0.000182, l3: 0.000225, l4: 0.000516, l5: 0.000978, l6: 0.001028
[epoch: 318/1000, batch:   596/ 1052, ite: 83520] train loss: 0.005068, tar: 0.000273 
l0: 0.000099, l1: 0.000098, l2: 0.000134, l3: 0.000176, l4: 0.000370, l5: 0.000731, l6: 0.001472
[epoch: 318/1000, batch:   676/ 1052, ite: 83540] train loss: 0.005070, tar: 0.000273 
l0: 0.000443, l1: 0.000451, l2: 0.000515, l3: 0.000544, l4: 0.000790, l5: 0.001168, l6: 0.002956
[epoch: 318/1000, batch:   756/ 1052, ite: 83560] train loss: 0.005070, tar: 0.000273 
l0: 0.000159, l1: 0.000154, l2: 0.000212, l3: 0.000315, l4: 0.000486, l5: 0.001065, l6: 0.001722
[epoch: 318/1000, batch:   836/ 1052, ite: 83580] train loss: 0.005069, tar: 0.000273 
l0: 0.000109, l1: 0.000111, l2: 0.000160, l3: 0.000156, l4: 0.000374, l5: 0.000676, l6: 0.001286
[epoch: 318/1000, batch:   916/ 1052, ite: 83600] train loss: 0.005068, tar: 0.000273 
l0: 0.000239, l1: 0.000243, l2: 0.000273, l3: 0.000423, l4: 0.001114, l5: 0.001558, l6: 0.002257
[epoch: 318/1000, batch:   996/ 1052, ite: 83620] train loss: 0.005067, tar: 0.000273 
[Epoch 318/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000116, l1: 0.000115, l2: 0.000153, l3: 0.000176, l4: 0.000311, l5: 0.000429, l6: 0.001332
[epoch: 319/1000, batch:    24/ 1052, ite: 83640] train loss: 0.005068, tar: 0.000273 
l0: 0.000175, l1: 0.000192, l2: 0.000171, l3: 0.000198, l4: 0.000416, l5: 0.000923, l6: 0.001320
[epoch: 319/1000, batch:   104/ 1052, ite: 83660] train loss: 0.005066, tar: 0.000273 
l0: 0.000243, l1: 0.000245, l2: 0.000313, l3: 0.000464, l4: 0.000774, l5: 0.001727, l6: 0.001909
[epoch: 319/1000, batch:   184/ 1052, ite: 83680] train loss: 0.005067, tar: 0.000272 
l0: 0.000257, l1: 0.000259, l2: 0.000278, l3: 0.000305, l4: 0.000626, l5: 0.001170, l6: 0.001848
[epoch: 319/1000, batch:   264/ 1052, ite: 83700] train loss: 0.005066, tar: 0.000272 
l0: 0.000168, l1: 0.000172, l2: 0.000194, l3: 0.000269, l4: 0.000580, l5: 0.000769, l6: 0.002251
[epoch: 319/1000, batch:   344/ 1052, ite: 83720] train loss: 0.005066, tar: 0.000272 
l0: 0.000219, l1: 0.000205, l2: 0.000271, l3: 0.000590, l4: 0.001197, l5: 0.001355, l6: 0.002269
[epoch: 319/1000, batch:   424/ 1052, ite: 83740] train loss: 0.005066, tar: 0.000272 
l0: 0.000131, l1: 0.000133, l2: 0.000150, l3: 0.000166, l4: 0.000280, l5: 0.000689, l6: 0.001178
[epoch: 319/1000, batch:   504/ 1052, ite: 83760] train loss: 0.005066, tar: 0.000272 
l0: 0.000298, l1: 0.000322, l2: 0.000308, l3: 0.000340, l4: 0.000605, l5: 0.001329, l6: 0.002731
[epoch: 319/1000, batch:   584/ 1052, ite: 83780] train loss: 0.005065, tar: 0.000272 
l0: 0.000161, l1: 0.000172, l2: 0.000175, l3: 0.000203, l4: 0.000576, l5: 0.000654, l6: 0.001742
[epoch: 319/1000, batch:   664/ 1052, ite: 83800] train loss: 0.005064, tar: 0.000272 
l0: 0.000284, l1: 0.000289, l2: 0.000312, l3: 0.000350, l4: 0.000574, l5: 0.001213, l6: 0.002470
[epoch: 319/1000, batch:   744/ 1052, ite: 83820] train loss: 0.005064, tar: 0.000272 
l0: 0.000220, l1: 0.000219, l2: 0.000259, l3: 0.000392, l4: 0.000749, l5: 0.001851, l6: 0.001879
[epoch: 319/1000, batch:   824/ 1052, ite: 83840] train loss: 0.005065, tar: 0.000272 
l0: 0.000308, l1: 0.000329, l2: 0.000333, l3: 0.000398, l4: 0.000701, l5: 0.001121, l6: 0.002652
[epoch: 319/1000, batch:   904/ 1052, ite: 83860] train loss: 0.005064, tar: 0.000272 
l0: 0.000172, l1: 0.000173, l2: 0.000182, l3: 0.000229, l4: 0.000388, l5: 0.000673, l6: 0.001153
[epoch: 319/1000, batch:   984/ 1052, ite: 83880] train loss: 0.005062, tar: 0.000272 
[Epoch 319/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000117, l1: 0.000116, l2: 0.000120, l3: 0.000168, l4: 0.000243, l5: 0.000541, l6: 0.000990
[epoch: 320/1000, batch:    12/ 1052, ite: 83900] train loss: 0.005062, tar: 0.000272 
l0: 0.000384, l1: 0.000412, l2: 0.000393, l3: 0.000384, l4: 0.000650, l5: 0.001394, l6: 0.002051
[epoch: 320/1000, batch:    92/ 1052, ite: 83920] train loss: 0.005064, tar: 0.000272 
l0: 0.000252, l1: 0.000258, l2: 0.000283, l3: 0.000373, l4: 0.000727, l5: 0.001100, l6: 0.002178
[epoch: 320/1000, batch:   172/ 1052, ite: 83940] train loss: 0.005063, tar: 0.000272 
l0: 0.000279, l1: 0.000284, l2: 0.000368, l3: 0.000334, l4: 0.000692, l5: 0.001643, l6: 0.002318
[epoch: 320/1000, batch:   252/ 1052, ite: 83960] train loss: 0.005063, tar: 0.000272 
l0: 0.000240, l1: 0.000228, l2: 0.000288, l3: 0.000348, l4: 0.000577, l5: 0.000810, l6: 0.001762
[epoch: 320/1000, batch:   332/ 1052, ite: 83980] train loss: 0.005063, tar: 0.000272 
l0: 0.000123, l1: 0.000126, l2: 0.000160, l3: 0.000204, l4: 0.000422, l5: 0.000754, l6: 0.001555
[epoch: 320/1000, batch:   412/ 1052, ite: 84000] train loss: 0.005062, tar: 0.000271 
l0: 0.000091, l1: 0.000090, l2: 0.000122, l3: 0.000203, l4: 0.000380, l5: 0.000950, l6: 0.001585
[epoch: 320/1000, batch:   492/ 1052, ite: 84020] train loss: 0.005062, tar: 0.000271 
l0: 0.000189, l1: 0.000188, l2: 0.000224, l3: 0.000268, l4: 0.000391, l5: 0.000823, l6: 0.001793
[epoch: 320/1000, batch:   572/ 1052, ite: 84040] train loss: 0.005062, tar: 0.000271 
l0: 0.000321, l1: 0.000316, l2: 0.000359, l3: 0.000492, l4: 0.000693, l5: 0.001307, l6: 0.002230
[epoch: 320/1000, batch:   652/ 1052, ite: 84060] train loss: 0.005060, tar: 0.000271 
l0: 0.000168, l1: 0.000171, l2: 0.000194, l3: 0.000237, l4: 0.000441, l5: 0.000730, l6: 0.001302
[epoch: 320/1000, batch:   732/ 1052, ite: 84080] train loss: 0.005059, tar: 0.000271 
l0: 0.000135, l1: 0.000139, l2: 0.000156, l3: 0.000203, l4: 0.000364, l5: 0.000660, l6: 0.001576
[epoch: 320/1000, batch:   812/ 1052, ite: 84100] train loss: 0.005059, tar: 0.000271 
l0: 0.000139, l1: 0.000135, l2: 0.000182, l3: 0.000272, l4: 0.000550, l5: 0.001176, l6: 0.001976
[epoch: 320/1000, batch:   892/ 1052, ite: 84120] train loss: 0.005058, tar: 0.000271 
l0: 0.000213, l1: 0.000217, l2: 0.000249, l3: 0.000286, l4: 0.000464, l5: 0.001165, l6: 0.001819
[epoch: 320/1000, batch:   972/ 1052, ite: 84140] train loss: 0.005058, tar: 0.000271 
l0: 0.000441, l1: 0.000460, l2: 0.000460, l3: 0.000520, l4: 0.000670, l5: 0.001148, l6: 0.003538
[epoch: 320/1000, batch:  1052/ 1052, ite: 84160] train loss: 0.005057, tar: 0.000271 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_84160.pkl
[Epoch 320/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000136, l1: 0.000138, l2: 0.000164, l3: 0.000254, l4: 0.000379, l5: 0.000820, l6: 0.002182
[epoch: 321/1000, batch:    80/ 1052, ite: 84180] train loss: 0.004888, tar: 0.000239 
l0: 0.000210, l1: 0.000199, l2: 0.000196, l3: 0.000294, l4: 0.000389, l5: 0.000682, l6: 0.001292
[epoch: 321/1000, batch:   160/ 1052, ite: 84200] train loss: 0.004843, tar: 0.000234 
l0: 0.000366, l1: 0.000368, l2: 0.000407, l3: 0.000557, l4: 0.001041, l5: 0.001557, l6: 0.002592
[epoch: 321/1000, batch:   240/ 1052, ite: 84220] train loss: 0.005008, tar: 0.000242 
l0: 0.000225, l1: 0.000231, l2: 0.000264, l3: 0.000309, l4: 0.000547, l5: 0.001186, l6: 0.003128
[epoch: 321/1000, batch:   320/ 1052, ite: 84240] train loss: 0.005108, tar: 0.000242 
l0: 0.000314, l1: 0.000317, l2: 0.000371, l3: 0.000389, l4: 0.000668, l5: 0.000907, l6: 0.001783
[epoch: 321/1000, batch:   400/ 1052, ite: 84260] train loss: 0.004991, tar: 0.000240 
l0: 0.000108, l1: 0.000111, l2: 0.000140, l3: 0.000171, l4: 0.000328, l5: 0.000927, l6: 0.001619
[epoch: 321/1000, batch:   480/ 1052, ite: 84280] train loss: 0.004907, tar: 0.000237 
l0: 0.000436, l1: 0.000447, l2: 0.000467, l3: 0.000557, l4: 0.000666, l5: 0.001922, l6: 0.003330
[epoch: 321/1000, batch:   560/ 1052, ite: 84300] train loss: 0.004888, tar: 0.000235 
l0: 0.000183, l1: 0.000187, l2: 0.000210, l3: 0.000224, l4: 0.000291, l5: 0.000738, l6: 0.000880
[epoch: 321/1000, batch:   640/ 1052, ite: 84320] train loss: 0.004938, tar: 0.000237 
l0: 0.000523, l1: 0.000516, l2: 0.000510, l3: 0.000740, l4: 0.000898, l5: 0.001350, l6: 0.002938
[epoch: 321/1000, batch:   720/ 1052, ite: 84340] train loss: 0.004910, tar: 0.000236 
l0: 0.000162, l1: 0.000164, l2: 0.000232, l3: 0.000297, l4: 0.000824, l5: 0.001377, l6: 0.001905
[epoch: 321/1000, batch:   800/ 1052, ite: 84360] train loss: 0.004868, tar: 0.000233 
l0: 0.000164, l1: 0.000167, l2: 0.000196, l3: 0.000240, l4: 0.000298, l5: 0.000579, l6: 0.001609
[epoch: 321/1000, batch:   880/ 1052, ite: 84380] train loss: 0.004909, tar: 0.000236 
l0: 0.000196, l1: 0.000204, l2: 0.000249, l3: 0.000331, l4: 0.000679, l5: 0.001313, l6: 0.001235
[epoch: 321/1000, batch:   960/ 1052, ite: 84400] train loss: 0.004864, tar: 0.000234 
l0: 0.000282, l1: 0.000292, l2: 0.000336, l3: 0.000379, l4: 0.000703, l5: 0.001084, l6: 0.002733
[epoch: 321/1000, batch:  1040/ 1052, ite: 84420] train loss: 0.004858, tar: 0.000235 
[Epoch 321/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000307, l1: 0.000310, l2: 0.000327, l3: 0.000417, l4: 0.000825, l5: 0.001439, l6: 0.002912
[epoch: 322/1000, batch:    68/ 1052, ite: 84440] train loss: 0.004840, tar: 0.000234 
l0: 0.000170, l1: 0.000170, l2: 0.000218, l3: 0.000378, l4: 0.000804, l5: 0.001584, l6: 0.001843
[epoch: 322/1000, batch:   148/ 1052, ite: 84460] train loss: 0.004880, tar: 0.000236 
l0: 0.000181, l1: 0.000184, l2: 0.000198, l3: 0.000241, l4: 0.000464, l5: 0.001050, l6: 0.001221
[epoch: 322/1000, batch:   228/ 1052, ite: 84480] train loss: 0.004882, tar: 0.000236 
l0: 0.000160, l1: 0.000169, l2: 0.000167, l3: 0.000195, l4: 0.000541, l5: 0.000659, l6: 0.001450
[epoch: 322/1000, batch:   308/ 1052, ite: 84500] train loss: 0.004874, tar: 0.000235 
l0: 0.000296, l1: 0.000316, l2: 0.000283, l3: 0.000335, l4: 0.000641, l5: 0.001228, l6: 0.002654
[epoch: 322/1000, batch:   388/ 1052, ite: 84520] train loss: 0.004875, tar: 0.000236 
l0: 0.000156, l1: 0.000159, l2: 0.000175, l3: 0.000208, l4: 0.000313, l5: 0.000523, l6: 0.001029
[epoch: 322/1000, batch:   468/ 1052, ite: 84540] train loss: 0.004850, tar: 0.000234 
l0: 0.000151, l1: 0.000161, l2: 0.000183, l3: 0.000229, l4: 0.000558, l5: 0.001398, l6: 0.001364
[epoch: 322/1000, batch:   548/ 1052, ite: 84560] train loss: 0.004867, tar: 0.000235 
l0: 0.000170, l1: 0.000178, l2: 0.000196, l3: 0.000215, l4: 0.000342, l5: 0.000637, l6: 0.001984
[epoch: 322/1000, batch:   628/ 1052, ite: 84580] train loss: 0.004838, tar: 0.000233 
l0: 0.000303, l1: 0.000298, l2: 0.000379, l3: 0.000414, l4: 0.000810, l5: 0.001809, l6: 0.002079
[epoch: 322/1000, batch:   708/ 1052, ite: 84600] train loss: 0.004842, tar: 0.000232 
l0: 0.000356, l1: 0.000354, l2: 0.000348, l3: 0.000526, l4: 0.000683, l5: 0.001585, l6: 0.002858
[epoch: 322/1000, batch:   788/ 1052, ite: 84620] train loss: 0.004841, tar: 0.000232 
l0: 0.000137, l1: 0.000143, l2: 0.000138, l3: 0.000176, l4: 0.000350, l5: 0.000553, l6: 0.001596
[epoch: 322/1000, batch:   868/ 1052, ite: 84640] train loss: 0.004830, tar: 0.000232 
l0: 0.000147, l1: 0.000148, l2: 0.000168, l3: 0.000220, l4: 0.000510, l5: 0.000599, l6: 0.001442
[epoch: 322/1000, batch:   948/ 1052, ite: 84660] train loss: 0.004827, tar: 0.000231 
l0: 0.000090, l1: 0.000098, l2: 0.000097, l3: 0.000135, l4: 0.000406, l5: 0.000747, l6: 0.000820
[epoch: 322/1000, batch:  1028/ 1052, ite: 84680] train loss: 0.004826, tar: 0.000231 
[Epoch 322/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000320, l1: 0.000316, l2: 0.000360, l3: 0.000612, l4: 0.001185, l5: 0.001499, l6: 0.002630
[epoch: 323/1000, batch:    56/ 1052, ite: 84700] train loss: 0.004836, tar: 0.000231 
l0: 0.000211, l1: 0.000206, l2: 0.000224, l3: 0.000354, l4: 0.000585, l5: 0.001167, l6: 0.002266
[epoch: 323/1000, batch:   136/ 1052, ite: 84720] train loss: 0.004827, tar: 0.000231 
l0: 0.000397, l1: 0.000407, l2: 0.000468, l3: 0.000493, l4: 0.000782, l5: 0.001645, l6: 0.003249
[epoch: 323/1000, batch:   216/ 1052, ite: 84740] train loss: 0.004844, tar: 0.000232 
l0: 0.000127, l1: 0.000125, l2: 0.000153, l3: 0.000215, l4: 0.000455, l5: 0.000836, l6: 0.001361
[epoch: 323/1000, batch:   296/ 1052, ite: 84760] train loss: 0.004847, tar: 0.000232 
l0: 0.000170, l1: 0.000177, l2: 0.000183, l3: 0.000267, l4: 0.000417, l5: 0.001011, l6: 0.001371
[epoch: 323/1000, batch:   376/ 1052, ite: 84780] train loss: 0.004850, tar: 0.000232 
l0: 0.000214, l1: 0.000223, l2: 0.000242, l3: 0.000307, l4: 0.000567, l5: 0.001206, l6: 0.001656
[epoch: 323/1000, batch:   456/ 1052, ite: 84800] train loss: 0.004847, tar: 0.000232 
l0: 0.000201, l1: 0.000204, l2: 0.000245, l3: 0.000305, l4: 0.000413, l5: 0.000971, l6: 0.001536
[epoch: 323/1000, batch:   536/ 1052, ite: 84820] train loss: 0.004847, tar: 0.000232 
l0: 0.000170, l1: 0.000171, l2: 0.000228, l3: 0.000253, l4: 0.000476, l5: 0.000985, l6: 0.001743
[epoch: 323/1000, batch:   616/ 1052, ite: 84840] train loss: 0.004835, tar: 0.000232 
l0: 0.000160, l1: 0.000170, l2: 0.000170, l3: 0.000217, l4: 0.000387, l5: 0.000827, l6: 0.000997
[epoch: 323/1000, batch:   696/ 1052, ite: 84860] train loss: 0.004830, tar: 0.000232 
l0: 0.000234, l1: 0.000237, l2: 0.000291, l3: 0.000323, l4: 0.000445, l5: 0.000816, l6: 0.001781
[epoch: 323/1000, batch:   776/ 1052, ite: 84880] train loss: 0.004841, tar: 0.000232 
l0: 0.000404, l1: 0.000414, l2: 0.000479, l3: 0.000535, l4: 0.000743, l5: 0.001389, l6: 0.003072
[epoch: 323/1000, batch:   856/ 1052, ite: 84900] train loss: 0.004849, tar: 0.000233 
l0: 0.000397, l1: 0.000416, l2: 0.000423, l3: 0.000434, l4: 0.000605, l5: 0.000958, l6: 0.001582
[epoch: 323/1000, batch:   936/ 1052, ite: 84920] train loss: 0.004861, tar: 0.000234 
l0: 0.000160, l1: 0.000152, l2: 0.000206, l3: 0.000236, l4: 0.000367, l5: 0.000746, l6: 0.002046
[epoch: 323/1000, batch:  1016/ 1052, ite: 84940] train loss: 0.004859, tar: 0.000234 
[Epoch 323/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000119, l1: 0.000116, l2: 0.000139, l3: 0.000223, l4: 0.000481, l5: 0.001238, l6: 0.001916
[epoch: 324/1000, batch:    44/ 1052, ite: 84960] train loss: 0.004851, tar: 0.000233 
l0: 0.000384, l1: 0.000386, l2: 0.000350, l3: 0.000515, l4: 0.000909, l5: 0.001477, l6: 0.002524
[epoch: 324/1000, batch:   124/ 1052, ite: 84980] train loss: 0.004846, tar: 0.000233 
l0: 0.000251, l1: 0.000241, l2: 0.000277, l3: 0.000516, l4: 0.000754, l5: 0.001562, l6: 0.002799
[epoch: 324/1000, batch:   204/ 1052, ite: 85000] train loss: 0.004845, tar: 0.000233 
l0: 0.000160, l1: 0.000164, l2: 0.000174, l3: 0.000203, l4: 0.000433, l5: 0.000933, l6: 0.001281
[epoch: 324/1000, batch:   284/ 1052, ite: 85020] train loss: 0.004856, tar: 0.000233 
l0: 0.000289, l1: 0.000300, l2: 0.000319, l3: 0.000403, l4: 0.001060, l5: 0.001884, l6: 0.003054
[epoch: 324/1000, batch:   364/ 1052, ite: 85040] train loss: 0.004872, tar: 0.000234 
l0: 0.000162, l1: 0.000162, l2: 0.000177, l3: 0.000224, l4: 0.000481, l5: 0.000955, l6: 0.001557
[epoch: 324/1000, batch:   444/ 1052, ite: 85060] train loss: 0.004864, tar: 0.000234 
l0: 0.000547, l1: 0.000577, l2: 0.000990, l3: 0.000611, l4: 0.001213, l5: 0.002114, l6: 0.003410
[epoch: 324/1000, batch:   524/ 1052, ite: 85080] train loss: 0.004860, tar: 0.000235 
l0: 0.000219, l1: 0.000241, l2: 0.000271, l3: 0.000282, l4: 0.000544, l5: 0.001067, l6: 0.001953
[epoch: 324/1000, batch:   604/ 1052, ite: 85100] train loss: 0.004863, tar: 0.000234 
l0: 0.000182, l1: 0.000190, l2: 0.000207, l3: 0.000235, l4: 0.000335, l5: 0.001198, l6: 0.002339
[epoch: 324/1000, batch:   684/ 1052, ite: 85120] train loss: 0.004851, tar: 0.000234 
l0: 0.000307, l1: 0.000299, l2: 0.000363, l3: 0.000491, l4: 0.000710, l5: 0.001370, l6: 0.002431
[epoch: 324/1000, batch:   764/ 1052, ite: 85140] train loss: 0.004859, tar: 0.000235 
l0: 0.000132, l1: 0.000129, l2: 0.000195, l3: 0.000203, l4: 0.000341, l5: 0.000834, l6: 0.001369
[epoch: 324/1000, batch:   844/ 1052, ite: 85160] train loss: 0.004858, tar: 0.000236 
l0: 0.000154, l1: 0.000148, l2: 0.000191, l3: 0.000243, l4: 0.000545, l5: 0.001242, l6: 0.001315
[epoch: 324/1000, batch:   924/ 1052, ite: 85180] train loss: 0.004866, tar: 0.000236 
l0: 0.000142, l1: 0.000154, l2: 0.000152, l3: 0.000186, l4: 0.000402, l5: 0.000644, l6: 0.001660
[epoch: 324/1000, batch:  1004/ 1052, ite: 85200] train loss: 0.004863, tar: 0.000236 
[Epoch 324/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000223, l1: 0.000227, l2: 0.000246, l3: 0.000273, l4: 0.000446, l5: 0.000848, l6: 0.002054
[epoch: 325/1000, batch:    32/ 1052, ite: 85220] train loss: 0.004869, tar: 0.000237 
l0: 0.000247, l1: 0.000258, l2: 0.000254, l3: 0.000248, l4: 0.000381, l5: 0.001085, l6: 0.001565
[epoch: 325/1000, batch:   112/ 1052, ite: 85240] train loss: 0.004877, tar: 0.000237 
l0: 0.000190, l1: 0.000183, l2: 0.000226, l3: 0.000313, l4: 0.000612, l5: 0.000883, l6: 0.001782
[epoch: 325/1000, batch:   192/ 1052, ite: 85260] train loss: 0.004869, tar: 0.000236 
l0: 0.000166, l1: 0.000170, l2: 0.000206, l3: 0.000297, l4: 0.000377, l5: 0.000963, l6: 0.001651
[epoch: 325/1000, batch:   272/ 1052, ite: 85280] train loss: 0.004864, tar: 0.000236 
l0: 0.000223, l1: 0.000231, l2: 0.000241, l3: 0.000290, l4: 0.000473, l5: 0.000977, l6: 0.002050
[epoch: 325/1000, batch:   352/ 1052, ite: 85300] train loss: 0.004869, tar: 0.000237 
l0: 0.000232, l1: 0.000242, l2: 0.000241, l3: 0.000249, l4: 0.000619, l5: 0.000801, l6: 0.001659
[epoch: 325/1000, batch:   432/ 1052, ite: 85320] train loss: 0.004863, tar: 0.000237 
l0: 0.000223, l1: 0.000221, l2: 0.000262, l3: 0.000285, l4: 0.000562, l5: 0.000982, l6: 0.002151
[epoch: 325/1000, batch:   512/ 1052, ite: 85340] train loss: 0.004857, tar: 0.000236 
l0: 0.000112, l1: 0.000111, l2: 0.000146, l3: 0.000213, l4: 0.000378, l5: 0.000663, l6: 0.001222
[epoch: 325/1000, batch:   592/ 1052, ite: 85360] train loss: 0.004844, tar: 0.000236 
l0: 0.000375, l1: 0.000373, l2: 0.000398, l3: 0.000491, l4: 0.000851, l5: 0.001322, l6: 0.002264
[epoch: 325/1000, batch:   672/ 1052, ite: 85380] train loss: 0.004856, tar: 0.000237 
l0: 0.000126, l1: 0.000126, l2: 0.000160, l3: 0.000176, l4: 0.000455, l5: 0.000696, l6: 0.000915
[epoch: 325/1000, batch:   752/ 1052, ite: 85400] train loss: 0.004861, tar: 0.000237 
l0: 0.000319, l1: 0.000332, l2: 0.000315, l3: 0.000405, l4: 0.000588, l5: 0.001600, l6: 0.002871
[epoch: 325/1000, batch:   832/ 1052, ite: 85420] train loss: 0.004866, tar: 0.000238 
l0: 0.000114, l1: 0.000114, l2: 0.000141, l3: 0.000201, l4: 0.000375, l5: 0.001045, l6: 0.001001
[epoch: 325/1000, batch:   912/ 1052, ite: 85440] train loss: 0.004873, tar: 0.000240 
l0: 0.000496, l1: 0.000519, l2: 0.000545, l3: 0.000625, l4: 0.000896, l5: 0.001393, l6: 0.004833
[epoch: 325/1000, batch:   992/ 1052, ite: 85460] train loss: 0.004899, tar: 0.000242 
[Epoch 325/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000404, l1: 0.000464, l2: 0.000473, l3: 0.000423, l4: 0.000647, l5: 0.001479, l6: 0.002137
[epoch: 326/1000, batch:    20/ 1052, ite: 85480] train loss: 0.004913, tar: 0.000244 
l0: 0.000506, l1: 0.000572, l2: 0.000498, l3: 0.000714, l4: 0.000630, l5: 0.001024, l6: 0.002115
[epoch: 326/1000, batch:   100/ 1052, ite: 85500] train loss: 0.004920, tar: 0.000245 
l0: 0.000348, l1: 0.000363, l2: 0.000373, l3: 0.000381, l4: 0.000552, l5: 0.001922, l6: 0.002598
[epoch: 326/1000, batch:   180/ 1052, ite: 85520] train loss: 0.004925, tar: 0.000247 
l0: 0.000224, l1: 0.000241, l2: 0.000248, l3: 0.000238, l4: 0.000405, l5: 0.000869, l6: 0.000990
[epoch: 326/1000, batch:   260/ 1052, ite: 85540] train loss: 0.004926, tar: 0.000247 
l0: 0.000340, l1: 0.000337, l2: 0.000377, l3: 0.000474, l4: 0.000671, l5: 0.001496, l6: 0.003350
[epoch: 326/1000, batch:   340/ 1052, ite: 85560] train loss: 0.004941, tar: 0.000248 
l0: 0.000362, l1: 0.000390, l2: 0.000386, l3: 0.000539, l4: 0.000899, l5: 0.001309, l6: 0.003496
[epoch: 326/1000, batch:   420/ 1052, ite: 85580] train loss: 0.004938, tar: 0.000249 
l0: 0.000363, l1: 0.000384, l2: 0.000433, l3: 0.000376, l4: 0.000525, l5: 0.001112, l6: 0.001838
[epoch: 326/1000, batch:   500/ 1052, ite: 85600] train loss: 0.004954, tar: 0.000250 
l0: 0.000279, l1: 0.000293, l2: 0.000298, l3: 0.000411, l4: 0.000565, l5: 0.001167, l6: 0.002197
[epoch: 326/1000, batch:   580/ 1052, ite: 85620] train loss: 0.004960, tar: 0.000251 
l0: 0.000296, l1: 0.000296, l2: 0.000331, l3: 0.000395, l4: 0.000624, l5: 0.001066, l6: 0.001991
[epoch: 326/1000, batch:   660/ 1052, ite: 85640] train loss: 0.004962, tar: 0.000252 
l0: 0.000227, l1: 0.000231, l2: 0.000259, l3: 0.000339, l4: 0.000577, l5: 0.001204, l6: 0.001889
[epoch: 326/1000, batch:   740/ 1052, ite: 85660] train loss: 0.004967, tar: 0.000252 
l0: 0.000233, l1: 0.000265, l2: 0.000261, l3: 0.000285, l4: 0.000443, l5: 0.000882, l6: 0.001723
[epoch: 326/1000, batch:   820/ 1052, ite: 85680] train loss: 0.004971, tar: 0.000253 
l0: 0.000253, l1: 0.000252, l2: 0.000286, l3: 0.000292, l4: 0.000454, l5: 0.001121, l6: 0.002145
[epoch: 326/1000, batch:   900/ 1052, ite: 85700] train loss: 0.004965, tar: 0.000253 
l0: 0.000474, l1: 0.000512, l2: 0.000540, l3: 0.000579, l4: 0.000779, l5: 0.001446, l6: 0.001884
[epoch: 326/1000, batch:   980/ 1052, ite: 85720] train loss: 0.004969, tar: 0.000253 
[Epoch 326/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000211, l1: 0.000215, l2: 0.000242, l3: 0.000261, l4: 0.000535, l5: 0.000954, l6: 0.002207
[epoch: 327/1000, batch:     8/ 1052, ite: 85740] train loss: 0.004970, tar: 0.000253 
l0: 0.000247, l1: 0.000252, l2: 0.000310, l3: 0.000403, l4: 0.000768, l5: 0.001765, l6: 0.002470
[epoch: 327/1000, batch:    88/ 1052, ite: 85760] train loss: 0.004973, tar: 0.000253 
l0: 0.000169, l1: 0.000176, l2: 0.000196, l3: 0.000249, l4: 0.000628, l5: 0.001500, l6: 0.002179
[epoch: 327/1000, batch:   168/ 1052, ite: 85780] train loss: 0.004963, tar: 0.000252 
l0: 0.000313, l1: 0.000300, l2: 0.000410, l3: 0.000682, l4: 0.000964, l5: 0.001881, l6: 0.003519
[epoch: 327/1000, batch:   248/ 1052, ite: 85800] train loss: 0.004967, tar: 0.000253 
l0: 0.000275, l1: 0.000295, l2: 0.000304, l3: 0.000301, l4: 0.000456, l5: 0.001175, l6: 0.001832
[epoch: 327/1000, batch:   328/ 1052, ite: 85820] train loss: 0.004971, tar: 0.000253 
l0: 0.000160, l1: 0.000175, l2: 0.000177, l3: 0.000187, l4: 0.000403, l5: 0.000556, l6: 0.001227
[epoch: 327/1000, batch:   408/ 1052, ite: 85840] train loss: 0.004977, tar: 0.000253 
l0: 0.000169, l1: 0.000166, l2: 0.000224, l3: 0.000276, l4: 0.000643, l5: 0.000511, l6: 0.001969
[epoch: 327/1000, batch:   488/ 1052, ite: 85860] train loss: 0.004980, tar: 0.000254 
l0: 0.000114, l1: 0.000111, l2: 0.000151, l3: 0.000208, l4: 0.000314, l5: 0.000631, l6: 0.000692
[epoch: 327/1000, batch:   568/ 1052, ite: 85880] train loss: 0.004981, tar: 0.000254 
l0: 0.000169, l1: 0.000169, l2: 0.000197, l3: 0.000221, l4: 0.000448, l5: 0.000927, l6: 0.001180
[epoch: 327/1000, batch:   648/ 1052, ite: 85900] train loss: 0.004974, tar: 0.000253 
l0: 0.000243, l1: 0.000253, l2: 0.000266, l3: 0.000276, l4: 0.000484, l5: 0.001052, l6: 0.002600
[epoch: 327/1000, batch:   728/ 1052, ite: 85920] train loss: 0.004976, tar: 0.000253 
l0: 0.000307, l1: 0.000299, l2: 0.000367, l3: 0.000529, l4: 0.000978, l5: 0.001745, l6: 0.002607
[epoch: 327/1000, batch:   808/ 1052, ite: 85940] train loss: 0.004983, tar: 0.000253 
l0: 0.000268, l1: 0.000274, l2: 0.000312, l3: 0.000319, l4: 0.000568, l5: 0.001143, l6: 0.002694
[epoch: 327/1000, batch:   888/ 1052, ite: 85960] train loss: 0.004988, tar: 0.000254 
l0: 0.000142, l1: 0.000138, l2: 0.000172, l3: 0.000258, l4: 0.000226, l5: 0.000801, l6: 0.001357
[epoch: 327/1000, batch:   968/ 1052, ite: 85980] train loss: 0.004986, tar: 0.000254 
l0: 0.000118, l1: 0.000116, l2: 0.000130, l3: 0.000189, l4: 0.000545, l5: 0.000829, l6: 0.001217
[epoch: 327/1000, batch:  1048/ 1052, ite: 86000] train loss: 0.004986, tar: 0.000254 
[Epoch 327/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000294, l1: 0.000290, l2: 0.000343, l3: 0.000566, l4: 0.001313, l5: 0.001759, l6: 0.002335
[epoch: 328/1000, batch:    76/ 1052, ite: 86020] train loss: 0.004986, tar: 0.000253 
l0: 0.000168, l1: 0.000169, l2: 0.000184, l3: 0.000282, l4: 0.000503, l5: 0.000695, l6: 0.001918
[epoch: 328/1000, batch:   156/ 1052, ite: 86040] train loss: 0.004982, tar: 0.000253 
l0: 0.000189, l1: 0.000184, l2: 0.000264, l3: 0.000383, l4: 0.000470, l5: 0.001933, l6: 0.002192
[epoch: 328/1000, batch:   236/ 1052, ite: 86060] train loss: 0.004977, tar: 0.000253 
l0: 0.000220, l1: 0.000221, l2: 0.000252, l3: 0.000269, l4: 0.000443, l5: 0.001114, l6: 0.001497
[epoch: 328/1000, batch:   316/ 1052, ite: 86080] train loss: 0.004978, tar: 0.000253 
l0: 0.000449, l1: 0.000445, l2: 0.000469, l3: 0.000737, l4: 0.001135, l5: 0.001378, l6: 0.002792
[epoch: 328/1000, batch:   396/ 1052, ite: 86100] train loss: 0.004976, tar: 0.000252 
l0: 0.000178, l1: 0.000181, l2: 0.000202, l3: 0.000271, l4: 0.000535, l5: 0.000934, l6: 0.001912
[epoch: 328/1000, batch:   476/ 1052, ite: 86120] train loss: 0.004978, tar: 0.000252 
l0: 0.000350, l1: 0.000362, l2: 0.000400, l3: 0.000534, l4: 0.000821, l5: 0.001433, l6: 0.002904
[epoch: 328/1000, batch:   556/ 1052, ite: 86140] train loss: 0.004981, tar: 0.000252 
l0: 0.000106, l1: 0.000103, l2: 0.000153, l3: 0.000225, l4: 0.000388, l5: 0.001087, l6: 0.001513
[epoch: 328/1000, batch:   636/ 1052, ite: 86160] train loss: 0.004973, tar: 0.000252 
l0: 0.000238, l1: 0.000242, l2: 0.000249, l3: 0.000352, l4: 0.000526, l5: 0.001108, l6: 0.001633
[epoch: 328/1000, batch:   716/ 1052, ite: 86180] train loss: 0.004971, tar: 0.000251 
l0: 0.000269, l1: 0.000268, l2: 0.000338, l3: 0.000360, l4: 0.000533, l5: 0.001359, l6: 0.002450
[epoch: 328/1000, batch:   796/ 1052, ite: 86200] train loss: 0.004971, tar: 0.000251 
l0: 0.000155, l1: 0.000156, l2: 0.000197, l3: 0.000238, l4: 0.000525, l5: 0.001323, l6: 0.001778
[epoch: 328/1000, batch:   876/ 1052, ite: 86220] train loss: 0.004968, tar: 0.000251 
l0: 0.000122, l1: 0.000136, l2: 0.000159, l3: 0.000168, l4: 0.000405, l5: 0.000843, l6: 0.000884
[epoch: 328/1000, batch:   956/ 1052, ite: 86240] train loss: 0.004969, tar: 0.000251 
l0: 0.000084, l1: 0.000087, l2: 0.000108, l3: 0.000178, l4: 0.000268, l5: 0.000797, l6: 0.001093
[epoch: 328/1000, batch:  1036/ 1052, ite: 86260] train loss: 0.004971, tar: 0.000251 
[Epoch 328/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000163, l1: 0.000168, l2: 0.000237, l3: 0.000362, l4: 0.000573, l5: 0.001019, l6: 0.001753
[epoch: 329/1000, batch:    64/ 1052, ite: 86280] train loss: 0.004975, tar: 0.000251 
l0: 0.000255, l1: 0.000255, l2: 0.000282, l3: 0.000377, l4: 0.000565, l5: 0.001577, l6: 0.002648
[epoch: 329/1000, batch:   144/ 1052, ite: 86300] train loss: 0.004979, tar: 0.000250 
l0: 0.000367, l1: 0.000359, l2: 0.000437, l3: 0.000589, l4: 0.000904, l5: 0.001767, l6: 0.001997
[epoch: 329/1000, batch:   224/ 1052, ite: 86320] train loss: 0.004971, tar: 0.000250 
l0: 0.000322, l1: 0.000333, l2: 0.000337, l3: 0.000362, l4: 0.000663, l5: 0.001276, l6: 0.001705
[epoch: 329/1000, batch:   304/ 1052, ite: 86340] train loss: 0.004973, tar: 0.000250 
l0: 0.000429, l1: 0.000429, l2: 0.000517, l3: 0.000528, l4: 0.000862, l5: 0.001457, l6: 0.002698
[epoch: 329/1000, batch:   384/ 1052, ite: 86360] train loss: 0.004970, tar: 0.000250 
l0: 0.000323, l1: 0.000338, l2: 0.000348, l3: 0.000402, l4: 0.000584, l5: 0.001407, l6: 0.002386
[epoch: 329/1000, batch:   464/ 1052, ite: 86380] train loss: 0.004966, tar: 0.000250 
l0: 0.000328, l1: 0.000327, l2: 0.000370, l3: 0.000442, l4: 0.000861, l5: 0.002131, l6: 0.002764
[epoch: 329/1000, batch:   544/ 1052, ite: 86400] train loss: 0.004964, tar: 0.000249 
l0: 0.000236, l1: 0.000244, l2: 0.000278, l3: 0.000367, l4: 0.000702, l5: 0.001061, l6: 0.002262
[epoch: 329/1000, batch:   624/ 1052, ite: 86420] train loss: 0.004967, tar: 0.000249 
l0: 0.000341, l1: 0.000357, l2: 0.000356, l3: 0.000280, l4: 0.000493, l5: 0.001043, l6: 0.002016
[epoch: 329/1000, batch:   704/ 1052, ite: 86440] train loss: 0.004965, tar: 0.000249 
l0: 0.000229, l1: 0.000240, l2: 0.000269, l3: 0.000370, l4: 0.000724, l5: 0.001110, l6: 0.001907
[epoch: 329/1000, batch:   784/ 1052, ite: 86460] train loss: 0.004960, tar: 0.000249 
l0: 0.000494, l1: 0.000495, l2: 0.000514, l3: 0.000706, l4: 0.000981, l5: 0.001325, l6: 0.002629
[epoch: 329/1000, batch:   864/ 1052, ite: 86480] train loss: 0.004963, tar: 0.000249 
l0: 0.000235, l1: 0.000233, l2: 0.000306, l3: 0.000386, l4: 0.000642, l5: 0.001241, l6: 0.002087
[epoch: 329/1000, batch:   944/ 1052, ite: 86500] train loss: 0.004960, tar: 0.000248 
l0: 0.000228, l1: 0.000241, l2: 0.000282, l3: 0.000226, l4: 0.000495, l5: 0.001286, l6: 0.001192
[epoch: 329/1000, batch:  1024/ 1052, ite: 86520] train loss: 0.004955, tar: 0.000248 
[Epoch 329/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000236, l1: 0.000233, l2: 0.000266, l3: 0.000310, l4: 0.000433, l5: 0.001022, l6: 0.001934
[epoch: 330/1000, batch:    52/ 1052, ite: 86540] train loss: 0.004952, tar: 0.000248 
l0: 0.000111, l1: 0.000111, l2: 0.000120, l3: 0.000185, l4: 0.000361, l5: 0.000626, l6: 0.000822
[epoch: 330/1000, batch:   132/ 1052, ite: 86560] train loss: 0.004948, tar: 0.000247 
l0: 0.000136, l1: 0.000135, l2: 0.000173, l3: 0.000271, l4: 0.000411, l5: 0.001153, l6: 0.001541
[epoch: 330/1000, batch:   212/ 1052, ite: 86580] train loss: 0.004942, tar: 0.000247 
l0: 0.000153, l1: 0.000153, l2: 0.000190, l3: 0.000240, l4: 0.000545, l5: 0.001094, l6: 0.002350
[epoch: 330/1000, batch:   292/ 1052, ite: 86600] train loss: 0.004940, tar: 0.000247 
l0: 0.000301, l1: 0.000313, l2: 0.000323, l3: 0.000344, l4: 0.000612, l5: 0.001275, l6: 0.002363
[epoch: 330/1000, batch:   372/ 1052, ite: 86620] train loss: 0.004936, tar: 0.000246 
l0: 0.000167, l1: 0.000159, l2: 0.000205, l3: 0.000243, l4: 0.000379, l5: 0.000718, l6: 0.001017
[epoch: 330/1000, batch:   452/ 1052, ite: 86640] train loss: 0.004935, tar: 0.000246 
l0: 0.000104, l1: 0.000104, l2: 0.000144, l3: 0.000170, l4: 0.000364, l5: 0.000631, l6: 0.001607
[epoch: 330/1000, batch:   532/ 1052, ite: 86660] train loss: 0.004933, tar: 0.000246 
l0: 0.000203, l1: 0.000207, l2: 0.000231, l3: 0.000328, l4: 0.000643, l5: 0.001295, l6: 0.002456
[epoch: 330/1000, batch:   612/ 1052, ite: 86680] train loss: 0.004935, tar: 0.000246 
l0: 0.000183, l1: 0.000183, l2: 0.000194, l3: 0.000263, l4: 0.000530, l5: 0.001060, l6: 0.002147
[epoch: 330/1000, batch:   692/ 1052, ite: 86700] train loss: 0.004935, tar: 0.000246 
l0: 0.000122, l1: 0.000118, l2: 0.000141, l3: 0.000188, l4: 0.000322, l5: 0.000999, l6: 0.001686
[epoch: 330/1000, batch:   772/ 1052, ite: 86720] train loss: 0.004937, tar: 0.000246 
l0: 0.000189, l1: 0.000182, l2: 0.000213, l3: 0.000284, l4: 0.000516, l5: 0.000760, l6: 0.001628
[epoch: 330/1000, batch:   852/ 1052, ite: 86740] train loss: 0.004934, tar: 0.000246 
l0: 0.000376, l1: 0.000385, l2: 0.000374, l3: 0.000449, l4: 0.000800, l5: 0.001469, l6: 0.003230
[epoch: 330/1000, batch:   932/ 1052, ite: 86760] train loss: 0.004938, tar: 0.000246 
l0: 0.000152, l1: 0.000166, l2: 0.000126, l3: 0.000152, l4: 0.000254, l5: 0.000275, l6: 0.000845
[epoch: 330/1000, batch:  1012/ 1052, ite: 86780] train loss: 0.004936, tar: 0.000245 
[Epoch 330/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000118, l1: 0.000118, l2: 0.000167, l3: 0.000200, l4: 0.000299, l5: 0.000748, l6: 0.001390
[epoch: 331/1000, batch:    40/ 1052, ite: 86800] train loss: 0.004939, tar: 0.000245 
l0: 0.000257, l1: 0.000264, l2: 0.000333, l3: 0.000347, l4: 0.000586, l5: 0.001102, l6: 0.003309
[epoch: 331/1000, batch:   120/ 1052, ite: 86820] train loss: 0.004937, tar: 0.000245 
l0: 0.000342, l1: 0.000346, l2: 0.000379, l3: 0.000409, l4: 0.000879, l5: 0.001134, l6: 0.003348
[epoch: 331/1000, batch:   200/ 1052, ite: 86840] train loss: 0.004937, tar: 0.000245 
l0: 0.000156, l1: 0.000160, l2: 0.000175, l3: 0.000241, l4: 0.000453, l5: 0.001314, l6: 0.002086
[epoch: 331/1000, batch:   280/ 1052, ite: 86860] train loss: 0.004936, tar: 0.000245 
l0: 0.000191, l1: 0.000186, l2: 0.000226, l3: 0.000278, l4: 0.000460, l5: 0.001174, l6: 0.001733
[epoch: 331/1000, batch:   360/ 1052, ite: 86880] train loss: 0.004938, tar: 0.000245 
l0: 0.000120, l1: 0.000118, l2: 0.000169, l3: 0.000221, l4: 0.000384, l5: 0.001144, l6: 0.001850
[epoch: 331/1000, batch:   440/ 1052, ite: 86900] train loss: 0.004934, tar: 0.000244 
l0: 0.000392, l1: 0.000388, l2: 0.000459, l3: 0.000667, l4: 0.001245, l5: 0.001835, l6: 0.003173
[epoch: 331/1000, batch:   520/ 1052, ite: 86920] train loss: 0.004930, tar: 0.000244 
l0: 0.000362, l1: 0.000376, l2: 0.000385, l3: 0.000354, l4: 0.000568, l5: 0.001218, l6: 0.001631
[epoch: 331/1000, batch:   600/ 1052, ite: 86940] train loss: 0.004929, tar: 0.000244 
l0: 0.000252, l1: 0.000252, l2: 0.000257, l3: 0.000381, l4: 0.000488, l5: 0.000787, l6: 0.001278
[epoch: 331/1000, batch:   680/ 1052, ite: 86960] train loss: 0.004927, tar: 0.000244 
l0: 0.000160, l1: 0.000161, l2: 0.000173, l3: 0.000209, l4: 0.000377, l5: 0.000658, l6: 0.001283
[epoch: 331/1000, batch:   760/ 1052, ite: 86980] train loss: 0.004927, tar: 0.000244 
l0: 0.000659, l1: 0.000675, l2: 0.000689, l3: 0.000749, l4: 0.001223, l5: 0.002913, l6: 0.006558
[epoch: 331/1000, batch:   840/ 1052, ite: 87000] train loss: 0.004930, tar: 0.000244 
l0: 0.000180, l1: 0.000184, l2: 0.000206, l3: 0.000220, l4: 0.000590, l5: 0.001527, l6: 0.003716
[epoch: 331/1000, batch:   920/ 1052, ite: 87020] train loss: 0.004929, tar: 0.000243 
l0: 0.000298, l1: 0.000302, l2: 0.000363, l3: 0.000441, l4: 0.000782, l5: 0.001445, l6: 0.002191
[epoch: 331/1000, batch:  1000/ 1052, ite: 87040] train loss: 0.004930, tar: 0.000244 
[Epoch 331/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000087, l1: 0.000094, l2: 0.000099, l3: 0.000147, l4: 0.000347, l5: 0.000798, l6: 0.001026
[epoch: 332/1000, batch:    28/ 1052, ite: 87060] train loss: 0.004929, tar: 0.000244 
l0: 0.000136, l1: 0.000140, l2: 0.000159, l3: 0.000227, l4: 0.000266, l5: 0.000765, l6: 0.001391
[epoch: 332/1000, batch:   108/ 1052, ite: 87080] train loss: 0.004925, tar: 0.000243 
l0: 0.000190, l1: 0.000193, l2: 0.000211, l3: 0.000234, l4: 0.000415, l5: 0.000808, l6: 0.001561
[epoch: 332/1000, batch:   188/ 1052, ite: 87100] train loss: 0.004926, tar: 0.000243 
l0: 0.000151, l1: 0.000152, l2: 0.000180, l3: 0.000189, l4: 0.000423, l5: 0.000635, l6: 0.000926
[epoch: 332/1000, batch:   268/ 1052, ite: 87120] train loss: 0.004927, tar: 0.000243 
l0: 0.000209, l1: 0.000207, l2: 0.000276, l3: 0.000359, l4: 0.000521, l5: 0.001162, l6: 0.001493
[epoch: 332/1000, batch:   348/ 1052, ite: 87140] train loss: 0.004929, tar: 0.000243 
l0: 0.000260, l1: 0.000259, l2: 0.000305, l3: 0.000451, l4: 0.000704, l5: 0.001481, l6: 0.002623
[epoch: 332/1000, batch:   428/ 1052, ite: 87160] train loss: 0.004928, tar: 0.000243 
l0: 0.000227, l1: 0.000226, l2: 0.000236, l3: 0.000340, l4: 0.000493, l5: 0.000795, l6: 0.001514
[epoch: 332/1000, batch:   508/ 1052, ite: 87180] train loss: 0.004925, tar: 0.000242 
l0: 0.000209, l1: 0.000211, l2: 0.000229, l3: 0.000255, l4: 0.000397, l5: 0.001063, l6: 0.001758
[epoch: 332/1000, batch:   588/ 1052, ite: 87200] train loss: 0.004927, tar: 0.000243 
l0: 0.000165, l1: 0.000170, l2: 0.000188, l3: 0.000345, l4: 0.000657, l5: 0.001213, l6: 0.001945
[epoch: 332/1000, batch:   668/ 1052, ite: 87220] train loss: 0.004926, tar: 0.000242 
l0: 0.000531, l1: 0.000524, l2: 0.000608, l3: 0.000742, l4: 0.001029, l5: 0.001905, l6: 0.003660
[epoch: 332/1000, batch:   748/ 1052, ite: 87240] train loss: 0.004929, tar: 0.000242 
l0: 0.000183, l1: 0.000186, l2: 0.000250, l3: 0.000286, l4: 0.000482, l5: 0.000859, l6: 0.001510
[epoch: 332/1000, batch:   828/ 1052, ite: 87260] train loss: 0.004927, tar: 0.000242 
l0: 0.000177, l1: 0.000175, l2: 0.000257, l3: 0.000445, l4: 0.001083, l5: 0.001512, l6: 0.001838
[epoch: 332/1000, batch:   908/ 1052, ite: 87280] train loss: 0.004924, tar: 0.000242 
l0: 0.000202, l1: 0.000207, l2: 0.000212, l3: 0.000331, l4: 0.000634, l5: 0.001326, l6: 0.002029
[epoch: 332/1000, batch:   988/ 1052, ite: 87300] train loss: 0.004923, tar: 0.000242 
[Epoch 332/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000327, l1: 0.000339, l2: 0.000366, l3: 0.000447, l4: 0.000848, l5: 0.002418, l6: 0.003707
[epoch: 333/1000, batch:    16/ 1052, ite: 87320] train loss: 0.004922, tar: 0.000242 
l0: 0.000237, l1: 0.000238, l2: 0.000272, l3: 0.000346, l4: 0.000574, l5: 0.000956, l6: 0.001743
[epoch: 333/1000, batch:    96/ 1052, ite: 87340] train loss: 0.004917, tar: 0.000242 
l0: 0.000132, l1: 0.000138, l2: 0.000143, l3: 0.000213, l4: 0.000422, l5: 0.000816, l6: 0.000953
[epoch: 333/1000, batch:   176/ 1052, ite: 87360] train loss: 0.004917, tar: 0.000241 
l0: 0.000167, l1: 0.000167, l2: 0.000188, l3: 0.000261, l4: 0.000392, l5: 0.000849, l6: 0.001699
[epoch: 333/1000, batch:   256/ 1052, ite: 87380] train loss: 0.004920, tar: 0.000241 
l0: 0.000124, l1: 0.000123, l2: 0.000143, l3: 0.000177, l4: 0.000365, l5: 0.000939, l6: 0.001809
[epoch: 333/1000, batch:   336/ 1052, ite: 87400] train loss: 0.004917, tar: 0.000241 
l0: 0.000164, l1: 0.000167, l2: 0.000208, l3: 0.000221, l4: 0.000363, l5: 0.001022, l6: 0.001808
[epoch: 333/1000, batch:   416/ 1052, ite: 87420] train loss: 0.004919, tar: 0.000241 
l0: 0.000131, l1: 0.000130, l2: 0.000133, l3: 0.000214, l4: 0.000305, l5: 0.000629, l6: 0.001009
[epoch: 333/1000, batch:   496/ 1052, ite: 87440] train loss: 0.004918, tar: 0.000241 
l0: 0.000259, l1: 0.000250, l2: 0.000337, l3: 0.000442, l4: 0.000697, l5: 0.000932, l6: 0.002040
[epoch: 333/1000, batch:   576/ 1052, ite: 87460] train loss: 0.004918, tar: 0.000241 
l0: 0.000211, l1: 0.000213, l2: 0.000220, l3: 0.000271, l4: 0.000416, l5: 0.000787, l6: 0.001881
[epoch: 333/1000, batch:   656/ 1052, ite: 87480] train loss: 0.004917, tar: 0.000241 
l0: 0.000153, l1: 0.000155, l2: 0.000191, l3: 0.000235, l4: 0.000435, l5: 0.000587, l6: 0.001976
[epoch: 333/1000, batch:   736/ 1052, ite: 87500] train loss: 0.004916, tar: 0.000241 
l0: 0.000114, l1: 0.000112, l2: 0.000164, l3: 0.000234, l4: 0.000318, l5: 0.000640, l6: 0.001559
[epoch: 333/1000, batch:   816/ 1052, ite: 87520] train loss: 0.004910, tar: 0.000240 
l0: 0.000203, l1: 0.000201, l2: 0.000223, l3: 0.000340, l4: 0.000692, l5: 0.002219, l6: 0.003170
[epoch: 333/1000, batch:   896/ 1052, ite: 87540] train loss: 0.004914, tar: 0.000241 
l0: 0.000374, l1: 0.000381, l2: 0.000404, l3: 0.000598, l4: 0.000980, l5: 0.001283, l6: 0.001469
[epoch: 333/1000, batch:   976/ 1052, ite: 87560] train loss: 0.004911, tar: 0.000240 
[Epoch 333/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000355, l1: 0.000370, l2: 0.000376, l3: 0.000507, l4: 0.001133, l5: 0.002329, l6: 0.003814
[epoch: 334/1000, batch:     4/ 1052, ite: 87580] train loss: 0.004913, tar: 0.000240 
l0: 0.000325, l1: 0.000328, l2: 0.000320, l3: 0.000382, l4: 0.000731, l5: 0.000896, l6: 0.001975
[epoch: 334/1000, batch:    84/ 1052, ite: 87600] train loss: 0.004910, tar: 0.000240 
l0: 0.000261, l1: 0.000260, l2: 0.000317, l3: 0.000503, l4: 0.000819, l5: 0.001619, l6: 0.004454
[epoch: 334/1000, batch:   164/ 1052, ite: 87620] train loss: 0.004910, tar: 0.000240 
l0: 0.000119, l1: 0.000122, l2: 0.000132, l3: 0.000178, l4: 0.000407, l5: 0.000744, l6: 0.000791
[epoch: 334/1000, batch:   244/ 1052, ite: 87640] train loss: 0.004910, tar: 0.000240 
l0: 0.000464, l1: 0.000481, l2: 0.000506, l3: 0.000595, l4: 0.001328, l5: 0.002423, l6: 0.004632
[epoch: 334/1000, batch:   324/ 1052, ite: 87660] train loss: 0.004911, tar: 0.000240 
l0: 0.000198, l1: 0.000167, l2: 0.000211, l3: 0.000375, l4: 0.000641, l5: 0.001369, l6: 0.002063
[epoch: 334/1000, batch:   404/ 1052, ite: 87680] train loss: 0.004909, tar: 0.000239 
l0: 0.000251, l1: 0.000254, l2: 0.000267, l3: 0.000320, l4: 0.000565, l5: 0.002207, l6: 0.002738
[epoch: 334/1000, batch:   484/ 1052, ite: 87700] train loss: 0.004908, tar: 0.000239 
l0: 0.000555, l1: 0.000576, l2: 0.000592, l3: 0.000693, l4: 0.000826, l5: 0.001729, l6: 0.003737
[epoch: 334/1000, batch:   564/ 1052, ite: 87720] train loss: 0.004910, tar: 0.000239 
l0: 0.000144, l1: 0.000147, l2: 0.000178, l3: 0.000196, l4: 0.000284, l5: 0.000676, l6: 0.001200
[epoch: 334/1000, batch:   644/ 1052, ite: 87740] train loss: 0.004911, tar: 0.000239 
l0: 0.000124, l1: 0.000126, l2: 0.000128, l3: 0.000130, l4: 0.000324, l5: 0.000759, l6: 0.000899
[epoch: 334/1000, batch:   724/ 1052, ite: 87760] train loss: 0.004909, tar: 0.000239 
l0: 0.000060, l1: 0.000064, l2: 0.000081, l3: 0.000092, l4: 0.000251, l5: 0.000780, l6: 0.001259
[epoch: 334/1000, batch:   804/ 1052, ite: 87780] train loss: 0.004907, tar: 0.000239 
l0: 0.000313, l1: 0.000313, l2: 0.000359, l3: 0.000444, l4: 0.000871, l5: 0.001282, l6: 0.002389
[epoch: 334/1000, batch:   884/ 1052, ite: 87800] train loss: 0.004908, tar: 0.000239 
l0: 0.000179, l1: 0.000188, l2: 0.000203, l3: 0.000245, l4: 0.000539, l5: 0.001085, l6: 0.001685
[epoch: 334/1000, batch:   964/ 1052, ite: 87820] train loss: 0.004906, tar: 0.000239 
l0: 0.000132, l1: 0.000136, l2: 0.000150, l3: 0.000189, l4: 0.000457, l5: 0.000966, l6: 0.001863
[epoch: 334/1000, batch:  1044/ 1052, ite: 87840] train loss: 0.004905, tar: 0.000239 
[Epoch 334/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000120, l1: 0.000123, l2: 0.000157, l3: 0.000195, l4: 0.000372, l5: 0.000961, l6: 0.001958
[epoch: 335/1000, batch:    72/ 1052, ite: 87860] train loss: 0.004903, tar: 0.000238 
l0: 0.000167, l1: 0.000182, l2: 0.000191, l3: 0.000257, l4: 0.000539, l5: 0.001514, l6: 0.001575
[epoch: 335/1000, batch:   152/ 1052, ite: 87880] train loss: 0.004904, tar: 0.000238 
l0: 0.000127, l1: 0.000129, l2: 0.000177, l3: 0.000226, l4: 0.000397, l5: 0.000658, l6: 0.001153
[epoch: 335/1000, batch:   232/ 1052, ite: 87900] train loss: 0.004906, tar: 0.000238 
l0: 0.000072, l1: 0.000073, l2: 0.000057, l3: 0.000093, l4: 0.000330, l5: 0.000457, l6: 0.000713
[epoch: 335/1000, batch:   312/ 1052, ite: 87920] train loss: 0.004905, tar: 0.000238 
l0: 0.000322, l1: 0.000333, l2: 0.000361, l3: 0.000385, l4: 0.000735, l5: 0.001524, l6: 0.002018
[epoch: 335/1000, batch:   392/ 1052, ite: 87940] train loss: 0.004905, tar: 0.000238 
l0: 0.000212, l1: 0.000216, l2: 0.000267, l3: 0.000349, l4: 0.000614, l5: 0.001611, l6: 0.002202
[epoch: 335/1000, batch:   472/ 1052, ite: 87960] train loss: 0.004906, tar: 0.000238 
l0: 0.000138, l1: 0.000141, l2: 0.000127, l3: 0.000154, l4: 0.000198, l5: 0.000467, l6: 0.000676
[epoch: 335/1000, batch:   552/ 1052, ite: 87980] train loss: 0.004902, tar: 0.000238 
l0: 0.000091, l1: 0.000090, l2: 0.000107, l3: 0.000156, l4: 0.000314, l5: 0.000595, l6: 0.000676
[epoch: 335/1000, batch:   632/ 1052, ite: 88000] train loss: 0.004899, tar: 0.000238 
l0: 0.000341, l1: 0.000342, l2: 0.000358, l3: 0.000423, l4: 0.000645, l5: 0.001320, l6: 0.003316
[epoch: 335/1000, batch:   712/ 1052, ite: 88020] train loss: 0.004900, tar: 0.000237 
l0: 0.000309, l1: 0.000320, l2: 0.000351, l3: 0.000395, l4: 0.000724, l5: 0.001626, l6: 0.002626
[epoch: 335/1000, batch:   792/ 1052, ite: 88040] train loss: 0.004899, tar: 0.000237 
l0: 0.000342, l1: 0.000357, l2: 0.000357, l3: 0.000414, l4: 0.000632, l5: 0.000979, l6: 0.002514
[epoch: 335/1000, batch:   872/ 1052, ite: 88060] train loss: 0.004898, tar: 0.000237 
l0: 0.000130, l1: 0.000129, l2: 0.000152, l3: 0.000196, l4: 0.000373, l5: 0.000808, l6: 0.001061
[epoch: 335/1000, batch:   952/ 1052, ite: 88080] train loss: 0.004898, tar: 0.000237 
l0: 0.000217, l1: 0.000220, l2: 0.000257, l3: 0.000262, l4: 0.000591, l5: 0.001123, l6: 0.001736
[epoch: 335/1000, batch:  1032/ 1052, ite: 88100] train loss: 0.004898, tar: 0.000237 
[Epoch 335/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000270, l1: 0.000275, l2: 0.000314, l3: 0.000398, l4: 0.000665, l5: 0.000824, l6: 0.001878
[epoch: 336/1000, batch:    60/ 1052, ite: 88120] train loss: 0.004898, tar: 0.000237 
l0: 0.000150, l1: 0.000144, l2: 0.000168, l3: 0.000243, l4: 0.000365, l5: 0.001189, l6: 0.001029
[epoch: 336/1000, batch:   140/ 1052, ite: 88140] train loss: 0.004897, tar: 0.000237 
l0: 0.000171, l1: 0.000172, l2: 0.000218, l3: 0.000262, l4: 0.000392, l5: 0.001038, l6: 0.002098
[epoch: 336/1000, batch:   220/ 1052, ite: 88160] train loss: 0.004897, tar: 0.000237 
l0: 0.000170, l1: 0.000164, l2: 0.000211, l3: 0.000247, l4: 0.000612, l5: 0.001194, l6: 0.001433
[epoch: 336/1000, batch:   300/ 1052, ite: 88180] train loss: 0.004897, tar: 0.000237 
l0: 0.000269, l1: 0.000289, l2: 0.000283, l3: 0.000304, l4: 0.000536, l5: 0.001299, l6: 0.002457
[epoch: 336/1000, batch:   380/ 1052, ite: 88200] train loss: 0.004896, tar: 0.000237 
l0: 0.000130, l1: 0.000124, l2: 0.000213, l3: 0.000281, l4: 0.000491, l5: 0.000823, l6: 0.001348
[epoch: 336/1000, batch:   460/ 1052, ite: 88220] train loss: 0.004891, tar: 0.000236 
l0: 0.000284, l1: 0.000295, l2: 0.000316, l3: 0.000372, l4: 0.000658, l5: 0.001278, l6: 0.003140
[epoch: 336/1000, batch:   540/ 1052, ite: 88240] train loss: 0.004888, tar: 0.000236 
l0: 0.000218, l1: 0.000225, l2: 0.000236, l3: 0.000251, l4: 0.000447, l5: 0.000971, l6: 0.001629
[epoch: 336/1000, batch:   620/ 1052, ite: 88260] train loss: 0.004889, tar: 0.000236 
l0: 0.000274, l1: 0.000290, l2: 0.000311, l3: 0.000328, l4: 0.000636, l5: 0.001964, l6: 0.003095
[epoch: 336/1000, batch:   700/ 1052, ite: 88280] train loss: 0.004891, tar: 0.000236 
l0: 0.000184, l1: 0.000187, l2: 0.000207, l3: 0.000245, l4: 0.000432, l5: 0.000893, l6: 0.001789
[epoch: 336/1000, batch:   780/ 1052, ite: 88300] train loss: 0.004890, tar: 0.000236 
l0: 0.000240, l1: 0.000249, l2: 0.000286, l3: 0.000338, l4: 0.000664, l5: 0.001292, l6: 0.001699
[epoch: 336/1000, batch:   860/ 1052, ite: 88320] train loss: 0.004888, tar: 0.000236 
l0: 0.000238, l1: 0.000241, l2: 0.000296, l3: 0.000353, l4: 0.000711, l5: 0.001147, l6: 0.002078
[epoch: 336/1000, batch:   940/ 1052, ite: 88340] train loss: 0.004887, tar: 0.000236 
l0: 0.000140, l1: 0.000147, l2: 0.000123, l3: 0.000161, l4: 0.000301, l5: 0.000497, l6: 0.001228
[epoch: 336/1000, batch:  1020/ 1052, ite: 88360] train loss: 0.004888, tar: 0.000236 
[Epoch 336/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000236, l1: 0.000234, l2: 0.000285, l3: 0.000340, l4: 0.000558, l5: 0.001281, l6: 0.002205
[epoch: 337/1000, batch:    48/ 1052, ite: 88380] train loss: 0.004888, tar: 0.000236 
l0: 0.000108, l1: 0.000108, l2: 0.000129, l3: 0.000185, l4: 0.000336, l5: 0.000524, l6: 0.001184
[epoch: 337/1000, batch:   128/ 1052, ite: 88400] train loss: 0.004887, tar: 0.000236 
l0: 0.000377, l1: 0.000385, l2: 0.000469, l3: 0.000559, l4: 0.000992, l5: 0.001940, l6: 0.003990
[epoch: 337/1000, batch:   208/ 1052, ite: 88420] train loss: 0.004887, tar: 0.000235 
l0: 0.000137, l1: 0.000134, l2: 0.000186, l3: 0.000254, l4: 0.000514, l5: 0.000945, l6: 0.001321
[epoch: 337/1000, batch:   288/ 1052, ite: 88440] train loss: 0.004887, tar: 0.000235 
l0: 0.000306, l1: 0.000322, l2: 0.000334, l3: 0.000391, l4: 0.000594, l5: 0.001058, l6: 0.001640
[epoch: 337/1000, batch:   368/ 1052, ite: 88460] train loss: 0.004887, tar: 0.000235 
l0: 0.000213, l1: 0.000215, l2: 0.000273, l3: 0.000361, l4: 0.000494, l5: 0.001029, l6: 0.001886
[epoch: 337/1000, batch:   448/ 1052, ite: 88480] train loss: 0.004889, tar: 0.000236 
l0: 0.000234, l1: 0.000240, l2: 0.000274, l3: 0.000311, l4: 0.000434, l5: 0.000830, l6: 0.002187
[epoch: 337/1000, batch:   528/ 1052, ite: 88500] train loss: 0.004886, tar: 0.000235 
l0: 0.000249, l1: 0.000251, l2: 0.000268, l3: 0.000339, l4: 0.000674, l5: 0.001159, l6: 0.002170
[epoch: 337/1000, batch:   608/ 1052, ite: 88520] train loss: 0.004884, tar: 0.000235 
l0: 0.000247, l1: 0.000242, l2: 0.000336, l3: 0.000389, l4: 0.000668, l5: 0.001571, l6: 0.002597
[epoch: 337/1000, batch:   688/ 1052, ite: 88540] train loss: 0.004883, tar: 0.000235 
l0: 0.000208, l1: 0.000215, l2: 0.000277, l3: 0.000290, l4: 0.000630, l5: 0.000915, l6: 0.001835
[epoch: 337/1000, batch:   768/ 1052, ite: 88560] train loss: 0.004885, tar: 0.000236 
l0: 0.000401, l1: 0.000412, l2: 0.000382, l3: 0.000420, l4: 0.000547, l5: 0.000962, l6: 0.002688
[epoch: 337/1000, batch:   848/ 1052, ite: 88580] train loss: 0.004886, tar: 0.000236 
l0: 0.000275, l1: 0.000279, l2: 0.000296, l3: 0.000375, l4: 0.000483, l5: 0.001144, l6: 0.002175
[epoch: 337/1000, batch:   928/ 1052, ite: 88600] train loss: 0.004885, tar: 0.000236 
l0: 0.000187, l1: 0.000193, l2: 0.000201, l3: 0.000297, l4: 0.000589, l5: 0.001017, l6: 0.002050
[epoch: 337/1000, batch:  1008/ 1052, ite: 88620] train loss: 0.004887, tar: 0.000236 
[Epoch 337/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000309, l1: 0.000325, l2: 0.000331, l3: 0.000395, l4: 0.000672, l5: 0.002260, l6: 0.003399
[epoch: 338/1000, batch:    36/ 1052, ite: 88640] train loss: 0.004883, tar: 0.000235 
l0: 0.000211, l1: 0.000208, l2: 0.000214, l3: 0.000299, l4: 0.000483, l5: 0.000996, l6: 0.001480
[epoch: 338/1000, batch:   116/ 1052, ite: 88660] train loss: 0.004882, tar: 0.000235 
l0: 0.000198, l1: 0.000207, l2: 0.000222, l3: 0.000248, l4: 0.000451, l5: 0.000913, l6: 0.001692
[epoch: 338/1000, batch:   196/ 1052, ite: 88680] train loss: 0.004880, tar: 0.000235 
l0: 0.000212, l1: 0.000207, l2: 0.000274, l3: 0.000414, l4: 0.000842, l5: 0.002445, l6: 0.003023
[epoch: 338/1000, batch:   276/ 1052, ite: 88700] train loss: 0.004881, tar: 0.000235 
l0: 0.000283, l1: 0.000297, l2: 0.000323, l3: 0.000320, l4: 0.000658, l5: 0.001331, l6: 0.001984
[epoch: 338/1000, batch:   356/ 1052, ite: 88720] train loss: 0.004882, tar: 0.000235 
l0: 0.000240, l1: 0.000241, l2: 0.000279, l3: 0.000375, l4: 0.000750, l5: 0.001277, l6: 0.002503
[epoch: 338/1000, batch:   436/ 1052, ite: 88740] train loss: 0.004881, tar: 0.000235 
l0: 0.000342, l1: 0.000361, l2: 0.000351, l3: 0.000472, l4: 0.000672, l5: 0.001141, l6: 0.002338
[epoch: 338/1000, batch:   516/ 1052, ite: 88760] train loss: 0.004879, tar: 0.000235 
l0: 0.000096, l1: 0.000091, l2: 0.000110, l3: 0.000203, l4: 0.000483, l5: 0.000835, l6: 0.001513
[epoch: 338/1000, batch:   596/ 1052, ite: 88780] train loss: 0.004876, tar: 0.000235 
l0: 0.000233, l1: 0.000242, l2: 0.000239, l3: 0.000362, l4: 0.000641, l5: 0.001129, l6: 0.001509
[epoch: 338/1000, batch:   676/ 1052, ite: 88800] train loss: 0.004877, tar: 0.000235 
l0: 0.000312, l1: 0.000343, l2: 0.000336, l3: 0.000433, l4: 0.000787, l5: 0.001702, l6: 0.002718
[epoch: 338/1000, batch:   756/ 1052, ite: 88820] train loss: 0.004876, tar: 0.000235 
l0: 0.000112, l1: 0.000114, l2: 0.000152, l3: 0.000174, l4: 0.000288, l5: 0.000831, l6: 0.001013
[epoch: 338/1000, batch:   836/ 1052, ite: 88840] train loss: 0.004874, tar: 0.000235 
l0: 0.000168, l1: 0.000170, l2: 0.000211, l3: 0.000243, l4: 0.000276, l5: 0.000955, l6: 0.001695
[epoch: 338/1000, batch:   916/ 1052, ite: 88860] train loss: 0.004876, tar: 0.000235 
l0: 0.000246, l1: 0.000249, l2: 0.000244, l3: 0.000345, l4: 0.000479, l5: 0.000937, l6: 0.001592
[epoch: 338/1000, batch:   996/ 1052, ite: 88880] train loss: 0.004878, tar: 0.000235 
[Epoch 338/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000123, l1: 0.000113, l2: 0.000145, l3: 0.000232, l4: 0.000441, l5: 0.000955, l6: 0.001458
[epoch: 339/1000, batch:    24/ 1052, ite: 88900] train loss: 0.004878, tar: 0.000235 
l0: 0.000110, l1: 0.000121, l2: 0.000130, l3: 0.000162, l4: 0.000298, l5: 0.001080, l6: 0.001705
[epoch: 339/1000, batch:   104/ 1052, ite: 88920] train loss: 0.004876, tar: 0.000235 
l0: 0.000132, l1: 0.000131, l2: 0.000152, l3: 0.000206, l4: 0.000526, l5: 0.000923, l6: 0.001899
[epoch: 339/1000, batch:   184/ 1052, ite: 88940] train loss: 0.004874, tar: 0.000234 
l0: 0.000194, l1: 0.000198, l2: 0.000220, l3: 0.000324, l4: 0.000627, l5: 0.001734, l6: 0.002671
[epoch: 339/1000, batch:   264/ 1052, ite: 88960] train loss: 0.004876, tar: 0.000235 
l0: 0.000200, l1: 0.000204, l2: 0.000261, l3: 0.000342, l4: 0.000554, l5: 0.000830, l6: 0.002336
[epoch: 339/1000, batch:   344/ 1052, ite: 88980] train loss: 0.004877, tar: 0.000234 
l0: 0.000130, l1: 0.000132, l2: 0.000172, l3: 0.000171, l4: 0.000374, l5: 0.000860, l6: 0.001168
[epoch: 339/1000, batch:   424/ 1052, ite: 89000] train loss: 0.004876, tar: 0.000234 
l0: 0.000410, l1: 0.000424, l2: 0.000394, l3: 0.000466, l4: 0.001046, l5: 0.002265, l6: 0.003248
[epoch: 339/1000, batch:   504/ 1052, ite: 89020] train loss: 0.004878, tar: 0.000234 
l0: 0.000261, l1: 0.000264, l2: 0.000261, l3: 0.000299, l4: 0.000456, l5: 0.000830, l6: 0.001532
[epoch: 339/1000, batch:   584/ 1052, ite: 89040] train loss: 0.004879, tar: 0.000234 
l0: 0.000223, l1: 0.000205, l2: 0.000230, l3: 0.000324, l4: 0.000515, l5: 0.000798, l6: 0.001297
[epoch: 339/1000, batch:   664/ 1052, ite: 89060] train loss: 0.004881, tar: 0.000234 
l0: 0.000196, l1: 0.000201, l2: 0.000213, l3: 0.000257, l4: 0.000401, l5: 0.000887, l6: 0.002378
[epoch: 339/1000, batch:   744/ 1052, ite: 89080] train loss: 0.004878, tar: 0.000234 
l0: 0.000262, l1: 0.000258, l2: 0.000291, l3: 0.000425, l4: 0.000547, l5: 0.001511, l6: 0.001282
[epoch: 339/1000, batch:   824/ 1052, ite: 89100] train loss: 0.004875, tar: 0.000234 
l0: 0.000210, l1: 0.000205, l2: 0.000217, l3: 0.000311, l4: 0.000556, l5: 0.000689, l6: 0.001653
[epoch: 339/1000, batch:   904/ 1052, ite: 89120] train loss: 0.004874, tar: 0.000234 
l0: 0.000201, l1: 0.000193, l2: 0.000263, l3: 0.000459, l4: 0.000868, l5: 0.001255, l6: 0.002749
[epoch: 339/1000, batch:   984/ 1052, ite: 89140] train loss: 0.004874, tar: 0.000234 
[Epoch 339/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000265, l1: 0.000276, l2: 0.000311, l3: 0.000298, l4: 0.000604, l5: 0.001463, l6: 0.001858
[epoch: 340/1000, batch:    12/ 1052, ite: 89160] train loss: 0.004873, tar: 0.000234 
l0: 0.000278, l1: 0.000296, l2: 0.000305, l3: 0.000311, l4: 0.000493, l5: 0.001090, l6: 0.001713
[epoch: 340/1000, batch:    92/ 1052, ite: 89180] train loss: 0.004869, tar: 0.000234 
l0: 0.000210, l1: 0.000213, l2: 0.000253, l3: 0.000318, l4: 0.000692, l5: 0.001548, l6: 0.002069
[epoch: 340/1000, batch:   172/ 1052, ite: 89200] train loss: 0.004868, tar: 0.000234 
l0: 0.000195, l1: 0.000199, l2: 0.000220, l3: 0.000284, l4: 0.000461, l5: 0.000786, l6: 0.001095
[epoch: 340/1000, batch:   252/ 1052, ite: 89220] train loss: 0.004865, tar: 0.000233 
l0: 0.000292, l1: 0.000289, l2: 0.000351, l3: 0.000411, l4: 0.000559, l5: 0.001095, l6: 0.002284
[epoch: 340/1000, batch:   332/ 1052, ite: 89240] train loss: 0.004865, tar: 0.000233 
l0: 0.000102, l1: 0.000099, l2: 0.000154, l3: 0.000195, l4: 0.000316, l5: 0.000781, l6: 0.001015
[epoch: 340/1000, batch:   412/ 1052, ite: 89260] train loss: 0.004864, tar: 0.000233 
l0: 0.000069, l1: 0.000065, l2: 0.000106, l3: 0.000160, l4: 0.000338, l5: 0.000611, l6: 0.000882
[epoch: 340/1000, batch:   492/ 1052, ite: 89280] train loss: 0.004862, tar: 0.000233 
l0: 0.000260, l1: 0.000256, l2: 0.000316, l3: 0.000445, l4: 0.000946, l5: 0.001770, l6: 0.002812
[epoch: 340/1000, batch:   572/ 1052, ite: 89300] train loss: 0.004862, tar: 0.000233 
l0: 0.000166, l1: 0.000175, l2: 0.000148, l3: 0.000198, l4: 0.000531, l5: 0.001202, l6: 0.001403
[epoch: 340/1000, batch:   652/ 1052, ite: 89320] train loss: 0.004866, tar: 0.000233 
l0: 0.000395, l1: 0.000398, l2: 0.000390, l3: 0.000589, l4: 0.000785, l5: 0.001028, l6: 0.002253
[epoch: 340/1000, batch:   732/ 1052, ite: 89340] train loss: 0.004868, tar: 0.000233 
l0: 0.000195, l1: 0.000198, l2: 0.000218, l3: 0.000251, l4: 0.000587, l5: 0.000893, l6: 0.002177
[epoch: 340/1000, batch:   812/ 1052, ite: 89360] train loss: 0.004866, tar: 0.000233 
l0: 0.000240, l1: 0.000249, l2: 0.000226, l3: 0.000312, l4: 0.000578, l5: 0.001268, l6: 0.001812
[epoch: 340/1000, batch:   892/ 1052, ite: 89380] train loss: 0.004866, tar: 0.000233 
l0: 0.000167, l1: 0.000173, l2: 0.000182, l3: 0.000220, l4: 0.000521, l5: 0.000928, l6: 0.001327
[epoch: 340/1000, batch:   972/ 1052, ite: 89400] train loss: 0.004868, tar: 0.000233 
l0: 0.000291, l1: 0.000293, l2: 0.000335, l3: 0.000367, l4: 0.000646, l5: 0.001691, l6: 0.001354
[epoch: 340/1000, batch:  1052/ 1052, ite: 89420] train loss: 0.004869, tar: 0.000233 
[Epoch 340/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000211, l1: 0.000219, l2: 0.000250, l3: 0.000288, l4: 0.000697, l5: 0.001003, l6: 0.001918
[epoch: 341/1000, batch:    80/ 1052, ite: 89440] train loss: 0.004868, tar: 0.000233 
l0: 0.000349, l1: 0.000349, l2: 0.000345, l3: 0.000487, l4: 0.000811, l5: 0.001481, l6: 0.003057
[epoch: 341/1000, batch:   160/ 1052, ite: 89460] train loss: 0.004868, tar: 0.000233 
l0: 0.000330, l1: 0.000327, l2: 0.000414, l3: 0.000516, l4: 0.000771, l5: 0.001255, l6: 0.004255
[epoch: 341/1000, batch:   240/ 1052, ite: 89480] train loss: 0.004868, tar: 0.000233 
l0: 0.000180, l1: 0.000188, l2: 0.000222, l3: 0.000297, l4: 0.000576, l5: 0.001115, l6: 0.001752
[epoch: 341/1000, batch:   320/ 1052, ite: 89500] train loss: 0.004867, tar: 0.000233 
l0: 0.000198, l1: 0.000197, l2: 0.000244, l3: 0.000351, l4: 0.000771, l5: 0.001692, l6: 0.002789
[epoch: 341/1000, batch:   400/ 1052, ite: 89520] train loss: 0.004865, tar: 0.000233 
l0: 0.000171, l1: 0.000168, l2: 0.000224, l3: 0.000274, l4: 0.000553, l5: 0.001672, l6: 0.002186
[epoch: 341/1000, batch:   480/ 1052, ite: 89540] train loss: 0.004867, tar: 0.000233 
l0: 0.000252, l1: 0.000258, l2: 0.000280, l3: 0.000340, l4: 0.000610, l5: 0.001166, l6: 0.002232
[epoch: 341/1000, batch:   560/ 1052, ite: 89560] train loss: 0.004865, tar: 0.000233 
l0: 0.000258, l1: 0.000259, l2: 0.000272, l3: 0.000381, l4: 0.000610, l5: 0.001006, l6: 0.002057
[epoch: 341/1000, batch:   640/ 1052, ite: 89580] train loss: 0.004866, tar: 0.000233 
l0: 0.000306, l1: 0.000312, l2: 0.000308, l3: 0.000417, l4: 0.000556, l5: 0.001271, l6: 0.002101
[epoch: 341/1000, batch:   720/ 1052, ite: 89600] train loss: 0.004865, tar: 0.000233 
l0: 0.000206, l1: 0.000207, l2: 0.000232, l3: 0.000265, l4: 0.000359, l5: 0.001509, l6: 0.001801
[epoch: 341/1000, batch:   800/ 1052, ite: 89620] train loss: 0.004864, tar: 0.000233 
l0: 0.000201, l1: 0.000198, l2: 0.000210, l3: 0.000256, l4: 0.000463, l5: 0.001157, l6: 0.001492
[epoch: 341/1000, batch:   880/ 1052, ite: 89640] train loss: 0.004863, tar: 0.000232 
l0: 0.000103, l1: 0.000100, l2: 0.000133, l3: 0.000157, l4: 0.000321, l5: 0.000698, l6: 0.001155
[epoch: 341/1000, batch:   960/ 1052, ite: 89660] train loss: 0.004863, tar: 0.000232 
l0: 0.000145, l1: 0.000136, l2: 0.000177, l3: 0.000330, l4: 0.000524, l5: 0.001297, l6: 0.001898
[epoch: 341/1000, batch:  1040/ 1052, ite: 89680] train loss: 0.004864, tar: 0.000232 
[Epoch 341/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000152, l1: 0.000155, l2: 0.000202, l3: 0.000244, l4: 0.000347, l5: 0.000737, l6: 0.001610
[epoch: 342/1000, batch:    68/ 1052, ite: 89700] train loss: 0.004864, tar: 0.000232 
l0: 0.000063, l1: 0.000067, l2: 0.000085, l3: 0.000094, l4: 0.000294, l5: 0.000653, l6: 0.001252
[epoch: 342/1000, batch:   148/ 1052, ite: 89720] train loss: 0.004864, tar: 0.000232 
l0: 0.000330, l1: 0.000344, l2: 0.000348, l3: 0.000463, l4: 0.000655, l5: 0.001919, l6: 0.003160
[epoch: 342/1000, batch:   228/ 1052, ite: 89740] train loss: 0.004864, tar: 0.000232 
l0: 0.000110, l1: 0.000111, l2: 0.000126, l3: 0.000164, l4: 0.000373, l5: 0.000783, l6: 0.000997
[epoch: 342/1000, batch:   308/ 1052, ite: 89760] train loss: 0.004863, tar: 0.000232 
l0: 0.000409, l1: 0.000408, l2: 0.000495, l3: 0.000640, l4: 0.001024, l5: 0.002247, l6: 0.004114
[epoch: 342/1000, batch:   388/ 1052, ite: 89780] train loss: 0.004862, tar: 0.000232 
l0: 0.000088, l1: 0.000088, l2: 0.000126, l3: 0.000169, l4: 0.000520, l5: 0.000752, l6: 0.000925
[epoch: 342/1000, batch:   468/ 1052, ite: 89800] train loss: 0.004861, tar: 0.000232 
l0: 0.000198, l1: 0.000197, l2: 0.000261, l3: 0.000294, l4: 0.000709, l5: 0.001121, l6: 0.002402
[epoch: 342/1000, batch:   548/ 1052, ite: 89820] train loss: 0.004860, tar: 0.000232 
l0: 0.000232, l1: 0.000225, l2: 0.000310, l3: 0.000492, l4: 0.001146, l5: 0.002062, l6: 0.001607
[epoch: 342/1000, batch:   628/ 1052, ite: 89840] train loss: 0.004859, tar: 0.000232 
l0: 0.000312, l1: 0.000325, l2: 0.000343, l3: 0.000357, l4: 0.000546, l5: 0.000819, l6: 0.001875
[epoch: 342/1000, batch:   708/ 1052, ite: 89860] train loss: 0.004858, tar: 0.000232 
l0: 0.000150, l1: 0.000151, l2: 0.000157, l3: 0.000225, l4: 0.000588, l5: 0.001031, l6: 0.001780
[epoch: 342/1000, batch:   788/ 1052, ite: 89880] train loss: 0.004855, tar: 0.000231 
l0: 0.000596, l1: 0.000582, l2: 0.000628, l3: 0.000789, l4: 0.001102, l5: 0.002205, l6: 0.004023
[epoch: 342/1000, batch:   868/ 1052, ite: 89900] train loss: 0.004857, tar: 0.000231 
l0: 0.000178, l1: 0.000169, l2: 0.000201, l3: 0.000274, l4: 0.000465, l5: 0.001185, l6: 0.000875
[epoch: 342/1000, batch:   948/ 1052, ite: 89920] train loss: 0.004860, tar: 0.000231 
l0: 0.000150, l1: 0.000147, l2: 0.000211, l3: 0.000270, l4: 0.000584, l5: 0.000935, l6: 0.001745
[epoch: 342/1000, batch:  1028/ 1052, ite: 89940] train loss: 0.004859, tar: 0.000231 
[Epoch 342/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000156, l1: 0.000167, l2: 0.000168, l3: 0.000240, l4: 0.000605, l5: 0.001141, l6: 0.001111
[epoch: 343/1000, batch:    56/ 1052, ite: 89960] train loss: 0.004858, tar: 0.000231 
l0: 0.000270, l1: 0.000283, l2: 0.000332, l3: 0.000311, l4: 0.000477, l5: 0.001415, l6: 0.001843
[epoch: 343/1000, batch:   136/ 1052, ite: 89980] train loss: 0.004856, tar: 0.000231 
l0: 0.000167, l1: 0.000159, l2: 0.000197, l3: 0.000330, l4: 0.000314, l5: 0.000564, l6: 0.001336
[epoch: 343/1000, batch:   216/ 1052, ite: 90000] train loss: 0.004854, tar: 0.000231 
l0: 0.000509, l1: 0.000533, l2: 0.000575, l3: 0.000803, l4: 0.001374, l5: 0.002437, l6: 0.005029
[epoch: 343/1000, batch:   296/ 1052, ite: 90020] train loss: 0.004855, tar: 0.000231 
l0: 0.000367, l1: 0.000392, l2: 0.000348, l3: 0.000435, l4: 0.000526, l5: 0.000797, l6: 0.002720
[epoch: 343/1000, batch:   376/ 1052, ite: 90040] train loss: 0.004855, tar: 0.000231 
l0: 0.000272, l1: 0.000276, l2: 0.000339, l3: 0.000361, l4: 0.000476, l5: 0.001445, l6: 0.003079
[epoch: 343/1000, batch:   456/ 1052, ite: 90060] train loss: 0.004854, tar: 0.000231 
l0: 0.000178, l1: 0.000179, l2: 0.000195, l3: 0.000291, l4: 0.000355, l5: 0.000846, l6: 0.000966
[epoch: 343/1000, batch:   536/ 1052, ite: 90080] train loss: 0.004856, tar: 0.000231 
l0: 0.000062, l1: 0.000059, l2: 0.000104, l3: 0.000097, l4: 0.000326, l5: 0.000517, l6: 0.000691
[epoch: 343/1000, batch:   616/ 1052, ite: 90100] train loss: 0.004855, tar: 0.000231 
l0: 0.000102, l1: 0.000104, l2: 0.000148, l3: 0.000186, l4: 0.000402, l5: 0.000657, l6: 0.001374
[epoch: 343/1000, batch:   696/ 1052, ite: 90120] train loss: 0.004855, tar: 0.000231 
l0: 0.000203, l1: 0.000208, l2: 0.000222, l3: 0.000331, l4: 0.000442, l5: 0.000952, l6: 0.002467
[epoch: 343/1000, batch:   776/ 1052, ite: 90140] train loss: 0.004854, tar: 0.000230 
l0: 0.000127, l1: 0.000133, l2: 0.000161, l3: 0.000193, l4: 0.000310, l5: 0.000836, l6: 0.001220
[epoch: 343/1000, batch:   856/ 1052, ite: 90160] train loss: 0.004851, tar: 0.000230 
l0: 0.000135, l1: 0.000143, l2: 0.000161, l3: 0.000203, l4: 0.000307, l5: 0.000642, l6: 0.001812
[epoch: 343/1000, batch:   936/ 1052, ite: 90180] train loss: 0.004854, tar: 0.000230 
l0: 0.000199, l1: 0.000200, l2: 0.000223, l3: 0.000340, l4: 0.000701, l5: 0.001048, l6: 0.001661
[epoch: 343/1000, batch:  1016/ 1052, ite: 90200] train loss: 0.004854, tar: 0.000230 
[Epoch 343/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000135, l1: 0.000134, l2: 0.000199, l3: 0.000248, l4: 0.000530, l5: 0.001190, l6: 0.001766
[epoch: 344/1000, batch:    44/ 1052, ite: 90220] train loss: 0.004852, tar: 0.000230 
l0: 0.000134, l1: 0.000141, l2: 0.000151, l3: 0.000188, l4: 0.000434, l5: 0.000680, l6: 0.001307
[epoch: 344/1000, batch:   124/ 1052, ite: 90240] train loss: 0.004849, tar: 0.000230 
l0: 0.000124, l1: 0.000121, l2: 0.000153, l3: 0.000212, l4: 0.000373, l5: 0.000713, l6: 0.001588
[epoch: 344/1000, batch:   204/ 1052, ite: 90260] train loss: 0.004849, tar: 0.000230 
l0: 0.000124, l1: 0.000124, l2: 0.000179, l3: 0.000374, l4: 0.000857, l5: 0.001231, l6: 0.001611
[epoch: 344/1000, batch:   284/ 1052, ite: 90280] train loss: 0.004848, tar: 0.000230 
l0: 0.000236, l1: 0.000238, l2: 0.000242, l3: 0.000294, l4: 0.000632, l5: 0.001382, l6: 0.001576
[epoch: 344/1000, batch:   364/ 1052, ite: 90300] train loss: 0.004849, tar: 0.000230 
l0: 0.000208, l1: 0.000204, l2: 0.000255, l3: 0.000353, l4: 0.000466, l5: 0.001219, l6: 0.001971
[epoch: 344/1000, batch:   444/ 1052, ite: 90320] train loss: 0.004848, tar: 0.000230 
l0: 0.000496, l1: 0.000507, l2: 0.000547, l3: 0.000591, l4: 0.001158, l5: 0.001738, l6: 0.003172
[epoch: 344/1000, batch:   524/ 1052, ite: 90340] train loss: 0.004847, tar: 0.000230 
l0: 0.000160, l1: 0.000165, l2: 0.000193, l3: 0.000219, l4: 0.000402, l5: 0.001105, l6: 0.002005
[epoch: 344/1000, batch:   604/ 1052, ite: 90360] train loss: 0.004846, tar: 0.000230 
l0: 0.000305, l1: 0.000314, l2: 0.000375, l3: 0.000426, l4: 0.000732, l5: 0.001319, l6: 0.003440
[epoch: 344/1000, batch:   684/ 1052, ite: 90380] train loss: 0.004847, tar: 0.000230 
l0: 0.000192, l1: 0.000196, l2: 0.000246, l3: 0.000232, l4: 0.000424, l5: 0.000908, l6: 0.000990
[epoch: 344/1000, batch:   764/ 1052, ite: 90400] train loss: 0.004847, tar: 0.000229 
l0: 0.000133, l1: 0.000129, l2: 0.000178, l3: 0.000261, l4: 0.000539, l5: 0.001053, l6: 0.001708
[epoch: 344/1000, batch:   844/ 1052, ite: 90420] train loss: 0.004847, tar: 0.000229 
l0: 0.000144, l1: 0.000145, l2: 0.000170, l3: 0.000207, l4: 0.000506, l5: 0.000844, l6: 0.001008
[epoch: 344/1000, batch:   924/ 1052, ite: 90440] train loss: 0.004845, tar: 0.000229 
l0: 0.000198, l1: 0.000203, l2: 0.000214, l3: 0.000227, l4: 0.000378, l5: 0.000858, l6: 0.001613
[epoch: 344/1000, batch:  1004/ 1052, ite: 90460] train loss: 0.004847, tar: 0.000229 
[Epoch 344/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000148, l1: 0.000149, l2: 0.000205, l3: 0.000316, l4: 0.000651, l5: 0.001642, l6: 0.002229
[epoch: 345/1000, batch:    32/ 1052, ite: 90480] train loss: 0.004847, tar: 0.000229 
l0: 0.000125, l1: 0.000130, l2: 0.000152, l3: 0.000192, l4: 0.000412, l5: 0.000887, l6: 0.001677
[epoch: 345/1000, batch:   112/ 1052, ite: 90500] train loss: 0.004847, tar: 0.000229 
l0: 0.000126, l1: 0.000127, l2: 0.000144, l3: 0.000245, l4: 0.000597, l5: 0.001082, l6: 0.002542
[epoch: 345/1000, batch:   192/ 1052, ite: 90520] train loss: 0.004846, tar: 0.000229 
l0: 0.000151, l1: 0.000153, l2: 0.000196, l3: 0.000244, l4: 0.000579, l5: 0.001113, l6: 0.001798
[epoch: 345/1000, batch:   272/ 1052, ite: 90540] train loss: 0.004846, tar: 0.000229 
l0: 0.000093, l1: 0.000099, l2: 0.000092, l3: 0.000126, l4: 0.000295, l5: 0.000602, l6: 0.001093
[epoch: 345/1000, batch:   352/ 1052, ite: 90560] train loss: 0.004844, tar: 0.000229 
l0: 0.000322, l1: 0.000346, l2: 0.000512, l3: 0.000333, l4: 0.000541, l5: 0.000794, l6: 0.000507
[epoch: 345/1000, batch:   432/ 1052, ite: 90580] train loss: 0.004844, tar: 0.000229 
l0: 0.000352, l1: 0.000352, l2: 0.000385, l3: 0.000542, l4: 0.001016, l5: 0.001743, l6: 0.003228
[epoch: 345/1000, batch:   512/ 1052, ite: 90600] train loss: 0.004845, tar: 0.000230 
l0: 0.000268, l1: 0.000293, l2: 0.000319, l3: 0.000352, l4: 0.000515, l5: 0.000967, l6: 0.001583
[epoch: 345/1000, batch:   592/ 1052, ite: 90620] train loss: 0.004847, tar: 0.000230 
l0: 0.000567, l1: 0.000590, l2: 0.000705, l3: 0.000650, l4: 0.000862, l5: 0.001874, l6: 0.002764
[epoch: 345/1000, batch:   672/ 1052, ite: 90640] train loss: 0.004850, tar: 0.000230 
l0: 0.000505, l1: 0.000522, l2: 0.000536, l3: 0.000728, l4: 0.001130, l5: 0.001438, l6: 0.002958
[epoch: 345/1000, batch:   752/ 1052, ite: 90660] train loss: 0.004853, tar: 0.000230 
l0: 0.000149, l1: 0.000145, l2: 0.000173, l3: 0.000184, l4: 0.000449, l5: 0.000802, l6: 0.001384
[epoch: 345/1000, batch:   832/ 1052, ite: 90680] train loss: 0.004852, tar: 0.000230 
l0: 0.000181, l1: 0.000177, l2: 0.000249, l3: 0.000273, l4: 0.000547, l5: 0.001080, l6: 0.002747
[epoch: 345/1000, batch:   912/ 1052, ite: 90700] train loss: 0.004853, tar: 0.000231 
l0: 0.000529, l1: 0.000538, l2: 0.000581, l3: 0.000683, l4: 0.001236, l5: 0.003574, l6: 0.006734
[epoch: 345/1000, batch:   992/ 1052, ite: 90720] train loss: 0.004854, tar: 0.000231 
[Epoch 345/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000303, l1: 0.000294, l2: 0.000367, l3: 0.000413, l4: 0.000577, l5: 0.001058, l6: 0.002325
[epoch: 346/1000, batch:    20/ 1052, ite: 90740] train loss: 0.004856, tar: 0.000231 
l0: 0.000134, l1: 0.000270, l2: 0.000140, l3: 0.000226, l4: 0.000497, l5: 0.000773, l6: 0.001260
[epoch: 346/1000, batch:   100/ 1052, ite: 90760] train loss: 0.004857, tar: 0.000231 
l0: 0.000212, l1: 0.000223, l2: 0.000241, l3: 0.000308, l4: 0.000564, l5: 0.000836, l6: 0.001903
[epoch: 346/1000, batch:   180/ 1052, ite: 90780] train loss: 0.004859, tar: 0.000232 
l0: 0.000486, l1: 0.000508, l2: 0.000576, l3: 0.000464, l4: 0.000676, l5: 0.001354, l6: 0.003263
[epoch: 346/1000, batch:   260/ 1052, ite: 90800] train loss: 0.004860, tar: 0.000232 
l0: 0.000466, l1: 0.000452, l2: 0.000557, l3: 0.000674, l4: 0.000885, l5: 0.001450, l6: 0.002328
[epoch: 346/1000, batch:   340/ 1052, ite: 90820] train loss: 0.004862, tar: 0.000232 
l0: 0.000124, l1: 0.000119, l2: 0.000172, l3: 0.000192, l4: 0.000408, l5: 0.001039, l6: 0.001196
[epoch: 346/1000, batch:   420/ 1052, ite: 90840] train loss: 0.004862, tar: 0.000232 
l0: 0.000221, l1: 0.000220, l2: 0.000273, l3: 0.000345, l4: 0.000531, l5: 0.001070, l6: 0.001660
[epoch: 346/1000, batch:   500/ 1052, ite: 90860] train loss: 0.004863, tar: 0.000232 
l0: 0.000904, l1: 0.000898, l2: 0.001015, l3: 0.001138, l4: 0.001335, l5: 0.002130, l6: 0.003584
[epoch: 346/1000, batch:   580/ 1052, ite: 90880] train loss: 0.004865, tar: 0.000233 
l0: 0.000271, l1: 0.000272, l2: 0.000318, l3: 0.000381, l4: 0.000631, l5: 0.001244, l6: 0.001887
[epoch: 346/1000, batch:   660/ 1052, ite: 90900] train loss: 0.004869, tar: 0.000233 
l0: 0.000259, l1: 0.000268, l2: 0.000284, l3: 0.000325, l4: 0.000617, l5: 0.000692, l6: 0.001580
[epoch: 346/1000, batch:   740/ 1052, ite: 90920] train loss: 0.004870, tar: 0.000233 
l0: 0.000396, l1: 0.000409, l2: 0.000395, l3: 0.000339, l4: 0.000697, l5: 0.001055, l6: 0.002227
[epoch: 346/1000, batch:   820/ 1052, ite: 90940] train loss: 0.004871, tar: 0.000234 
l0: 0.000268, l1: 0.000319, l2: 0.000461, l3: 0.000358, l4: 0.000454, l5: 0.000794, l6: 0.001993
[epoch: 346/1000, batch:   900/ 1052, ite: 90960] train loss: 0.004895, tar: 0.000237 
l0: 0.001914, l1: 0.002051, l2: 0.001972, l3: 0.002178, l4: 0.002508, l5: 0.005491, l6: 0.006647
[epoch: 346/1000, batch:   980/ 1052, ite: 90980] train loss: 0.004912, tar: 0.000239 
[Epoch 346/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.001342, l1: 0.001679, l2: 0.000920, l3: 0.001213, l4: 0.001099, l5: 0.001371, l6: 0.002352
[epoch: 347/1000, batch:     8/ 1052, ite: 91000] train loss: 0.004926, tar: 0.000241 
l0: 0.000410, l1: 0.000427, l2: 0.000430, l3: 0.000513, l4: 0.000596, l5: 0.001025, l6: 0.001972
[epoch: 347/1000, batch:    88/ 1052, ite: 91020] train loss: 0.004930, tar: 0.000241 
l0: 0.000291, l1: 0.000313, l2: 0.000324, l3: 0.000356, l4: 0.000645, l5: 0.001182, l6: 0.001740
[epoch: 347/1000, batch:   168/ 1052, ite: 91040] train loss: 0.004938, tar: 0.000242 
l0: 0.000505, l1: 0.000536, l2: 0.000637, l3: 0.000515, l4: 0.001404, l5: 0.001577, l6: 0.002819
[epoch: 347/1000, batch:   248/ 1052, ite: 91060] train loss: 0.004945, tar: 0.000243 
l0: 0.000298, l1: 0.000330, l2: 0.000339, l3: 0.000376, l4: 0.000431, l5: 0.000993, l6: 0.001616
[epoch: 347/1000, batch:   328/ 1052, ite: 91080] train loss: 0.004948, tar: 0.000244 
l0: 0.000348, l1: 0.000373, l2: 0.000363, l3: 0.000361, l4: 0.000538, l5: 0.001287, l6: 0.002743
[epoch: 347/1000, batch:   408/ 1052, ite: 91100] train loss: 0.004952, tar: 0.000244 
l0: 0.000512, l1: 0.000521, l2: 0.000575, l3: 0.000525, l4: 0.000605, l5: 0.001668, l6: 0.002220
[epoch: 347/1000, batch:   488/ 1052, ite: 91120] train loss: 0.004960, tar: 0.000245 
l0: 0.000246, l1: 0.000254, l2: 0.000295, l3: 0.000308, l4: 0.000597, l5: 0.001306, l6: 0.001717
[epoch: 347/1000, batch:   568/ 1052, ite: 91140] train loss: 0.004962, tar: 0.000245 
l0: 0.000463, l1: 0.000471, l2: 0.000462, l3: 0.000464, l4: 0.000648, l5: 0.001234, l6: 0.002237
[epoch: 347/1000, batch:   648/ 1052, ite: 91160] train loss: 0.004966, tar: 0.000246 
l0: 0.000300, l1: 0.000306, l2: 0.000354, l3: 0.000377, l4: 0.000432, l5: 0.000980, l6: 0.001788
[epoch: 347/1000, batch:   728/ 1052, ite: 91180] train loss: 0.004967, tar: 0.000246 
l0: 0.000356, l1: 0.000350, l2: 0.000381, l3: 0.000617, l4: 0.000544, l5: 0.000998, l6: 0.002109
[epoch: 347/1000, batch:   808/ 1052, ite: 91200] train loss: 0.004969, tar: 0.000246 
l0: 0.000255, l1: 0.000247, l2: 0.000278, l3: 0.000411, l4: 0.000525, l5: 0.001129, l6: 0.001856
[epoch: 347/1000, batch:   888/ 1052, ite: 91220] train loss: 0.004970, tar: 0.000247 
l0: 0.000291, l1: 0.000289, l2: 0.000315, l3: 0.000395, l4: 0.000604, l5: 0.000990, l6: 0.001191
[epoch: 347/1000, batch:   968/ 1052, ite: 91240] train loss: 0.004969, tar: 0.000247 
l0: 0.000253, l1: 0.000257, l2: 0.000249, l3: 0.000319, l4: 0.000515, l5: 0.001276, l6: 0.001878
[epoch: 347/1000, batch:  1048/ 1052, ite: 91260] train loss: 0.004970, tar: 0.000247 
[Epoch 347/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000367, l1: 0.000379, l2: 0.000407, l3: 0.000479, l4: 0.000696, l5: 0.001938, l6: 0.003644
[epoch: 348/1000, batch:    76/ 1052, ite: 91280] train loss: 0.004970, tar: 0.000247 
l0: 0.000097, l1: 0.000099, l2: 0.000107, l3: 0.000148, l4: 0.000380, l5: 0.000654, l6: 0.001254
[epoch: 348/1000, batch:   156/ 1052, ite: 91300] train loss: 0.004973, tar: 0.000247 
l0: 0.000334, l1: 0.000325, l2: 0.000370, l3: 0.000484, l4: 0.000650, l5: 0.001780, l6: 0.002574
[epoch: 348/1000, batch:   236/ 1052, ite: 91320] train loss: 0.004974, tar: 0.000247 
l0: 0.000156, l1: 0.000156, l2: 0.000175, l3: 0.000217, l4: 0.000489, l5: 0.000736, l6: 0.001638
[epoch: 348/1000, batch:   316/ 1052, ite: 91340] train loss: 0.004975, tar: 0.000248 
l0: 0.000337, l1: 0.000345, l2: 0.000373, l3: 0.000431, l4: 0.000650, l5: 0.000870, l6: 0.001595
[epoch: 348/1000, batch:   396/ 1052, ite: 91360] train loss: 0.004976, tar: 0.000248 
l0: 0.000396, l1: 0.000375, l2: 0.000411, l3: 0.000656, l4: 0.000929, l5: 0.001416, l6: 0.002836
[epoch: 348/1000, batch:   476/ 1052, ite: 91380] train loss: 0.004977, tar: 0.000248 
l0: 0.000270, l1: 0.000297, l2: 0.000313, l3: 0.000345, l4: 0.000512, l5: 0.000986, l6: 0.001971
[epoch: 348/1000, batch:   556/ 1052, ite: 91400] train loss: 0.004979, tar: 0.000248 
l0: 0.000287, l1: 0.000284, l2: 0.000335, l3: 0.000419, l4: 0.000613, l5: 0.001889, l6: 0.003285
[epoch: 348/1000, batch:   636/ 1052, ite: 91420] train loss: 0.004979, tar: 0.000248 
l0: 0.000230, l1: 0.000243, l2: 0.000263, l3: 0.000299, l4: 0.000498, l5: 0.001087, l6: 0.001945
[epoch: 348/1000, batch:   716/ 1052, ite: 91440] train loss: 0.004982, tar: 0.000249 
l0: 0.000245, l1: 0.000264, l2: 0.000288, l3: 0.000331, l4: 0.000377, l5: 0.001158, l6: 0.002028
[epoch: 348/1000, batch:   796/ 1052, ite: 91460] train loss: 0.004983, tar: 0.000249 
l0: 0.000181, l1: 0.000177, l2: 0.000202, l3: 0.000232, l4: 0.000398, l5: 0.000911, l6: 0.001589
[epoch: 348/1000, batch:   876/ 1052, ite: 91480] train loss: 0.004983, tar: 0.000249 
l0: 0.000198, l1: 0.000194, l2: 0.000226, l3: 0.000289, l4: 0.000580, l5: 0.000887, l6: 0.001795
[epoch: 348/1000, batch:   956/ 1052, ite: 91500] train loss: 0.004983, tar: 0.000249 
l0: 0.000156, l1: 0.000162, l2: 0.000172, l3: 0.000214, l4: 0.000369, l5: 0.000898, l6: 0.002136
[epoch: 348/1000, batch:  1036/ 1052, ite: 91520] train loss: 0.004984, tar: 0.000249 
[Epoch 348/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000131, l1: 0.000137, l2: 0.000137, l3: 0.000155, l4: 0.000397, l5: 0.000589, l6: 0.001157
[epoch: 349/1000, batch:    64/ 1052, ite: 91540] train loss: 0.004983, tar: 0.000249 
l0: 0.000117, l1: 0.000117, l2: 0.000132, l3: 0.000175, l4: 0.000436, l5: 0.000793, l6: 0.001368
[epoch: 349/1000, batch:   144/ 1052, ite: 91560] train loss: 0.004984, tar: 0.000249 
l0: 0.000201, l1: 0.000198, l2: 0.000235, l3: 0.000281, l4: 0.000429, l5: 0.001069, l6: 0.001813
[epoch: 349/1000, batch:   224/ 1052, ite: 91580] train loss: 0.004985, tar: 0.000249 
l0: 0.000245, l1: 0.000260, l2: 0.000268, l3: 0.000316, l4: 0.000502, l5: 0.001179, l6: 0.001822
[epoch: 349/1000, batch:   304/ 1052, ite: 91600] train loss: 0.004985, tar: 0.000249 
l0: 0.000199, l1: 0.000204, l2: 0.000230, l3: 0.000264, l4: 0.000418, l5: 0.001005, l6: 0.001615
[epoch: 349/1000, batch:   384/ 1052, ite: 91620] train loss: 0.004986, tar: 0.000249 
l0: 0.000369, l1: 0.000397, l2: 0.000404, l3: 0.000414, l4: 0.000588, l5: 0.001724, l6: 0.002973
[epoch: 349/1000, batch:   464/ 1052, ite: 91640] train loss: 0.004988, tar: 0.000249 
l0: 0.000381, l1: 0.000382, l2: 0.000391, l3: 0.000569, l4: 0.000751, l5: 0.001252, l6: 0.003871
[epoch: 349/1000, batch:   544/ 1052, ite: 91660] train loss: 0.004988, tar: 0.000249 
l0: 0.000251, l1: 0.000256, l2: 0.000289, l3: 0.000352, l4: 0.000492, l5: 0.001695, l6: 0.003021
[epoch: 349/1000, batch:   624/ 1052, ite: 91680] train loss: 0.004988, tar: 0.000249 
l0: 0.000226, l1: 0.000234, l2: 0.000234, l3: 0.000261, l4: 0.000426, l5: 0.000742, l6: 0.001143
[epoch: 349/1000, batch:   704/ 1052, ite: 91700] train loss: 0.004989, tar: 0.000249 
l0: 0.000403, l1: 0.000390, l2: 0.000457, l3: 0.000563, l4: 0.000889, l5: 0.001699, l6: 0.002804
[epoch: 349/1000, batch:   784/ 1052, ite: 91720] train loss: 0.004989, tar: 0.000249 
l0: 0.000162, l1: 0.000175, l2: 0.000171, l3: 0.000199, l4: 0.000279, l5: 0.001043, l6: 0.001386
[epoch: 349/1000, batch:   864/ 1052, ite: 91740] train loss: 0.004989, tar: 0.000249 
l0: 0.000195, l1: 0.000199, l2: 0.000203, l3: 0.000238, l4: 0.000484, l5: 0.000822, l6: 0.001265
[epoch: 349/1000, batch:   944/ 1052, ite: 91760] train loss: 0.004989, tar: 0.000249 
l0: 0.000162, l1: 0.000159, l2: 0.000209, l3: 0.000286, l4: 0.000697, l5: 0.001224, l6: 0.001757
[epoch: 349/1000, batch:  1024/ 1052, ite: 91780] train loss: 0.004989, tar: 0.000249 
[Epoch 349/1000] Test loss: 0.009, Test target loss: 0.001
l0: 0.000117, l1: 0.000124, l2: 0.000129, l3: 0.000174, l4: 0.000306, l5: 0.000731, l6: 0.001313
[epoch: 350/1000, batch:    52/ 1052, ite: 91800] train loss: 0.004990, tar: 0.000250 
l0: 0.000293, l1: 0.000289, l2: 0.000319, l3: 0.000565, l4: 0.000796, l5: 0.001350, l6: 0.001520
[epoch: 350/1000, batch:   132/ 1052, ite: 91820] train loss: 0.004990, tar: 0.000250 
l0: 0.000496, l1: 0.000514, l2: 0.000564, l3: 0.000659, l4: 0.000935, l5: 0.002193, l6: 0.003954
[epoch: 350/1000, batch:   212/ 1052, ite: 91840] train loss: 0.004991, tar: 0.000250 
l0: 0.000150, l1: 0.000131, l2: 0.000146, l3: 0.000362, l4: 0.000525, l5: 0.000954, l6: 0.001083
[epoch: 350/1000, batch:   292/ 1052, ite: 91860] train loss: 0.004993, tar: 0.000250 
l0: 0.000257, l1: 0.000261, l2: 0.000320, l3: 0.000347, l4: 0.000478, l5: 0.001026, l6: 0.002337
[epoch: 350/1000, batch:   372/ 1052, ite: 91880] train loss: 0.004993, tar: 0.000250 
l0: 0.000171, l1: 0.000164, l2: 0.000225, l3: 0.000295, l4: 0.000558, l5: 0.001056, l6: 0.002536
[epoch: 350/1000, batch:   452/ 1052, ite: 91900] train loss: 0.004993, tar: 0.000250 
l0: 0.000232, l1: 0.000236, l2: 0.000286, l3: 0.000376, l4: 0.000625, l5: 0.001202, l6: 0.001262
[epoch: 350/1000, batch:   532/ 1052, ite: 91920] train loss: 0.004992, tar: 0.000249 
l0: 0.000174, l1: 0.000164, l2: 0.000190, l3: 0.000258, l4: 0.000470, l5: 0.000877, l6: 0.001380
[epoch: 350/1000, batch:   612/ 1052, ite: 91940] train loss: 0.004989, tar: 0.000249 
l0: 0.000166, l1: 0.000193, l2: 0.000197, l3: 0.000185, l4: 0.000319, l5: 0.000782, l6: 0.001850
[epoch: 350/1000, batch:   692/ 1052, ite: 91960] train loss: 0.004989, tar: 0.000249 
l0: 0.000163, l1: 0.000152, l2: 0.000197, l3: 0.000319, l4: 0.000559, l5: 0.001067, l6: 0.001567
[epoch: 350/1000, batch:   772/ 1052, ite: 91980] train loss: 0.004989, tar: 0.000249 
l0: 0.000307, l1: 0.000318, l2: 0.000346, l3: 0.000396, l4: 0.000633, l5: 0.001100, l6: 0.002197
[epoch: 350/1000, batch:   852/ 1052, ite: 92000] train loss: 0.004988, tar: 0.000249 
l0: 0.000204, l1: 0.000205, l2: 0.000227, l3: 0.000229, l4: 0.000410, l5: 0.001005, l6: 0.001151
[epoch: 350/1000, batch:   932/ 1052, ite: 92020] train loss: 0.004989, tar: 0.000249 
l0: 0.000316, l1: 0.000319, l2: 0.000380, l3: 0.000402, l4: 0.000650, l5: 0.001372, l6: 0.002222
[epoch: 350/1000, batch:  1012/ 1052, ite: 92040] train loss: 0.004989, tar: 0.000249 
[Epoch 350/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000282, l1: 0.000277, l2: 0.000323, l3: 0.000423, l4: 0.000887, l5: 0.001425, l6: 0.001875
[epoch: 351/1000, batch:    40/ 1052, ite: 92060] train loss: 0.004987, tar: 0.000249 
l0: 0.000305, l1: 0.000285, l2: 0.000340, l3: 0.000541, l4: 0.001091, l5: 0.001818, l6: 0.002682
[epoch: 351/1000, batch:   120/ 1052, ite: 92080] train loss: 0.004987, tar: 0.000249 
l0: 0.000233, l1: 0.000231, l2: 0.000273, l3: 0.000322, l4: 0.000693, l5: 0.000919, l6: 0.001952
[epoch: 351/1000, batch:   200/ 1052, ite: 92100] train loss: 0.004987, tar: 0.000249 
l0: 0.000168, l1: 0.000164, l2: 0.000224, l3: 0.000301, l4: 0.000623, l5: 0.001302, l6: 0.002102
[epoch: 351/1000, batch:   280/ 1052, ite: 92120] train loss: 0.004986, tar: 0.000249 
l0: 0.000236, l1: 0.000251, l2: 0.000263, l3: 0.000280, l4: 0.000644, l5: 0.001223, l6: 0.001738
[epoch: 351/1000, batch:   360/ 1052, ite: 92140] train loss: 0.004985, tar: 0.000249 
l0: 0.000229, l1: 0.000247, l2: 0.000237, l3: 0.000263, l4: 0.000464, l5: 0.001165, l6: 0.002206
[epoch: 351/1000, batch:   440/ 1052, ite: 92160] train loss: 0.004985, tar: 0.000249 
l0: 0.000133, l1: 0.000134, l2: 0.000152, l3: 0.000206, l4: 0.000381, l5: 0.000797, l6: 0.001703
[epoch: 351/1000, batch:   520/ 1052, ite: 92180] train loss: 0.004987, tar: 0.000249 
l0: 0.000251, l1: 0.000254, l2: 0.000223, l3: 0.000310, l4: 0.000633, l5: 0.001164, l6: 0.001400
[epoch: 351/1000, batch:   600/ 1052, ite: 92200] train loss: 0.004986, tar: 0.000249 
l0: 0.000265, l1: 0.000270, l2: 0.000296, l3: 0.000436, l4: 0.000886, l5: 0.001353, l6: 0.002761
[epoch: 351/1000, batch:   680/ 1052, ite: 92220] train loss: 0.004986, tar: 0.000249 
l0: 0.000198, l1: 0.000197, l2: 0.000216, l3: 0.000292, l4: 0.000785, l5: 0.001018, l6: 0.001904
[epoch: 351/1000, batch:   760/ 1052, ite: 92240] train loss: 0.004986, tar: 0.000249 
l0: 0.000253, l1: 0.000255, l2: 0.000280, l3: 0.000320, l4: 0.000591, l5: 0.000984, l6: 0.002110
[epoch: 351/1000, batch:   840/ 1052, ite: 92260] train loss: 0.004985, tar: 0.000249 
l0: 0.000517, l1: 0.000494, l2: 0.000562, l3: 0.000571, l4: 0.000835, l5: 0.001973, l6: 0.002456
[epoch: 351/1000, batch:   920/ 1052, ite: 92280] train loss: 0.004986, tar: 0.000249 
l0: 0.000329, l1: 0.000352, l2: 0.000354, l3: 0.000442, l4: 0.000646, l5: 0.001422, l6: 0.002776
[epoch: 351/1000, batch:  1000/ 1052, ite: 92300] train loss: 0.004986, tar: 0.000249 
[Epoch 351/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000202, l1: 0.000208, l2: 0.000245, l3: 0.000391, l4: 0.000801, l5: 0.001659, l6: 0.002286
[epoch: 352/1000, batch:    28/ 1052, ite: 92320] train loss: 0.004986, tar: 0.000249 
l0: 0.000420, l1: 0.000435, l2: 0.000447, l3: 0.000546, l4: 0.000977, l5: 0.001860, l6: 0.003811
[epoch: 352/1000, batch:   108/ 1052, ite: 92340] train loss: 0.004987, tar: 0.000249 
l0: 0.000203, l1: 0.000203, l2: 0.000266, l3: 0.000371, l4: 0.000717, l5: 0.001820, l6: 0.002962
[epoch: 352/1000, batch:   188/ 1052, ite: 92360] train loss: 0.004986, tar: 0.000249 
l0: 0.000336, l1: 0.000336, l2: 0.000405, l3: 0.000438, l4: 0.000675, l5: 0.001204, l6: 0.002007
[epoch: 352/1000, batch:   268/ 1052, ite: 92380] train loss: 0.004985, tar: 0.000249 
l0: 0.000118, l1: 0.000122, l2: 0.000126, l3: 0.000173, l4: 0.000497, l5: 0.000797, l6: 0.001375
[epoch: 352/1000, batch:   348/ 1052, ite: 92400] train loss: 0.004987, tar: 0.000249 
l0: 0.000250, l1: 0.000251, l2: 0.000349, l3: 0.000456, l4: 0.000828, l5: 0.001666, l6: 0.003099
[epoch: 352/1000, batch:   428/ 1052, ite: 92420] train loss: 0.004987, tar: 0.000249 
l0: 0.000159, l1: 0.000149, l2: 0.000211, l3: 0.000276, l4: 0.000637, l5: 0.001250, l6: 0.002183
[epoch: 352/1000, batch:   508/ 1052, ite: 92440] train loss: 0.004987, tar: 0.000249 
l0: 0.000368, l1: 0.000381, l2: 0.000387, l3: 0.000439, l4: 0.000698, l5: 0.001335, l6: 0.002711
[epoch: 352/1000, batch:   588/ 1052, ite: 92460] train loss: 0.004985, tar: 0.000249 
l0: 0.000133, l1: 0.000135, l2: 0.000165, l3: 0.000215, l4: 0.000392, l5: 0.000595, l6: 0.001102
[epoch: 352/1000, batch:   668/ 1052, ite: 92480] train loss: 0.004984, tar: 0.000248 
l0: 0.000283, l1: 0.000273, l2: 0.000351, l3: 0.000642, l4: 0.001124, l5: 0.002069, l6: 0.003154
[epoch: 352/1000, batch:   748/ 1052, ite: 92500] train loss: 0.004983, tar: 0.000248 
l0: 0.000234, l1: 0.000246, l2: 0.000233, l3: 0.000269, l4: 0.000759, l5: 0.001743, l6: 0.002924
[epoch: 352/1000, batch:   828/ 1052, ite: 92520] train loss: 0.004982, tar: 0.000248 
l0: 0.000186, l1: 0.000194, l2: 0.000189, l3: 0.000205, l4: 0.000317, l5: 0.000674, l6: 0.001552
[epoch: 352/1000, batch:   908/ 1052, ite: 92540] train loss: 0.004982, tar: 0.000248 
l0: 0.000199, l1: 0.000208, l2: 0.000244, l3: 0.000293, l4: 0.000456, l5: 0.001115, l6: 0.002818
[epoch: 352/1000, batch:   988/ 1052, ite: 92560] train loss: 0.004982, tar: 0.000248 
[Epoch 352/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000329, l1: 0.000343, l2: 0.000410, l3: 0.000435, l4: 0.001127, l5: 0.002664, l6: 0.003699
[epoch: 353/1000, batch:    16/ 1052, ite: 92580] train loss: 0.004983, tar: 0.000248 
l0: 0.000195, l1: 0.000187, l2: 0.000215, l3: 0.000274, l4: 0.000567, l5: 0.000720, l6: 0.001779
[epoch: 353/1000, batch:    96/ 1052, ite: 92600] train loss: 0.004983, tar: 0.000248 
l0: 0.000143, l1: 0.000142, l2: 0.000154, l3: 0.000219, l4: 0.000529, l5: 0.001000, l6: 0.001611
[epoch: 353/1000, batch:   176/ 1052, ite: 92620] train loss: 0.004981, tar: 0.000248 
l0: 0.000366, l1: 0.000365, l2: 0.000380, l3: 0.000588, l4: 0.000977, l5: 0.002346, l6: 0.003418
[epoch: 353/1000, batch:   256/ 1052, ite: 92640] train loss: 0.004981, tar: 0.000248 
l0: 0.000144, l1: 0.000147, l2: 0.000153, l3: 0.000208, l4: 0.000320, l5: 0.000822, l6: 0.001293
[epoch: 353/1000, batch:   336/ 1052, ite: 92660] train loss: 0.004980, tar: 0.000248 
l0: 0.000136, l1: 0.000141, l2: 0.000168, l3: 0.000295, l4: 0.000641, l5: 0.000948, l6: 0.002102
[epoch: 353/1000, batch:   416/ 1052, ite: 92680] train loss: 0.004980, tar: 0.000248 
l0: 0.000154, l1: 0.000158, l2: 0.000196, l3: 0.000223, l4: 0.000542, l5: 0.001012, l6: 0.001236
[epoch: 353/1000, batch:   496/ 1052, ite: 92700] train loss: 0.004979, tar: 0.000247 
l0: 0.000237, l1: 0.000232, l2: 0.000285, l3: 0.000314, l4: 0.000484, l5: 0.001283, l6: 0.003004
[epoch: 353/1000, batch:   576/ 1052, ite: 92720] train loss: 0.004979, tar: 0.000247 
l0: 0.000184, l1: 0.000180, l2: 0.000236, l3: 0.000319, l4: 0.000613, l5: 0.001259, l6: 0.002864
[epoch: 353/1000, batch:   656/ 1052, ite: 92740] train loss: 0.004978, tar: 0.000247 
l0: 0.000103, l1: 0.000096, l2: 0.000152, l3: 0.000237, l4: 0.000411, l5: 0.001035, l6: 0.001997
[epoch: 353/1000, batch:   736/ 1052, ite: 92760] train loss: 0.004979, tar: 0.000247 
l0: 0.000274, l1: 0.000275, l2: 0.000307, l3: 0.000426, l4: 0.000687, l5: 0.001134, l6: 0.002832
[epoch: 353/1000, batch:   816/ 1052, ite: 92780] train loss: 0.004977, tar: 0.000247 
l0: 0.000210, l1: 0.000203, l2: 0.000297, l3: 0.000554, l4: 0.000820, l5: 0.001810, l6: 0.003194
[epoch: 353/1000, batch:   896/ 1052, ite: 92800] train loss: 0.004978, tar: 0.000247 
l0: 0.000096, l1: 0.000093, l2: 0.000129, l3: 0.000182, l4: 0.000334, l5: 0.000746, l6: 0.001082
[epoch: 353/1000, batch:   976/ 1052, ite: 92820] train loss: 0.004977, tar: 0.000247 
[Epoch 353/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000176, l1: 0.000170, l2: 0.000206, l3: 0.000271, l4: 0.000364, l5: 0.000862, l6: 0.002059
[epoch: 354/1000, batch:     4/ 1052, ite: 92840] train loss: 0.004978, tar: 0.000247 
l0: 0.000202, l1: 0.000196, l2: 0.000267, l3: 0.000281, l4: 0.000658, l5: 0.001576, l6: 0.002423
[epoch: 354/1000, batch:    84/ 1052, ite: 92860] train loss: 0.004977, tar: 0.000247 
l0: 0.000285, l1: 0.000297, l2: 0.000288, l3: 0.000324, l4: 0.000748, l5: 0.001321, l6: 0.002907
[epoch: 354/1000, batch:   164/ 1052, ite: 92880] train loss: 0.004978, tar: 0.000247 
l0: 0.000145, l1: 0.000144, l2: 0.000200, l3: 0.000290, l4: 0.000557, l5: 0.000792, l6: 0.001453
[epoch: 354/1000, batch:   244/ 1052, ite: 92900] train loss: 0.004977, tar: 0.000247 
l0: 0.000131, l1: 0.000144, l2: 0.000144, l3: 0.000172, l4: 0.000446, l5: 0.000744, l6: 0.002000
[epoch: 354/1000, batch:   324/ 1052, ite: 92920] train loss: 0.004976, tar: 0.000247 
l0: 0.000237, l1: 0.000249, l2: 0.000263, l3: 0.000366, l4: 0.000536, l5: 0.000793, l6: 0.001655
[epoch: 354/1000, batch:   404/ 1052, ite: 92940] train loss: 0.004975, tar: 0.000246 
l0: 0.000222, l1: 0.000221, l2: 0.000276, l3: 0.000350, l4: 0.000758, l5: 0.001881, l6: 0.002388
[epoch: 354/1000, batch:   484/ 1052, ite: 92960] train loss: 0.004974, tar: 0.000246 
l0: 0.000086, l1: 0.000090, l2: 0.000103, l3: 0.000175, l4: 0.000339, l5: 0.000849, l6: 0.001254
[epoch: 354/1000, batch:   564/ 1052, ite: 92980] train loss: 0.004974, tar: 0.000246 
l0: 0.000147, l1: 0.000154, l2: 0.000175, l3: 0.000244, l4: 0.000279, l5: 0.000436, l6: 0.001031
[epoch: 354/1000, batch:   644/ 1052, ite: 93000] train loss: 0.004976, tar: 0.000246 
l0: 0.000455, l1: 0.000379, l2: 0.000438, l3: 0.000640, l4: 0.001058, l5: 0.001648, l6: 0.002572
[epoch: 354/1000, batch:   724/ 1052, ite: 93020] train loss: 0.004975, tar: 0.000246 
l0: 0.000173, l1: 0.000176, l2: 0.000191, l3: 0.000277, l4: 0.000469, l5: 0.000834, l6: 0.002667
[epoch: 354/1000, batch:   804/ 1052, ite: 93040] train loss: 0.004973, tar: 0.000246 
l0: 0.000145, l1: 0.000156, l2: 0.000199, l3: 0.000197, l4: 0.000295, l5: 0.000795, l6: 0.002102
[epoch: 354/1000, batch:   884/ 1052, ite: 93060] train loss: 0.004971, tar: 0.000246 
l0: 0.000173, l1: 0.000179, l2: 0.000208, l3: 0.000276, l4: 0.000495, l5: 0.000945, l6: 0.002033
[epoch: 354/1000, batch:   964/ 1052, ite: 93080] train loss: 0.004969, tar: 0.000246 
l0: 0.000205, l1: 0.000209, l2: 0.000210, l3: 0.000296, l4: 0.000431, l5: 0.001695, l6: 0.002874
[epoch: 354/1000, batch:  1044/ 1052, ite: 93100] train loss: 0.004969, tar: 0.000246 
[Epoch 354/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000146, l1: 0.000140, l2: 0.000196, l3: 0.000264, l4: 0.000316, l5: 0.000512, l6: 0.000690
[epoch: 355/1000, batch:    72/ 1052, ite: 93120] train loss: 0.004968, tar: 0.000246 
l0: 0.000075, l1: 0.000071, l2: 0.000108, l3: 0.000217, l4: 0.000255, l5: 0.000759, l6: 0.001228
[epoch: 355/1000, batch:   152/ 1052, ite: 93140] train loss: 0.004966, tar: 0.000246 
l0: 0.000114, l1: 0.000111, l2: 0.000162, l3: 0.000215, l4: 0.000476, l5: 0.000834, l6: 0.001354
[epoch: 355/1000, batch:   232/ 1052, ite: 93160] train loss: 0.004965, tar: 0.000245 
l0: 0.000244, l1: 0.000249, l2: 0.000296, l3: 0.000514, l4: 0.000898, l5: 0.001649, l6: 0.002510
[epoch: 355/1000, batch:   312/ 1052, ite: 93180] train loss: 0.004966, tar: 0.000245 
l0: 0.000270, l1: 0.000270, l2: 0.000332, l3: 0.000542, l4: 0.000914, l5: 0.001578, l6: 0.002889
[epoch: 355/1000, batch:   392/ 1052, ite: 93200] train loss: 0.004965, tar: 0.000245 
l0: 0.000192, l1: 0.000186, l2: 0.000247, l3: 0.000405, l4: 0.000725, l5: 0.001558, l6: 0.002693
[epoch: 355/1000, batch:   472/ 1052, ite: 93220] train loss: 0.004965, tar: 0.000245 
l0: 0.000480, l1: 0.000498, l2: 0.000584, l3: 0.000581, l4: 0.000994, l5: 0.002505, l6: 0.004858
[epoch: 355/1000, batch:   552/ 1052, ite: 93240] train loss: 0.004964, tar: 0.000245 
l0: 0.000134, l1: 0.000133, l2: 0.000218, l3: 0.000279, l4: 0.000575, l5: 0.001581, l6: 0.002177
[epoch: 355/1000, batch:   632/ 1052, ite: 93260] train loss: 0.004964, tar: 0.000245 
l0: 0.000721, l1: 0.000729, l2: 0.000766, l3: 0.000808, l4: 0.000907, l5: 0.001111, l6: 0.002477
[epoch: 355/1000, batch:   712/ 1052, ite: 93280] train loss: 0.004964, tar: 0.000245 
l0: 0.000169, l1: 0.000170, l2: 0.000207, l3: 0.000213, l4: 0.000512, l5: 0.000687, l6: 0.001783
[epoch: 355/1000, batch:   792/ 1052, ite: 93300] train loss: 0.004963, tar: 0.000245 
l0: 0.000622, l1: 0.000621, l2: 0.000684, l3: 0.000866, l4: 0.001132, l5: 0.002846, l6: 0.003860
[epoch: 355/1000, batch:   872/ 1052, ite: 93320] train loss: 0.004963, tar: 0.000245 
l0: 0.000281, l1: 0.000290, l2: 0.000335, l3: 0.000369, l4: 0.000836, l5: 0.002103, l6: 0.002620
[epoch: 355/1000, batch:   952/ 1052, ite: 93340] train loss: 0.004964, tar: 0.000245 
l0: 0.000168, l1: 0.000171, l2: 0.000220, l3: 0.000301, l4: 0.000636, l5: 0.001112, l6: 0.001307
[epoch: 355/1000, batch:  1032/ 1052, ite: 93360] train loss: 0.004963, tar: 0.000245 
[Epoch 355/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000097, l1: 0.000097, l2: 0.000131, l3: 0.000214, l4: 0.000389, l5: 0.001094, l6: 0.001662
[epoch: 356/1000, batch:    60/ 1052, ite: 93380] train loss: 0.004963, tar: 0.000245 
l0: 0.000092, l1: 0.000092, l2: 0.000117, l3: 0.000198, l4: 0.000341, l5: 0.000659, l6: 0.001272
[epoch: 356/1000, batch:   140/ 1052, ite: 93400] train loss: 0.004962, tar: 0.000245 
l0: 0.000219, l1: 0.000215, l2: 0.000235, l3: 0.000337, l4: 0.000480, l5: 0.000933, l6: 0.001592
[epoch: 356/1000, batch:   220/ 1052, ite: 93420] train loss: 0.004961, tar: 0.000244 
l0: 0.000164, l1: 0.000166, l2: 0.000218, l3: 0.000296, l4: 0.000551, l5: 0.001195, l6: 0.002607
[epoch: 356/1000, batch:   300/ 1052, ite: 93440] train loss: 0.004960, tar: 0.000244 
l0: 0.000188, l1: 0.000191, l2: 0.000209, l3: 0.000237, l4: 0.000361, l5: 0.000837, l6: 0.001628
[epoch: 356/1000, batch:   380/ 1052, ite: 93460] train loss: 0.004960, tar: 0.000244 
l0: 0.000236, l1: 0.000254, l2: 0.000217, l3: 0.000363, l4: 0.000623, l5: 0.000824, l6: 0.002153
[epoch: 356/1000, batch:   460/ 1052, ite: 93480] train loss: 0.004962, tar: 0.000244 
l0: 0.000123, l1: 0.000120, l2: 0.000130, l3: 0.000219, l4: 0.000457, l5: 0.000618, l6: 0.000775
[epoch: 356/1000, batch:   540/ 1052, ite: 93500] train loss: 0.004961, tar: 0.000244 
l0: 0.000234, l1: 0.000226, l2: 0.000282, l3: 0.000418, l4: 0.000636, l5: 0.001254, l6: 0.001896
[epoch: 356/1000, batch:   620/ 1052, ite: 93520] train loss: 0.004960, tar: 0.000244 
l0: 0.000216, l1: 0.000228, l2: 0.000241, l3: 0.000316, l4: 0.000703, l5: 0.001035, l6: 0.002404
[epoch: 356/1000, batch:   700/ 1052, ite: 93540] train loss: 0.004961, tar: 0.000244 
l0: 0.000347, l1: 0.000360, l2: 0.000379, l3: 0.000447, l4: 0.000624, l5: 0.001215, l6: 0.002040
[epoch: 356/1000, batch:   780/ 1052, ite: 93560] train loss: 0.004960, tar: 0.000244 
l0: 0.000324, l1: 0.000332, l2: 0.000358, l3: 0.000437, l4: 0.000588, l5: 0.001161, l6: 0.003104
[epoch: 356/1000, batch:   860/ 1052, ite: 93580] train loss: 0.004960, tar: 0.000244 
l0: 0.000179, l1: 0.000187, l2: 0.000215, l3: 0.000324, l4: 0.000636, l5: 0.001408, l6: 0.001592
[epoch: 356/1000, batch:   940/ 1052, ite: 93600] train loss: 0.004960, tar: 0.000244 
l0: 0.000141, l1: 0.000135, l2: 0.000153, l3: 0.000228, l4: 0.000363, l5: 0.000821, l6: 0.001242
[epoch: 356/1000, batch:  1020/ 1052, ite: 93620] train loss: 0.004960, tar: 0.000244 
[Epoch 356/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000570, l1: 0.000565, l2: 0.000631, l3: 0.000835, l4: 0.001493, l5: 0.002723, l6: 0.004232
[epoch: 357/1000, batch:    48/ 1052, ite: 93640] train loss: 0.004961, tar: 0.000244 
l0: 0.000439, l1: 0.000446, l2: 0.000431, l3: 0.000534, l4: 0.000899, l5: 0.001658, l6: 0.002496
[epoch: 357/1000, batch:   128/ 1052, ite: 93660] train loss: 0.004961, tar: 0.000244 
l0: 0.000136, l1: 0.000144, l2: 0.000153, l3: 0.000191, l4: 0.000267, l5: 0.000706, l6: 0.001770
[epoch: 357/1000, batch:   208/ 1052, ite: 93680] train loss: 0.004961, tar: 0.000244 
l0: 0.000135, l1: 0.000134, l2: 0.000151, l3: 0.000229, l4: 0.000573, l5: 0.001165, l6: 0.001831
[epoch: 357/1000, batch:   288/ 1052, ite: 93700] train loss: 0.004961, tar: 0.000244 
l0: 0.000105, l1: 0.000107, l2: 0.000122, l3: 0.000174, l4: 0.000458, l5: 0.000657, l6: 0.001160
[epoch: 357/1000, batch:   368/ 1052, ite: 93720] train loss: 0.004960, tar: 0.000244 
l0: 0.000137, l1: 0.000145, l2: 0.000167, l3: 0.000254, l4: 0.000529, l5: 0.001458, l6: 0.002425
[epoch: 357/1000, batch:   448/ 1052, ite: 93740] train loss: 0.004959, tar: 0.000244 
l0: 0.000156, l1: 0.000166, l2: 0.000183, l3: 0.000294, l4: 0.000502, l5: 0.000980, l6: 0.002090
[epoch: 357/1000, batch:   528/ 1052, ite: 93760] train loss: 0.004958, tar: 0.000244 
l0: 0.000190, l1: 0.000196, l2: 0.000247, l3: 0.000295, l4: 0.000635, l5: 0.001222, l6: 0.002411
[epoch: 357/1000, batch:   608/ 1052, ite: 93780] train loss: 0.004957, tar: 0.000244 
l0: 0.000558, l1: 0.000583, l2: 0.000625, l3: 0.000806, l4: 0.001312, l5: 0.001984, l6: 0.005437
[epoch: 357/1000, batch:   688/ 1052, ite: 93800] train loss: 0.004957, tar: 0.000243 
l0: 0.000164, l1: 0.000166, l2: 0.000201, l3: 0.000262, l4: 0.000340, l5: 0.000833, l6: 0.002461
[epoch: 357/1000, batch:   768/ 1052, ite: 93820] train loss: 0.004957, tar: 0.000243 
l0: 0.000227, l1: 0.000233, l2: 0.000278, l3: 0.000338, l4: 0.000581, l5: 0.001089, l6: 0.002775
[epoch: 357/1000, batch:   848/ 1052, ite: 93840] train loss: 0.004957, tar: 0.000243 
l0: 0.000207, l1: 0.000212, l2: 0.000215, l3: 0.000308, l4: 0.000545, l5: 0.001015, l6: 0.002551
[epoch: 357/1000, batch:   928/ 1052, ite: 93860] train loss: 0.004957, tar: 0.000243 
l0: 0.000225, l1: 0.000224, l2: 0.000307, l3: 0.000376, l4: 0.000990, l5: 0.001599, l6: 0.002269
[epoch: 357/1000, batch:  1008/ 1052, ite: 93880] train loss: 0.004957, tar: 0.000243 
[Epoch 357/1000] Test loss: 0.010, Test target loss: 0.001
l0: 0.000282, l1: 0.000279, l2: 0.000338, l3: 0.000454, l4: 0.000851, l5: 0.002231, l6: 0.003214
[epoch: 358/1000, batch:    36/ 1052, ite: 93900] train loss: 0.004957, tar: 0.000243 
l0: 0.000234, l1: 0.000232, l2: 0.000281, l3: 0.000408, l4: 0.000670, l5: 0.000922, l6: 0.001841
[epoch: 358/1000, batch:   116/ 1052, ite: 93920] train loss: 0.004957, tar: 0.000243 
l0: 0.000347, l1: 0.000343, l2: 0.000393, l3: 0.000478, l4: 0.000846, l5: 0.001965, l6: 0.003231
[epoch: 358/1000, batch:   196/ 1052, ite: 93940] train loss: 0.004957, tar: 0.000243 
l0: 0.000165, l1: 0.000162, l2: 0.000209, l3: 0.000313, l4: 0.000724, l5: 0.001007, l6: 0.001818
[epoch: 358/1000, batch:   276/ 1052, ite: 93960] train loss: 0.004957, tar: 0.000243 
l0: 0.000355, l1: 0.000361, l2: 0.000393, l3: 0.000466, l4: 0.000725, l5: 0.001260, l6: 0.002276
[epoch: 358/1000, batch:   356/ 1052, ite: 93980] train loss: 0.004958, tar: 0.000243 
l0: 0.000301, l1: 0.000305, l2: 0.000347, l3: 0.000458, l4: 0.000839, l5: 0.001230, l6: 0.004981
[epoch: 358/1000, batch:   436/ 1052, ite: 94000] train loss: 0.004958, tar: 0.000243 
l0: 0.000184, l1: 0.000193, l2: 0.000212, l3: 0.000246, l4: 0.000414, l5: 0.000821, l6: 0.001391
[epoch: 358/1000, batch:   516/ 1052, ite: 94020] train loss: 0.004955, tar: 0.000243 
l0: 0.000207, l1: 0.000217, l2: 0.000210, l3: 0.000227, l4: 0.000391, l5: 0.001178, l6: 0.001977
[epoch: 358/1000, batch:   596/ 1052, ite: 94040] train loss: 0.004954, tar: 0.000243 
l0: 0.000228, l1: 0.000226, l2: 0.000255, l3: 0.000288, l4: 0.000406, l5: 0.000811, l6: 0.002008
[epoch: 358/1000, batch:   676/ 1052, ite: 94060] train loss: 0.004954, tar: 0.000243 
l0: 0.000123, l1: 0.000125, l2: 0.000152, l3: 0.000224, l4: 0.000489, l5: 0.000643, l6: 0.001390
[epoch: 358/1000, batch:   756/ 1052, ite: 94080] train loss: 0.004954, tar: 0.000242 
l0: 0.000168, l1: 0.000158, l2: 0.000222, l3: 0.000274, l4: 0.000524, l5: 0.001120, l6: 0.001363
[epoch: 358/1000, batch:   836/ 1052, ite: 94100] train loss: 0.004953, tar: 0.000242 
l0: 0.000219, l1: 0.000219, l2: 0.000267, l3: 0.000369, l4: 0.000584, l5: 0.001046, l6: 0.001787
[epoch: 358/1000, batch:   916/ 1052, ite: 94120] train loss: 0.004954, tar: 0.000242 
l0: 0.000156, l1: 0.000153, l2: 0.000197, l3: 0.000270, l4: 0.000464, l5: 0.001187, l6: 0.002113
[epoch: 358/1000, batch:   996/ 1052, ite: 94140] train loss: 0.004952, tar: 0.000242 
[Epoch 358/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000149, l1: 0.000152, l2: 0.000171, l3: 0.000199, l4: 0.000320, l5: 0.000651, l6: 0.000678
[epoch: 359/1000, batch:    24/ 1052, ite: 94160] train loss: 0.004951, tar: 0.000242 
l0: 0.000255, l1: 0.000241, l2: 0.000278, l3: 0.000404, l4: 0.000807, l5: 0.001786, l6: 0.002757
[epoch: 359/1000, batch:   104/ 1052, ite: 94180] train loss: 0.004951, tar: 0.000242 
l0: 0.000136, l1: 0.000131, l2: 0.000170, l3: 0.000238, l4: 0.000408, l5: 0.001187, l6: 0.002164
[epoch: 359/1000, batch:   184/ 1052, ite: 94200] train loss: 0.004950, tar: 0.000242 
l0: 0.000110, l1: 0.000107, l2: 0.000150, l3: 0.000228, l4: 0.000425, l5: 0.000775, l6: 0.002066
[epoch: 359/1000, batch:   264/ 1052, ite: 94220] train loss: 0.004950, tar: 0.000242 
l0: 0.000226, l1: 0.000232, l2: 0.000258, l3: 0.000362, l4: 0.000657, l5: 0.001280, l6: 0.002193
[epoch: 359/1000, batch:   344/ 1052, ite: 94240] train loss: 0.004949, tar: 0.000242 
l0: 0.000413, l1: 0.000413, l2: 0.000452, l3: 0.000515, l4: 0.000884, l5: 0.002055, l6: 0.003812
[epoch: 359/1000, batch:   424/ 1052, ite: 94260] train loss: 0.004949, tar: 0.000242 
l0: 0.000118, l1: 0.000121, l2: 0.000167, l3: 0.000201, l4: 0.000376, l5: 0.001029, l6: 0.001428
[epoch: 359/1000, batch:   504/ 1052, ite: 94280] train loss: 0.004950, tar: 0.000242 
l0: 0.000212, l1: 0.000212, l2: 0.000255, l3: 0.000418, l4: 0.000833, l5: 0.000952, l6: 0.002055
[epoch: 359/1000, batch:   584/ 1052, ite: 94300] train loss: 0.004950, tar: 0.000242 
l0: 0.000271, l1: 0.000270, l2: 0.000308, l3: 0.000354, l4: 0.000754, l5: 0.001162, l6: 0.003579
[epoch: 359/1000, batch:   664/ 1052, ite: 94320] train loss: 0.004950, tar: 0.000242 
l0: 0.000202, l1: 0.000196, l2: 0.000225, l3: 0.000254, l4: 0.000547, l5: 0.000886, l6: 0.001019
[epoch: 359/1000, batch:   744/ 1052, ite: 94340] train loss: 0.004948, tar: 0.000241 
l0: 0.000211, l1: 0.000212, l2: 0.000261, l3: 0.000356, l4: 0.000641, l5: 0.001592, l6: 0.002782
[epoch: 359/1000, batch:   824/ 1052, ite: 94360] train loss: 0.004948, tar: 0.000241 
l0: 0.000104, l1: 0.000105, l2: 0.000155, l3: 0.000253, l4: 0.000460, l5: 0.000988, l6: 0.001329
[epoch: 359/1000, batch:   904/ 1052, ite: 94380] train loss: 0.004948, tar: 0.000241 
l0: 0.000262, l1: 0.000260, l2: 0.000309, l3: 0.000398, l4: 0.000574, l5: 0.001287, l6: 0.002078
[epoch: 359/1000, batch:   984/ 1052, ite: 94400] train loss: 0.004947, tar: 0.000241 
[Epoch 359/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000203, l1: 0.000205, l2: 0.000251, l3: 0.000369, l4: 0.000534, l5: 0.001035, l6: 0.001990
[epoch: 360/1000, batch:    12/ 1052, ite: 94420] train loss: 0.004946, tar: 0.000241 
l0: 0.000144, l1: 0.000147, l2: 0.000153, l3: 0.000209, l4: 0.000373, l5: 0.001292, l6: 0.002042
[epoch: 360/1000, batch:    92/ 1052, ite: 94440] train loss: 0.004946, tar: 0.000241 
l0: 0.000291, l1: 0.000298, l2: 0.000331, l3: 0.000463, l4: 0.000860, l5: 0.002032, l6: 0.003432
[epoch: 360/1000, batch:   172/ 1052, ite: 94460] train loss: 0.004947, tar: 0.000241 
l0: 0.000125, l1: 0.000133, l2: 0.000141, l3: 0.000120, l4: 0.000438, l5: 0.000741, l6: 0.001146
[epoch: 360/1000, batch:   252/ 1052, ite: 94480] train loss: 0.004945, tar: 0.000241 
l0: 0.000208, l1: 0.000210, l2: 0.000245, l3: 0.000265, l4: 0.000414, l5: 0.001222, l6: 0.002114
[epoch: 360/1000, batch:   332/ 1052, ite: 94500] train loss: 0.004946, tar: 0.000241 
l0: 0.000338, l1: 0.000350, l2: 0.000370, l3: 0.000506, l4: 0.000930, l5: 0.002009, l6: 0.003050
[epoch: 360/1000, batch:   412/ 1052, ite: 94520] train loss: 0.004946, tar: 0.000241 
l0: 0.000166, l1: 0.000168, l2: 0.000220, l3: 0.000291, l4: 0.000590, l5: 0.001190, l6: 0.002128
[epoch: 360/1000, batch:   492/ 1052, ite: 94540] train loss: 0.004945, tar: 0.000241 
l0: 0.000119, l1: 0.000120, l2: 0.000168, l3: 0.000221, l4: 0.000350, l5: 0.001063, l6: 0.001585
[epoch: 360/1000, batch:   572/ 1052, ite: 94560] train loss: 0.004945, tar: 0.000241 
l0: 0.000056, l1: 0.000060, l2: 0.000072, l3: 0.000135, l4: 0.000385, l5: 0.000614, l6: 0.001190
[epoch: 360/1000, batch:   652/ 1052, ite: 94580] train loss: 0.004943, tar: 0.000240 
l0: 0.000213, l1: 0.000218, l2: 0.000268, l3: 0.000357, l4: 0.000661, l5: 0.000914, l6: 0.002568
[epoch: 360/1000, batch:   732/ 1052, ite: 94600] train loss: 0.004943, tar: 0.000240 
l0: 0.000196, l1: 0.000193, l2: 0.000239, l3: 0.000306, l4: 0.000473, l5: 0.001362, l6: 0.001830
[epoch: 360/1000, batch:   812/ 1052, ite: 94620] train loss: 0.004942, tar: 0.000240 
l0: 0.000121, l1: 0.000122, l2: 0.000133, l3: 0.000201, l4: 0.000299, l5: 0.000916, l6: 0.001575
[epoch: 360/1000, batch:   892/ 1052, ite: 94640] train loss: 0.004943, tar: 0.000240 
l0: 0.000130, l1: 0.000132, l2: 0.000142, l3: 0.000138, l4: 0.000337, l5: 0.000613, l6: 0.000999
[epoch: 360/1000, batch:   972/ 1052, ite: 94660] train loss: 0.004942, tar: 0.000240 
l0: 0.000206, l1: 0.000200, l2: 0.000222, l3: 0.000290, l4: 0.000544, l5: 0.000772, l6: 0.001685
[epoch: 360/1000, batch:  1052/ 1052, ite: 94680] train loss: 0.004941, tar: 0.000240 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_94680.pkl
[Epoch 360/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000207, l1: 0.000200, l2: 0.000241, l3: 0.000330, l4: 0.000672, l5: 0.001281, l6: 0.001949
[epoch: 361/1000, batch:    80/ 1052, ite: 94700] train loss: 0.004544, tar: 0.000173 
l0: 0.000256, l1: 0.000255, l2: 0.000274, l3: 0.000333, l4: 0.000583, l5: 0.001305, l6: 0.002901
[epoch: 361/1000, batch:   160/ 1052, ite: 94720] train loss: 0.004684, tar: 0.000192 
l0: 0.000192, l1: 0.000203, l2: 0.000199, l3: 0.000218, l4: 0.000381, l5: 0.000651, l6: 0.001746
[epoch: 361/1000, batch:   240/ 1052, ite: 94740] train loss: 0.004647, tar: 0.000195 
l0: 0.000209, l1: 0.000215, l2: 0.000245, l3: 0.000344, l4: 0.000605, l5: 0.000817, l6: 0.001579
[epoch: 361/1000, batch:   320/ 1052, ite: 94760] train loss: 0.004720, tar: 0.000199 
l0: 0.000178, l1: 0.000180, l2: 0.000211, l3: 0.000300, l4: 0.000521, l5: 0.000898, l6: 0.001846
[epoch: 361/1000, batch:   400/ 1052, ite: 94780] train loss: 0.004737, tar: 0.000198 
l0: 0.000086, l1: 0.000088, l2: 0.000149, l3: 0.000229, l4: 0.000538, l5: 0.001279, l6: 0.002020
[epoch: 361/1000, batch:   480/ 1052, ite: 94800] train loss: 0.004884, tar: 0.000202 
l0: 0.000171, l1: 0.000174, l2: 0.000199, l3: 0.000257, l4: 0.000484, l5: 0.001134, l6: 0.001773
[epoch: 361/1000, batch:   560/ 1052, ite: 94820] train loss: 0.004793, tar: 0.000197 
l0: 0.000155, l1: 0.000156, l2: 0.000169, l3: 0.000248, l4: 0.000468, l5: 0.000784, l6: 0.001659
[epoch: 361/1000, batch:   640/ 1052, ite: 94840] train loss: 0.004682, tar: 0.000192 
l0: 0.000208, l1: 0.000210, l2: 0.000223, l3: 0.000306, l4: 0.000644, l5: 0.001237, l6: 0.002393
[epoch: 361/1000, batch:   720/ 1052, ite: 94860] train loss: 0.004609, tar: 0.000189 
l0: 0.000183, l1: 0.000188, l2: 0.000186, l3: 0.000236, l4: 0.000513, l5: 0.000746, l6: 0.002147
[epoch: 361/1000, batch:   800/ 1052, ite: 94880] train loss: 0.004567, tar: 0.000187 
l0: 0.000211, l1: 0.000215, l2: 0.000270, l3: 0.000418, l4: 0.000645, l5: 0.001853, l6: 0.003138
[epoch: 361/1000, batch:   880/ 1052, ite: 94900] train loss: 0.004628, tar: 0.000193 
l0: 0.000128, l1: 0.000131, l2: 0.000153, l3: 0.000182, l4: 0.000356, l5: 0.000706, l6: 0.001621
[epoch: 361/1000, batch:   960/ 1052, ite: 94920] train loss: 0.004610, tar: 0.000191 
l0: 0.000089, l1: 0.000091, l2: 0.000131, l3: 0.000153, l4: 0.000306, l5: 0.000817, l6: 0.001405
[epoch: 361/1000, batch:  1040/ 1052, ite: 94940] train loss: 0.004652, tar: 0.000193 
[Epoch 361/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000277, l1: 0.000272, l2: 0.000303, l3: 0.000450, l4: 0.001006, l5: 0.001772, l6: 0.002915
[epoch: 362/1000, batch:    68/ 1052, ite: 94960] train loss: 0.004687, tar: 0.000195 
l0: 0.000086, l1: 0.000081, l2: 0.000127, l3: 0.000197, l4: 0.000475, l5: 0.001116, l6: 0.001834
[epoch: 362/1000, batch:   148/ 1052, ite: 94980] train loss: 0.004678, tar: 0.000195 
l0: 0.000221, l1: 0.000210, l2: 0.000358, l3: 0.000673, l4: 0.001121, l5: 0.002137, l6: 0.003673
[epoch: 362/1000, batch:   228/ 1052, ite: 95000] train loss: 0.004698, tar: 0.000196 
l0: 0.000162, l1: 0.000161, l2: 0.000196, l3: 0.000256, l4: 0.000414, l5: 0.000807, l6: 0.001290
[epoch: 362/1000, batch:   308/ 1052, ite: 95020] train loss: 0.004712, tar: 0.000197 
l0: 0.000226, l1: 0.000235, l2: 0.000215, l3: 0.000303, l4: 0.000443, l5: 0.000803, l6: 0.001726
[epoch: 362/1000, batch:   388/ 1052, ite: 95040] train loss: 0.004720, tar: 0.000197 
l0: 0.000300, l1: 0.000303, l2: 0.000331, l3: 0.000428, l4: 0.000570, l5: 0.000903, l6: 0.002925
[epoch: 362/1000, batch:   468/ 1052, ite: 95060] train loss: 0.004756, tar: 0.000198 
l0: 0.000192, l1: 0.000195, l2: 0.000209, l3: 0.000255, l4: 0.000533, l5: 0.000796, l6: 0.000953
[epoch: 362/1000, batch:   548/ 1052, ite: 95080] train loss: 0.004723, tar: 0.000197 
l0: 0.000153, l1: 0.000164, l2: 0.000152, l3: 0.000159, l4: 0.000286, l5: 0.000964, l6: 0.001054
[epoch: 362/1000, batch:   628/ 1052, ite: 95100] train loss: 0.004689, tar: 0.000196 
l0: 0.000104, l1: 0.000104, l2: 0.000132, l3: 0.000214, l4: 0.000346, l5: 0.000723, l6: 0.001419
[epoch: 362/1000, batch:   708/ 1052, ite: 95120] train loss: 0.004665, tar: 0.000194 
l0: 0.000199, l1: 0.000203, l2: 0.000247, l3: 0.000318, l4: 0.000537, l5: 0.001306, l6: 0.003530
[epoch: 362/1000, batch:   788/ 1052, ite: 95140] train loss: 0.004668, tar: 0.000194 
l0: 0.000262, l1: 0.000257, l2: 0.000324, l3: 0.000409, l4: 0.000717, l5: 0.001943, l6: 0.002589
[epoch: 362/1000, batch:   868/ 1052, ite: 95160] train loss: 0.004673, tar: 0.000194 
l0: 0.000221, l1: 0.000226, l2: 0.000230, l3: 0.000308, l4: 0.000521, l5: 0.001820, l6: 0.003111
[epoch: 362/1000, batch:   948/ 1052, ite: 95180] train loss: 0.004663, tar: 0.000194 
l0: 0.000279, l1: 0.000280, l2: 0.000306, l3: 0.000422, l4: 0.000666, l5: 0.001740, l6: 0.002121
[epoch: 362/1000, batch:  1028/ 1052, ite: 95200] train loss: 0.004658, tar: 0.000194 
[Epoch 362/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000147, l1: 0.000144, l2: 0.000175, l3: 0.000221, l4: 0.000391, l5: 0.000852, l6: 0.001333
[epoch: 363/1000, batch:    56/ 1052, ite: 95220] train loss: 0.004676, tar: 0.000195 
l0: 0.000221, l1: 0.000224, l2: 0.000251, l3: 0.000397, l4: 0.000709, l5: 0.001169, l6: 0.002708
[epoch: 363/1000, batch:   136/ 1052, ite: 95240] train loss: 0.004666, tar: 0.000194 
l0: 0.000189, l1: 0.000186, l2: 0.000271, l3: 0.000524, l4: 0.000825, l5: 0.001096, l6: 0.002489
[epoch: 363/1000, batch:   216/ 1052, ite: 95260] train loss: 0.004660, tar: 0.000194 
l0: 0.000134, l1: 0.000136, l2: 0.000176, l3: 0.000299, l4: 0.000577, l5: 0.001286, l6: 0.002160
[epoch: 363/1000, batch:   296/ 1052, ite: 95280] train loss: 0.004659, tar: 0.000193 
l0: 0.000273, l1: 0.000280, l2: 0.000292, l3: 0.000459, l4: 0.000933, l5: 0.002413, l6: 0.004214
[epoch: 363/1000, batch:   376/ 1052, ite: 95300] train loss: 0.004671, tar: 0.000194 
l0: 0.000108, l1: 0.000109, l2: 0.000159, l3: 0.000199, l4: 0.000552, l5: 0.001182, l6: 0.001845
[epoch: 363/1000, batch:   456/ 1052, ite: 95320] train loss: 0.004654, tar: 0.000194 
l0: 0.000106, l1: 0.000104, l2: 0.000155, l3: 0.000201, l4: 0.000362, l5: 0.000922, l6: 0.001301
[epoch: 363/1000, batch:   536/ 1052, ite: 95340] train loss: 0.004629, tar: 0.000192 
l0: 0.000230, l1: 0.000232, l2: 0.000262, l3: 0.000344, l4: 0.000636, l5: 0.001007, l6: 0.003012
[epoch: 363/1000, batch:   616/ 1052, ite: 95360] train loss: 0.004646, tar: 0.000194 
l0: 0.000229, l1: 0.000229, l2: 0.000281, l3: 0.000290, l4: 0.000362, l5: 0.000997, l6: 0.001363
[epoch: 363/1000, batch:   696/ 1052, ite: 95380] train loss: 0.004643, tar: 0.000193 
l0: 0.000261, l1: 0.000258, l2: 0.000327, l3: 0.000432, l4: 0.001044, l5: 0.001317, l6: 0.002342
[epoch: 363/1000, batch:   776/ 1052, ite: 95400] train loss: 0.004646, tar: 0.000193 
l0: 0.000221, l1: 0.000231, l2: 0.000235, l3: 0.000342, l4: 0.000594, l5: 0.001746, l6: 0.003885
[epoch: 363/1000, batch:   856/ 1052, ite: 95420] train loss: 0.004672, tar: 0.000193 
l0: 0.000239, l1: 0.000234, l2: 0.000275, l3: 0.000375, l4: 0.000362, l5: 0.001008, l6: 0.001996
[epoch: 363/1000, batch:   936/ 1052, ite: 95440] train loss: 0.004672, tar: 0.000194 
l0: 0.000152, l1: 0.000162, l2: 0.000193, l3: 0.000184, l4: 0.000350, l5: 0.000692, l6: 0.001934
[epoch: 363/1000, batch:  1016/ 1052, ite: 95460] train loss: 0.004661, tar: 0.000193 
[Epoch 363/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000115, l1: 0.000115, l2: 0.000167, l3: 0.000292, l4: 0.000849, l5: 0.001365, l6: 0.001296
[epoch: 364/1000, batch:    44/ 1052, ite: 95480] train loss: 0.004665, tar: 0.000193 
l0: 0.000305, l1: 0.000317, l2: 0.000369, l3: 0.000454, l4: 0.000680, l5: 0.001615, l6: 0.001384
[epoch: 364/1000, batch:   124/ 1052, ite: 95500] train loss: 0.004667, tar: 0.000193 
l0: 0.000331, l1: 0.000337, l2: 0.000378, l3: 0.000523, l4: 0.000952, l5: 0.001841, l6: 0.003932
[epoch: 364/1000, batch:   204/ 1052, ite: 95520] train loss: 0.004677, tar: 0.000193 
l0: 0.000058, l1: 0.000057, l2: 0.000086, l3: 0.000140, l4: 0.000447, l5: 0.000785, l6: 0.001320
[epoch: 364/1000, batch:   284/ 1052, ite: 95540] train loss: 0.004676, tar: 0.000193 
l0: 0.000424, l1: 0.000426, l2: 0.000483, l3: 0.000631, l4: 0.000886, l5: 0.001950, l6: 0.002713
[epoch: 364/1000, batch:   364/ 1052, ite: 95560] train loss: 0.004667, tar: 0.000193 
l0: 0.000418, l1: 0.000415, l2: 0.000521, l3: 0.000645, l4: 0.001209, l5: 0.001632, l6: 0.002536
[epoch: 364/1000, batch:   444/ 1052, ite: 95580] train loss: 0.004669, tar: 0.000193 
l0: 0.000225, l1: 0.000230, l2: 0.000276, l3: 0.000353, l4: 0.000591, l5: 0.001239, l6: 0.001828
[epoch: 364/1000, batch:   524/ 1052, ite: 95600] train loss: 0.004665, tar: 0.000193 
l0: 0.000214, l1: 0.000206, l2: 0.000271, l3: 0.000353, l4: 0.000754, l5: 0.001291, l6: 0.002613
[epoch: 364/1000, batch:   604/ 1052, ite: 95620] train loss: 0.004665, tar: 0.000192 
l0: 0.000179, l1: 0.000177, l2: 0.000260, l3: 0.000333, l4: 0.000540, l5: 0.000966, l6: 0.001758
[epoch: 364/1000, batch:   684/ 1052, ite: 95640] train loss: 0.004659, tar: 0.000192 
l0: 0.000213, l1: 0.000211, l2: 0.000222, l3: 0.000303, l4: 0.000524, l5: 0.001027, l6: 0.002229
[epoch: 364/1000, batch:   764/ 1052, ite: 95660] train loss: 0.004671, tar: 0.000193 
l0: 0.000167, l1: 0.000173, l2: 0.000208, l3: 0.000348, l4: 0.000639, l5: 0.000789, l6: 0.001618
[epoch: 364/1000, batch:   844/ 1052, ite: 95680] train loss: 0.004670, tar: 0.000193 
l0: 0.000176, l1: 0.000175, l2: 0.000241, l3: 0.000313, l4: 0.000388, l5: 0.000917, l6: 0.001342
[epoch: 364/1000, batch:   924/ 1052, ite: 95700] train loss: 0.004677, tar: 0.000193 
l0: 0.000113, l1: 0.000113, l2: 0.000125, l3: 0.000206, l4: 0.000346, l5: 0.000893, l6: 0.001775
[epoch: 364/1000, batch:  1004/ 1052, ite: 95720] train loss: 0.004677, tar: 0.000192 
[Epoch 364/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000194, l1: 0.000197, l2: 0.000223, l3: 0.000300, l4: 0.000433, l5: 0.001486, l6: 0.001970
[epoch: 365/1000, batch:    32/ 1052, ite: 95740] train loss: 0.004677, tar: 0.000193 
l0: 0.000185, l1: 0.000185, l2: 0.000253, l3: 0.000336, l4: 0.000683, l5: 0.001717, l6: 0.002217
[epoch: 365/1000, batch:   112/ 1052, ite: 95760] train loss: 0.004677, tar: 0.000193 
l0: 0.000188, l1: 0.000186, l2: 0.000216, l3: 0.000336, l4: 0.000543, l5: 0.000991, l6: 0.001692
[epoch: 365/1000, batch:   192/ 1052, ite: 95780] train loss: 0.004669, tar: 0.000193 
l0: 0.000219, l1: 0.000223, l2: 0.000278, l3: 0.000441, l4: 0.000760, l5: 0.001108, l6: 0.002451
[epoch: 365/1000, batch:   272/ 1052, ite: 95800] train loss: 0.004669, tar: 0.000193 
l0: 0.000405, l1: 0.000389, l2: 0.000400, l3: 0.000448, l4: 0.000731, l5: 0.001255, l6: 0.001546
[epoch: 365/1000, batch:   352/ 1052, ite: 95820] train loss: 0.004675, tar: 0.000193 
l0: 0.000159, l1: 0.000157, l2: 0.000204, l3: 0.000237, l4: 0.000411, l5: 0.000690, l6: 0.001263
[epoch: 365/1000, batch:   432/ 1052, ite: 95840] train loss: 0.004674, tar: 0.000193 
l0: 0.000128, l1: 0.000131, l2: 0.000137, l3: 0.000164, l4: 0.000347, l5: 0.000730, l6: 0.001521
[epoch: 365/1000, batch:   512/ 1052, ite: 95860] train loss: 0.004666, tar: 0.000193 
l0: 0.000280, l1: 0.000280, l2: 0.000306, l3: 0.000399, l4: 0.000695, l5: 0.001309, l6: 0.002711
[epoch: 365/1000, batch:   592/ 1052, ite: 95880] train loss: 0.004669, tar: 0.000193 
l0: 0.000265, l1: 0.000265, l2: 0.000320, l3: 0.000427, l4: 0.000614, l5: 0.001295, l6: 0.002868
[epoch: 365/1000, batch:   672/ 1052, ite: 95900] train loss: 0.004664, tar: 0.000193 
l0: 0.000279, l1: 0.000290, l2: 0.000302, l3: 0.000306, l4: 0.000520, l5: 0.001034, l6: 0.001821
[epoch: 365/1000, batch:   752/ 1052, ite: 95920] train loss: 0.004667, tar: 0.000192 
l0: 0.000102, l1: 0.000101, l2: 0.000151, l3: 0.000201, l4: 0.000378, l5: 0.001288, l6: 0.002572
[epoch: 365/1000, batch:   832/ 1052, ite: 95940] train loss: 0.004672, tar: 0.000193 
l0: 0.000215, l1: 0.000219, l2: 0.000270, l3: 0.000337, l4: 0.000711, l5: 0.001688, l6: 0.002739
[epoch: 365/1000, batch:   912/ 1052, ite: 95960] train loss: 0.004674, tar: 0.000192 
l0: 0.000261, l1: 0.000260, l2: 0.000265, l3: 0.000436, l4: 0.000626, l5: 0.001026, l6: 0.002651
[epoch: 365/1000, batch:   992/ 1052, ite: 95980] train loss: 0.004679, tar: 0.000193 
[Epoch 365/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000200, l1: 0.000203, l2: 0.000228, l3: 0.000262, l4: 0.000489, l5: 0.001002, l6: 0.002364
[epoch: 366/1000, batch:    20/ 1052, ite: 96000] train loss: 0.004668, tar: 0.000192 
l0: 0.000069, l1: 0.000069, l2: 0.000072, l3: 0.000101, l4: 0.000206, l5: 0.000428, l6: 0.000880
[epoch: 366/1000, batch:   100/ 1052, ite: 96020] train loss: 0.004659, tar: 0.000192 
l0: 0.000100, l1: 0.000106, l2: 0.000137, l3: 0.000142, l4: 0.000414, l5: 0.000645, l6: 0.001540
[epoch: 366/1000, batch:   180/ 1052, ite: 96040] train loss: 0.004671, tar: 0.000192 
l0: 0.000152, l1: 0.000155, l2: 0.000174, l3: 0.000239, l4: 0.000381, l5: 0.000919, l6: 0.001122
[epoch: 366/1000, batch:   260/ 1052, ite: 96060] train loss: 0.004666, tar: 0.000192 
l0: 0.000191, l1: 0.000194, l2: 0.000258, l3: 0.000377, l4: 0.000491, l5: 0.001030, l6: 0.002187
[epoch: 366/1000, batch:   340/ 1052, ite: 96080] train loss: 0.004661, tar: 0.000192 
l0: 0.000241, l1: 0.000238, l2: 0.000274, l3: 0.000331, l4: 0.000674, l5: 0.001355, l6: 0.001508
[epoch: 366/1000, batch:   420/ 1052, ite: 96100] train loss: 0.004657, tar: 0.000192 
l0: 0.000284, l1: 0.000290, l2: 0.000324, l3: 0.000455, l4: 0.000888, l5: 0.001654, l6: 0.003073
[epoch: 366/1000, batch:   500/ 1052, ite: 96120] train loss: 0.004653, tar: 0.000192 
l0: 0.000074, l1: 0.000071, l2: 0.000104, l3: 0.000231, l4: 0.000215, l5: 0.000586, l6: 0.001116
[epoch: 366/1000, batch:   580/ 1052, ite: 96140] train loss: 0.004658, tar: 0.000191 
l0: 0.000114, l1: 0.000116, l2: 0.000154, l3: 0.000169, l4: 0.000362, l5: 0.000889, l6: 0.001256
[epoch: 366/1000, batch:   660/ 1052, ite: 96160] train loss: 0.004660, tar: 0.000192 
l0: 0.000212, l1: 0.000214, l2: 0.000223, l3: 0.000327, l4: 0.000531, l5: 0.001153, l6: 0.001598
[epoch: 366/1000, batch:   740/ 1052, ite: 96180] train loss: 0.004662, tar: 0.000192 
l0: 0.000191, l1: 0.000189, l2: 0.000217, l3: 0.000355, l4: 0.000538, l5: 0.001576, l6: 0.001705
[epoch: 366/1000, batch:   820/ 1052, ite: 96200] train loss: 0.004661, tar: 0.000191 
l0: 0.000401, l1: 0.000426, l2: 0.000422, l3: 0.000484, l4: 0.000414, l5: 0.000724, l6: 0.001371
[epoch: 366/1000, batch:   900/ 1052, ite: 96220] train loss: 0.004672, tar: 0.000192 
l0: 0.000140, l1: 0.000144, l2: 0.000168, l3: 0.000253, l4: 0.000348, l5: 0.000879, l6: 0.001465
[epoch: 366/1000, batch:   980/ 1052, ite: 96240] train loss: 0.004672, tar: 0.000192 
[Epoch 366/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000238, l1: 0.000243, l2: 0.000221, l3: 0.000236, l4: 0.000400, l5: 0.001030, l6: 0.001535
[epoch: 367/1000, batch:     8/ 1052, ite: 96260] train loss: 0.004667, tar: 0.000192 
l0: 0.000114, l1: 0.000118, l2: 0.000138, l3: 0.000192, l4: 0.000514, l5: 0.000875, l6: 0.001714
[epoch: 367/1000, batch:    88/ 1052, ite: 96280] train loss: 0.004665, tar: 0.000192 
l0: 0.000352, l1: 0.000368, l2: 0.000352, l3: 0.000516, l4: 0.000633, l5: 0.001558, l6: 0.002848
[epoch: 367/1000, batch:   168/ 1052, ite: 96300] train loss: 0.004667, tar: 0.000192 
l0: 0.000117, l1: 0.000118, l2: 0.000152, l3: 0.000182, l4: 0.000447, l5: 0.000770, l6: 0.001645
[epoch: 367/1000, batch:   248/ 1052, ite: 96320] train loss: 0.004668, tar: 0.000192 
l0: 0.000486, l1: 0.000474, l2: 0.000580, l3: 0.000649, l4: 0.000893, l5: 0.001533, l6: 0.003510
[epoch: 367/1000, batch:   328/ 1052, ite: 96340] train loss: 0.004667, tar: 0.000192 
l0: 0.000221, l1: 0.000220, l2: 0.000266, l3: 0.000295, l4: 0.000662, l5: 0.000980, l6: 0.001887
[epoch: 367/1000, batch:   408/ 1052, ite: 96360] train loss: 0.004669, tar: 0.000193 
l0: 0.000129, l1: 0.000132, l2: 0.000146, l3: 0.000193, l4: 0.000420, l5: 0.000915, l6: 0.001819
[epoch: 367/1000, batch:   488/ 1052, ite: 96380] train loss: 0.004665, tar: 0.000192 
l0: 0.000139, l1: 0.000138, l2: 0.000174, l3: 0.000203, l4: 0.000412, l5: 0.001139, l6: 0.002333
[epoch: 367/1000, batch:   568/ 1052, ite: 96400] train loss: 0.004662, tar: 0.000192 
l0: 0.000110, l1: 0.000109, l2: 0.000133, l3: 0.000161, l4: 0.000533, l5: 0.000640, l6: 0.001331
[epoch: 367/1000, batch:   648/ 1052, ite: 96420] train loss: 0.004661, tar: 0.000192 
l0: 0.000488, l1: 0.000528, l2: 0.000496, l3: 0.000644, l4: 0.001110, l5: 0.002159, l6: 0.003889
[epoch: 367/1000, batch:   728/ 1052, ite: 96440] train loss: 0.004665, tar: 0.000192 
l0: 0.000280, l1: 0.000279, l2: 0.000327, l3: 0.000364, l4: 0.000648, l5: 0.001604, l6: 0.002489
[epoch: 367/1000, batch:   808/ 1052, ite: 96460] train loss: 0.004663, tar: 0.000192 
l0: 0.000281, l1: 0.000288, l2: 0.000328, l3: 0.000410, l4: 0.000583, l5: 0.001186, l6: 0.002157
[epoch: 367/1000, batch:   888/ 1052, ite: 96480] train loss: 0.004666, tar: 0.000193 
l0: 0.000244, l1: 0.000229, l2: 0.000333, l3: 0.000504, l4: 0.000954, l5: 0.001393, l6: 0.002477
[epoch: 367/1000, batch:   968/ 1052, ite: 96500] train loss: 0.004669, tar: 0.000193 
l0: 0.000156, l1: 0.000155, l2: 0.000165, l3: 0.000285, l4: 0.000525, l5: 0.000801, l6: 0.001291
[epoch: 367/1000, batch:  1048/ 1052, ite: 96520] train loss: 0.004677, tar: 0.000193 
[Epoch 367/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000092, l1: 0.000092, l2: 0.000120, l3: 0.000148, l4: 0.000309, l5: 0.000691, l6: 0.001288
[epoch: 368/1000, batch:    76/ 1052, ite: 96540] train loss: 0.004678, tar: 0.000193 
l0: 0.000180, l1: 0.000189, l2: 0.000183, l3: 0.000209, l4: 0.000621, l5: 0.001078, l6: 0.001101
[epoch: 368/1000, batch:   156/ 1052, ite: 96560] train loss: 0.004677, tar: 0.000193 
l0: 0.000228, l1: 0.000232, l2: 0.000321, l3: 0.000575, l4: 0.000809, l5: 0.001921, l6: 0.004797
[epoch: 368/1000, batch:   236/ 1052, ite: 96580] train loss: 0.004676, tar: 0.000193 
l0: 0.000320, l1: 0.000323, l2: 0.000329, l3: 0.000404, l4: 0.000777, l5: 0.001713, l6: 0.002418
[epoch: 368/1000, batch:   316/ 1052, ite: 96600] train loss: 0.004674, tar: 0.000193 
l0: 0.000214, l1: 0.000215, l2: 0.000325, l3: 0.000391, l4: 0.000658, l5: 0.001382, l6: 0.002419
[epoch: 368/1000, batch:   396/ 1052, ite: 96620] train loss: 0.004679, tar: 0.000193 
l0: 0.000098, l1: 0.000100, l2: 0.000110, l3: 0.000140, l4: 0.000321, l5: 0.000350, l6: 0.000850
[epoch: 368/1000, batch:   476/ 1052, ite: 96640] train loss: 0.004675, tar: 0.000193 
l0: 0.000208, l1: 0.000207, l2: 0.000267, l3: 0.000253, l4: 0.000475, l5: 0.000821, l6: 0.002031
[epoch: 368/1000, batch:   556/ 1052, ite: 96660] train loss: 0.004676, tar: 0.000193 
l0: 0.000341, l1: 0.000343, l2: 0.000391, l3: 0.000517, l4: 0.000826, l5: 0.002002, l6: 0.003243
[epoch: 368/1000, batch:   636/ 1052, ite: 96680] train loss: 0.004681, tar: 0.000193 
l0: 0.000089, l1: 0.000089, l2: 0.000118, l3: 0.000143, l4: 0.000290, l5: 0.000809, l6: 0.001179
[epoch: 368/1000, batch:   716/ 1052, ite: 96700] train loss: 0.004679, tar: 0.000193 
l0: 0.000293, l1: 0.000288, l2: 0.000345, l3: 0.000558, l4: 0.000801, l5: 0.001598, l6: 0.002053
[epoch: 368/1000, batch:   796/ 1052, ite: 96720] train loss: 0.004676, tar: 0.000193 
l0: 0.000251, l1: 0.000259, l2: 0.000284, l3: 0.000312, l4: 0.000771, l5: 0.001739, l6: 0.003140
[epoch: 368/1000, batch:   876/ 1052, ite: 96740] train loss: 0.004677, tar: 0.000193 
l0: 0.000076, l1: 0.000080, l2: 0.000094, l3: 0.000138, l4: 0.000381, l5: 0.000822, l6: 0.001112
[epoch: 368/1000, batch:   956/ 1052, ite: 96760] train loss: 0.004679, tar: 0.000193 
l0: 0.000346, l1: 0.000346, l2: 0.000431, l3: 0.000527, l4: 0.001159, l5: 0.001938, l6: 0.003182
[epoch: 368/1000, batch:  1036/ 1052, ite: 96780] train loss: 0.004682, tar: 0.000194 
[Epoch 368/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000233, l1: 0.000234, l2: 0.000255, l3: 0.000271, l4: 0.000515, l5: 0.000788, l6: 0.001679
[epoch: 369/1000, batch:    64/ 1052, ite: 96800] train loss: 0.004679, tar: 0.000194 
l0: 0.000367, l1: 0.000374, l2: 0.000397, l3: 0.000428, l4: 0.000707, l5: 0.001297, l6: 0.002988
[epoch: 369/1000, batch:   144/ 1052, ite: 96820] train loss: 0.004682, tar: 0.000194 
l0: 0.000161, l1: 0.000158, l2: 0.000231, l3: 0.000288, l4: 0.000638, l5: 0.001050, l6: 0.001597
[epoch: 369/1000, batch:   224/ 1052, ite: 96840] train loss: 0.004684, tar: 0.000194 
l0: 0.000130, l1: 0.000131, l2: 0.000167, l3: 0.000266, l4: 0.000350, l5: 0.001197, l6: 0.001409
[epoch: 369/1000, batch:   304/ 1052, ite: 96860] train loss: 0.004682, tar: 0.000194 
l0: 0.000160, l1: 0.000158, l2: 0.000200, l3: 0.000226, l4: 0.000290, l5: 0.000701, l6: 0.001849
[epoch: 369/1000, batch:   384/ 1052, ite: 96880] train loss: 0.004679, tar: 0.000194 
l0: 0.000304, l1: 0.000306, l2: 0.000389, l3: 0.000484, l4: 0.000810, l5: 0.001531, l6: 0.002036
[epoch: 369/1000, batch:   464/ 1052, ite: 96900] train loss: 0.004680, tar: 0.000194 
l0: 0.000307, l1: 0.000316, l2: 0.000380, l3: 0.000442, l4: 0.000716, l5: 0.001792, l6: 0.002613
[epoch: 369/1000, batch:   544/ 1052, ite: 96920] train loss: 0.004682, tar: 0.000194 
l0: 0.000323, l1: 0.000317, l2: 0.000409, l3: 0.000485, l4: 0.000780, l5: 0.000953, l6: 0.001927
[epoch: 369/1000, batch:   624/ 1052, ite: 96940] train loss: 0.004683, tar: 0.000194 
l0: 0.000127, l1: 0.000123, l2: 0.000172, l3: 0.000277, l4: 0.000742, l5: 0.001335, l6: 0.001802
[epoch: 369/1000, batch:   704/ 1052, ite: 96960] train loss: 0.004684, tar: 0.000194 
l0: 0.000382, l1: 0.000387, l2: 0.000423, l3: 0.000524, l4: 0.000861, l5: 0.002650, l6: 0.004693
[epoch: 369/1000, batch:   784/ 1052, ite: 96980] train loss: 0.004684, tar: 0.000194 
l0: 0.000313, l1: 0.000307, l2: 0.000357, l3: 0.000459, l4: 0.000757, l5: 0.001151, l6: 0.002711
[epoch: 369/1000, batch:   864/ 1052, ite: 97000] train loss: 0.004683, tar: 0.000194 
l0: 0.000436, l1: 0.000452, l2: 0.000452, l3: 0.000554, l4: 0.001015, l5: 0.001750, l6: 0.004124
[epoch: 369/1000, batch:   944/ 1052, ite: 97020] train loss: 0.004686, tar: 0.000194 
l0: 0.000213, l1: 0.000217, l2: 0.000263, l3: 0.000441, l4: 0.000953, l5: 0.002325, l6: 0.002899
[epoch: 369/1000, batch:  1024/ 1052, ite: 97040] train loss: 0.004687, tar: 0.000194 
[Epoch 369/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000133, l1: 0.000143, l2: 0.000130, l3: 0.000168, l4: 0.000507, l5: 0.001092, l6: 0.001454
[epoch: 370/1000, batch:    52/ 1052, ite: 97060] train loss: 0.004683, tar: 0.000194 
l0: 0.000282, l1: 0.000280, l2: 0.000310, l3: 0.000488, l4: 0.000922, l5: 0.001714, l6: 0.003325
[epoch: 370/1000, batch:   132/ 1052, ite: 97080] train loss: 0.004688, tar: 0.000194 
l0: 0.000142, l1: 0.000135, l2: 0.000184, l3: 0.000255, l4: 0.000524, l5: 0.000700, l6: 0.001986
[epoch: 370/1000, batch:   212/ 1052, ite: 97100] train loss: 0.004689, tar: 0.000194 
l0: 0.000178, l1: 0.000182, l2: 0.000182, l3: 0.000210, l4: 0.000301, l5: 0.000754, l6: 0.001330
[epoch: 370/1000, batch:   292/ 1052, ite: 97120] train loss: 0.004687, tar: 0.000194 
l0: 0.000175, l1: 0.000175, l2: 0.000217, l3: 0.000251, l4: 0.000401, l5: 0.000767, l6: 0.001484
[epoch: 370/1000, batch:   372/ 1052, ite: 97140] train loss: 0.004689, tar: 0.000194 
l0: 0.000097, l1: 0.000099, l2: 0.000107, l3: 0.000188, l4: 0.000308, l5: 0.001161, l6: 0.001559
[epoch: 370/1000, batch:   452/ 1052, ite: 97160] train loss: 0.004686, tar: 0.000194 
l0: 0.000099, l1: 0.000099, l2: 0.000139, l3: 0.000150, l4: 0.000488, l5: 0.000807, l6: 0.001152
[epoch: 370/1000, batch:   532/ 1052, ite: 97180] train loss: 0.004685, tar: 0.000194 
l0: 0.000134, l1: 0.000125, l2: 0.000172, l3: 0.000274, l4: 0.000400, l5: 0.001077, l6: 0.001701
[epoch: 370/1000, batch:   612/ 1052, ite: 97200] train loss: 0.004688, tar: 0.000194 
l0: 0.000095, l1: 0.000098, l2: 0.000115, l3: 0.000136, l4: 0.000337, l5: 0.000638, l6: 0.001397
[epoch: 370/1000, batch:   692/ 1052, ite: 97220] train loss: 0.004687, tar: 0.000194 
l0: 0.000083, l1: 0.000084, l2: 0.000104, l3: 0.000134, l4: 0.000333, l5: 0.000656, l6: 0.001174
[epoch: 370/1000, batch:   772/ 1052, ite: 97240] train loss: 0.004686, tar: 0.000194 
l0: 0.000087, l1: 0.000089, l2: 0.000104, l3: 0.000140, l4: 0.000257, l5: 0.000750, l6: 0.001521
[epoch: 370/1000, batch:   852/ 1052, ite: 97260] train loss: 0.004685, tar: 0.000194 
l0: 0.000135, l1: 0.000133, l2: 0.000174, l3: 0.000308, l4: 0.000437, l5: 0.001004, l6: 0.002042
[epoch: 370/1000, batch:   932/ 1052, ite: 97280] train loss: 0.004681, tar: 0.000193 
l0: 0.000121, l1: 0.000124, l2: 0.000150, l3: 0.000183, l4: 0.000299, l5: 0.000761, l6: 0.001596
[epoch: 370/1000, batch:  1012/ 1052, ite: 97300] train loss: 0.004681, tar: 0.000193 
[Epoch 370/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000151, l1: 0.000156, l2: 0.000165, l3: 0.000189, l4: 0.000531, l5: 0.000989, l6: 0.001560
[epoch: 371/1000, batch:    40/ 1052, ite: 97320] train loss: 0.004679, tar: 0.000193 
l0: 0.000192, l1: 0.000193, l2: 0.000208, l3: 0.000249, l4: 0.000442, l5: 0.001128, l6: 0.001814
[epoch: 371/1000, batch:   120/ 1052, ite: 97340] train loss: 0.004679, tar: 0.000193 
l0: 0.000298, l1: 0.000294, l2: 0.000342, l3: 0.000480, l4: 0.000734, l5: 0.001967, l6: 0.002376
[epoch: 371/1000, batch:   200/ 1052, ite: 97360] train loss: 0.004679, tar: 0.000193 
l0: 0.000030, l1: 0.000028, l2: 0.000058, l3: 0.000093, l4: 0.000249, l5: 0.000449, l6: 0.000988
[epoch: 371/1000, batch:   280/ 1052, ite: 97380] train loss: 0.004680, tar: 0.000193 
l0: 0.000231, l1: 0.000231, l2: 0.000273, l3: 0.000331, l4: 0.000738, l5: 0.000966, l6: 0.002012
[epoch: 371/1000, batch:   360/ 1052, ite: 97400] train loss: 0.004680, tar: 0.000193 
l0: 0.000290, l1: 0.000310, l2: 0.000325, l3: 0.000356, l4: 0.000695, l5: 0.001351, l6: 0.002765
[epoch: 371/1000, batch:   440/ 1052, ite: 97420] train loss: 0.004677, tar: 0.000193 
l0: 0.000178, l1: 0.000182, l2: 0.000165, l3: 0.000236, l4: 0.000437, l5: 0.000842, l6: 0.001578
[epoch: 371/1000, batch:   520/ 1052, ite: 97440] train loss: 0.004677, tar: 0.000193 
l0: 0.000254, l1: 0.000251, l2: 0.000328, l3: 0.000320, l4: 0.000615, l5: 0.001841, l6: 0.003912
[epoch: 371/1000, batch:   600/ 1052, ite: 97460] train loss: 0.004679, tar: 0.000193 
l0: 0.000167, l1: 0.000170, l2: 0.000186, l3: 0.000300, l4: 0.000398, l5: 0.001074, l6: 0.002104
[epoch: 371/1000, batch:   680/ 1052, ite: 97480] train loss: 0.004676, tar: 0.000193 
l0: 0.000206, l1: 0.000207, l2: 0.000243, l3: 0.000310, l4: 0.000586, l5: 0.001071, l6: 0.001669
[epoch: 371/1000, batch:   760/ 1052, ite: 97500] train loss: 0.004677, tar: 0.000193 
l0: 0.000182, l1: 0.000181, l2: 0.000226, l3: 0.000259, l4: 0.000565, l5: 0.000806, l6: 0.001842
[epoch: 371/1000, batch:   840/ 1052, ite: 97520] train loss: 0.004673, tar: 0.000193 
l0: 0.000124, l1: 0.000127, l2: 0.000142, l3: 0.000180, l4: 0.000259, l5: 0.000762, l6: 0.001719
[epoch: 371/1000, batch:   920/ 1052, ite: 97540] train loss: 0.004672, tar: 0.000193 
l0: 0.000360, l1: 0.000380, l2: 0.000411, l3: 0.000486, l4: 0.000849, l5: 0.001666, l6: 0.004073
[epoch: 371/1000, batch:  1000/ 1052, ite: 97560] train loss: 0.004677, tar: 0.000193 
[Epoch 371/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000120, l1: 0.000123, l2: 0.000128, l3: 0.000224, l4: 0.000444, l5: 0.000739, l6: 0.001036
[epoch: 372/1000, batch:    28/ 1052, ite: 97580] train loss: 0.004676, tar: 0.000193 
l0: 0.000151, l1: 0.000149, l2: 0.000171, l3: 0.000231, l4: 0.000428, l5: 0.001019, l6: 0.002103
[epoch: 372/1000, batch:   108/ 1052, ite: 97600] train loss: 0.004676, tar: 0.000193 
l0: 0.000341, l1: 0.000362, l2: 0.000331, l3: 0.000335, l4: 0.000856, l5: 0.002121, l6: 0.002671
[epoch: 372/1000, batch:   188/ 1052, ite: 97620] train loss: 0.004675, tar: 0.000193 
l0: 0.000169, l1: 0.000187, l2: 0.000188, l3: 0.000269, l4: 0.000640, l5: 0.000761, l6: 0.001350
[epoch: 372/1000, batch:   268/ 1052, ite: 97640] train loss: 0.004672, tar: 0.000193 
l0: 0.000188, l1: 0.000188, l2: 0.000193, l3: 0.000333, l4: 0.000636, l5: 0.001126, l6: 0.001699
[epoch: 372/1000, batch:   348/ 1052, ite: 97660] train loss: 0.004667, tar: 0.000193 
l0: 0.000190, l1: 0.000194, l2: 0.000200, l3: 0.000181, l4: 0.000324, l5: 0.000678, l6: 0.000905
[epoch: 372/1000, batch:   428/ 1052, ite: 97680] train loss: 0.004669, tar: 0.000193 
l0: 0.000109, l1: 0.000113, l2: 0.000127, l3: 0.000193, l4: 0.000489, l5: 0.001062, l6: 0.002188
[epoch: 372/1000, batch:   508/ 1052, ite: 97700] train loss: 0.004668, tar: 0.000193 
l0: 0.000213, l1: 0.000216, l2: 0.000252, l3: 0.000337, l4: 0.000649, l5: 0.001559, l6: 0.003040
[epoch: 372/1000, batch:   588/ 1052, ite: 97720] train loss: 0.004670, tar: 0.000193 
l0: 0.000103, l1: 0.000103, l2: 0.000168, l3: 0.000295, l4: 0.000573, l5: 0.000909, l6: 0.001764
[epoch: 372/1000, batch:   668/ 1052, ite: 97740] train loss: 0.004671, tar: 0.000193 
l0: 0.000180, l1: 0.000186, l2: 0.000177, l3: 0.000320, l4: 0.000657, l5: 0.001132, l6: 0.001427
[epoch: 372/1000, batch:   748/ 1052, ite: 97760] train loss: 0.004668, tar: 0.000193 
l0: 0.000207, l1: 0.000214, l2: 0.000211, l3: 0.000260, l4: 0.000452, l5: 0.001323, l6: 0.002757
[epoch: 372/1000, batch:   828/ 1052, ite: 97780] train loss: 0.004669, tar: 0.000193 
l0: 0.000115, l1: 0.000115, l2: 0.000130, l3: 0.000173, l4: 0.000402, l5: 0.000683, l6: 0.000764
[epoch: 372/1000, batch:   908/ 1052, ite: 97800] train loss: 0.004669, tar: 0.000193 
l0: 0.000123, l1: 0.000124, l2: 0.000173, l3: 0.000347, l4: 0.000533, l5: 0.001421, l6: 0.001766
[epoch: 372/1000, batch:   988/ 1052, ite: 97820] train loss: 0.004669, tar: 0.000193 
[Epoch 372/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000148, l1: 0.000150, l2: 0.000208, l3: 0.000225, l4: 0.000433, l5: 0.001221, l6: 0.002090
[epoch: 373/1000, batch:    16/ 1052, ite: 97840] train loss: 0.004672, tar: 0.000193 
l0: 0.000312, l1: 0.000315, l2: 0.000369, l3: 0.000441, l4: 0.001008, l5: 0.002014, l6: 0.003584
[epoch: 373/1000, batch:    96/ 1052, ite: 97860] train loss: 0.004670, tar: 0.000193 
l0: 0.000186, l1: 0.000184, l2: 0.000222, l3: 0.000316, l4: 0.000606, l5: 0.001287, l6: 0.001349
[epoch: 373/1000, batch:   176/ 1052, ite: 97880] train loss: 0.004669, tar: 0.000192 
l0: 0.000093, l1: 0.000093, l2: 0.000098, l3: 0.000176, l4: 0.000327, l5: 0.000832, l6: 0.000988
[epoch: 373/1000, batch:   256/ 1052, ite: 97900] train loss: 0.004671, tar: 0.000192 
l0: 0.000159, l1: 0.000162, l2: 0.000189, l3: 0.000216, l4: 0.000307, l5: 0.000649, l6: 0.001176
[epoch: 373/1000, batch:   336/ 1052, ite: 97920] train loss: 0.004668, tar: 0.000192 
l0: 0.000271, l1: 0.000279, l2: 0.000302, l3: 0.000364, l4: 0.000846, l5: 0.001378, l6: 0.003190
[epoch: 373/1000, batch:   416/ 1052, ite: 97940] train loss: 0.004666, tar: 0.000192 
l0: 0.000108, l1: 0.000111, l2: 0.000124, l3: 0.000188, l4: 0.000319, l5: 0.000913, l6: 0.001862
[epoch: 373/1000, batch:   496/ 1052, ite: 97960] train loss: 0.004669, tar: 0.000192 
l0: 0.000279, l1: 0.000280, l2: 0.000332, l3: 0.000395, l4: 0.000793, l5: 0.001965, l6: 0.003012
[epoch: 373/1000, batch:   576/ 1052, ite: 97980] train loss: 0.004670, tar: 0.000193 
l0: 0.000085, l1: 0.000082, l2: 0.000101, l3: 0.000166, l4: 0.000324, l5: 0.000685, l6: 0.001089
[epoch: 373/1000, batch:   656/ 1052, ite: 98000] train loss: 0.004673, tar: 0.000193 
l0: 0.000270, l1: 0.000270, l2: 0.000272, l3: 0.000329, l4: 0.000708, l5: 0.001344, l6: 0.002009
[epoch: 373/1000, batch:   736/ 1052, ite: 98020] train loss: 0.004670, tar: 0.000193 
l0: 0.000133, l1: 0.000131, l2: 0.000161, l3: 0.000248, l4: 0.000455, l5: 0.001065, l6: 0.001798
[epoch: 373/1000, batch:   816/ 1052, ite: 98040] train loss: 0.004670, tar: 0.000193 
l0: 0.000164, l1: 0.000163, l2: 0.000210, l3: 0.000296, l4: 0.000725, l5: 0.001068, l6: 0.002327
[epoch: 373/1000, batch:   896/ 1052, ite: 98060] train loss: 0.004668, tar: 0.000193 
l0: 0.000142, l1: 0.000150, l2: 0.000166, l3: 0.000271, l4: 0.000476, l5: 0.001434, l6: 0.001639
[epoch: 373/1000, batch:   976/ 1052, ite: 98080] train loss: 0.004667, tar: 0.000193 
[Epoch 373/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000205, l1: 0.000203, l2: 0.000224, l3: 0.000304, l4: 0.000409, l5: 0.001312, l6: 0.001372
[epoch: 374/1000, batch:     4/ 1052, ite: 98100] train loss: 0.004665, tar: 0.000192 
l0: 0.000140, l1: 0.000141, l2: 0.000173, l3: 0.000259, l4: 0.000541, l5: 0.001021, l6: 0.001696
[epoch: 374/1000, batch:    84/ 1052, ite: 98120] train loss: 0.004666, tar: 0.000192 
l0: 0.000154, l1: 0.000150, l2: 0.000212, l3: 0.000301, l4: 0.000722, l5: 0.001171, l6: 0.002022
[epoch: 374/1000, batch:   164/ 1052, ite: 98140] train loss: 0.004666, tar: 0.000192 
l0: 0.000244, l1: 0.000245, l2: 0.000281, l3: 0.000407, l4: 0.000856, l5: 0.001390, l6: 0.002813
[epoch: 374/1000, batch:   244/ 1052, ite: 98160] train loss: 0.004668, tar: 0.000192 
l0: 0.000279, l1: 0.000285, l2: 0.000272, l3: 0.000310, l4: 0.000626, l5: 0.001251, l6: 0.003083
[epoch: 374/1000, batch:   324/ 1052, ite: 98180] train loss: 0.004666, tar: 0.000192 
l0: 0.000297, l1: 0.000296, l2: 0.000320, l3: 0.000414, l4: 0.000726, l5: 0.001783, l6: 0.002823
[epoch: 374/1000, batch:   404/ 1052, ite: 98200] train loss: 0.004665, tar: 0.000192 
l0: 0.000081, l1: 0.000079, l2: 0.000129, l3: 0.000190, l4: 0.000258, l5: 0.000606, l6: 0.002421
[epoch: 374/1000, batch:   484/ 1052, ite: 98220] train loss: 0.004662, tar: 0.000192 
l0: 0.000164, l1: 0.000168, l2: 0.000171, l3: 0.000253, l4: 0.000582, l5: 0.001363, l6: 0.002517
[epoch: 374/1000, batch:   564/ 1052, ite: 98240] train loss: 0.004665, tar: 0.000192 
l0: 0.000190, l1: 0.000186, l2: 0.000223, l3: 0.000414, l4: 0.000810, l5: 0.001466, l6: 0.001745
[epoch: 374/1000, batch:   644/ 1052, ite: 98260] train loss: 0.004662, tar: 0.000192 
l0: 0.000272, l1: 0.000275, l2: 0.000328, l3: 0.000452, l4: 0.000854, l5: 0.001589, l6: 0.003751
[epoch: 374/1000, batch:   724/ 1052, ite: 98280] train loss: 0.004661, tar: 0.000192 
l0: 0.000106, l1: 0.000109, l2: 0.000127, l3: 0.000176, l4: 0.000326, l5: 0.000628, l6: 0.002147
[epoch: 374/1000, batch:   804/ 1052, ite: 98300] train loss: 0.004659, tar: 0.000192 
l0: 0.000253, l1: 0.000258, l2: 0.000264, l3: 0.000396, l4: 0.000677, l5: 0.001214, l6: 0.002968
[epoch: 374/1000, batch:   884/ 1052, ite: 98320] train loss: 0.004662, tar: 0.000192 
l0: 0.000194, l1: 0.000195, l2: 0.000235, l3: 0.000325, l4: 0.000625, l5: 0.001011, l6: 0.001420
[epoch: 374/1000, batch:   964/ 1052, ite: 98340] train loss: 0.004662, tar: 0.000192 
l0: 0.000248, l1: 0.000246, l2: 0.000270, l3: 0.000340, l4: 0.000443, l5: 0.000959, l6: 0.002634
[epoch: 374/1000, batch:  1044/ 1052, ite: 98360] train loss: 0.004661, tar: 0.000192 
[Epoch 374/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000188, l1: 0.000191, l2: 0.000202, l3: 0.000227, l4: 0.000336, l5: 0.000811, l6: 0.001407
[epoch: 375/1000, batch:    72/ 1052, ite: 98380] train loss: 0.004664, tar: 0.000192 
l0: 0.000115, l1: 0.000115, l2: 0.000153, l3: 0.000274, l4: 0.000605, l5: 0.001158, l6: 0.002166
[epoch: 375/1000, batch:   152/ 1052, ite: 98400] train loss: 0.004663, tar: 0.000192 
l0: 0.000184, l1: 0.000187, l2: 0.000196, l3: 0.000273, l4: 0.000558, l5: 0.001644, l6: 0.001684
[epoch: 375/1000, batch:   232/ 1052, ite: 98420] train loss: 0.004665, tar: 0.000192 
l0: 0.000147, l1: 0.000152, l2: 0.000156, l3: 0.000225, l4: 0.000374, l5: 0.000496, l6: 0.001453
[epoch: 375/1000, batch:   312/ 1052, ite: 98440] train loss: 0.004666, tar: 0.000192 
l0: 0.000188, l1: 0.000197, l2: 0.000217, l3: 0.000295, l4: 0.000658, l5: 0.001533, l6: 0.002290
[epoch: 375/1000, batch:   392/ 1052, ite: 98460] train loss: 0.004665, tar: 0.000192 
l0: 0.000304, l1: 0.000302, l2: 0.000339, l3: 0.000458, l4: 0.000776, l5: 0.001383, l6: 0.001991
[epoch: 375/1000, batch:   472/ 1052, ite: 98480] train loss: 0.004666, tar: 0.000192 
l0: 0.000291, l1: 0.000306, l2: 0.000294, l3: 0.000313, l4: 0.000637, l5: 0.001300, l6: 0.002283
[epoch: 375/1000, batch:   552/ 1052, ite: 98500] train loss: 0.004665, tar: 0.000192 
l0: 0.000226, l1: 0.000228, l2: 0.000301, l3: 0.000392, l4: 0.000802, l5: 0.001759, l6: 0.002646
[epoch: 375/1000, batch:   632/ 1052, ite: 98520] train loss: 0.004662, tar: 0.000192 
l0: 0.000205, l1: 0.000206, l2: 0.000263, l3: 0.000349, l4: 0.000646, l5: 0.001385, l6: 0.002934
[epoch: 375/1000, batch:   712/ 1052, ite: 98540] train loss: 0.004664, tar: 0.000192 
l0: 0.000156, l1: 0.000155, l2: 0.000217, l3: 0.000493, l4: 0.000966, l5: 0.001680, l6: 0.002597
[epoch: 375/1000, batch:   792/ 1052, ite: 98560] train loss: 0.004664, tar: 0.000192 
l0: 0.000113, l1: 0.000110, l2: 0.000143, l3: 0.000217, l4: 0.000315, l5: 0.000616, l6: 0.001474
[epoch: 375/1000, batch:   872/ 1052, ite: 98580] train loss: 0.004664, tar: 0.000192 
l0: 0.000173, l1: 0.000175, l2: 0.000185, l3: 0.000271, l4: 0.000501, l5: 0.001219, l6: 0.002888
[epoch: 375/1000, batch:   952/ 1052, ite: 98600] train loss: 0.004663, tar: 0.000191 
l0: 0.000209, l1: 0.000208, l2: 0.000246, l3: 0.000304, l4: 0.000524, l5: 0.000795, l6: 0.001150
[epoch: 375/1000, batch:  1032/ 1052, ite: 98620] train loss: 0.004663, tar: 0.000192 
[Epoch 375/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000186, l1: 0.000188, l2: 0.000202, l3: 0.000283, l4: 0.000374, l5: 0.000865, l6: 0.001547
[epoch: 376/1000, batch:    60/ 1052, ite: 98640] train loss: 0.004661, tar: 0.000191 
l0: 0.000248, l1: 0.000253, l2: 0.000251, l3: 0.000316, l4: 0.000607, l5: 0.001506, l6: 0.003131
[epoch: 376/1000, batch:   140/ 1052, ite: 98660] train loss: 0.004658, tar: 0.000191 
l0: 0.000120, l1: 0.000123, l2: 0.000132, l3: 0.000182, l4: 0.000390, l5: 0.000869, l6: 0.001678
[epoch: 376/1000, batch:   220/ 1052, ite: 98680] train loss: 0.004659, tar: 0.000191 
l0: 0.000064, l1: 0.000065, l2: 0.000075, l3: 0.000145, l4: 0.000328, l5: 0.000586, l6: 0.001111
[epoch: 376/1000, batch:   300/ 1052, ite: 98700] train loss: 0.004662, tar: 0.000191 
l0: 0.000069, l1: 0.000067, l2: 0.000103, l3: 0.000159, l4: 0.000375, l5: 0.001083, l6: 0.001230
[epoch: 376/1000, batch:   380/ 1052, ite: 98720] train loss: 0.004664, tar: 0.000191 
l0: 0.000068, l1: 0.000066, l2: 0.000115, l3: 0.000187, l4: 0.000376, l5: 0.000474, l6: 0.001071
[epoch: 376/1000, batch:   460/ 1052, ite: 98740] train loss: 0.004664, tar: 0.000191 
l0: 0.000169, l1: 0.000173, l2: 0.000210, l3: 0.000182, l4: 0.000455, l5: 0.000978, l6: 0.002022
[epoch: 376/1000, batch:   540/ 1052, ite: 98760] train loss: 0.004663, tar: 0.000191 
l0: 0.000130, l1: 0.000131, l2: 0.000176, l3: 0.000208, l4: 0.000567, l5: 0.001031, l6: 0.001750
[epoch: 376/1000, batch:   620/ 1052, ite: 98780] train loss: 0.004663, tar: 0.000191 
l0: 0.000101, l1: 0.000100, l2: 0.000133, l3: 0.000137, l4: 0.000344, l5: 0.000628, l6: 0.001220
[epoch: 376/1000, batch:   700/ 1052, ite: 98800] train loss: 0.004660, tar: 0.000191 
l0: 0.000266, l1: 0.000273, l2: 0.000291, l3: 0.000304, l4: 0.000447, l5: 0.000890, l6: 0.002756
[epoch: 376/1000, batch:   780/ 1052, ite: 98820] train loss: 0.004660, tar: 0.000191 
l0: 0.000185, l1: 0.000193, l2: 0.000237, l3: 0.000257, l4: 0.000550, l5: 0.000850, l6: 0.002776
[epoch: 376/1000, batch:   860/ 1052, ite: 98840] train loss: 0.004660, tar: 0.000191 
l0: 0.000149, l1: 0.000149, l2: 0.000197, l3: 0.000274, l4: 0.000415, l5: 0.000922, l6: 0.001950
[epoch: 376/1000, batch:   940/ 1052, ite: 98860] train loss: 0.004662, tar: 0.000191 
l0: 0.000153, l1: 0.000160, l2: 0.000169, l3: 0.000241, l4: 0.000707, l5: 0.002095, l6: 0.002928
[epoch: 376/1000, batch:  1020/ 1052, ite: 98880] train loss: 0.004662, tar: 0.000191 
[Epoch 376/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000361, l1: 0.000365, l2: 0.000453, l3: 0.000513, l4: 0.001086, l5: 0.001674, l6: 0.003253
[epoch: 377/1000, batch:    48/ 1052, ite: 98900] train loss: 0.004664, tar: 0.000191 
l0: 0.000143, l1: 0.000144, l2: 0.000193, l3: 0.000293, l4: 0.000410, l5: 0.001079, l6: 0.002718
[epoch: 377/1000, batch:   128/ 1052, ite: 98920] train loss: 0.004665, tar: 0.000191 
l0: 0.000108, l1: 0.000106, l2: 0.000132, l3: 0.000214, l4: 0.000618, l5: 0.000648, l6: 0.001432
[epoch: 377/1000, batch:   208/ 1052, ite: 98940] train loss: 0.004664, tar: 0.000192 
l0: 0.000179, l1: 0.000181, l2: 0.000186, l3: 0.000218, l4: 0.000509, l5: 0.000686, l6: 0.001902
[epoch: 377/1000, batch:   288/ 1052, ite: 98960] train loss: 0.004666, tar: 0.000192 
l0: 0.000222, l1: 0.000228, l2: 0.000222, l3: 0.000369, l4: 0.000702, l5: 0.001409, l6: 0.001676
[epoch: 377/1000, batch:   368/ 1052, ite: 98980] train loss: 0.004667, tar: 0.000192 
l0: 0.000178, l1: 0.000175, l2: 0.000242, l3: 0.000351, l4: 0.000486, l5: 0.001270, l6: 0.002079
[epoch: 377/1000, batch:   448/ 1052, ite: 99000] train loss: 0.004667, tar: 0.000192 
l0: 0.000222, l1: 0.000221, l2: 0.000267, l3: 0.000331, l4: 0.000554, l5: 0.001231, l6: 0.001836
[epoch: 377/1000, batch:   528/ 1052, ite: 99020] train loss: 0.004671, tar: 0.000192 
l0: 0.000260, l1: 0.000263, l2: 0.000292, l3: 0.000384, l4: 0.000622, l5: 0.001263, l6: 0.002384
[epoch: 377/1000, batch:   608/ 1052, ite: 99040] train loss: 0.004671, tar: 0.000193 
l0: 0.000219, l1: 0.000214, l2: 0.000250, l3: 0.000390, l4: 0.000724, l5: 0.002331, l6: 0.003389
[epoch: 377/1000, batch:   688/ 1052, ite: 99060] train loss: 0.004673, tar: 0.000193 
l0: 0.000286, l1: 0.000289, l2: 0.000360, l3: 0.000378, l4: 0.000588, l5: 0.001063, l6: 0.002504
[epoch: 377/1000, batch:   768/ 1052, ite: 99080] train loss: 0.004672, tar: 0.000193 
l0: 0.000280, l1: 0.000276, l2: 0.000321, l3: 0.000442, l4: 0.000711, l5: 0.001321, l6: 0.002231
[epoch: 377/1000, batch:   848/ 1052, ite: 99100] train loss: 0.004671, tar: 0.000193 
l0: 0.000247, l1: 0.000252, l2: 0.000307, l3: 0.000411, l4: 0.001124, l5: 0.001503, l6: 0.003663
[epoch: 377/1000, batch:   928/ 1052, ite: 99120] train loss: 0.004669, tar: 0.000193 
l0: 0.000071, l1: 0.000072, l2: 0.000089, l3: 0.000124, l4: 0.000399, l5: 0.000707, l6: 0.001449
[epoch: 377/1000, batch:  1008/ 1052, ite: 99140] train loss: 0.004667, tar: 0.000193 
[Epoch 377/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000150, l1: 0.000147, l2: 0.000211, l3: 0.000270, l4: 0.000553, l5: 0.000707, l6: 0.001621
[epoch: 378/1000, batch:    36/ 1052, ite: 99160] train loss: 0.004668, tar: 0.000193 
l0: 0.000122, l1: 0.000126, l2: 0.000134, l3: 0.000157, l4: 0.000421, l5: 0.000663, l6: 0.001272
[epoch: 378/1000, batch:   116/ 1052, ite: 99180] train loss: 0.004668, tar: 0.000193 
l0: 0.000078, l1: 0.000080, l2: 0.000079, l3: 0.000180, l4: 0.000404, l5: 0.000822, l6: 0.001275
[epoch: 378/1000, batch:   196/ 1052, ite: 99200] train loss: 0.004669, tar: 0.000193 
l0: 0.000108, l1: 0.000115, l2: 0.000131, l3: 0.000140, l4: 0.000304, l5: 0.000863, l6: 0.000890
[epoch: 378/1000, batch:   276/ 1052, ite: 99220] train loss: 0.004670, tar: 0.000193 
l0: 0.000121, l1: 0.000115, l2: 0.000160, l3: 0.000262, l4: 0.000533, l5: 0.000799, l6: 0.001607
[epoch: 378/1000, batch:   356/ 1052, ite: 99240] train loss: 0.004670, tar: 0.000193 
l0: 0.000172, l1: 0.000169, l2: 0.000199, l3: 0.000246, l4: 0.000404, l5: 0.001043, l6: 0.002098
[epoch: 378/1000, batch:   436/ 1052, ite: 99260] train loss: 0.004672, tar: 0.000193 
l0: 0.000098, l1: 0.000096, l2: 0.000118, l3: 0.000155, l4: 0.000420, l5: 0.000694, l6: 0.001315
[epoch: 378/1000, batch:   516/ 1052, ite: 99280] train loss: 0.004669, tar: 0.000193 
l0: 0.000131, l1: 0.000128, l2: 0.000195, l3: 0.000245, l4: 0.000378, l5: 0.001141, l6: 0.001729
[epoch: 378/1000, batch:   596/ 1052, ite: 99300] train loss: 0.004668, tar: 0.000193 
l0: 0.000166, l1: 0.000162, l2: 0.000218, l3: 0.000295, l4: 0.000633, l5: 0.001182, l6: 0.001994
[epoch: 378/1000, batch:   676/ 1052, ite: 99320] train loss: 0.004668, tar: 0.000193 
l0: 0.000348, l1: 0.000353, l2: 0.000342, l3: 0.000366, l4: 0.000457, l5: 0.000686, l6: 0.001143
[epoch: 378/1000, batch:   756/ 1052, ite: 99340] train loss: 0.004668, tar: 0.000193 
l0: 0.000240, l1: 0.000237, l2: 0.000277, l3: 0.000384, l4: 0.000671, l5: 0.001353, l6: 0.001588
[epoch: 378/1000, batch:   836/ 1052, ite: 99360] train loss: 0.004667, tar: 0.000193 
l0: 0.000186, l1: 0.000181, l2: 0.000240, l3: 0.000290, l4: 0.000454, l5: 0.000894, l6: 0.001441
[epoch: 378/1000, batch:   916/ 1052, ite: 99380] train loss: 0.004670, tar: 0.000193 
l0: 0.000124, l1: 0.000128, l2: 0.000167, l3: 0.000204, l4: 0.000551, l5: 0.001149, l6: 0.001773
[epoch: 378/1000, batch:   996/ 1052, ite: 99400] train loss: 0.004669, tar: 0.000193 
[Epoch 378/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000325, l1: 0.000333, l2: 0.000344, l3: 0.000408, l4: 0.000381, l5: 0.000692, l6: 0.000963
[epoch: 379/1000, batch:    24/ 1052, ite: 99420] train loss: 0.004668, tar: 0.000193 
l0: 0.000266, l1: 0.000273, l2: 0.000253, l3: 0.000284, l4: 0.000430, l5: 0.001170, l6: 0.002933
[epoch: 379/1000, batch:   104/ 1052, ite: 99440] train loss: 0.004666, tar: 0.000193 
l0: 0.000323, l1: 0.000339, l2: 0.000368, l3: 0.000373, l4: 0.000512, l5: 0.001020, l6: 0.001786
[epoch: 379/1000, batch:   184/ 1052, ite: 99460] train loss: 0.004667, tar: 0.000193 
l0: 0.000150, l1: 0.000157, l2: 0.000170, l3: 0.000183, l4: 0.000404, l5: 0.000940, l6: 0.000965
[epoch: 379/1000, batch:   264/ 1052, ite: 99480] train loss: 0.004664, tar: 0.000192 
l0: 0.000223, l1: 0.000219, l2: 0.000315, l3: 0.000536, l4: 0.000837, l5: 0.001775, l6: 0.003431
[epoch: 379/1000, batch:   344/ 1052, ite: 99500] train loss: 0.004665, tar: 0.000192 
l0: 0.000102, l1: 0.000103, l2: 0.000139, l3: 0.000274, l4: 0.000625, l5: 0.001292, l6: 0.002226
[epoch: 379/1000, batch:   424/ 1052, ite: 99520] train loss: 0.004668, tar: 0.000193 
l0: 0.000122, l1: 0.000125, l2: 0.000143, l3: 0.000202, l4: 0.000362, l5: 0.000885, l6: 0.001033
[epoch: 379/1000, batch:   504/ 1052, ite: 99540] train loss: 0.004667, tar: 0.000193 
l0: 0.000152, l1: 0.000148, l2: 0.000192, l3: 0.000253, l4: 0.000745, l5: 0.001398, l6: 0.001894
[epoch: 379/1000, batch:   584/ 1052, ite: 99560] train loss: 0.004665, tar: 0.000192 
l0: 0.000203, l1: 0.000204, l2: 0.000234, l3: 0.000291, l4: 0.000489, l5: 0.001793, l6: 0.001518
[epoch: 379/1000, batch:   664/ 1052, ite: 99580] train loss: 0.004663, tar: 0.000192 
l0: 0.000207, l1: 0.000213, l2: 0.000228, l3: 0.000298, l4: 0.000423, l5: 0.001300, l6: 0.002423
[epoch: 379/1000, batch:   744/ 1052, ite: 99600] train loss: 0.004664, tar: 0.000192 
l0: 0.000100, l1: 0.000103, l2: 0.000120, l3: 0.000176, l4: 0.000302, l5: 0.000650, l6: 0.001295
[epoch: 379/1000, batch:   824/ 1052, ite: 99620] train loss: 0.004666, tar: 0.000192 
l0: 0.000202, l1: 0.000204, l2: 0.000239, l3: 0.000304, l4: 0.000620, l5: 0.001836, l6: 0.002620
[epoch: 379/1000, batch:   904/ 1052, ite: 99640] train loss: 0.004666, tar: 0.000192 
l0: 0.000228, l1: 0.000234, l2: 0.000230, l3: 0.000314, l4: 0.000517, l5: 0.001282, l6: 0.001985
[epoch: 379/1000, batch:   984/ 1052, ite: 99660] train loss: 0.004667, tar: 0.000192 
[Epoch 379/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000156, l1: 0.000159, l2: 0.000199, l3: 0.000233, l4: 0.000508, l5: 0.001217, l6: 0.001575
[epoch: 380/1000, batch:    12/ 1052, ite: 99680] train loss: 0.004669, tar: 0.000193 
l0: 0.000285, l1: 0.000295, l2: 0.000310, l3: 0.000424, l4: 0.000673, l5: 0.001416, l6: 0.002006
[epoch: 380/1000, batch:    92/ 1052, ite: 99700] train loss: 0.004668, tar: 0.000192 
l0: 0.000225, l1: 0.000240, l2: 0.000220, l3: 0.000232, l4: 0.000500, l5: 0.000732, l6: 0.001871
[epoch: 380/1000, batch:   172/ 1052, ite: 99720] train loss: 0.004667, tar: 0.000192 
l0: 0.000213, l1: 0.000216, l2: 0.000236, l3: 0.000352, l4: 0.000673, l5: 0.001377, l6: 0.002175
[epoch: 380/1000, batch:   252/ 1052, ite: 99740] train loss: 0.004668, tar: 0.000192 
l0: 0.000284, l1: 0.000288, l2: 0.000308, l3: 0.000481, l4: 0.000807, l5: 0.001349, l6: 0.002358
[epoch: 380/1000, batch:   332/ 1052, ite: 99760] train loss: 0.004669, tar: 0.000192 
l0: 0.000161, l1: 0.000159, l2: 0.000192, l3: 0.000283, l4: 0.000515, l5: 0.000982, l6: 0.001494
[epoch: 380/1000, batch:   412/ 1052, ite: 99780] train loss: 0.004668, tar: 0.000192 
l0: 0.000232, l1: 0.000237, l2: 0.000247, l3: 0.000358, l4: 0.000903, l5: 0.001440, l6: 0.003431
[epoch: 380/1000, batch:   492/ 1052, ite: 99800] train loss: 0.004667, tar: 0.000192 
l0: 0.000178, l1: 0.000178, l2: 0.000254, l3: 0.000442, l4: 0.000772, l5: 0.001653, l6: 0.002958
[epoch: 380/1000, batch:   572/ 1052, ite: 99820] train loss: 0.004668, tar: 0.000192 
l0: 0.000097, l1: 0.000098, l2: 0.000137, l3: 0.000182, l4: 0.000415, l5: 0.000775, l6: 0.001136
[epoch: 380/1000, batch:   652/ 1052, ite: 99840] train loss: 0.004669, tar: 0.000192 
l0: 0.000299, l1: 0.000300, l2: 0.000448, l3: 0.000648, l4: 0.000922, l5: 0.001636, l6: 0.002457
[epoch: 380/1000, batch:   732/ 1052, ite: 99860] train loss: 0.004669, tar: 0.000192 
l0: 0.000169, l1: 0.000172, l2: 0.000222, l3: 0.000358, l4: 0.000392, l5: 0.001055, l6: 0.002915
[epoch: 380/1000, batch:   812/ 1052, ite: 99880] train loss: 0.004668, tar: 0.000192 
l0: 0.000296, l1: 0.000294, l2: 0.000378, l3: 0.000463, l4: 0.000682, l5: 0.001415, l6: 0.002271
[epoch: 380/1000, batch:   892/ 1052, ite: 99900] train loss: 0.004668, tar: 0.000192 
l0: 0.000088, l1: 0.000092, l2: 0.000108, l3: 0.000167, l4: 0.000572, l5: 0.000895, l6: 0.001690
[epoch: 380/1000, batch:   972/ 1052, ite: 99920] train loss: 0.004666, tar: 0.000192 
l0: 0.000342, l1: 0.000351, l2: 0.000347, l3: 0.000406, l4: 0.000695, l5: 0.001609, l6: 0.002587
[epoch: 380/1000, batch:  1052/ 1052, ite: 99940] train loss: 0.004665, tar: 0.000192 
[Epoch 380/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000133, l1: 0.000134, l2: 0.000168, l3: 0.000200, l4: 0.000352, l5: 0.000805, l6: 0.001175
[epoch: 381/1000, batch:    80/ 1052, ite: 99960] train loss: 0.004665, tar: 0.000192 
l0: 0.000434, l1: 0.000428, l2: 0.000471, l3: 0.000579, l4: 0.000568, l5: 0.001205, l6: 0.001701
[epoch: 381/1000, batch:   160/ 1052, ite: 99980] train loss: 0.004666, tar: 0.000192 
l0: 0.000111, l1: 0.000111, l2: 0.000133, l3: 0.000172, l4: 0.000294, l5: 0.000747, l6: 0.002211
[epoch: 381/1000, batch:   240/ 1052, ite: 100000] train loss: 0.004665, tar: 0.000192 
l0: 0.000087, l1: 0.000089, l2: 0.000102, l3: 0.000166, l4: 0.000316, l5: 0.000760, l6: 0.001665
[epoch: 381/1000, batch:   320/ 1052, ite: 100020] train loss: 0.004666, tar: 0.000192 
l0: 0.000061, l1: 0.000063, l2: 0.000100, l3: 0.000193, l4: 0.000362, l5: 0.000650, l6: 0.001842
[epoch: 381/1000, batch:   400/ 1052, ite: 100040] train loss: 0.004665, tar: 0.000192 
l0: 0.000098, l1: 0.000098, l2: 0.000126, l3: 0.000226, l4: 0.000355, l5: 0.000895, l6: 0.001359
[epoch: 381/1000, batch:   480/ 1052, ite: 100060] train loss: 0.004665, tar: 0.000192 
l0: 0.000129, l1: 0.000130, l2: 0.000164, l3: 0.000235, l4: 0.000332, l5: 0.000801, l6: 0.002540
[epoch: 381/1000, batch:   560/ 1052, ite: 100080] train loss: 0.004666, tar: 0.000192 
l0: 0.000087, l1: 0.000096, l2: 0.000095, l3: 0.000130, l4: 0.000390, l5: 0.000487, l6: 0.001061
[epoch: 381/1000, batch:   640/ 1052, ite: 100100] train loss: 0.004664, tar: 0.000192 
l0: 0.000115, l1: 0.000113, l2: 0.000131, l3: 0.000186, l4: 0.000383, l5: 0.000634, l6: 0.000865
[epoch: 381/1000, batch:   720/ 1052, ite: 100120] train loss: 0.004663, tar: 0.000192 
l0: 0.000116, l1: 0.000117, l2: 0.000131, l3: 0.000194, l4: 0.000415, l5: 0.000951, l6: 0.001237
[epoch: 381/1000, batch:   800/ 1052, ite: 100140] train loss: 0.004662, tar: 0.000192 
l0: 0.000247, l1: 0.000244, l2: 0.000318, l3: 0.000419, l4: 0.000486, l5: 0.001561, l6: 0.001911
[epoch: 381/1000, batch:   880/ 1052, ite: 100160] train loss: 0.004662, tar: 0.000192 
l0: 0.000138, l1: 0.000142, l2: 0.000137, l3: 0.000162, l4: 0.000264, l5: 0.000531, l6: 0.001217
[epoch: 381/1000, batch:   960/ 1052, ite: 100180] train loss: 0.004661, tar: 0.000192 
l0: 0.000118, l1: 0.000118, l2: 0.000134, l3: 0.000226, l4: 0.000418, l5: 0.000948, l6: 0.001104
[epoch: 381/1000, batch:  1040/ 1052, ite: 100200] train loss: 0.004662, tar: 0.000192 
[Epoch 381/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000154, l1: 0.000155, l2: 0.000201, l3: 0.000387, l4: 0.000657, l5: 0.001050, l6: 0.002286
[epoch: 382/1000, batch:    68/ 1052, ite: 100220] train loss: 0.004661, tar: 0.000192 
l0: 0.000163, l1: 0.000163, l2: 0.000168, l3: 0.000268, l4: 0.000378, l5: 0.000977, l6: 0.001371
[epoch: 382/1000, batch:   148/ 1052, ite: 100240] train loss: 0.004660, tar: 0.000192 
l0: 0.000235, l1: 0.000225, l2: 0.000257, l3: 0.000365, l4: 0.000500, l5: 0.001013, l6: 0.001672
[epoch: 382/1000, batch:   228/ 1052, ite: 100260] train loss: 0.004660, tar: 0.000192 
l0: 0.000426, l1: 0.000457, l2: 0.000396, l3: 0.000399, l4: 0.000532, l5: 0.001280, l6: 0.003471
[epoch: 382/1000, batch:   308/ 1052, ite: 100280] train loss: 0.004659, tar: 0.000192 
l0: 0.000226, l1: 0.000227, l2: 0.000278, l3: 0.000364, l4: 0.000531, l5: 0.001048, l6: 0.002257
[epoch: 382/1000, batch:   388/ 1052, ite: 100300] train loss: 0.004658, tar: 0.000191 
l0: 0.000137, l1: 0.000146, l2: 0.000134, l3: 0.000161, l4: 0.000393, l5: 0.000670, l6: 0.001239
[epoch: 382/1000, batch:   468/ 1052, ite: 100320] train loss: 0.004659, tar: 0.000191 
l0: 0.000138, l1: 0.000139, l2: 0.000181, l3: 0.000251, l4: 0.000543, l5: 0.001284, l6: 0.003146
[epoch: 382/1000, batch:   548/ 1052, ite: 100340] train loss: 0.004658, tar: 0.000191 
l0: 0.000173, l1: 0.000174, l2: 0.000204, l3: 0.000251, l4: 0.000403, l5: 0.001072, l6: 0.002939
[epoch: 382/1000, batch:   628/ 1052, ite: 100360] train loss: 0.004658, tar: 0.000191 
l0: 0.000074, l1: 0.000071, l2: 0.000093, l3: 0.000138, l4: 0.000274, l5: 0.000386, l6: 0.000771
[epoch: 382/1000, batch:   708/ 1052, ite: 100380] train loss: 0.004654, tar: 0.000191 
l0: 0.000309, l1: 0.000314, l2: 0.000343, l3: 0.000409, l4: 0.000632, l5: 0.001223, l6: 0.002794
[epoch: 382/1000, batch:   788/ 1052, ite: 100400] train loss: 0.004655, tar: 0.000191 
l0: 0.000154, l1: 0.000156, l2: 0.000191, l3: 0.000271, l4: 0.000669, l5: 0.000783, l6: 0.003097
[epoch: 382/1000, batch:   868/ 1052, ite: 100420] train loss: 0.004655, tar: 0.000191 
l0: 0.000197, l1: 0.000197, l2: 0.000231, l3: 0.000247, l4: 0.000359, l5: 0.000798, l6: 0.001482
[epoch: 382/1000, batch:   948/ 1052, ite: 100440] train loss: 0.004655, tar: 0.000191 
l0: 0.000188, l1: 0.000196, l2: 0.000207, l3: 0.000249, l4: 0.000518, l5: 0.000819, l6: 0.002368
[epoch: 382/1000, batch:  1028/ 1052, ite: 100460] train loss: 0.004655, tar: 0.000191 
[Epoch 382/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000277, l1: 0.000276, l2: 0.000321, l3: 0.000471, l4: 0.000608, l5: 0.001344, l6: 0.002789
[epoch: 383/1000, batch:    56/ 1052, ite: 100480] train loss: 0.004656, tar: 0.000191 
l0: 0.000163, l1: 0.000160, l2: 0.000222, l3: 0.000260, l4: 0.000510, l5: 0.001239, l6: 0.002411
[epoch: 383/1000, batch:   136/ 1052, ite: 100500] train loss: 0.004655, tar: 0.000191 
l0: 0.000301, l1: 0.000306, l2: 0.000339, l3: 0.000383, l4: 0.000748, l5: 0.001454, l6: 0.003988
[epoch: 383/1000, batch:   216/ 1052, ite: 100520] train loss: 0.004655, tar: 0.000191 
l0: 0.000175, l1: 0.000175, l2: 0.000229, l3: 0.000357, l4: 0.000849, l5: 0.002164, l6: 0.003304
[epoch: 383/1000, batch:   296/ 1052, ite: 100540] train loss: 0.004656, tar: 0.000191 
l0: 0.000144, l1: 0.000149, l2: 0.000169, l3: 0.000277, l4: 0.000555, l5: 0.000768, l6: 0.001495
[epoch: 383/1000, batch:   376/ 1052, ite: 100560] train loss: 0.004655, tar: 0.000191 
l0: 0.000091, l1: 0.000095, l2: 0.000112, l3: 0.000137, l4: 0.000392, l5: 0.000839, l6: 0.001200
[epoch: 383/1000, batch:   456/ 1052, ite: 100580] train loss: 0.004657, tar: 0.000191 
l0: 0.000199, l1: 0.000201, l2: 0.000205, l3: 0.000315, l4: 0.000554, l5: 0.001342, l6: 0.002624
[epoch: 383/1000, batch:   536/ 1052, ite: 100600] train loss: 0.004659, tar: 0.000191 
l0: 0.000081, l1: 0.000078, l2: 0.000115, l3: 0.000220, l4: 0.000406, l5: 0.000996, l6: 0.000953
[epoch: 383/1000, batch:   616/ 1052, ite: 100620] train loss: 0.004657, tar: 0.000191 
l0: 0.000144, l1: 0.000148, l2: 0.000164, l3: 0.000187, l4: 0.000356, l5: 0.000836, l6: 0.001480
[epoch: 383/1000, batch:   696/ 1052, ite: 100640] train loss: 0.004658, tar: 0.000191 
l0: 0.000105, l1: 0.000103, l2: 0.000157, l3: 0.000191, l4: 0.000497, l5: 0.000857, l6: 0.001382
[epoch: 383/1000, batch:   776/ 1052, ite: 100660] train loss: 0.004659, tar: 0.000191 
l0: 0.000159, l1: 0.000157, l2: 0.000216, l3: 0.000325, l4: 0.000475, l5: 0.000827, l6: 0.001775
[epoch: 383/1000, batch:   856/ 1052, ite: 100680] train loss: 0.004659, tar: 0.000191 
l0: 0.000149, l1: 0.000150, l2: 0.000169, l3: 0.000187, l4: 0.000521, l5: 0.000666, l6: 0.001333
[epoch: 383/1000, batch:   936/ 1052, ite: 100700] train loss: 0.004658, tar: 0.000191 
l0: 0.000099, l1: 0.000100, l2: 0.000153, l3: 0.000172, l4: 0.000302, l5: 0.000769, l6: 0.001109
[epoch: 383/1000, batch:  1016/ 1052, ite: 100720] train loss: 0.004657, tar: 0.000191 
[Epoch 383/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000126, l1: 0.000127, l2: 0.000142, l3: 0.000180, l4: 0.000324, l5: 0.000589, l6: 0.001389
[epoch: 384/1000, batch:    44/ 1052, ite: 100740] train loss: 0.004655, tar: 0.000191 
l0: 0.000107, l1: 0.000108, l2: 0.000134, l3: 0.000225, l4: 0.000504, l5: 0.001126, l6: 0.001365
[epoch: 384/1000, batch:   124/ 1052, ite: 100760] train loss: 0.004656, tar: 0.000191 
l0: 0.000123, l1: 0.000126, l2: 0.000129, l3: 0.000163, l4: 0.000312, l5: 0.000539, l6: 0.001016
[epoch: 384/1000, batch:   204/ 1052, ite: 100780] train loss: 0.004656, tar: 0.000191 
l0: 0.000077, l1: 0.000078, l2: 0.000116, l3: 0.000210, l4: 0.000245, l5: 0.000876, l6: 0.001552
[epoch: 384/1000, batch:   284/ 1052, ite: 100800] train loss: 0.004655, tar: 0.000191 
l0: 0.000091, l1: 0.000087, l2: 0.000132, l3: 0.000230, l4: 0.000294, l5: 0.000695, l6: 0.001707
[epoch: 384/1000, batch:   364/ 1052, ite: 100820] train loss: 0.004655, tar: 0.000191 
l0: 0.000203, l1: 0.000199, l2: 0.000246, l3: 0.000313, l4: 0.000595, l5: 0.001455, l6: 0.002999
[epoch: 384/1000, batch:   444/ 1052, ite: 100840] train loss: 0.004655, tar: 0.000191 
l0: 0.000339, l1: 0.000346, l2: 0.000357, l3: 0.000409, l4: 0.000788, l5: 0.002198, l6: 0.003069
[epoch: 384/1000, batch:   524/ 1052, ite: 100860] train loss: 0.004657, tar: 0.000191 
l0: 0.000152, l1: 0.000149, l2: 0.000188, l3: 0.000260, l4: 0.000394, l5: 0.001018, l6: 0.001817
[epoch: 384/1000, batch:   604/ 1052, ite: 100880] train loss: 0.004657, tar: 0.000191 
l0: 0.000333, l1: 0.000332, l2: 0.000363, l3: 0.000411, l4: 0.000880, l5: 0.001560, l6: 0.003273
[epoch: 384/1000, batch:   684/ 1052, ite: 100900] train loss: 0.004656, tar: 0.000191 
l0: 0.000173, l1: 0.000177, l2: 0.000203, l3: 0.000224, l4: 0.000364, l5: 0.001008, l6: 0.001223
[epoch: 384/1000, batch:   764/ 1052, ite: 100920] train loss: 0.004656, tar: 0.000191 
l0: 0.000086, l1: 0.000083, l2: 0.000120, l3: 0.000311, l4: 0.000591, l5: 0.000689, l6: 0.001654
[epoch: 384/1000, batch:   844/ 1052, ite: 100940] train loss: 0.004657, tar: 0.000191 
l0: 0.000144, l1: 0.000144, l2: 0.000168, l3: 0.000248, l4: 0.000344, l5: 0.001222, l6: 0.001717
[epoch: 384/1000, batch:   924/ 1052, ite: 100960] train loss: 0.004657, tar: 0.000191 
l0: 0.000229, l1: 0.000234, l2: 0.000298, l3: 0.000496, l4: 0.000849, l5: 0.001852, l6: 0.003065
[epoch: 384/1000, batch:  1004/ 1052, ite: 100980] train loss: 0.004656, tar: 0.000191 
[Epoch 384/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000126, l1: 0.000127, l2: 0.000162, l3: 0.000208, l4: 0.000440, l5: 0.001146, l6: 0.001827
[epoch: 385/1000, batch:    32/ 1052, ite: 101000] train loss: 0.004656, tar: 0.000191 
l0: 0.000309, l1: 0.000310, l2: 0.000339, l3: 0.000429, l4: 0.000868, l5: 0.001640, l6: 0.002929
[epoch: 385/1000, batch:   112/ 1052, ite: 101020] train loss: 0.004656, tar: 0.000191 
l0: 0.000094, l1: 0.000089, l2: 0.000141, l3: 0.000223, l4: 0.000408, l5: 0.000777, l6: 0.001605
[epoch: 385/1000, batch:   192/ 1052, ite: 101040] train loss: 0.004658, tar: 0.000191 
l0: 0.000144, l1: 0.000143, l2: 0.000150, l3: 0.000207, l4: 0.000346, l5: 0.000697, l6: 0.001445
[epoch: 385/1000, batch:   272/ 1052, ite: 101060] train loss: 0.004657, tar: 0.000190 
l0: 0.000194, l1: 0.000191, l2: 0.000245, l3: 0.000352, l4: 0.000564, l5: 0.001268, l6: 0.002234
[epoch: 385/1000, batch:   352/ 1052, ite: 101080] train loss: 0.004656, tar: 0.000190 
l0: 0.000080, l1: 0.000081, l2: 0.000103, l3: 0.000144, l4: 0.000194, l5: 0.000541, l6: 0.001220
[epoch: 385/1000, batch:   432/ 1052, ite: 101100] train loss: 0.004658, tar: 0.000190 
l0: 0.000220, l1: 0.000225, l2: 0.000243, l3: 0.000397, l4: 0.000722, l5: 0.001358, l6: 0.002215
[epoch: 385/1000, batch:   512/ 1052, ite: 101120] train loss: 0.004658, tar: 0.000190 
l0: 0.000217, l1: 0.000217, l2: 0.000279, l3: 0.000600, l4: 0.000913, l5: 0.001940, l6: 0.003706
[epoch: 385/1000, batch:   592/ 1052, ite: 101140] train loss: 0.004657, tar: 0.000190 
l0: 0.000194, l1: 0.000192, l2: 0.000190, l3: 0.000254, l4: 0.000310, l5: 0.000953, l6: 0.001355
[epoch: 385/1000, batch:   672/ 1052, ite: 101160] train loss: 0.004655, tar: 0.000190 
l0: 0.000108, l1: 0.000111, l2: 0.000146, l3: 0.000191, l4: 0.000413, l5: 0.001025, l6: 0.001040
[epoch: 385/1000, batch:   752/ 1052, ite: 101180] train loss: 0.004653, tar: 0.000190 
l0: 0.000061, l1: 0.000062, l2: 0.000094, l3: 0.000131, l4: 0.000369, l5: 0.000619, l6: 0.000836
[epoch: 385/1000, batch:   832/ 1052, ite: 101200] train loss: 0.004652, tar: 0.000190 
l0: 0.000121, l1: 0.000121, l2: 0.000164, l3: 0.000146, l4: 0.000390, l5: 0.000876, l6: 0.001569
[epoch: 385/1000, batch:   912/ 1052, ite: 101220] train loss: 0.004653, tar: 0.000190 
l0: 0.000225, l1: 0.000225, l2: 0.000306, l3: 0.000395, l4: 0.000665, l5: 0.001528, l6: 0.003163
[epoch: 385/1000, batch:   992/ 1052, ite: 101240] train loss: 0.004653, tar: 0.000190 
[Epoch 385/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000320, l1: 0.000322, l2: 0.000341, l3: 0.000381, l4: 0.000572, l5: 0.001334, l6: 0.002839
[epoch: 386/1000, batch:    20/ 1052, ite: 101260] train loss: 0.004652, tar: 0.000190 
l0: 0.000191, l1: 0.000193, l2: 0.000227, l3: 0.000275, l4: 0.000423, l5: 0.001365, l6: 0.001256
[epoch: 386/1000, batch:   100/ 1052, ite: 101280] train loss: 0.004653, tar: 0.000190 
l0: 0.000543, l1: 0.000561, l2: 0.000590, l3: 0.000679, l4: 0.001308, l5: 0.002329, l6: 0.004546
[epoch: 386/1000, batch:   180/ 1052, ite: 101300] train loss: 0.004655, tar: 0.000190 
l0: 0.000263, l1: 0.000270, l2: 0.000263, l3: 0.000350, l4: 0.000602, l5: 0.001084, l6: 0.003468
[epoch: 386/1000, batch:   260/ 1052, ite: 101320] train loss: 0.004655, tar: 0.000190 
l0: 0.000258, l1: 0.000256, l2: 0.000320, l3: 0.000436, l4: 0.000982, l5: 0.001763, l6: 0.002575
[epoch: 386/1000, batch:   340/ 1052, ite: 101340] train loss: 0.004653, tar: 0.000190 
l0: 0.000076, l1: 0.000076, l2: 0.000096, l3: 0.000143, l4: 0.000387, l5: 0.000495, l6: 0.001035
[epoch: 386/1000, batch:   420/ 1052, ite: 101360] train loss: 0.004652, tar: 0.000190 
l0: 0.000267, l1: 0.000267, l2: 0.000335, l3: 0.000489, l4: 0.000878, l5: 0.001297, l6: 0.003086
[epoch: 386/1000, batch:   500/ 1052, ite: 101380] train loss: 0.004656, tar: 0.000190 
l0: 0.000211, l1: 0.000217, l2: 0.000284, l3: 0.000302, l4: 0.000720, l5: 0.000993, l6: 0.002538
[epoch: 386/1000, batch:   580/ 1052, ite: 101400] train loss: 0.004655, tar: 0.000190 
l0: 0.000052, l1: 0.000054, l2: 0.000063, l3: 0.000109, l4: 0.000293, l5: 0.000705, l6: 0.001228
[epoch: 386/1000, batch:   660/ 1052, ite: 101420] train loss: 0.004655, tar: 0.000190 
l0: 0.000248, l1: 0.000251, l2: 0.000319, l3: 0.000356, l4: 0.000759, l5: 0.001171, l6: 0.003673
[epoch: 386/1000, batch:   740/ 1052, ite: 101440] train loss: 0.004656, tar: 0.000190 
l0: 0.000167, l1: 0.000162, l2: 0.000204, l3: 0.000270, l4: 0.000558, l5: 0.001059, l6: 0.001623
[epoch: 386/1000, batch:   820/ 1052, ite: 101460] train loss: 0.004655, tar: 0.000190 
l0: 0.000098, l1: 0.000102, l2: 0.000163, l3: 0.000176, l4: 0.000664, l5: 0.001327, l6: 0.001857
[epoch: 386/1000, batch:   900/ 1052, ite: 101480] train loss: 0.004655, tar: 0.000190 
l0: 0.000032, l1: 0.000034, l2: 0.000058, l3: 0.000113, l4: 0.000200, l5: 0.000689, l6: 0.001045
[epoch: 386/1000, batch:   980/ 1052, ite: 101500] train loss: 0.004653, tar: 0.000190 
[Epoch 386/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000132, l1: 0.000131, l2: 0.000148, l3: 0.000220, l4: 0.000288, l5: 0.000784, l6: 0.001598
[epoch: 387/1000, batch:     8/ 1052, ite: 101520] train loss: 0.004652, tar: 0.000189 
l0: 0.000179, l1: 0.000175, l2: 0.000214, l3: 0.000300, l4: 0.000596, l5: 0.001489, l6: 0.002071
[epoch: 387/1000, batch:    88/ 1052, ite: 101540] train loss: 0.004651, tar: 0.000189 
l0: 0.000138, l1: 0.000138, l2: 0.000190, l3: 0.000337, l4: 0.000621, l5: 0.001406, l6: 0.001738
[epoch: 387/1000, batch:   168/ 1052, ite: 101560] train loss: 0.004652, tar: 0.000189 
l0: 0.000313, l1: 0.000320, l2: 0.000351, l3: 0.000362, l4: 0.000535, l5: 0.000735, l6: 0.001448
[epoch: 387/1000, batch:   248/ 1052, ite: 101580] train loss: 0.004653, tar: 0.000189 
l0: 0.000018, l1: 0.000018, l2: 0.000023, l3: 0.000041, l4: 0.000132, l5: 0.000205, l6: 0.000264
[epoch: 387/1000, batch:   328/ 1052, ite: 101600] train loss: 0.004652, tar: 0.000189 
l0: 0.000249, l1: 0.000257, l2: 0.000280, l3: 0.000357, l4: 0.000597, l5: 0.001180, l6: 0.002388
[epoch: 387/1000, batch:   408/ 1052, ite: 101620] train loss: 0.004653, tar: 0.000189 
l0: 0.000203, l1: 0.000202, l2: 0.000246, l3: 0.000388, l4: 0.001010, l5: 0.001382, l6: 0.003297
[epoch: 387/1000, batch:   488/ 1052, ite: 101640] train loss: 0.004653, tar: 0.000189 
l0: 0.000083, l1: 0.000083, l2: 0.000095, l3: 0.000152, l4: 0.000213, l5: 0.000643, l6: 0.000698
[epoch: 387/1000, batch:   568/ 1052, ite: 101660] train loss: 0.004654, tar: 0.000189 
l0: 0.000274, l1: 0.000272, l2: 0.000319, l3: 0.000384, l4: 0.000597, l5: 0.001181, l6: 0.002970
[epoch: 387/1000, batch:   648/ 1052, ite: 101680] train loss: 0.004653, tar: 0.000189 
l0: 0.000228, l1: 0.000236, l2: 0.000263, l3: 0.000292, l4: 0.000638, l5: 0.001320, l6: 0.002426
[epoch: 387/1000, batch:   728/ 1052, ite: 101700] train loss: 0.004651, tar: 0.000189 
l0: 0.000106, l1: 0.000109, l2: 0.000161, l3: 0.000201, l4: 0.000290, l5: 0.000679, l6: 0.001661
[epoch: 387/1000, batch:   808/ 1052, ite: 101720] train loss: 0.004652, tar: 0.000189 
l0: 0.000177, l1: 0.000175, l2: 0.000231, l3: 0.000394, l4: 0.000954, l5: 0.001173, l6: 0.003479
[epoch: 387/1000, batch:   888/ 1052, ite: 101740] train loss: 0.004653, tar: 0.000189 
l0: 0.000136, l1: 0.000136, l2: 0.000189, l3: 0.000273, l4: 0.000578, l5: 0.001615, l6: 0.002399
[epoch: 387/1000, batch:   968/ 1052, ite: 101760] train loss: 0.004653, tar: 0.000189 
l0: 0.000124, l1: 0.000120, l2: 0.000160, l3: 0.000294, l4: 0.000471, l5: 0.001174, l6: 0.002147
[epoch: 387/1000, batch:  1048/ 1052, ite: 101780] train loss: 0.004652, tar: 0.000189 
[Epoch 387/1000] Test loss: 0.011, Test target loss: 0.002
l0: 0.000274, l1: 0.000275, l2: 0.000290, l3: 0.000430, l4: 0.000598, l5: 0.001292, l6: 0.002636
[epoch: 388/1000, batch:    76/ 1052, ite: 101800] train loss: 0.004651, tar: 0.000189 
l0: 0.000133, l1: 0.000134, l2: 0.000180, l3: 0.000263, l4: 0.000316, l5: 0.001004, l6: 0.002118
[epoch: 388/1000, batch:   156/ 1052, ite: 101820] train loss: 0.004651, tar: 0.000189 
l0: 0.000232, l1: 0.000233, l2: 0.000280, l3: 0.000392, l4: 0.000833, l5: 0.001440, l6: 0.002261
[epoch: 388/1000, batch:   236/ 1052, ite: 101840] train loss: 0.004651, tar: 0.000189 
l0: 0.000070, l1: 0.000069, l2: 0.000128, l3: 0.000206, l4: 0.000447, l5: 0.001174, l6: 0.001830
[epoch: 388/1000, batch:   316/ 1052, ite: 101860] train loss: 0.004651, tar: 0.000189 
l0: 0.000114, l1: 0.000111, l2: 0.000188, l3: 0.000284, l4: 0.000487, l5: 0.000944, l6: 0.002192
[epoch: 388/1000, batch:   396/ 1052, ite: 101880] train loss: 0.004648, tar: 0.000189 
l0: 0.000131, l1: 0.000132, l2: 0.000155, l3: 0.000233, l4: 0.000333, l5: 0.000900, l6: 0.000779
[epoch: 388/1000, batch:   476/ 1052, ite: 101900] train loss: 0.004647, tar: 0.000189 
l0: 0.000335, l1: 0.000343, l2: 0.000360, l3: 0.000452, l4: 0.000970, l5: 0.001444, l6: 0.002903
[epoch: 388/1000, batch:   556/ 1052, ite: 101920] train loss: 0.004649, tar: 0.000189 
l0: 0.000372, l1: 0.000374, l2: 0.000372, l3: 0.000424, l4: 0.000604, l5: 0.001203, l6: 0.001530
[epoch: 388/1000, batch:   636/ 1052, ite: 101940] train loss: 0.004650, tar: 0.000189 
l0: 0.000097, l1: 0.000093, l2: 0.000174, l3: 0.000260, l4: 0.000983, l5: 0.001356, l6: 0.002217
[epoch: 388/1000, batch:   716/ 1052, ite: 101960] train loss: 0.004651, tar: 0.000189 
l0: 0.000492, l1: 0.000506, l2: 0.000510, l3: 0.000632, l4: 0.000823, l5: 0.002162, l6: 0.004597
[epoch: 388/1000, batch:   796/ 1052, ite: 101980] train loss: 0.004653, tar: 0.000189 
l0: 0.000118, l1: 0.000116, l2: 0.000185, l3: 0.000273, l4: 0.000650, l5: 0.000834, l6: 0.001744
[epoch: 388/1000, batch:   876/ 1052, ite: 102000] train loss: 0.004652, tar: 0.000189 
l0: 0.000119, l1: 0.000114, l2: 0.000170, l3: 0.000260, l4: 0.000590, l5: 0.001131, l6: 0.001753
[epoch: 388/1000, batch:   956/ 1052, ite: 102020] train loss: 0.004653, tar: 0.000189 
l0: 0.000208, l1: 0.000210, l2: 0.000267, l3: 0.000332, l4: 0.000432, l5: 0.000956, l6: 0.001925
[epoch: 388/1000, batch:  1036/ 1052, ite: 102040] train loss: 0.004652, tar: 0.000189 
[Epoch 388/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000092, l1: 0.000125, l2: 0.000130, l3: 0.000145, l4: 0.000402, l5: 0.000666, l6: 0.001526
[epoch: 389/1000, batch:    64/ 1052, ite: 102060] train loss: 0.004653, tar: 0.000189 
l0: 0.000109, l1: 0.000111, l2: 0.000125, l3: 0.000186, l4: 0.000373, l5: 0.001254, l6: 0.002096
[epoch: 389/1000, batch:   144/ 1052, ite: 102080] train loss: 0.004654, tar: 0.000189 
l0: 0.000174, l1: 0.000179, l2: 0.000202, l3: 0.000248, l4: 0.000346, l5: 0.000785, l6: 0.001510
[epoch: 389/1000, batch:   224/ 1052, ite: 102100] train loss: 0.004654, tar: 0.000189 
l0: 0.000273, l1: 0.000292, l2: 0.000294, l3: 0.000417, l4: 0.000663, l5: 0.001183, l6: 0.004011
[epoch: 389/1000, batch:   304/ 1052, ite: 102120] train loss: 0.004655, tar: 0.000189 
l0: 0.000104, l1: 0.000108, l2: 0.000139, l3: 0.000221, l4: 0.000464, l5: 0.000796, l6: 0.001379
[epoch: 389/1000, batch:   384/ 1052, ite: 102140] train loss: 0.004654, tar: 0.000189 
l0: 0.000125, l1: 0.000164, l2: 0.000130, l3: 0.000212, l4: 0.000391, l5: 0.000552, l6: 0.001048
[epoch: 389/1000, batch:   464/ 1052, ite: 102160] train loss: 0.004653, tar: 0.000189 
l0: 0.000107, l1: 0.000108, l2: 0.000140, l3: 0.000207, l4: 0.000473, l5: 0.000972, l6: 0.002157
[epoch: 389/1000, batch:   544/ 1052, ite: 102180] train loss: 0.004652, tar: 0.000189 
l0: 0.000207, l1: 0.000213, l2: 0.000204, l3: 0.000254, l4: 0.000341, l5: 0.000917, l6: 0.001412
[epoch: 389/1000, batch:   624/ 1052, ite: 102200] train loss: 0.004653, tar: 0.000189 
l0: 0.000117, l1: 0.000117, l2: 0.000148, l3: 0.000216, l4: 0.000561, l5: 0.000796, l6: 0.001589
[epoch: 389/1000, batch:   704/ 1052, ite: 102220] train loss: 0.004653, tar: 0.000189 
l0: 0.000153, l1: 0.000152, l2: 0.000190, l3: 0.000335, l4: 0.000737, l5: 0.001042, l6: 0.002018
[epoch: 389/1000, batch:   784/ 1052, ite: 102240] train loss: 0.004650, tar: 0.000189 
l0: 0.000134, l1: 0.000128, l2: 0.000197, l3: 0.000314, l4: 0.000604, l5: 0.001058, l6: 0.002652
[epoch: 389/1000, batch:   864/ 1052, ite: 102260] train loss: 0.004651, tar: 0.000189 
l0: 0.000119, l1: 0.000118, l2: 0.000142, l3: 0.000177, l4: 0.000233, l5: 0.000532, l6: 0.000809
[epoch: 389/1000, batch:   944/ 1052, ite: 102280] train loss: 0.004650, tar: 0.000189 
l0: 0.000129, l1: 0.000128, l2: 0.000173, l3: 0.000241, l4: 0.000482, l5: 0.000830, l6: 0.002661
[epoch: 389/1000, batch:  1024/ 1052, ite: 102300] train loss: 0.004651, tar: 0.000189 
[Epoch 389/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000188, l1: 0.000199, l2: 0.000236, l3: 0.000264, l4: 0.000557, l5: 0.001038, l6: 0.001810
[epoch: 390/1000, batch:    52/ 1052, ite: 102320] train loss: 0.004652, tar: 0.000189 
l0: 0.000325, l1: 0.000334, l2: 0.000344, l3: 0.000486, l4: 0.000772, l5: 0.000921, l6: 0.001798
[epoch: 390/1000, batch:   132/ 1052, ite: 102340] train loss: 0.004652, tar: 0.000189 
l0: 0.000177, l1: 0.000176, l2: 0.000211, l3: 0.000294, l4: 0.000566, l5: 0.001542, l6: 0.002100
[epoch: 390/1000, batch:   212/ 1052, ite: 102360] train loss: 0.004651, tar: 0.000189 
l0: 0.000144, l1: 0.000148, l2: 0.000150, l3: 0.000234, l4: 0.000458, l5: 0.001002, l6: 0.001432
[epoch: 390/1000, batch:   292/ 1052, ite: 102380] train loss: 0.004651, tar: 0.000188 
l0: 0.000232, l1: 0.000244, l2: 0.000290, l3: 0.000421, l4: 0.000950, l5: 0.001709, l6: 0.004217
[epoch: 390/1000, batch:   372/ 1052, ite: 102400] train loss: 0.004652, tar: 0.000188 
l0: 0.000316, l1: 0.000308, l2: 0.000355, l3: 0.000524, l4: 0.000945, l5: 0.001944, l6: 0.002267
[epoch: 390/1000, batch:   452/ 1052, ite: 102420] train loss: 0.004653, tar: 0.000188 
l0: 0.000215, l1: 0.000223, l2: 0.000232, l3: 0.000292, l4: 0.000527, l5: 0.000892, l6: 0.001292
[epoch: 390/1000, batch:   532/ 1052, ite: 102440] train loss: 0.004651, tar: 0.000188 
l0: 0.000114, l1: 0.000114, l2: 0.000132, l3: 0.000201, l4: 0.000439, l5: 0.000988, l6: 0.001177
[epoch: 390/1000, batch:   612/ 1052, ite: 102460] train loss: 0.004650, tar: 0.000188 
l0: 0.000166, l1: 0.000172, l2: 0.000162, l3: 0.000206, l4: 0.000425, l5: 0.000649, l6: 0.000933
[epoch: 390/1000, batch:   692/ 1052, ite: 102480] train loss: 0.004651, tar: 0.000188 
l0: 0.000186, l1: 0.000189, l2: 0.000213, l3: 0.000284, l4: 0.000526, l5: 0.001300, l6: 0.001888
[epoch: 390/1000, batch:   772/ 1052, ite: 102500] train loss: 0.004652, tar: 0.000188 
l0: 0.000146, l1: 0.000151, l2: 0.000196, l3: 0.000266, l4: 0.000535, l5: 0.000720, l6: 0.002426
[epoch: 390/1000, batch:   852/ 1052, ite: 102520] train loss: 0.004651, tar: 0.000188 
l0: 0.000100, l1: 0.000101, l2: 0.000134, l3: 0.000188, l4: 0.000544, l5: 0.000924, l6: 0.001556
[epoch: 390/1000, batch:   932/ 1052, ite: 102540] train loss: 0.004649, tar: 0.000188 
l0: 0.000347, l1: 0.000361, l2: 0.000380, l3: 0.000431, l4: 0.000858, l5: 0.001624, l6: 0.002075
[epoch: 390/1000, batch:  1012/ 1052, ite: 102560] train loss: 0.004650, tar: 0.000188 
[Epoch 390/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000128, l1: 0.000129, l2: 0.000125, l3: 0.000194, l4: 0.000311, l5: 0.001072, l6: 0.001773
[epoch: 391/1000, batch:    40/ 1052, ite: 102580] train loss: 0.004650, tar: 0.000188 
l0: 0.000153, l1: 0.000155, l2: 0.000167, l3: 0.000228, l4: 0.000479, l5: 0.000889, l6: 0.002656
[epoch: 391/1000, batch:   120/ 1052, ite: 102600] train loss: 0.004649, tar: 0.000188 
l0: 0.000154, l1: 0.000157, l2: 0.000186, l3: 0.000219, l4: 0.000459, l5: 0.000861, l6: 0.001497
[epoch: 391/1000, batch:   200/ 1052, ite: 102620] train loss: 0.004647, tar: 0.000188 
l0: 0.000131, l1: 0.000124, l2: 0.000195, l3: 0.000285, l4: 0.000520, l5: 0.001035, l6: 0.002038
[epoch: 391/1000, batch:   280/ 1052, ite: 102640] train loss: 0.004647, tar: 0.000188 
l0: 0.000073, l1: 0.000069, l2: 0.000117, l3: 0.000186, l4: 0.000550, l5: 0.000888, l6: 0.002003
[epoch: 391/1000, batch:   360/ 1052, ite: 102660] train loss: 0.004648, tar: 0.000188 
l0: 0.000076, l1: 0.000080, l2: 0.000099, l3: 0.000129, l4: 0.000356, l5: 0.000966, l6: 0.001332
[epoch: 391/1000, batch:   440/ 1052, ite: 102680] train loss: 0.004647, tar: 0.000188 
l0: 0.000134, l1: 0.000135, l2: 0.000181, l3: 0.000271, l4: 0.000369, l5: 0.000826, l6: 0.001469
[epoch: 391/1000, batch:   520/ 1052, ite: 102700] train loss: 0.004645, tar: 0.000188 
l0: 0.000141, l1: 0.000139, l2: 0.000152, l3: 0.000187, l4: 0.000506, l5: 0.001029, l6: 0.001105
[epoch: 391/1000, batch:   600/ 1052, ite: 102720] train loss: 0.004645, tar: 0.000188 
l0: 0.000293, l1: 0.000301, l2: 0.000354, l3: 0.000429, l4: 0.000649, l5: 0.001775, l6: 0.003453
[epoch: 391/1000, batch:   680/ 1052, ite: 102740] train loss: 0.004645, tar: 0.000188 
l0: 0.000216, l1: 0.000218, l2: 0.000274, l3: 0.000408, l4: 0.000951, l5: 0.001591, l6: 0.002943
[epoch: 391/1000, batch:   760/ 1052, ite: 102760] train loss: 0.004647, tar: 0.000188 
l0: 0.000276, l1: 0.000277, l2: 0.000302, l3: 0.000425, l4: 0.000587, l5: 0.000809, l6: 0.001899
[epoch: 391/1000, batch:   840/ 1052, ite: 102780] train loss: 0.004647, tar: 0.000188 
l0: 0.000339, l1: 0.000336, l2: 0.000404, l3: 0.000583, l4: 0.000803, l5: 0.001685, l6: 0.003420
[epoch: 391/1000, batch:   920/ 1052, ite: 102800] train loss: 0.004647, tar: 0.000188 
l0: 0.000110, l1: 0.000117, l2: 0.000104, l3: 0.000151, l4: 0.000290, l5: 0.000962, l6: 0.001086
[epoch: 391/1000, batch:  1000/ 1052, ite: 102820] train loss: 0.004646, tar: 0.000187 
[Epoch 391/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000156, l1: 0.000161, l2: 0.000182, l3: 0.000271, l4: 0.000502, l5: 0.000651, l6: 0.002082
[epoch: 392/1000, batch:    28/ 1052, ite: 102840] train loss: 0.004646, tar: 0.000187 
l0: 0.000173, l1: 0.000171, l2: 0.000237, l3: 0.000304, l4: 0.000600, l5: 0.000892, l6: 0.002176
[epoch: 392/1000, batch:   108/ 1052, ite: 102860] train loss: 0.004647, tar: 0.000187 
l0: 0.000128, l1: 0.000125, l2: 0.000171, l3: 0.000247, l4: 0.000367, l5: 0.000799, l6: 0.001579
[epoch: 392/1000, batch:   188/ 1052, ite: 102880] train loss: 0.004646, tar: 0.000187 
l0: 0.000144, l1: 0.000140, l2: 0.000178, l3: 0.000246, l4: 0.000328, l5: 0.000910, l6: 0.001111
[epoch: 392/1000, batch:   268/ 1052, ite: 102900] train loss: 0.004646, tar: 0.000187 
l0: 0.000089, l1: 0.000092, l2: 0.000142, l3: 0.000217, l4: 0.000368, l5: 0.001174, l6: 0.001725
[epoch: 392/1000, batch:   348/ 1052, ite: 102920] train loss: 0.004644, tar: 0.000187 
l0: 0.000098, l1: 0.000101, l2: 0.000127, l3: 0.000172, l4: 0.000425, l5: 0.000665, l6: 0.001801
[epoch: 392/1000, batch:   428/ 1052, ite: 102940] train loss: 0.004645, tar: 0.000187 
l0: 0.000243, l1: 0.000254, l2: 0.000333, l3: 0.000428, l4: 0.000739, l5: 0.001785, l6: 0.002992
[epoch: 392/1000, batch:   508/ 1052, ite: 102960] train loss: 0.004646, tar: 0.000187 
l0: 0.000169, l1: 0.000168, l2: 0.000233, l3: 0.000381, l4: 0.000825, l5: 0.001479, l6: 0.002642
[epoch: 392/1000, batch:   588/ 1052, ite: 102980] train loss: 0.004645, tar: 0.000187 
l0: 0.000157, l1: 0.000158, l2: 0.000223, l3: 0.000336, l4: 0.000680, l5: 0.001435, l6: 0.002335
[epoch: 392/1000, batch:   668/ 1052, ite: 103000] train loss: 0.004645, tar: 0.000187 
l0: 0.000213, l1: 0.000219, l2: 0.000251, l3: 0.000388, l4: 0.000597, l5: 0.001032, l6: 0.003420
[epoch: 392/1000, batch:   748/ 1052, ite: 103020] train loss: 0.004643, tar: 0.000187 
l0: 0.000292, l1: 0.000301, l2: 0.000328, l3: 0.000406, l4: 0.000883, l5: 0.001627, l6: 0.003556
[epoch: 392/1000, batch:   828/ 1052, ite: 103040] train loss: 0.004644, tar: 0.000187 
l0: 0.000157, l1: 0.000157, l2: 0.000200, l3: 0.000294, l4: 0.000591, l5: 0.001143, l6: 0.002046
[epoch: 392/1000, batch:   908/ 1052, ite: 103060] train loss: 0.004643, tar: 0.000187 
l0: 0.000122, l1: 0.000125, l2: 0.000135, l3: 0.000196, l4: 0.000305, l5: 0.000970, l6: 0.001325
[epoch: 392/1000, batch:   988/ 1052, ite: 103080] train loss: 0.004643, tar: 0.000187 
[Epoch 392/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000218, l1: 0.000215, l2: 0.000265, l3: 0.000341, l4: 0.000708, l5: 0.001495, l6: 0.002092
[epoch: 393/1000, batch:    16/ 1052, ite: 103100] train loss: 0.004643, tar: 0.000187 
l0: 0.000076, l1: 0.000076, l2: 0.000115, l3: 0.000248, l4: 0.000575, l5: 0.000938, l6: 0.001324
[epoch: 393/1000, batch:    96/ 1052, ite: 103120] train loss: 0.004642, tar: 0.000187 
l0: 0.000228, l1: 0.000226, l2: 0.000280, l3: 0.000478, l4: 0.000756, l5: 0.001651, l6: 0.002514
[epoch: 393/1000, batch:   176/ 1052, ite: 103140] train loss: 0.004641, tar: 0.000186 
l0: 0.000290, l1: 0.000306, l2: 0.000334, l3: 0.000519, l4: 0.000961, l5: 0.002106, l6: 0.003531
[epoch: 393/1000, batch:   256/ 1052, ite: 103160] train loss: 0.004641, tar: 0.000186 
l0: 0.000166, l1: 0.000174, l2: 0.000180, l3: 0.000230, l4: 0.000447, l5: 0.001232, l6: 0.002166
[epoch: 393/1000, batch:   336/ 1052, ite: 103180] train loss: 0.004640, tar: 0.000186 
l0: 0.000490, l1: 0.000479, l2: 0.000590, l3: 0.000610, l4: 0.000689, l5: 0.001085, l6: 0.003802
[epoch: 393/1000, batch:   416/ 1052, ite: 103200] train loss: 0.004641, tar: 0.000186 
l0: 0.000197, l1: 0.000197, l2: 0.000252, l3: 0.000416, l4: 0.000653, l5: 0.000874, l6: 0.001411
[epoch: 393/1000, batch:   496/ 1052, ite: 103220] train loss: 0.004640, tar: 0.000186 
l0: 0.000154, l1: 0.000155, l2: 0.000190, l3: 0.000306, l4: 0.000614, l5: 0.001356, l6: 0.002287
[epoch: 393/1000, batch:   576/ 1052, ite: 103240] train loss: 0.004640, tar: 0.000186 
l0: 0.000142, l1: 0.000143, l2: 0.000180, l3: 0.000251, l4: 0.000400, l5: 0.001093, l6: 0.001794
[epoch: 393/1000, batch:   656/ 1052, ite: 103260] train loss: 0.004639, tar: 0.000186 
l0: 0.000168, l1: 0.000177, l2: 0.000214, l3: 0.000289, l4: 0.000534, l5: 0.000909, l6: 0.001862
[epoch: 393/1000, batch:   736/ 1052, ite: 103280] train loss: 0.004639, tar: 0.000186 
l0: 0.000099, l1: 0.000098, l2: 0.000143, l3: 0.000380, l4: 0.000680, l5: 0.001313, l6: 0.002195
[epoch: 393/1000, batch:   816/ 1052, ite: 103300] train loss: 0.004639, tar: 0.000186 
l0: 0.000111, l1: 0.000118, l2: 0.000127, l3: 0.000235, l4: 0.000718, l5: 0.001002, l6: 0.002541
[epoch: 393/1000, batch:   896/ 1052, ite: 103320] train loss: 0.004638, tar: 0.000186 
l0: 0.000377, l1: 0.000387, l2: 0.000463, l3: 0.000483, l4: 0.000820, l5: 0.001460, l6: 0.004268
[epoch: 393/1000, batch:   976/ 1052, ite: 103340] train loss: 0.004638, tar: 0.000186 
[Epoch 393/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000049, l1: 0.000050, l2: 0.000074, l3: 0.000155, l4: 0.000403, l5: 0.000858, l6: 0.001049
[epoch: 394/1000, batch:     4/ 1052, ite: 103360] train loss: 0.004639, tar: 0.000186 
l0: 0.000133, l1: 0.000135, l2: 0.000159, l3: 0.000260, l4: 0.000602, l5: 0.001683, l6: 0.001906
[epoch: 394/1000, batch:    84/ 1052, ite: 103380] train loss: 0.004639, tar: 0.000186 
l0: 0.000133, l1: 0.000133, l2: 0.000214, l3: 0.000208, l4: 0.000452, l5: 0.000893, l6: 0.001997
[epoch: 394/1000, batch:   164/ 1052, ite: 103400] train loss: 0.004638, tar: 0.000186 
l0: 0.000124, l1: 0.000122, l2: 0.000153, l3: 0.000228, l4: 0.000512, l5: 0.000937, l6: 0.001773
[epoch: 394/1000, batch:   244/ 1052, ite: 103420] train loss: 0.004639, tar: 0.000186 
l0: 0.000095, l1: 0.000098, l2: 0.000118, l3: 0.000194, l4: 0.000434, l5: 0.000580, l6: 0.001548
[epoch: 394/1000, batch:   324/ 1052, ite: 103440] train loss: 0.004638, tar: 0.000186 
l0: 0.000272, l1: 0.000283, l2: 0.000291, l3: 0.000397, l4: 0.000821, l5: 0.001193, l6: 0.002141
[epoch: 394/1000, batch:   404/ 1052, ite: 103460] train loss: 0.004638, tar: 0.000186 
l0: 0.000184, l1: 0.000188, l2: 0.000205, l3: 0.000242, l4: 0.000678, l5: 0.001105, l6: 0.002562
[epoch: 394/1000, batch:   484/ 1052, ite: 103480] train loss: 0.004637, tar: 0.000186 
l0: 0.000042, l1: 0.000041, l2: 0.000083, l3: 0.000195, l4: 0.000409, l5: 0.000905, l6: 0.001469
[epoch: 394/1000, batch:   564/ 1052, ite: 103500] train loss: 0.004637, tar: 0.000186 
l0: 0.000160, l1: 0.000160, l2: 0.000235, l3: 0.000347, l4: 0.000899, l5: 0.001597, l6: 0.002256
[epoch: 394/1000, batch:   644/ 1052, ite: 103520] train loss: 0.004637, tar: 0.000186 
l0: 0.000113, l1: 0.000123, l2: 0.000110, l3: 0.000154, l4: 0.000290, l5: 0.000677, l6: 0.000998
[epoch: 394/1000, batch:   724/ 1052, ite: 103540] train loss: 0.004636, tar: 0.000186 
l0: 0.000086, l1: 0.000088, l2: 0.000088, l3: 0.000101, l4: 0.000346, l5: 0.000439, l6: 0.000880
[epoch: 394/1000, batch:   804/ 1052, ite: 103560] train loss: 0.004635, tar: 0.000186 
l0: 0.000118, l1: 0.000118, l2: 0.000179, l3: 0.000350, l4: 0.000711, l5: 0.001721, l6: 0.003129
[epoch: 394/1000, batch:   884/ 1052, ite: 103580] train loss: 0.004635, tar: 0.000186 
l0: 0.000317, l1: 0.000318, l2: 0.000408, l3: 0.000469, l4: 0.000726, l5: 0.001284, l6: 0.002412
[epoch: 394/1000, batch:   964/ 1052, ite: 103600] train loss: 0.004635, tar: 0.000186 
l0: 0.000136, l1: 0.000135, l2: 0.000184, l3: 0.000320, l4: 0.000540, l5: 0.000993, l6: 0.002374
[epoch: 394/1000, batch:  1044/ 1052, ite: 103620] train loss: 0.004634, tar: 0.000186 
[Epoch 394/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000130, l1: 0.000129, l2: 0.000160, l3: 0.000202, l4: 0.000340, l5: 0.000530, l6: 0.001200
[epoch: 395/1000, batch:    72/ 1052, ite: 103640] train loss: 0.004634, tar: 0.000186 
l0: 0.000187, l1: 0.000186, l2: 0.000252, l3: 0.000434, l4: 0.000692, l5: 0.001885, l6: 0.003822
[epoch: 395/1000, batch:   152/ 1052, ite: 103660] train loss: 0.004634, tar: 0.000185 
l0: 0.000186, l1: 0.000194, l2: 0.000194, l3: 0.000227, l4: 0.000404, l5: 0.000930, l6: 0.001450
[epoch: 395/1000, batch:   232/ 1052, ite: 103680] train loss: 0.004633, tar: 0.000185 
l0: 0.000123, l1: 0.000122, l2: 0.000139, l3: 0.000205, l4: 0.000355, l5: 0.000819, l6: 0.001253
[epoch: 395/1000, batch:   312/ 1052, ite: 103700] train loss: 0.004633, tar: 0.000185 
l0: 0.000329, l1: 0.000335, l2: 0.000414, l3: 0.000425, l4: 0.000742, l5: 0.001393, l6: 0.003282
[epoch: 395/1000, batch:   392/ 1052, ite: 103720] train loss: 0.004632, tar: 0.000185 
l0: 0.000285, l1: 0.000285, l2: 0.000319, l3: 0.000358, l4: 0.000804, l5: 0.001280, l6: 0.002570
[epoch: 395/1000, batch:   472/ 1052, ite: 103740] train loss: 0.004634, tar: 0.000185 
l0: 0.000321, l1: 0.000322, l2: 0.000384, l3: 0.000421, l4: 0.000610, l5: 0.001205, l6: 0.002132
[epoch: 395/1000, batch:   552/ 1052, ite: 103760] train loss: 0.004635, tar: 0.000186 
l0: 0.000166, l1: 0.000178, l2: 0.000269, l3: 0.000299, l4: 0.000449, l5: 0.000736, l6: 0.001376
[epoch: 395/1000, batch:   632/ 1052, ite: 103780] train loss: 0.004638, tar: 0.000186 
l0: 0.000261, l1: 0.000285, l2: 0.000341, l3: 0.000372, l4: 0.000576, l5: 0.000947, l6: 0.001659
[epoch: 395/1000, batch:   712/ 1052, ite: 103800] train loss: 0.004641, tar: 0.000187 
l0: 0.000187, l1: 0.000190, l2: 0.000217, l3: 0.000312, l4: 0.000555, l5: 0.000981, l6: 0.001496
[epoch: 395/1000, batch:   792/ 1052, ite: 103820] train loss: 0.004643, tar: 0.000187 
l0: 0.000111, l1: 0.000112, l2: 0.000151, l3: 0.000154, l4: 0.000302, l5: 0.000745, l6: 0.000889
[epoch: 395/1000, batch:   872/ 1052, ite: 103840] train loss: 0.004644, tar: 0.000187 
l0: 0.000148, l1: 0.000144, l2: 0.000176, l3: 0.000247, l4: 0.000427, l5: 0.000727, l6: 0.001423
[epoch: 395/1000, batch:   952/ 1052, ite: 103860] train loss: 0.004645, tar: 0.000187 
l0: 0.000273, l1: 0.000272, l2: 0.000312, l3: 0.000456, l4: 0.000852, l5: 0.001569, l6: 0.002027
[epoch: 395/1000, batch:  1032/ 1052, ite: 103880] train loss: 0.004646, tar: 0.000188 
[Epoch 395/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000139, l1: 0.000146, l2: 0.000162, l3: 0.000204, l4: 0.000416, l5: 0.000638, l6: 0.001484
[epoch: 396/1000, batch:    60/ 1052, ite: 103900] train loss: 0.004647, tar: 0.000188 
l0: 0.000158, l1: 0.000152, l2: 0.000185, l3: 0.000290, l4: 0.000546, l5: 0.001090, l6: 0.002110
[epoch: 396/1000, batch:   140/ 1052, ite: 103920] train loss: 0.004649, tar: 0.000188 
l0: 0.000446, l1: 0.000443, l2: 0.000479, l3: 0.000656, l4: 0.001130, l5: 0.002191, l6: 0.003933
[epoch: 396/1000, batch:   220/ 1052, ite: 103940] train loss: 0.004650, tar: 0.000188 
l0: 0.000158, l1: 0.000151, l2: 0.000175, l3: 0.000284, l4: 0.000601, l5: 0.000970, l6: 0.001350
[epoch: 396/1000, batch:   300/ 1052, ite: 103960] train loss: 0.004650, tar: 0.000188 
l0: 0.000318, l1: 0.000287, l2: 0.000367, l3: 0.000413, l4: 0.000467, l5: 0.000870, l6: 0.001314
[epoch: 396/1000, batch:   380/ 1052, ite: 103980] train loss: 0.004650, tar: 0.000188 
l0: 0.000223, l1: 0.000237, l2: 0.000212, l3: 0.000281, l4: 0.000461, l5: 0.001034, l6: 0.001415
[epoch: 396/1000, batch:   460/ 1052, ite: 104000] train loss: 0.004651, tar: 0.000188 
l0: 0.000176, l1: 0.000182, l2: 0.000202, l3: 0.000181, l4: 0.000326, l5: 0.000758, l6: 0.001346
[epoch: 396/1000, batch:   540/ 1052, ite: 104020] train loss: 0.004652, tar: 0.000188 
l0: 0.000199, l1: 0.000189, l2: 0.000229, l3: 0.000385, l4: 0.000626, l5: 0.001071, l6: 0.001555
[epoch: 396/1000, batch:   620/ 1052, ite: 104040] train loss: 0.004651, tar: 0.000188 
l0: 0.000240, l1: 0.000233, l2: 0.000267, l3: 0.000394, l4: 0.000554, l5: 0.001140, l6: 0.001918
[epoch: 396/1000, batch:   700/ 1052, ite: 104060] train loss: 0.004650, tar: 0.000188 
l0: 0.000113, l1: 0.000112, l2: 0.000120, l3: 0.000229, l4: 0.000541, l5: 0.001087, l6: 0.001627
[epoch: 396/1000, batch:   780/ 1052, ite: 104080] train loss: 0.004649, tar: 0.000188 
l0: 0.000198, l1: 0.000203, l2: 0.000228, l3: 0.000347, l4: 0.000530, l5: 0.002237, l6: 0.002978
[epoch: 396/1000, batch:   860/ 1052, ite: 104100] train loss: 0.004650, tar: 0.000188 
l0: 0.000145, l1: 0.000138, l2: 0.000169, l3: 0.000231, l4: 0.000597, l5: 0.001178, l6: 0.002564
[epoch: 396/1000, batch:   940/ 1052, ite: 104120] train loss: 0.004651, tar: 0.000188 
l0: 0.000151, l1: 0.000154, l2: 0.000180, l3: 0.000224, l4: 0.000416, l5: 0.000707, l6: 0.001204
[epoch: 396/1000, batch:  1020/ 1052, ite: 104140] train loss: 0.004651, tar: 0.000188 
[Epoch 396/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000152, l1: 0.000152, l2: 0.000150, l3: 0.000246, l4: 0.000475, l5: 0.000984, l6: 0.001975
[epoch: 397/1000, batch:    48/ 1052, ite: 104160] train loss: 0.004652, tar: 0.000188 
l0: 0.000100, l1: 0.000096, l2: 0.000130, l3: 0.000219, l4: 0.000321, l5: 0.000764, l6: 0.001361
[epoch: 397/1000, batch:   128/ 1052, ite: 104180] train loss: 0.004650, tar: 0.000188 
l0: 0.000109, l1: 0.000102, l2: 0.000124, l3: 0.000192, l4: 0.000365, l5: 0.000457, l6: 0.000965
[epoch: 397/1000, batch:   208/ 1052, ite: 104200] train loss: 0.004651, tar: 0.000188 
l0: 0.000701, l1: 0.000664, l2: 0.000723, l3: 0.000766, l4: 0.001079, l5: 0.002503, l6: 0.002863
[epoch: 397/1000, batch:   288/ 1052, ite: 104220] train loss: 0.004652, tar: 0.000188 
l0: 0.000144, l1: 0.000140, l2: 0.000163, l3: 0.000242, l4: 0.000500, l5: 0.001213, l6: 0.002213
[epoch: 397/1000, batch:   368/ 1052, ite: 104240] train loss: 0.004652, tar: 0.000188 
l0: 0.000284, l1: 0.000284, l2: 0.000284, l3: 0.000327, l4: 0.000589, l5: 0.001257, l6: 0.002404
[epoch: 397/1000, batch:   448/ 1052, ite: 104260] train loss: 0.004652, tar: 0.000188 
l0: 0.000120, l1: 0.000115, l2: 0.000145, l3: 0.000180, l4: 0.000353, l5: 0.000612, l6: 0.001205
[epoch: 397/1000, batch:   528/ 1052, ite: 104280] train loss: 0.004652, tar: 0.000188 
l0: 0.000191, l1: 0.000188, l2: 0.000212, l3: 0.000332, l4: 0.000529, l5: 0.001499, l6: 0.002204
[epoch: 397/1000, batch:   608/ 1052, ite: 104300] train loss: 0.004653, tar: 0.000188 
l0: 0.000147, l1: 0.000147, l2: 0.000188, l3: 0.000223, l4: 0.000311, l5: 0.001302, l6: 0.002049
[epoch: 397/1000, batch:   688/ 1052, ite: 104320] train loss: 0.004652, tar: 0.000188 
l0: 0.000194, l1: 0.000205, l2: 0.000253, l3: 0.000414, l4: 0.000980, l5: 0.001171, l6: 0.002410
[epoch: 397/1000, batch:   768/ 1052, ite: 104340] train loss: 0.004652, tar: 0.000188 
l0: 0.000220, l1: 0.000225, l2: 0.000234, l3: 0.000316, l4: 0.000502, l5: 0.001189, l6: 0.001327
[epoch: 397/1000, batch:   848/ 1052, ite: 104360] train loss: 0.004652, tar: 0.000188 
l0: 0.000122, l1: 0.000121, l2: 0.000176, l3: 0.000278, l4: 0.000531, l5: 0.000880, l6: 0.002535
[epoch: 397/1000, batch:   928/ 1052, ite: 104380] train loss: 0.004653, tar: 0.000188 
l0: 0.000158, l1: 0.000161, l2: 0.000153, l3: 0.000205, l4: 0.000417, l5: 0.000654, l6: 0.001430
[epoch: 397/1000, batch:  1008/ 1052, ite: 104400] train loss: 0.004652, tar: 0.000188 
[Epoch 397/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000233, l1: 0.000237, l2: 0.000252, l3: 0.000266, l4: 0.000501, l5: 0.000983, l6: 0.002314
[epoch: 398/1000, batch:    36/ 1052, ite: 104420] train loss: 0.004651, tar: 0.000188 
l0: 0.000110, l1: 0.000108, l2: 0.000139, l3: 0.000176, l4: 0.000422, l5: 0.000770, l6: 0.001535
[epoch: 398/1000, batch:   116/ 1052, ite: 104440] train loss: 0.004652, tar: 0.000188 
l0: 0.000240, l1: 0.000229, l2: 0.000331, l3: 0.000499, l4: 0.000896, l5: 0.001831, l6: 0.002974
[epoch: 398/1000, batch:   196/ 1052, ite: 104460] train loss: 0.004651, tar: 0.000188 
l0: 0.000305, l1: 0.000319, l2: 0.000349, l3: 0.000446, l4: 0.000995, l5: 0.002306, l6: 0.003491
[epoch: 398/1000, batch:   276/ 1052, ite: 104480] train loss: 0.004650, tar: 0.000188 
l0: 0.000130, l1: 0.000126, l2: 0.000154, l3: 0.000240, l4: 0.000472, l5: 0.000972, l6: 0.001795
[epoch: 398/1000, batch:   356/ 1052, ite: 104500] train loss: 0.004649, tar: 0.000188 
l0: 0.000409, l1: 0.000416, l2: 0.000446, l3: 0.000602, l4: 0.001248, l5: 0.003181, l6: 0.006044
[epoch: 398/1000, batch:   436/ 1052, ite: 104520] train loss: 0.004649, tar: 0.000188 
l0: 0.000085, l1: 0.000084, l2: 0.000103, l3: 0.000146, l4: 0.000268, l5: 0.000620, l6: 0.001405
[epoch: 398/1000, batch:   516/ 1052, ite: 104540] train loss: 0.004649, tar: 0.000188 
l0: 0.000100, l1: 0.000098, l2: 0.000140, l3: 0.000216, l4: 0.000396, l5: 0.000583, l6: 0.001279
[epoch: 398/1000, batch:   596/ 1052, ite: 104560] train loss: 0.004648, tar: 0.000188 
l0: 0.000154, l1: 0.000158, l2: 0.000168, l3: 0.000239, l4: 0.000455, l5: 0.000997, l6: 0.002415
[epoch: 398/1000, batch:   676/ 1052, ite: 104580] train loss: 0.004646, tar: 0.000187 
l0: 0.000379, l1: 0.000389, l2: 0.000408, l3: 0.000510, l4: 0.000936, l5: 0.001663, l6: 0.005200
[epoch: 398/1000, batch:   756/ 1052, ite: 104600] train loss: 0.004647, tar: 0.000187 
l0: 0.000166, l1: 0.000173, l2: 0.000139, l3: 0.000172, l4: 0.000419, l5: 0.001058, l6: 0.001394
[epoch: 398/1000, batch:   836/ 1052, ite: 104620] train loss: 0.004648, tar: 0.000187 
l0: 0.000103, l1: 0.000099, l2: 0.000118, l3: 0.000212, l4: 0.000478, l5: 0.001013, l6: 0.001667
[epoch: 398/1000, batch:   916/ 1052, ite: 104640] train loss: 0.004647, tar: 0.000187 
l0: 0.000167, l1: 0.000167, l2: 0.000202, l3: 0.000252, l4: 0.000510, l5: 0.001082, l6: 0.002575
[epoch: 398/1000, batch:   996/ 1052, ite: 104660] train loss: 0.004649, tar: 0.000187 
[Epoch 398/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000120, l1: 0.000118, l2: 0.000144, l3: 0.000190, l4: 0.000406, l5: 0.000973, l6: 0.001282
[epoch: 399/1000, batch:    24/ 1052, ite: 104680] train loss: 0.004648, tar: 0.000187 
l0: 0.000156, l1: 0.000154, l2: 0.000165, l3: 0.000269, l4: 0.000545, l5: 0.001044, l6: 0.002496
[epoch: 399/1000, batch:   104/ 1052, ite: 104700] train loss: 0.004647, tar: 0.000187 
l0: 0.000315, l1: 0.000305, l2: 0.000368, l3: 0.000519, l4: 0.000773, l5: 0.001086, l6: 0.002166
[epoch: 399/1000, batch:   184/ 1052, ite: 104720] train loss: 0.004647, tar: 0.000187 
l0: 0.000169, l1: 0.000169, l2: 0.000204, l3: 0.000256, l4: 0.000390, l5: 0.001182, l6: 0.001645
[epoch: 399/1000, batch:   264/ 1052, ite: 104740] train loss: 0.004646, tar: 0.000187 
l0: 0.000080, l1: 0.000081, l2: 0.000094, l3: 0.000173, l4: 0.000283, l5: 0.000943, l6: 0.001327
[epoch: 399/1000, batch:   344/ 1052, ite: 104760] train loss: 0.004645, tar: 0.000187 
l0: 0.000132, l1: 0.000131, l2: 0.000164, l3: 0.000223, l4: 0.000445, l5: 0.001538, l6: 0.003016
[epoch: 399/1000, batch:   424/ 1052, ite: 104780] train loss: 0.004645, tar: 0.000187 
l0: 0.000129, l1: 0.000131, l2: 0.000172, l3: 0.000250, l4: 0.000562, l5: 0.001310, l6: 0.002602
[epoch: 399/1000, batch:   504/ 1052, ite: 104800] train loss: 0.004647, tar: 0.000187 
l0: 0.000147, l1: 0.000161, l2: 0.000172, l3: 0.000267, l4: 0.000453, l5: 0.001288, l6: 0.002222
[epoch: 399/1000, batch:   584/ 1052, ite: 104820] train loss: 0.004645, tar: 0.000187 
l0: 0.000125, l1: 0.000113, l2: 0.000153, l3: 0.000205, l4: 0.000346, l5: 0.000489, l6: 0.000916
[epoch: 399/1000, batch:   664/ 1052, ite: 104840] train loss: 0.004646, tar: 0.000187 
l0: 0.000135, l1: 0.000134, l2: 0.000148, l3: 0.000268, l4: 0.000604, l5: 0.000955, l6: 0.002068
[epoch: 399/1000, batch:   744/ 1052, ite: 104860] train loss: 0.004647, tar: 0.000187 
l0: 0.000084, l1: 0.000087, l2: 0.000095, l3: 0.000186, l4: 0.000305, l5: 0.000722, l6: 0.001258
[epoch: 399/1000, batch:   824/ 1052, ite: 104880] train loss: 0.004646, tar: 0.000187 
l0: 0.000138, l1: 0.000138, l2: 0.000161, l3: 0.000222, l4: 0.000535, l5: 0.001826, l6: 0.003049
[epoch: 399/1000, batch:   904/ 1052, ite: 104900] train loss: 0.004645, tar: 0.000187 
l0: 0.000147, l1: 0.000146, l2: 0.000165, l3: 0.000249, l4: 0.000482, l5: 0.000759, l6: 0.001312
[epoch: 399/1000, batch:   984/ 1052, ite: 104920] train loss: 0.004645, tar: 0.000187 
[Epoch 399/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000123, l1: 0.000122, l2: 0.000236, l3: 0.000277, l4: 0.000478, l5: 0.000908, l6: 0.001294
[epoch: 400/1000, batch:    12/ 1052, ite: 104940] train loss: 0.004645, tar: 0.000187 
l0: 0.000218, l1: 0.000223, l2: 0.000284, l3: 0.000313, l4: 0.000662, l5: 0.001032, l6: 0.002334
[epoch: 400/1000, batch:    92/ 1052, ite: 104960] train loss: 0.004645, tar: 0.000187 
l0: 0.000168, l1: 0.000172, l2: 0.000172, l3: 0.000203, l4: 0.000480, l5: 0.000987, l6: 0.001773
[epoch: 400/1000, batch:   172/ 1052, ite: 104980] train loss: 0.004646, tar: 0.000187 
l0: 0.000103, l1: 0.000102, l2: 0.000132, l3: 0.000153, l4: 0.000423, l5: 0.000866, l6: 0.001255
[epoch: 400/1000, batch:   252/ 1052, ite: 105000] train loss: 0.004645, tar: 0.000187 
l0: 0.000305, l1: 0.000307, l2: 0.000350, l3: 0.000472, l4: 0.000577, l5: 0.001193, l6: 0.003780
[epoch: 400/1000, batch:   332/ 1052, ite: 105020] train loss: 0.004646, tar: 0.000187 
l0: 0.000115, l1: 0.000120, l2: 0.000116, l3: 0.000157, l4: 0.000296, l5: 0.000888, l6: 0.001317
[epoch: 400/1000, batch:   412/ 1052, ite: 105040] train loss: 0.004645, tar: 0.000187 
l0: 0.000304, l1: 0.000303, l2: 0.000354, l3: 0.000383, l4: 0.000606, l5: 0.000988, l6: 0.001732
[epoch: 400/1000, batch:   492/ 1052, ite: 105060] train loss: 0.004643, tar: 0.000186 
l0: 0.000124, l1: 0.000119, l2: 0.000176, l3: 0.000288, l4: 0.000474, l5: 0.000741, l6: 0.001376
[epoch: 400/1000, batch:   572/ 1052, ite: 105080] train loss: 0.004643, tar: 0.000186 
l0: 0.000064, l1: 0.000069, l2: 0.000080, l3: 0.000134, l4: 0.000245, l5: 0.000795, l6: 0.001323
[epoch: 400/1000, batch:   652/ 1052, ite: 105100] train loss: 0.004643, tar: 0.000186 
l0: 0.000218, l1: 0.000218, l2: 0.000249, l3: 0.000346, l4: 0.000576, l5: 0.000893, l6: 0.002697
[epoch: 400/1000, batch:   732/ 1052, ite: 105120] train loss: 0.004644, tar: 0.000186 
l0: 0.000029, l1: 0.000027, l2: 0.000053, l3: 0.000113, l4: 0.000313, l5: 0.000305, l6: 0.000828
[epoch: 400/1000, batch:   812/ 1052, ite: 105140] train loss: 0.004644, tar: 0.000186 
l0: 0.000151, l1: 0.000149, l2: 0.000186, l3: 0.000239, l4: 0.000539, l5: 0.001076, l6: 0.001696
[epoch: 400/1000, batch:   892/ 1052, ite: 105160] train loss: 0.004644, tar: 0.000186 
l0: 0.000305, l1: 0.000299, l2: 0.000436, l3: 0.000548, l4: 0.000790, l5: 0.001318, l6: 0.003527
[epoch: 400/1000, batch:   972/ 1052, ite: 105180] train loss: 0.004643, tar: 0.000186 
l0: 0.000111, l1: 0.000107, l2: 0.000153, l3: 0.000440, l4: 0.000617, l5: 0.001236, l6: 0.002248
[epoch: 400/1000, batch:  1052/ 1052, ite: 105200] train loss: 0.004643, tar: 0.000186 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_105200.pkl
[Epoch 400/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000183, l1: 0.000184, l2: 0.000205, l3: 0.000252, l4: 0.000408, l5: 0.000809, l6: 0.001630
[epoch: 401/1000, batch:    80/ 1052, ite: 105220] train loss: 0.004519, tar: 0.000159 
l0: 0.000392, l1: 0.000411, l2: 0.000430, l3: 0.000611, l4: 0.001228, l5: 0.002642, l6: 0.004921
[epoch: 401/1000, batch:   160/ 1052, ite: 105240] train loss: 0.005209, tar: 0.000182 
l0: 0.000114, l1: 0.000114, l2: 0.000138, l3: 0.000166, l4: 0.000376, l5: 0.000646, l6: 0.001111
[epoch: 401/1000, batch:   240/ 1052, ite: 105260] train loss: 0.004860, tar: 0.000165 
l0: 0.000262, l1: 0.000270, l2: 0.000265, l3: 0.000309, l4: 0.000477, l5: 0.001100, l6: 0.003014
[epoch: 401/1000, batch:   320/ 1052, ite: 105280] train loss: 0.004747, tar: 0.000163 
l0: 0.000079, l1: 0.000082, l2: 0.000096, l3: 0.000118, l4: 0.000321, l5: 0.000469, l6: 0.000584
[epoch: 401/1000, batch:   400/ 1052, ite: 105300] train loss: 0.004750, tar: 0.000162 
l0: 0.000045, l1: 0.000042, l2: 0.000089, l3: 0.000217, l4: 0.000443, l5: 0.000668, l6: 0.001195
[epoch: 401/1000, batch:   480/ 1052, ite: 105320] train loss: 0.004660, tar: 0.000158 
l0: 0.000059, l1: 0.000064, l2: 0.000060, l3: 0.000101, l4: 0.000283, l5: 0.000485, l6: 0.001095
[epoch: 401/1000, batch:   560/ 1052, ite: 105340] train loss: 0.004650, tar: 0.000159 
l0: 0.000277, l1: 0.000274, l2: 0.000290, l3: 0.000357, l4: 0.000707, l5: 0.001774, l6: 0.002565
[epoch: 401/1000, batch:   640/ 1052, ite: 105360] train loss: 0.004702, tar: 0.000163 
l0: 0.000227, l1: 0.000227, l2: 0.000234, l3: 0.000311, l4: 0.000817, l5: 0.001424, l6: 0.002778
[epoch: 401/1000, batch:   720/ 1052, ite: 105380] train loss: 0.004646, tar: 0.000164 
l0: 0.000116, l1: 0.000113, l2: 0.000150, l3: 0.000193, l4: 0.000376, l5: 0.000942, l6: 0.001236
[epoch: 401/1000, batch:   800/ 1052, ite: 105400] train loss: 0.004634, tar: 0.000162 
l0: 0.000091, l1: 0.000092, l2: 0.000134, l3: 0.000176, l4: 0.000231, l5: 0.000522, l6: 0.001588
[epoch: 401/1000, batch:   880/ 1052, ite: 105420] train loss: 0.004589, tar: 0.000161 
l0: 0.000190, l1: 0.000182, l2: 0.000180, l3: 0.000243, l4: 0.000478, l5: 0.000759, l6: 0.001318
[epoch: 401/1000, batch:   960/ 1052, ite: 105440] train loss: 0.004552, tar: 0.000161 
l0: 0.000205, l1: 0.000211, l2: 0.000192, l3: 0.000334, l4: 0.000434, l5: 0.001257, l6: 0.002508
[epoch: 401/1000, batch:  1040/ 1052, ite: 105460] train loss: 0.004504, tar: 0.000161 
[Epoch 401/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000186, l1: 0.000188, l2: 0.000239, l3: 0.000365, l4: 0.000568, l5: 0.001078, l6: 0.001422
[epoch: 402/1000, batch:    68/ 1052, ite: 105480] train loss: 0.004468, tar: 0.000158 
l0: 0.000113, l1: 0.000120, l2: 0.000143, l3: 0.000239, l4: 0.000475, l5: 0.000964, l6: 0.001648
[epoch: 402/1000, batch:   148/ 1052, ite: 105500] train loss: 0.004447, tar: 0.000157 
l0: 0.000167, l1: 0.000171, l2: 0.000198, l3: 0.000271, l4: 0.000463, l5: 0.001042, l6: 0.001779
[epoch: 402/1000, batch:   228/ 1052, ite: 105520] train loss: 0.004475, tar: 0.000159 
l0: 0.000160, l1: 0.000157, l2: 0.000232, l3: 0.000344, l4: 0.000573, l5: 0.001400, l6: 0.002699
[epoch: 402/1000, batch:   308/ 1052, ite: 105540] train loss: 0.004495, tar: 0.000159 
l0: 0.000165, l1: 0.000168, l2: 0.000220, l3: 0.000288, l4: 0.000702, l5: 0.001916, l6: 0.003794
[epoch: 402/1000, batch:   388/ 1052, ite: 105560] train loss: 0.004494, tar: 0.000158 
l0: 0.000066, l1: 0.000068, l2: 0.000065, l3: 0.000151, l4: 0.000553, l5: 0.000953, l6: 0.001206
[epoch: 402/1000, batch:   468/ 1052, ite: 105580] train loss: 0.004494, tar: 0.000159 
l0: 0.000083, l1: 0.000089, l2: 0.000094, l3: 0.000125, l4: 0.000297, l5: 0.000427, l6: 0.001432
[epoch: 402/1000, batch:   548/ 1052, ite: 105600] train loss: 0.004472, tar: 0.000157 
l0: 0.000057, l1: 0.000057, l2: 0.000081, l3: 0.000158, l4: 0.000415, l5: 0.000860, l6: 0.001496
[epoch: 402/1000, batch:   628/ 1052, ite: 105620] train loss: 0.004489, tar: 0.000158 
l0: 0.000062, l1: 0.000060, l2: 0.000085, l3: 0.000121, l4: 0.000268, l5: 0.000484, l6: 0.000911
[epoch: 402/1000, batch:   708/ 1052, ite: 105640] train loss: 0.004481, tar: 0.000159 
l0: 0.000100, l1: 0.000100, l2: 0.000147, l3: 0.000235, l4: 0.000488, l5: 0.000730, l6: 0.001946
[epoch: 402/1000, batch:   788/ 1052, ite: 105660] train loss: 0.004500, tar: 0.000160 
l0: 0.000242, l1: 0.000242, l2: 0.000296, l3: 0.000393, l4: 0.000546, l5: 0.001160, l6: 0.003527
[epoch: 402/1000, batch:   868/ 1052, ite: 105680] train loss: 0.004474, tar: 0.000159 
l0: 0.000278, l1: 0.000274, l2: 0.000336, l3: 0.000406, l4: 0.000716, l5: 0.001552, l6: 0.003089
[epoch: 402/1000, batch:   948/ 1052, ite: 105700] train loss: 0.004489, tar: 0.000160 
l0: 0.000200, l1: 0.000204, l2: 0.000215, l3: 0.000292, l4: 0.000400, l5: 0.000867, l6: 0.001563
[epoch: 402/1000, batch:  1028/ 1052, ite: 105720] train loss: 0.004496, tar: 0.000160 
[Epoch 402/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000101, l1: 0.000099, l2: 0.000140, l3: 0.000241, l4: 0.000553, l5: 0.001105, l6: 0.001643
[epoch: 403/1000, batch:    56/ 1052, ite: 105740] train loss: 0.004484, tar: 0.000159 
l0: 0.000117, l1: 0.000119, l2: 0.000162, l3: 0.000201, l4: 0.000459, l5: 0.000987, l6: 0.001663
[epoch: 403/1000, batch:   136/ 1052, ite: 105760] train loss: 0.004479, tar: 0.000158 
l0: 0.000198, l1: 0.000197, l2: 0.000233, l3: 0.000354, l4: 0.000611, l5: 0.001348, l6: 0.002522
[epoch: 403/1000, batch:   216/ 1052, ite: 105780] train loss: 0.004504, tar: 0.000160 
l0: 0.000160, l1: 0.000162, l2: 0.000216, l3: 0.000387, l4: 0.000868, l5: 0.001374, l6: 0.003404
[epoch: 403/1000, batch:   296/ 1052, ite: 105800] train loss: 0.004519, tar: 0.000160 
l0: 0.000110, l1: 0.000113, l2: 0.000139, l3: 0.000222, l4: 0.000362, l5: 0.001060, l6: 0.001627
[epoch: 403/1000, batch:   376/ 1052, ite: 105820] train loss: 0.004498, tar: 0.000159 
l0: 0.000378, l1: 0.000376, l2: 0.000395, l3: 0.000434, l4: 0.000645, l5: 0.001009, l6: 0.002765
[epoch: 403/1000, batch:   456/ 1052, ite: 105840] train loss: 0.004494, tar: 0.000159 
l0: 0.000260, l1: 0.000268, l2: 0.000320, l3: 0.000396, l4: 0.000628, l5: 0.001482, l6: 0.002535
[epoch: 403/1000, batch:   536/ 1052, ite: 105860] train loss: 0.004489, tar: 0.000160 
l0: 0.000118, l1: 0.000113, l2: 0.000178, l3: 0.000275, l4: 0.000471, l5: 0.000819, l6: 0.001264
[epoch: 403/1000, batch:   616/ 1052, ite: 105880] train loss: 0.004485, tar: 0.000159 
l0: 0.000133, l1: 0.000140, l2: 0.000184, l3: 0.000175, l4: 0.000460, l5: 0.001126, l6: 0.002252
[epoch: 403/1000, batch:   696/ 1052, ite: 105900] train loss: 0.004490, tar: 0.000159 
l0: 0.000177, l1: 0.000180, l2: 0.000210, l3: 0.000212, l4: 0.000406, l5: 0.001042, l6: 0.001792
[epoch: 403/1000, batch:   776/ 1052, ite: 105920] train loss: 0.004479, tar: 0.000159 
l0: 0.000107, l1: 0.000106, l2: 0.000122, l3: 0.000145, l4: 0.000272, l5: 0.000646, l6: 0.000930
[epoch: 403/1000, batch:   856/ 1052, ite: 105940] train loss: 0.004479, tar: 0.000159 
l0: 0.000129, l1: 0.000134, l2: 0.000177, l3: 0.000262, l4: 0.000419, l5: 0.001230, l6: 0.001907
[epoch: 403/1000, batch:   936/ 1052, ite: 105960] train loss: 0.004470, tar: 0.000159 
l0: 0.000213, l1: 0.000210, l2: 0.000285, l3: 0.000582, l4: 0.001021, l5: 0.001379, l6: 0.002470
[epoch: 403/1000, batch:  1016/ 1052, ite: 105980] train loss: 0.004481, tar: 0.000159 
[Epoch 403/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000091, l1: 0.000092, l2: 0.000144, l3: 0.000288, l4: 0.001012, l5: 0.001902, l6: 0.002450
[epoch: 404/1000, batch:    44/ 1052, ite: 106000] train loss: 0.004477, tar: 0.000159 
l0: 0.000167, l1: 0.000168, l2: 0.000199, l3: 0.000244, l4: 0.000462, l5: 0.001005, l6: 0.001752
[epoch: 404/1000, batch:   124/ 1052, ite: 106020] train loss: 0.004488, tar: 0.000160 
l0: 0.000199, l1: 0.000201, l2: 0.000244, l3: 0.000333, l4: 0.000491, l5: 0.000796, l6: 0.001837
[epoch: 404/1000, batch:   204/ 1052, ite: 106040] train loss: 0.004485, tar: 0.000159 
l0: 0.000095, l1: 0.000096, l2: 0.000114, l3: 0.000156, l4: 0.000356, l5: 0.000507, l6: 0.001383
[epoch: 404/1000, batch:   284/ 1052, ite: 106060] train loss: 0.004490, tar: 0.000159 
l0: 0.000131, l1: 0.000131, l2: 0.000152, l3: 0.000229, l4: 0.000556, l5: 0.000981, l6: 0.001570
[epoch: 404/1000, batch:   364/ 1052, ite: 106080] train loss: 0.004478, tar: 0.000159 
l0: 0.000161, l1: 0.000159, l2: 0.000186, l3: 0.000257, l4: 0.000502, l5: 0.000824, l6: 0.001855
[epoch: 404/1000, batch:   444/ 1052, ite: 106100] train loss: 0.004462, tar: 0.000159 
l0: 0.000098, l1: 0.000096, l2: 0.000115, l3: 0.000203, l4: 0.000436, l5: 0.000922, l6: 0.001842
[epoch: 404/1000, batch:   524/ 1052, ite: 106120] train loss: 0.004465, tar: 0.000159 
l0: 0.000111, l1: 0.000115, l2: 0.000139, l3: 0.000296, l4: 0.001015, l5: 0.001168, l6: 0.002652
[epoch: 404/1000, batch:   604/ 1052, ite: 106140] train loss: 0.004468, tar: 0.000159 
l0: 0.000304, l1: 0.000304, l2: 0.000346, l3: 0.000435, l4: 0.000762, l5: 0.001768, l6: 0.002498
[epoch: 404/1000, batch:   684/ 1052, ite: 106160] train loss: 0.004466, tar: 0.000159 
l0: 0.000100, l1: 0.000101, l2: 0.000121, l3: 0.000184, l4: 0.000261, l5: 0.000744, l6: 0.001375
[epoch: 404/1000, batch:   764/ 1052, ite: 106180] train loss: 0.004462, tar: 0.000158 
l0: 0.000196, l1: 0.000203, l2: 0.000228, l3: 0.000218, l4: 0.000280, l5: 0.000989, l6: 0.001819
[epoch: 404/1000, batch:   844/ 1052, ite: 106200] train loss: 0.004468, tar: 0.000158 
l0: 0.000117, l1: 0.000116, l2: 0.000186, l3: 0.000212, l4: 0.000557, l5: 0.001244, l6: 0.001995
[epoch: 404/1000, batch:   924/ 1052, ite: 106220] train loss: 0.004473, tar: 0.000159 
l0: 0.000131, l1: 0.000130, l2: 0.000171, l3: 0.000283, l4: 0.000698, l5: 0.001657, l6: 0.002233
[epoch: 404/1000, batch:  1004/ 1052, ite: 106240] train loss: 0.004479, tar: 0.000159 
[Epoch 404/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000083, l1: 0.000086, l2: 0.000099, l3: 0.000139, l4: 0.000241, l5: 0.000592, l6: 0.001484
[epoch: 405/1000, batch:    32/ 1052, ite: 106260] train loss: 0.004477, tar: 0.000159 
l0: 0.000203, l1: 0.000191, l2: 0.000328, l3: 0.000463, l4: 0.000784, l5: 0.001783, l6: 0.002121
[epoch: 405/1000, batch:   112/ 1052, ite: 106280] train loss: 0.004478, tar: 0.000159 
l0: 0.000229, l1: 0.000227, l2: 0.000225, l3: 0.000328, l4: 0.000847, l5: 0.001409, l6: 0.001954
[epoch: 405/1000, batch:   192/ 1052, ite: 106300] train loss: 0.004483, tar: 0.000159 
l0: 0.000187, l1: 0.000188, l2: 0.000223, l3: 0.000300, l4: 0.000370, l5: 0.000986, l6: 0.002416
[epoch: 405/1000, batch:   272/ 1052, ite: 106320] train loss: 0.004475, tar: 0.000159 
l0: 0.000108, l1: 0.000109, l2: 0.000175, l3: 0.000246, l4: 0.000476, l5: 0.001181, l6: 0.001742
[epoch: 405/1000, batch:   352/ 1052, ite: 106340] train loss: 0.004473, tar: 0.000159 
l0: 0.000334, l1: 0.000331, l2: 0.000411, l3: 0.000538, l4: 0.000969, l5: 0.001828, l6: 0.003386
[epoch: 405/1000, batch:   432/ 1052, ite: 106360] train loss: 0.004475, tar: 0.000159 
l0: 0.000113, l1: 0.000115, l2: 0.000153, l3: 0.000269, l4: 0.000361, l5: 0.000911, l6: 0.001788
[epoch: 405/1000, batch:   512/ 1052, ite: 106380] train loss: 0.004480, tar: 0.000160 
l0: 0.000158, l1: 0.000157, l2: 0.000186, l3: 0.000287, l4: 0.000453, l5: 0.000663, l6: 0.002291
[epoch: 405/1000, batch:   592/ 1052, ite: 106400] train loss: 0.004482, tar: 0.000160 
l0: 0.000258, l1: 0.000266, l2: 0.000338, l3: 0.000522, l4: 0.001309, l5: 0.002169, l6: 0.004741
[epoch: 405/1000, batch:   672/ 1052, ite: 106420] train loss: 0.004488, tar: 0.000161 
l0: 0.000144, l1: 0.000150, l2: 0.000166, l3: 0.000173, l4: 0.000311, l5: 0.000833, l6: 0.001696
[epoch: 405/1000, batch:   752/ 1052, ite: 106440] train loss: 0.004489, tar: 0.000161 
l0: 0.000071, l1: 0.000072, l2: 0.000086, l3: 0.000176, l4: 0.000301, l5: 0.000552, l6: 0.000948
[epoch: 405/1000, batch:   832/ 1052, ite: 106460] train loss: 0.004493, tar: 0.000161 
l0: 0.000075, l1: 0.000078, l2: 0.000111, l3: 0.000150, l4: 0.000353, l5: 0.000564, l6: 0.001069
[epoch: 405/1000, batch:   912/ 1052, ite: 106480] train loss: 0.004491, tar: 0.000161 
l0: 0.000101, l1: 0.000101, l2: 0.000117, l3: 0.000200, l4: 0.000413, l5: 0.000667, l6: 0.001797
[epoch: 405/1000, batch:   992/ 1052, ite: 106500] train loss: 0.004488, tar: 0.000160 
[Epoch 405/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000200, l1: 0.000204, l2: 0.000201, l3: 0.000291, l4: 0.000620, l5: 0.001095, l6: 0.002008
[epoch: 406/1000, batch:    20/ 1052, ite: 106520] train loss: 0.004491, tar: 0.000161 
l0: 0.000211, l1: 0.000211, l2: 0.000239, l3: 0.000319, l4: 0.000579, l5: 0.000879, l6: 0.001358
[epoch: 406/1000, batch:   100/ 1052, ite: 106540] train loss: 0.004496, tar: 0.000161 
l0: 0.000088, l1: 0.000089, l2: 0.000133, l3: 0.000160, l4: 0.000387, l5: 0.000851, l6: 0.000975
[epoch: 406/1000, batch:   180/ 1052, ite: 106560] train loss: 0.004496, tar: 0.000161 
l0: 0.000228, l1: 0.000225, l2: 0.000274, l3: 0.000442, l4: 0.000869, l5: 0.001641, l6: 0.003722
[epoch: 406/1000, batch:   260/ 1052, ite: 106580] train loss: 0.004488, tar: 0.000161 
l0: 0.000157, l1: 0.000164, l2: 0.000183, l3: 0.000240, l4: 0.000724, l5: 0.001532, l6: 0.003164
[epoch: 406/1000, batch:   340/ 1052, ite: 106600] train loss: 0.004492, tar: 0.000161 
l0: 0.000081, l1: 0.000083, l2: 0.000089, l3: 0.000135, l4: 0.000345, l5: 0.000724, l6: 0.000843
[epoch: 406/1000, batch:   420/ 1052, ite: 106620] train loss: 0.004485, tar: 0.000160 
l0: 0.000170, l1: 0.000173, l2: 0.000230, l3: 0.000298, l4: 0.000944, l5: 0.001867, l6: 0.002103
[epoch: 406/1000, batch:   500/ 1052, ite: 106640] train loss: 0.004491, tar: 0.000160 
l0: 0.000146, l1: 0.000146, l2: 0.000174, l3: 0.000253, l4: 0.000403, l5: 0.000971, l6: 0.001153
[epoch: 406/1000, batch:   580/ 1052, ite: 106660] train loss: 0.004498, tar: 0.000160 
l0: 0.000096, l1: 0.000105, l2: 0.000110, l3: 0.000159, l4: 0.000230, l5: 0.000755, l6: 0.001548
[epoch: 406/1000, batch:   660/ 1052, ite: 106680] train loss: 0.004492, tar: 0.000160 
l0: 0.000198, l1: 0.000205, l2: 0.000284, l3: 0.000501, l4: 0.000801, l5: 0.001709, l6: 0.002423
[epoch: 406/1000, batch:   740/ 1052, ite: 106700] train loss: 0.004490, tar: 0.000160 
l0: 0.000297, l1: 0.000293, l2: 0.000344, l3: 0.000508, l4: 0.000798, l5: 0.002099, l6: 0.004247
[epoch: 406/1000, batch:   820/ 1052, ite: 106720] train loss: 0.004495, tar: 0.000160 
l0: 0.000125, l1: 0.000124, l2: 0.000150, l3: 0.000188, l4: 0.000515, l5: 0.001086, l6: 0.001888
[epoch: 406/1000, batch:   900/ 1052, ite: 106740] train loss: 0.004495, tar: 0.000160 
l0: 0.000189, l1: 0.000188, l2: 0.000234, l3: 0.000298, l4: 0.000504, l5: 0.001007, l6: 0.003504
[epoch: 406/1000, batch:   980/ 1052, ite: 106760] train loss: 0.004493, tar: 0.000160 
[Epoch 406/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000231, l1: 0.000238, l2: 0.000249, l3: 0.000354, l4: 0.000492, l5: 0.000958, l6: 0.001317
[epoch: 407/1000, batch:     8/ 1052, ite: 106780] train loss: 0.004500, tar: 0.000160 
l0: 0.000156, l1: 0.000157, l2: 0.000190, l3: 0.000293, l4: 0.000739, l5: 0.001951, l6: 0.003773
[epoch: 407/1000, batch:    88/ 1052, ite: 106800] train loss: 0.004494, tar: 0.000160 
l0: 0.000296, l1: 0.000297, l2: 0.000325, l3: 0.000340, l4: 0.000713, l5: 0.001503, l6: 0.003273
[epoch: 407/1000, batch:   168/ 1052, ite: 106820] train loss: 0.004493, tar: 0.000160 
l0: 0.000280, l1: 0.000283, l2: 0.000296, l3: 0.000401, l4: 0.000740, l5: 0.001201, l6: 0.002007
[epoch: 407/1000, batch:   248/ 1052, ite: 106840] train loss: 0.004496, tar: 0.000160 
l0: 0.000066, l1: 0.000068, l2: 0.000095, l3: 0.000187, l4: 0.000526, l5: 0.001084, l6: 0.001762
[epoch: 407/1000, batch:   328/ 1052, ite: 106860] train loss: 0.004495, tar: 0.000160 
l0: 0.000234, l1: 0.000235, l2: 0.000244, l3: 0.000300, l4: 0.000632, l5: 0.001269, l6: 0.002620
[epoch: 407/1000, batch:   408/ 1052, ite: 106880] train loss: 0.004495, tar: 0.000160 
l0: 0.000113, l1: 0.000116, l2: 0.000113, l3: 0.000173, l4: 0.000268, l5: 0.000799, l6: 0.001261
[epoch: 407/1000, batch:   488/ 1052, ite: 106900] train loss: 0.004501, tar: 0.000160 
l0: 0.000104, l1: 0.000104, l2: 0.000124, l3: 0.000202, l4: 0.000303, l5: 0.000898, l6: 0.001056
[epoch: 407/1000, batch:   568/ 1052, ite: 106920] train loss: 0.004501, tar: 0.000160 
l0: 0.000300, l1: 0.000297, l2: 0.000350, l3: 0.000461, l4: 0.000712, l5: 0.001230, l6: 0.002998
[epoch: 407/1000, batch:   648/ 1052, ite: 106940] train loss: 0.004496, tar: 0.000160 
l0: 0.000147, l1: 0.000151, l2: 0.000159, l3: 0.000224, l4: 0.000470, l5: 0.001053, l6: 0.001762
[epoch: 407/1000, batch:   728/ 1052, ite: 106960] train loss: 0.004492, tar: 0.000159 
l0: 0.000068, l1: 0.000066, l2: 0.000119, l3: 0.000226, l4: 0.000418, l5: 0.000897, l6: 0.001238
[epoch: 407/1000, batch:   808/ 1052, ite: 106980] train loss: 0.004497, tar: 0.000159 
l0: 0.000055, l1: 0.000057, l2: 0.000068, l3: 0.000099, l4: 0.000418, l5: 0.000378, l6: 0.001044
[epoch: 407/1000, batch:   888/ 1052, ite: 107000] train loss: 0.004487, tar: 0.000159 
l0: 0.000243, l1: 0.000245, l2: 0.000313, l3: 0.000438, l4: 0.000799, l5: 0.002434, l6: 0.002795
[epoch: 407/1000, batch:   968/ 1052, ite: 107020] train loss: 0.004494, tar: 0.000159 
l0: 0.000118, l1: 0.000117, l2: 0.000169, l3: 0.000265, l4: 0.000441, l5: 0.001457, l6: 0.002708
[epoch: 407/1000, batch:  1048/ 1052, ite: 107040] train loss: 0.004494, tar: 0.000159 
[Epoch 407/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000095, l1: 0.000099, l2: 0.000137, l3: 0.000162, l4: 0.000425, l5: 0.001043, l6: 0.001428
[epoch: 408/1000, batch:    76/ 1052, ite: 107060] train loss: 0.004494, tar: 0.000159 
l0: 0.000110, l1: 0.000114, l2: 0.000120, l3: 0.000203, l4: 0.000376, l5: 0.000740, l6: 0.001344
[epoch: 408/1000, batch:   156/ 1052, ite: 107080] train loss: 0.004490, tar: 0.000159 
l0: 0.000229, l1: 0.000231, l2: 0.000261, l3: 0.000353, l4: 0.000701, l5: 0.001649, l6: 0.003059
[epoch: 408/1000, batch:   236/ 1052, ite: 107100] train loss: 0.004486, tar: 0.000158 
l0: 0.000088, l1: 0.000086, l2: 0.000136, l3: 0.000185, l4: 0.000345, l5: 0.001443, l6: 0.002053
[epoch: 408/1000, batch:   316/ 1052, ite: 107120] train loss: 0.004478, tar: 0.000158 
l0: 0.000184, l1: 0.000191, l2: 0.000176, l3: 0.000317, l4: 0.000789, l5: 0.001306, l6: 0.002222
[epoch: 408/1000, batch:   396/ 1052, ite: 107140] train loss: 0.004479, tar: 0.000158 
l0: 0.000289, l1: 0.000294, l2: 0.000394, l3: 0.000515, l4: 0.000982, l5: 0.001697, l6: 0.003998
[epoch: 408/1000, batch:   476/ 1052, ite: 107160] train loss: 0.004487, tar: 0.000158 
l0: 0.000211, l1: 0.000220, l2: 0.000229, l3: 0.000302, l4: 0.000440, l5: 0.001342, l6: 0.002917
[epoch: 408/1000, batch:   556/ 1052, ite: 107180] train loss: 0.004491, tar: 0.000158 
l0: 0.000219, l1: 0.000216, l2: 0.000331, l3: 0.000533, l4: 0.001129, l5: 0.001653, l6: 0.003977
[epoch: 408/1000, batch:   636/ 1052, ite: 107200] train loss: 0.004490, tar: 0.000158 
l0: 0.000213, l1: 0.000213, l2: 0.000285, l3: 0.000436, l4: 0.000772, l5: 0.001355, l6: 0.001960
[epoch: 408/1000, batch:   716/ 1052, ite: 107220] train loss: 0.004486, tar: 0.000158 
l0: 0.000131, l1: 0.000132, l2: 0.000139, l3: 0.000185, l4: 0.000478, l5: 0.000925, l6: 0.001358
[epoch: 408/1000, batch:   796/ 1052, ite: 107240] train loss: 0.004484, tar: 0.000158 
l0: 0.000245, l1: 0.000250, l2: 0.000322, l3: 0.000424, l4: 0.000943, l5: 0.001074, l6: 0.002947
[epoch: 408/1000, batch:   876/ 1052, ite: 107260] train loss: 0.004486, tar: 0.000158 
l0: 0.000103, l1: 0.000102, l2: 0.000148, l3: 0.000230, l4: 0.000544, l5: 0.000737, l6: 0.001044
[epoch: 408/1000, batch:   956/ 1052, ite: 107280] train loss: 0.004485, tar: 0.000158 
l0: 0.000146, l1: 0.000149, l2: 0.000225, l3: 0.000286, l4: 0.000622, l5: 0.001446, l6: 0.003077
[epoch: 408/1000, batch:  1036/ 1052, ite: 107300] train loss: 0.004490, tar: 0.000158 
[Epoch 408/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000178, l1: 0.000182, l2: 0.000210, l3: 0.000308, l4: 0.000618, l5: 0.001524, l6: 0.001902
[epoch: 409/1000, batch:    64/ 1052, ite: 107320] train loss: 0.004488, tar: 0.000158 
l0: 0.000113, l1: 0.000113, l2: 0.000160, l3: 0.000221, l4: 0.000477, l5: 0.000831, l6: 0.001141
[epoch: 409/1000, batch:   144/ 1052, ite: 107340] train loss: 0.004486, tar: 0.000158 
l0: 0.000084, l1: 0.000084, l2: 0.000123, l3: 0.000192, l4: 0.000368, l5: 0.000774, l6: 0.001489
[epoch: 409/1000, batch:   224/ 1052, ite: 107360] train loss: 0.004487, tar: 0.000158 
l0: 0.000319, l1: 0.000320, l2: 0.000387, l3: 0.000486, l4: 0.001045, l5: 0.002631, l6: 0.005890
[epoch: 409/1000, batch:   304/ 1052, ite: 107380] train loss: 0.004488, tar: 0.000158 
l0: 0.000084, l1: 0.000088, l2: 0.000106, l3: 0.000215, l4: 0.000417, l5: 0.000510, l6: 0.001634
[epoch: 409/1000, batch:   384/ 1052, ite: 107400] train loss: 0.004490, tar: 0.000158 
l0: 0.000176, l1: 0.000176, l2: 0.000253, l3: 0.000279, l4: 0.000545, l5: 0.001192, l6: 0.002683
[epoch: 409/1000, batch:   464/ 1052, ite: 107420] train loss: 0.004487, tar: 0.000157 
l0: 0.000130, l1: 0.000134, l2: 0.000156, l3: 0.000155, l4: 0.000402, l5: 0.001049, l6: 0.001663
[epoch: 409/1000, batch:   544/ 1052, ite: 107440] train loss: 0.004486, tar: 0.000157 
l0: 0.000134, l1: 0.000144, l2: 0.000159, l3: 0.000220, l4: 0.000534, l5: 0.000655, l6: 0.002194
[epoch: 409/1000, batch:   624/ 1052, ite: 107460] train loss: 0.004480, tar: 0.000157 
l0: 0.000213, l1: 0.000213, l2: 0.000277, l3: 0.000356, l4: 0.000710, l5: 0.001297, l6: 0.002866
[epoch: 409/1000, batch:   704/ 1052, ite: 107480] train loss: 0.004477, tar: 0.000157 
l0: 0.000155, l1: 0.000163, l2: 0.000160, l3: 0.000277, l4: 0.000481, l5: 0.001074, l6: 0.002022
[epoch: 409/1000, batch:   784/ 1052, ite: 107500] train loss: 0.004480, tar: 0.000157 
l0: 0.000077, l1: 0.000080, l2: 0.000107, l3: 0.000155, l4: 0.000285, l5: 0.001104, l6: 0.001196
[epoch: 409/1000, batch:   864/ 1052, ite: 107520] train loss: 0.004486, tar: 0.000157 
l0: 0.000198, l1: 0.000195, l2: 0.000203, l3: 0.000301, l4: 0.000452, l5: 0.001170, l6: 0.002138
[epoch: 409/1000, batch:   944/ 1052, ite: 107540] train loss: 0.004485, tar: 0.000157 
l0: 0.000048, l1: 0.000058, l2: 0.000110, l3: 0.000183, l4: 0.000444, l5: 0.001285, l6: 0.001768
[epoch: 409/1000, batch:  1024/ 1052, ite: 107560] train loss: 0.004486, tar: 0.000157 
[Epoch 409/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000328, l1: 0.000334, l2: 0.000298, l3: 0.000369, l4: 0.000775, l5: 0.001519, l6: 0.001746
[epoch: 410/1000, batch:    52/ 1052, ite: 107580] train loss: 0.004486, tar: 0.000157 
l0: 0.000143, l1: 0.000147, l2: 0.000151, l3: 0.000216, l4: 0.000412, l5: 0.000707, l6: 0.001539
[epoch: 410/1000, batch:   132/ 1052, ite: 107600] train loss: 0.004487, tar: 0.000158 
l0: 0.000139, l1: 0.000137, l2: 0.000192, l3: 0.000284, l4: 0.000514, l5: 0.000799, l6: 0.002636
[epoch: 410/1000, batch:   212/ 1052, ite: 107620] train loss: 0.004487, tar: 0.000158 
l0: 0.000238, l1: 0.000245, l2: 0.000231, l3: 0.000279, l4: 0.000714, l5: 0.001329, l6: 0.002184
[epoch: 410/1000, batch:   292/ 1052, ite: 107640] train loss: 0.004487, tar: 0.000158 
l0: 0.000097, l1: 0.000096, l2: 0.000123, l3: 0.000175, l4: 0.000492, l5: 0.000976, l6: 0.001835
[epoch: 410/1000, batch:   372/ 1052, ite: 107660] train loss: 0.004486, tar: 0.000157 
l0: 0.000095, l1: 0.000096, l2: 0.000169, l3: 0.000268, l4: 0.000784, l5: 0.001020, l6: 0.002392
[epoch: 410/1000, batch:   452/ 1052, ite: 107680] train loss: 0.004487, tar: 0.000157 
l0: 0.000101, l1: 0.000101, l2: 0.000157, l3: 0.000233, l4: 0.000466, l5: 0.000729, l6: 0.001908
[epoch: 410/1000, batch:   532/ 1052, ite: 107700] train loss: 0.004489, tar: 0.000157 
l0: 0.000097, l1: 0.000095, l2: 0.000121, l3: 0.000163, l4: 0.000388, l5: 0.000841, l6: 0.001078
[epoch: 410/1000, batch:   612/ 1052, ite: 107720] train loss: 0.004491, tar: 0.000157 
l0: 0.000129, l1: 0.000130, l2: 0.000153, l3: 0.000196, l4: 0.000321, l5: 0.000722, l6: 0.001927
[epoch: 410/1000, batch:   692/ 1052, ite: 107740] train loss: 0.004491, tar: 0.000157 
l0: 0.000271, l1: 0.000280, l2: 0.000262, l3: 0.000323, l4: 0.000622, l5: 0.001419, l6: 0.002204
[epoch: 410/1000, batch:   772/ 1052, ite: 107760] train loss: 0.004493, tar: 0.000157 
l0: 0.000079, l1: 0.000081, l2: 0.000094, l3: 0.000134, l4: 0.000396, l5: 0.000679, l6: 0.001220
[epoch: 410/1000, batch:   852/ 1052, ite: 107780] train loss: 0.004490, tar: 0.000157 
l0: 0.000090, l1: 0.000092, l2: 0.000123, l3: 0.000204, l4: 0.000519, l5: 0.000592, l6: 0.001620
[epoch: 410/1000, batch:   932/ 1052, ite: 107800] train loss: 0.004494, tar: 0.000158 
l0: 0.000138, l1: 0.000138, l2: 0.000180, l3: 0.000203, l4: 0.000340, l5: 0.000996, l6: 0.001617
[epoch: 410/1000, batch:  1012/ 1052, ite: 107820] train loss: 0.004490, tar: 0.000157 
[Epoch 410/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000171, l1: 0.000174, l2: 0.000234, l3: 0.000344, l4: 0.000708, l5: 0.001323, l6: 0.002406
[epoch: 411/1000, batch:    40/ 1052, ite: 107840] train loss: 0.004488, tar: 0.000157 
l0: 0.000157, l1: 0.000159, l2: 0.000191, l3: 0.000228, l4: 0.000562, l5: 0.000875, l6: 0.001688
[epoch: 411/1000, batch:   120/ 1052, ite: 107860] train loss: 0.004491, tar: 0.000157 
l0: 0.000196, l1: 0.000197, l2: 0.000219, l3: 0.000310, l4: 0.000711, l5: 0.001238, l6: 0.001595
[epoch: 411/1000, batch:   200/ 1052, ite: 107880] train loss: 0.004491, tar: 0.000157 
l0: 0.000088, l1: 0.000091, l2: 0.000109, l3: 0.000154, l4: 0.000316, l5: 0.000759, l6: 0.001630
[epoch: 411/1000, batch:   280/ 1052, ite: 107900] train loss: 0.004489, tar: 0.000157 
l0: 0.000033, l1: 0.000033, l2: 0.000058, l3: 0.000121, l4: 0.000398, l5: 0.000627, l6: 0.001386
[epoch: 411/1000, batch:   360/ 1052, ite: 107920] train loss: 0.004487, tar: 0.000157 
l0: 0.000112, l1: 0.000113, l2: 0.000140, l3: 0.000217, l4: 0.000530, l5: 0.000970, l6: 0.002554
[epoch: 411/1000, batch:   440/ 1052, ite: 107940] train loss: 0.004487, tar: 0.000157 
l0: 0.000286, l1: 0.000284, l2: 0.000321, l3: 0.000295, l4: 0.000670, l5: 0.001383, l6: 0.002784
[epoch: 411/1000, batch:   520/ 1052, ite: 107960] train loss: 0.004488, tar: 0.000157 
l0: 0.000263, l1: 0.000261, l2: 0.000313, l3: 0.000544, l4: 0.000893, l5: 0.001961, l6: 0.002672
[epoch: 411/1000, batch:   600/ 1052, ite: 107980] train loss: 0.004485, tar: 0.000157 
l0: 0.000109, l1: 0.000110, l2: 0.000125, l3: 0.000207, l4: 0.000438, l5: 0.000780, l6: 0.001188
[epoch: 411/1000, batch:   680/ 1052, ite: 108000] train loss: 0.004488, tar: 0.000157 
l0: 0.000202, l1: 0.000203, l2: 0.000279, l3: 0.000495, l4: 0.000842, l5: 0.001473, l6: 0.003005
[epoch: 411/1000, batch:   760/ 1052, ite: 108020] train loss: 0.004487, tar: 0.000157 
l0: 0.000174, l1: 0.000177, l2: 0.000250, l3: 0.000357, l4: 0.000592, l5: 0.001002, l6: 0.002153
[epoch: 411/1000, batch:   840/ 1052, ite: 108040] train loss: 0.004486, tar: 0.000157 
l0: 0.000047, l1: 0.000046, l2: 0.000072, l3: 0.000106, l4: 0.000331, l5: 0.000627, l6: 0.001073
[epoch: 411/1000, batch:   920/ 1052, ite: 108060] train loss: 0.004485, tar: 0.000157 
l0: 0.000204, l1: 0.000210, l2: 0.000222, l3: 0.000292, l4: 0.000391, l5: 0.001490, l6: 0.002276
[epoch: 411/1000, batch:  1000/ 1052, ite: 108080] train loss: 0.004483, tar: 0.000157 
[Epoch 411/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000051, l1: 0.000051, l2: 0.000080, l3: 0.000122, l4: 0.000341, l5: 0.000523, l6: 0.000731
[epoch: 412/1000, batch:    28/ 1052, ite: 108100] train loss: 0.004484, tar: 0.000157 
l0: 0.000087, l1: 0.000085, l2: 0.000135, l3: 0.000195, l4: 0.000412, l5: 0.000797, l6: 0.002342
[epoch: 412/1000, batch:   108/ 1052, ite: 108120] train loss: 0.004487, tar: 0.000157 
l0: 0.000143, l1: 0.000142, l2: 0.000185, l3: 0.000272, l4: 0.000520, l5: 0.001214, l6: 0.001887
[epoch: 412/1000, batch:   188/ 1052, ite: 108140] train loss: 0.004485, tar: 0.000157 
l0: 0.000096, l1: 0.000094, l2: 0.000122, l3: 0.000222, l4: 0.000542, l5: 0.000737, l6: 0.001179
[epoch: 412/1000, batch:   268/ 1052, ite: 108160] train loss: 0.004488, tar: 0.000157 
l0: 0.000119, l1: 0.000121, l2: 0.000138, l3: 0.000185, l4: 0.000463, l5: 0.001037, l6: 0.001996
[epoch: 412/1000, batch:   348/ 1052, ite: 108180] train loss: 0.004487, tar: 0.000157 
l0: 0.000130, l1: 0.000129, l2: 0.000173, l3: 0.000253, l4: 0.000378, l5: 0.000881, l6: 0.002109
[epoch: 412/1000, batch:   428/ 1052, ite: 108200] train loss: 0.004490, tar: 0.000157 
l0: 0.000175, l1: 0.000171, l2: 0.000261, l3: 0.000454, l4: 0.000663, l5: 0.001267, l6: 0.002769
[epoch: 412/1000, batch:   508/ 1052, ite: 108220] train loss: 0.004490, tar: 0.000157 
l0: 0.000137, l1: 0.000140, l2: 0.000144, l3: 0.000181, l4: 0.000375, l5: 0.000734, l6: 0.001000
[epoch: 412/1000, batch:   588/ 1052, ite: 108240] train loss: 0.004491, tar: 0.000157 
l0: 0.000074, l1: 0.000074, l2: 0.000092, l3: 0.000152, l4: 0.000327, l5: 0.000652, l6: 0.001165
[epoch: 412/1000, batch:   668/ 1052, ite: 108260] train loss: 0.004491, tar: 0.000157 
l0: 0.000200, l1: 0.000204, l2: 0.000304, l3: 0.000369, l4: 0.000699, l5: 0.000969, l6: 0.002437
[epoch: 412/1000, batch:   748/ 1052, ite: 108280] train loss: 0.004489, tar: 0.000157 
l0: 0.000159, l1: 0.000160, l2: 0.000234, l3: 0.000292, l4: 0.000517, l5: 0.001337, l6: 0.002048
[epoch: 412/1000, batch:   828/ 1052, ite: 108300] train loss: 0.004489, tar: 0.000157 
l0: 0.000274, l1: 0.000271, l2: 0.000343, l3: 0.000472, l4: 0.000837, l5: 0.001388, l6: 0.002458
[epoch: 412/1000, batch:   908/ 1052, ite: 108320] train loss: 0.004490, tar: 0.000157 
l0: 0.000271, l1: 0.000270, l2: 0.000339, l3: 0.000478, l4: 0.000957, l5: 0.001868, l6: 0.003390
[epoch: 412/1000, batch:   988/ 1052, ite: 108340] train loss: 0.004490, tar: 0.000157 
[Epoch 412/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000231, l1: 0.000235, l2: 0.000263, l3: 0.000327, l4: 0.000589, l5: 0.001934, l6: 0.002134
[epoch: 413/1000, batch:    16/ 1052, ite: 108360] train loss: 0.004486, tar: 0.000157 
l0: 0.000061, l1: 0.000062, l2: 0.000098, l3: 0.000156, l4: 0.000383, l5: 0.000674, l6: 0.001167
[epoch: 413/1000, batch:    96/ 1052, ite: 108380] train loss: 0.004486, tar: 0.000157 
l0: 0.000101, l1: 0.000098, l2: 0.000142, l3: 0.000226, l4: 0.000424, l5: 0.000895, l6: 0.001421
[epoch: 413/1000, batch:   176/ 1052, ite: 108400] train loss: 0.004488, tar: 0.000157 
l0: 0.000078, l1: 0.000079, l2: 0.000082, l3: 0.000118, l4: 0.000347, l5: 0.000632, l6: 0.000884
[epoch: 413/1000, batch:   256/ 1052, ite: 108420] train loss: 0.004487, tar: 0.000157 
l0: 0.000174, l1: 0.000178, l2: 0.000264, l3: 0.000390, l4: 0.001029, l5: 0.001588, l6: 0.002950
[epoch: 413/1000, batch:   336/ 1052, ite: 108440] train loss: 0.004484, tar: 0.000157 
l0: 0.000098, l1: 0.000101, l2: 0.000128, l3: 0.000163, l4: 0.000219, l5: 0.000672, l6: 0.001679
[epoch: 413/1000, batch:   416/ 1052, ite: 108460] train loss: 0.004483, tar: 0.000157 
l0: 0.000197, l1: 0.000204, l2: 0.000234, l3: 0.000342, l4: 0.000552, l5: 0.000999, l6: 0.001816
[epoch: 413/1000, batch:   496/ 1052, ite: 108480] train loss: 0.004487, tar: 0.000157 
l0: 0.000175, l1: 0.000182, l2: 0.000191, l3: 0.000246, l4: 0.000659, l5: 0.001269, l6: 0.003471
[epoch: 413/1000, batch:   576/ 1052, ite: 108500] train loss: 0.004488, tar: 0.000157 
l0: 0.000027, l1: 0.000029, l2: 0.000039, l3: 0.000075, l4: 0.000147, l5: 0.000462, l6: 0.000918
[epoch: 413/1000, batch:   656/ 1052, ite: 108520] train loss: 0.004488, tar: 0.000157 
l0: 0.000068, l1: 0.000071, l2: 0.000112, l3: 0.000154, l4: 0.000352, l5: 0.000772, l6: 0.001852
[epoch: 413/1000, batch:   736/ 1052, ite: 108540] train loss: 0.004491, tar: 0.000157 
l0: 0.000133, l1: 0.000134, l2: 0.000168, l3: 0.000237, l4: 0.000488, l5: 0.001127, l6: 0.002092
[epoch: 413/1000, batch:   816/ 1052, ite: 108560] train loss: 0.004491, tar: 0.000157 
l0: 0.000052, l1: 0.000050, l2: 0.000083, l3: 0.000115, l4: 0.000272, l5: 0.000662, l6: 0.001149
[epoch: 413/1000, batch:   896/ 1052, ite: 108580] train loss: 0.004489, tar: 0.000157 
l0: 0.000210, l1: 0.000210, l2: 0.000293, l3: 0.000434, l4: 0.000758, l5: 0.001537, l6: 0.003459
[epoch: 413/1000, batch:   976/ 1052, ite: 108600] train loss: 0.004489, tar: 0.000157 
[Epoch 413/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000257, l1: 0.000257, l2: 0.000260, l3: 0.000322, l4: 0.000801, l5: 0.001459, l6: 0.001789
[epoch: 414/1000, batch:     4/ 1052, ite: 108620] train loss: 0.004489, tar: 0.000157 
l0: 0.000293, l1: 0.000299, l2: 0.000277, l3: 0.000363, l4: 0.000647, l5: 0.001457, l6: 0.002610
[epoch: 414/1000, batch:    84/ 1052, ite: 108640] train loss: 0.004492, tar: 0.000157 
l0: 0.000097, l1: 0.000092, l2: 0.000158, l3: 0.000237, l4: 0.000639, l5: 0.001272, l6: 0.001830
[epoch: 414/1000, batch:   164/ 1052, ite: 108660] train loss: 0.004489, tar: 0.000157 
l0: 0.000073, l1: 0.000071, l2: 0.000127, l3: 0.000232, l4: 0.000426, l5: 0.000788, l6: 0.001946
[epoch: 414/1000, batch:   244/ 1052, ite: 108680] train loss: 0.004492, tar: 0.000157 
l0: 0.000182, l1: 0.000187, l2: 0.000250, l3: 0.000373, l4: 0.000730, l5: 0.001016, l6: 0.002745
[epoch: 414/1000, batch:   324/ 1052, ite: 108700] train loss: 0.004490, tar: 0.000157 
l0: 0.000207, l1: 0.000214, l2: 0.000261, l3: 0.000336, l4: 0.000680, l5: 0.001393, l6: 0.002311
[epoch: 414/1000, batch:   404/ 1052, ite: 108720] train loss: 0.004489, tar: 0.000157 
l0: 0.000146, l1: 0.000150, l2: 0.000140, l3: 0.000201, l4: 0.000413, l5: 0.000685, l6: 0.001641
[epoch: 414/1000, batch:   484/ 1052, ite: 108740] train loss: 0.004492, tar: 0.000157 
l0: 0.000230, l1: 0.000230, l2: 0.000267, l3: 0.000311, l4: 0.000579, l5: 0.001772, l6: 0.003650
[epoch: 414/1000, batch:   564/ 1052, ite: 108760] train loss: 0.004493, tar: 0.000157 
l0: 0.000233, l1: 0.000242, l2: 0.000255, l3: 0.000335, l4: 0.000756, l5: 0.001510, l6: 0.002665
[epoch: 414/1000, batch:   644/ 1052, ite: 108780] train loss: 0.004497, tar: 0.000157 
l0: 0.000147, l1: 0.000143, l2: 0.000196, l3: 0.000307, l4: 0.000662, l5: 0.001165, l6: 0.001808
[epoch: 414/1000, batch:   724/ 1052, ite: 108800] train loss: 0.004498, tar: 0.000157 
l0: 0.000238, l1: 0.000238, l2: 0.000293, l3: 0.000398, l4: 0.000571, l5: 0.001070, l6: 0.002947
[epoch: 414/1000, batch:   804/ 1052, ite: 108820] train loss: 0.004496, tar: 0.000157 
l0: 0.000168, l1: 0.000171, l2: 0.000182, l3: 0.000245, l4: 0.000406, l5: 0.001053, l6: 0.001960
[epoch: 414/1000, batch:   884/ 1052, ite: 108840] train loss: 0.004494, tar: 0.000157 
l0: 0.000106, l1: 0.000109, l2: 0.000151, l3: 0.000179, l4: 0.000471, l5: 0.000930, l6: 0.001305
[epoch: 414/1000, batch:   964/ 1052, ite: 108860] train loss: 0.004491, tar: 0.000157 
l0: 0.000322, l1: 0.000333, l2: 0.000342, l3: 0.000413, l4: 0.000671, l5: 0.001965, l6: 0.003684
[epoch: 414/1000, batch:  1044/ 1052, ite: 108880] train loss: 0.004488, tar: 0.000157 
[Epoch 414/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000120, l1: 0.000119, l2: 0.000159, l3: 0.000253, l4: 0.000615, l5: 0.001122, l6: 0.002383
[epoch: 415/1000, batch:    72/ 1052, ite: 108900] train loss: 0.004490, tar: 0.000157 
l0: 0.000419, l1: 0.000424, l2: 0.000426, l3: 0.000597, l4: 0.001163, l5: 0.002630, l6: 0.004852
[epoch: 415/1000, batch:   152/ 1052, ite: 108920] train loss: 0.004489, tar: 0.000157 
l0: 0.000220, l1: 0.000221, l2: 0.000310, l3: 0.000432, l4: 0.000832, l5: 0.002274, l6: 0.003516
[epoch: 415/1000, batch:   232/ 1052, ite: 108940] train loss: 0.004488, tar: 0.000157 
l0: 0.000098, l1: 0.000097, l2: 0.000128, l3: 0.000185, l4: 0.000393, l5: 0.000773, l6: 0.001539
[epoch: 415/1000, batch:   312/ 1052, ite: 108960] train loss: 0.004488, tar: 0.000157 
l0: 0.000127, l1: 0.000128, l2: 0.000147, l3: 0.000176, l4: 0.000735, l5: 0.001067, l6: 0.001364
[epoch: 415/1000, batch:   392/ 1052, ite: 108980] train loss: 0.004486, tar: 0.000156 
l0: 0.000164, l1: 0.000160, l2: 0.000196, l3: 0.000262, l4: 0.000465, l5: 0.001106, l6: 0.000766
[epoch: 415/1000, batch:   472/ 1052, ite: 109000] train loss: 0.004484, tar: 0.000156 
l0: 0.000189, l1: 0.000184, l2: 0.000302, l3: 0.000534, l4: 0.000901, l5: 0.001575, l6: 0.002612
[epoch: 415/1000, batch:   552/ 1052, ite: 109020] train loss: 0.004486, tar: 0.000157 
l0: 0.000180, l1: 0.000176, l2: 0.000248, l3: 0.000422, l4: 0.000820, l5: 0.001368, l6: 0.002271
[epoch: 415/1000, batch:   632/ 1052, ite: 109040] train loss: 0.004487, tar: 0.000157 
l0: 0.000150, l1: 0.000158, l2: 0.000166, l3: 0.000171, l4: 0.000349, l5: 0.000770, l6: 0.001293
[epoch: 415/1000, batch:   712/ 1052, ite: 109060] train loss: 0.004487, tar: 0.000157 
l0: 0.000099, l1: 0.000103, l2: 0.000115, l3: 0.000184, l4: 0.000412, l5: 0.000751, l6: 0.001623
[epoch: 415/1000, batch:   792/ 1052, ite: 109080] train loss: 0.004485, tar: 0.000157 
l0: 0.000312, l1: 0.000320, l2: 0.000337, l3: 0.000326, l4: 0.000442, l5: 0.001161, l6: 0.002899
[epoch: 415/1000, batch:   872/ 1052, ite: 109100] train loss: 0.004486, tar: 0.000157 
l0: 0.000090, l1: 0.000089, l2: 0.000128, l3: 0.000188, l4: 0.000368, l5: 0.000667, l6: 0.001059
[epoch: 415/1000, batch:   952/ 1052, ite: 109120] train loss: 0.004485, tar: 0.000157 
l0: 0.000171, l1: 0.000168, l2: 0.000229, l3: 0.000341, l4: 0.000596, l5: 0.001125, l6: 0.001394
[epoch: 415/1000, batch:  1032/ 1052, ite: 109140] train loss: 0.004488, tar: 0.000157 
[Epoch 415/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000107, l1: 0.000108, l2: 0.000150, l3: 0.000200, l4: 0.000318, l5: 0.000838, l6: 0.001839
[epoch: 416/1000, batch:    60/ 1052, ite: 109160] train loss: 0.004485, tar: 0.000157 
l0: 0.000104, l1: 0.000107, l2: 0.000110, l3: 0.000192, l4: 0.000217, l5: 0.000776, l6: 0.001337
[epoch: 416/1000, batch:   140/ 1052, ite: 109180] train loss: 0.004483, tar: 0.000157 
l0: 0.000089, l1: 0.000091, l2: 0.000099, l3: 0.000151, l4: 0.000230, l5: 0.000872, l6: 0.001263
[epoch: 416/1000, batch:   220/ 1052, ite: 109200] train loss: 0.004487, tar: 0.000157 
l0: 0.000122, l1: 0.000130, l2: 0.000135, l3: 0.000203, l4: 0.000327, l5: 0.000802, l6: 0.001277
[epoch: 416/1000, batch:   300/ 1052, ite: 109220] train loss: 0.004487, tar: 0.000157 
l0: 0.000120, l1: 0.000121, l2: 0.000149, l3: 0.000232, l4: 0.000482, l5: 0.001253, l6: 0.001563
[epoch: 416/1000, batch:   380/ 1052, ite: 109240] train loss: 0.004486, tar: 0.000157 
l0: 0.000311, l1: 0.000312, l2: 0.000371, l3: 0.000477, l4: 0.000657, l5: 0.001283, l6: 0.003253
[epoch: 416/1000, batch:   460/ 1052, ite: 109260] train loss: 0.004489, tar: 0.000157 
l0: 0.000211, l1: 0.000212, l2: 0.000303, l3: 0.000551, l4: 0.000912, l5: 0.002013, l6: 0.003374
[epoch: 416/1000, batch:   540/ 1052, ite: 109280] train loss: 0.004487, tar: 0.000157 
l0: 0.000109, l1: 0.000111, l2: 0.000134, l3: 0.000170, l4: 0.000462, l5: 0.001067, l6: 0.001823
[epoch: 416/1000, batch:   620/ 1052, ite: 109300] train loss: 0.004488, tar: 0.000157 
l0: 0.000066, l1: 0.000065, l2: 0.000100, l3: 0.000130, l4: 0.000344, l5: 0.000662, l6: 0.001222
[epoch: 416/1000, batch:   700/ 1052, ite: 109320] train loss: 0.004489, tar: 0.000157 
l0: 0.000090, l1: 0.000095, l2: 0.000109, l3: 0.000144, l4: 0.000419, l5: 0.001147, l6: 0.001397
[epoch: 416/1000, batch:   780/ 1052, ite: 109340] train loss: 0.004492, tar: 0.000157 
l0: 0.000109, l1: 0.000110, l2: 0.000108, l3: 0.000140, l4: 0.000250, l5: 0.000540, l6: 0.001329
[epoch: 416/1000, batch:   860/ 1052, ite: 109360] train loss: 0.004491, tar: 0.000157 
l0: 0.000131, l1: 0.000132, l2: 0.000176, l3: 0.000214, l4: 0.000607, l5: 0.001300, l6: 0.002106
[epoch: 416/1000, batch:   940/ 1052, ite: 109380] train loss: 0.004494, tar: 0.000157 
l0: 0.000511, l1: 0.000489, l2: 0.000616, l3: 0.000489, l4: 0.000630, l5: 0.001124, l6: 0.001375
[epoch: 416/1000, batch:  1020/ 1052, ite: 109400] train loss: 0.004492, tar: 0.000157 
[Epoch 416/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000134, l1: 0.000135, l2: 0.000178, l3: 0.000293, l4: 0.000689, l5: 0.000865, l6: 0.001468
[epoch: 417/1000, batch:    48/ 1052, ite: 109420] train loss: 0.004494, tar: 0.000157 
l0: 0.000131, l1: 0.000134, l2: 0.000184, l3: 0.000218, l4: 0.000339, l5: 0.000815, l6: 0.001492
[epoch: 417/1000, batch:   128/ 1052, ite: 109440] train loss: 0.004494, tar: 0.000157 
l0: 0.000150, l1: 0.000152, l2: 0.000198, l3: 0.000234, l4: 0.000433, l5: 0.000864, l6: 0.001343
[epoch: 417/1000, batch:   208/ 1052, ite: 109460] train loss: 0.004494, tar: 0.000157 
l0: 0.000143, l1: 0.000143, l2: 0.000196, l3: 0.000355, l4: 0.000515, l5: 0.001001, l6: 0.001990
[epoch: 417/1000, batch:   288/ 1052, ite: 109480] train loss: 0.004490, tar: 0.000157 
l0: 0.000081, l1: 0.000079, l2: 0.000115, l3: 0.000151, l4: 0.000262, l5: 0.000695, l6: 0.001232
[epoch: 417/1000, batch:   368/ 1052, ite: 109500] train loss: 0.004491, tar: 0.000157 
l0: 0.000379, l1: 0.000389, l2: 0.000423, l3: 0.000751, l4: 0.001428, l5: 0.001829, l6: 0.004130
[epoch: 417/1000, batch:   448/ 1052, ite: 109520] train loss: 0.004493, tar: 0.000157 
l0: 0.000052, l1: 0.000053, l2: 0.000044, l3: 0.000110, l4: 0.000296, l5: 0.000656, l6: 0.000587
[epoch: 417/1000, batch:   528/ 1052, ite: 109540] train loss: 0.004493, tar: 0.000157 
l0: 0.000142, l1: 0.000141, l2: 0.000187, l3: 0.000300, l4: 0.000594, l5: 0.000991, l6: 0.001877
[epoch: 417/1000, batch:   608/ 1052, ite: 109560] train loss: 0.004495, tar: 0.000157 
l0: 0.000252, l1: 0.000256, l2: 0.000313, l3: 0.000413, l4: 0.000972, l5: 0.002481, l6: 0.003754
[epoch: 417/1000, batch:   688/ 1052, ite: 109580] train loss: 0.004496, tar: 0.000157 
l0: 0.000053, l1: 0.000053, l2: 0.000100, l3: 0.000147, l4: 0.000383, l5: 0.001124, l6: 0.001872
[epoch: 417/1000, batch:   768/ 1052, ite: 109600] train loss: 0.004497, tar: 0.000157 
l0: 0.000113, l1: 0.000115, l2: 0.000176, l3: 0.000249, l4: 0.000368, l5: 0.000780, l6: 0.001468
[epoch: 417/1000, batch:   848/ 1052, ite: 109620] train loss: 0.004498, tar: 0.000157 
l0: 0.000099, l1: 0.000100, l2: 0.000122, l3: 0.000234, l4: 0.000494, l5: 0.001055, l6: 0.001967
[epoch: 417/1000, batch:   928/ 1052, ite: 109640] train loss: 0.004498, tar: 0.000157 
l0: 0.000309, l1: 0.000309, l2: 0.000397, l3: 0.000494, l4: 0.000773, l5: 0.001717, l6: 0.002264
[epoch: 417/1000, batch:  1008/ 1052, ite: 109660] train loss: 0.004496, tar: 0.000157 
[Epoch 417/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000065, l1: 0.000066, l2: 0.000087, l3: 0.000117, l4: 0.000315, l5: 0.000311, l6: 0.000906
[epoch: 418/1000, batch:    36/ 1052, ite: 109680] train loss: 0.004494, tar: 0.000157 
l0: 0.000098, l1: 0.000099, l2: 0.000105, l3: 0.000153, l4: 0.000389, l5: 0.000508, l6: 0.001334
[epoch: 418/1000, batch:   116/ 1052, ite: 109700] train loss: 0.004497, tar: 0.000157 
l0: 0.000110, l1: 0.000113, l2: 0.000118, l3: 0.000135, l4: 0.000286, l5: 0.000844, l6: 0.001622
[epoch: 418/1000, batch:   196/ 1052, ite: 109720] train loss: 0.004496, tar: 0.000157 
l0: 0.000092, l1: 0.000095, l2: 0.000123, l3: 0.000231, l4: 0.000580, l5: 0.001267, l6: 0.001563
[epoch: 418/1000, batch:   276/ 1052, ite: 109740] train loss: 0.004496, tar: 0.000157 
l0: 0.000061, l1: 0.000063, l2: 0.000099, l3: 0.000148, l4: 0.000216, l5: 0.000753, l6: 0.001104
[epoch: 418/1000, batch:   356/ 1052, ite: 109760] train loss: 0.004496, tar: 0.000157 
l0: 0.000186, l1: 0.000192, l2: 0.000204, l3: 0.000264, l4: 0.000452, l5: 0.000993, l6: 0.001166
[epoch: 418/1000, batch:   436/ 1052, ite: 109780] train loss: 0.004496, tar: 0.000157 
l0: 0.000108, l1: 0.000104, l2: 0.000173, l3: 0.000217, l4: 0.000468, l5: 0.001038, l6: 0.001614
[epoch: 418/1000, batch:   516/ 1052, ite: 109800] train loss: 0.004495, tar: 0.000157 
l0: 0.000302, l1: 0.000299, l2: 0.000318, l3: 0.000360, l4: 0.000556, l5: 0.001138, l6: 0.002854
[epoch: 418/1000, batch:   596/ 1052, ite: 109820] train loss: 0.004493, tar: 0.000157 
l0: 0.000125, l1: 0.000120, l2: 0.000158, l3: 0.000367, l4: 0.000576, l5: 0.001441, l6: 0.002421
[epoch: 418/1000, batch:   676/ 1052, ite: 109840] train loss: 0.004491, tar: 0.000157 
l0: 0.000127, l1: 0.000125, l2: 0.000179, l3: 0.000255, l4: 0.000550, l5: 0.001169, l6: 0.002501
[epoch: 418/1000, batch:   756/ 1052, ite: 109860] train loss: 0.004490, tar: 0.000156 
l0: 0.000083, l1: 0.000085, l2: 0.000097, l3: 0.000140, l4: 0.000397, l5: 0.000436, l6: 0.001168
[epoch: 418/1000, batch:   836/ 1052, ite: 109880] train loss: 0.004494, tar: 0.000157 
l0: 0.000313, l1: 0.000309, l2: 0.000321, l3: 0.000337, l4: 0.000502, l5: 0.001240, l6: 0.001532
[epoch: 418/1000, batch:   916/ 1052, ite: 109900] train loss: 0.004493, tar: 0.000157 
l0: 0.000341, l1: 0.000340, l2: 0.000397, l3: 0.000526, l4: 0.000787, l5: 0.001647, l6: 0.003070
[epoch: 418/1000, batch:   996/ 1052, ite: 109920] train loss: 0.004494, tar: 0.000157 
[Epoch 418/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000198, l1: 0.000200, l2: 0.000225, l3: 0.000381, l4: 0.000648, l5: 0.001109, l6: 0.002179
[epoch: 419/1000, batch:    24/ 1052, ite: 109940] train loss: 0.004492, tar: 0.000157 
l0: 0.000204, l1: 0.000199, l2: 0.000263, l3: 0.000406, l4: 0.000718, l5: 0.001354, l6: 0.002643
[epoch: 419/1000, batch:   104/ 1052, ite: 109960] train loss: 0.004491, tar: 0.000157 
l0: 0.000155, l1: 0.000149, l2: 0.000252, l3: 0.000363, l4: 0.000737, l5: 0.001250, l6: 0.002661
[epoch: 419/1000, batch:   184/ 1052, ite: 109980] train loss: 0.004493, tar: 0.000157 
l0: 0.000205, l1: 0.000200, l2: 0.000275, l3: 0.000439, l4: 0.000666, l5: 0.001018, l6: 0.001816
[epoch: 419/1000, batch:   264/ 1052, ite: 110000] train loss: 0.004495, tar: 0.000157 
l0: 0.000095, l1: 0.000093, l2: 0.000139, l3: 0.000179, l4: 0.000513, l5: 0.000896, l6: 0.001492
[epoch: 419/1000, batch:   344/ 1052, ite: 110020] train loss: 0.004494, tar: 0.000157 
l0: 0.000119, l1: 0.000120, l2: 0.000155, l3: 0.000213, l4: 0.000327, l5: 0.000979, l6: 0.001846
[epoch: 419/1000, batch:   424/ 1052, ite: 110040] train loss: 0.004493, tar: 0.000157 
l0: 0.000186, l1: 0.000184, l2: 0.000219, l3: 0.000290, l4: 0.000535, l5: 0.001127, l6: 0.001675
[epoch: 419/1000, batch:   504/ 1052, ite: 110060] train loss: 0.004493, tar: 0.000157 
l0: 0.000125, l1: 0.000131, l2: 0.000149, l3: 0.000215, l4: 0.000291, l5: 0.000674, l6: 0.001658
[epoch: 419/1000, batch:   584/ 1052, ite: 110080] train loss: 0.004491, tar: 0.000156 
l0: 0.000065, l1: 0.000063, l2: 0.000099, l3: 0.000161, l4: 0.000368, l5: 0.000661, l6: 0.001526
[epoch: 419/1000, batch:   664/ 1052, ite: 110100] train loss: 0.004490, tar: 0.000156 
l0: 0.000109, l1: 0.000115, l2: 0.000136, l3: 0.000201, l4: 0.000528, l5: 0.000949, l6: 0.001942
[epoch: 419/1000, batch:   744/ 1052, ite: 110120] train loss: 0.004490, tar: 0.000156 
l0: 0.000125, l1: 0.000126, l2: 0.000175, l3: 0.000207, l4: 0.000314, l5: 0.000817, l6: 0.001298
[epoch: 419/1000, batch:   824/ 1052, ite: 110140] train loss: 0.004489, tar: 0.000156 
l0: 0.000196, l1: 0.000192, l2: 0.000256, l3: 0.000406, l4: 0.001000, l5: 0.001746, l6: 0.003519
[epoch: 419/1000, batch:   904/ 1052, ite: 110160] train loss: 0.004491, tar: 0.000156 
l0: 0.000032, l1: 0.000034, l2: 0.000096, l3: 0.000227, l4: 0.000605, l5: 0.000793, l6: 0.001402
[epoch: 419/1000, batch:   984/ 1052, ite: 110180] train loss: 0.004489, tar: 0.000156 
[Epoch 419/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000072, l1: 0.000070, l2: 0.000116, l3: 0.000227, l4: 0.000535, l5: 0.001196, l6: 0.001999
[epoch: 420/1000, batch:    12/ 1052, ite: 110200] train loss: 0.004490, tar: 0.000156 
l0: 0.000082, l1: 0.000082, l2: 0.000108, l3: 0.000138, l4: 0.000363, l5: 0.000798, l6: 0.000840
[epoch: 420/1000, batch:    92/ 1052, ite: 110220] train loss: 0.004489, tar: 0.000156 
l0: 0.000067, l1: 0.000070, l2: 0.000103, l3: 0.000170, l4: 0.000291, l5: 0.000710, l6: 0.001228
[epoch: 420/1000, batch:   172/ 1052, ite: 110240] train loss: 0.004487, tar: 0.000156 
l0: 0.000103, l1: 0.000099, l2: 0.000143, l3: 0.000286, l4: 0.000433, l5: 0.001332, l6: 0.001993
[epoch: 420/1000, batch:   252/ 1052, ite: 110260] train loss: 0.004489, tar: 0.000156 
l0: 0.000124, l1: 0.000126, l2: 0.000158, l3: 0.000278, l4: 0.000577, l5: 0.001355, l6: 0.001885
[epoch: 420/1000, batch:   332/ 1052, ite: 110280] train loss: 0.004489, tar: 0.000156 
l0: 0.000142, l1: 0.000144, l2: 0.000157, l3: 0.000223, l4: 0.000481, l5: 0.000938, l6: 0.001900
[epoch: 420/1000, batch:   412/ 1052, ite: 110300] train loss: 0.004489, tar: 0.000156 
l0: 0.000083, l1: 0.000085, l2: 0.000093, l3: 0.000132, l4: 0.000336, l5: 0.000488, l6: 0.001183
[epoch: 420/1000, batch:   492/ 1052, ite: 110320] train loss: 0.004488, tar: 0.000156 
l0: 0.000135, l1: 0.000143, l2: 0.000155, l3: 0.000243, l4: 0.000399, l5: 0.000918, l6: 0.001247
[epoch: 420/1000, batch:   572/ 1052, ite: 110340] train loss: 0.004490, tar: 0.000156 
l0: 0.000095, l1: 0.000097, l2: 0.000106, l3: 0.000171, l4: 0.000324, l5: 0.000328, l6: 0.001212
[epoch: 420/1000, batch:   652/ 1052, ite: 110360] train loss: 0.004488, tar: 0.000156 
l0: 0.000070, l1: 0.000071, l2: 0.000109, l3: 0.000188, l4: 0.000424, l5: 0.000909, l6: 0.001508
[epoch: 420/1000, batch:   732/ 1052, ite: 110380] train loss: 0.004488, tar: 0.000156 
l0: 0.000205, l1: 0.000207, l2: 0.000230, l3: 0.000348, l4: 0.000686, l5: 0.001301, l6: 0.001645
[epoch: 420/1000, batch:   812/ 1052, ite: 110400] train loss: 0.004489, tar: 0.000156 
l0: 0.000196, l1: 0.000199, l2: 0.000236, l3: 0.000276, l4: 0.000564, l5: 0.001248, l6: 0.001984
[epoch: 420/1000, batch:   892/ 1052, ite: 110420] train loss: 0.004487, tar: 0.000156 
l0: 0.000156, l1: 0.000158, l2: 0.000233, l3: 0.000260, l4: 0.000780, l5: 0.001363, l6: 0.002132
[epoch: 420/1000, batch:   972/ 1052, ite: 110440] train loss: 0.004488, tar: 0.000156 
l0: 0.000123, l1: 0.000123, l2: 0.000140, l3: 0.000183, l4: 0.000436, l5: 0.001227, l6: 0.002210
[epoch: 420/1000, batch:  1052/ 1052, ite: 110460] train loss: 0.004488, tar: 0.000156 
[Epoch 420/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000193, l1: 0.000211, l2: 0.000195, l3: 0.000202, l4: 0.000337, l5: 0.001060, l6: 0.001453
[epoch: 421/1000, batch:    80/ 1052, ite: 110480] train loss: 0.004486, tar: 0.000156 
l0: 0.000154, l1: 0.000155, l2: 0.000194, l3: 0.000302, l4: 0.000505, l5: 0.001207, l6: 0.002307
[epoch: 421/1000, batch:   160/ 1052, ite: 110500] train loss: 0.004485, tar: 0.000156 
l0: 0.000289, l1: 0.000280, l2: 0.000389, l3: 0.000473, l4: 0.000828, l5: 0.001488, l6: 0.003628
[epoch: 421/1000, batch:   240/ 1052, ite: 110520] train loss: 0.004485, tar: 0.000156 
l0: 0.000098, l1: 0.000095, l2: 0.000169, l3: 0.000279, l4: 0.000506, l5: 0.000892, l6: 0.002627
[epoch: 421/1000, batch:   320/ 1052, ite: 110540] train loss: 0.004485, tar: 0.000156 
l0: 0.000129, l1: 0.000137, l2: 0.000167, l3: 0.000170, l4: 0.000454, l5: 0.001182, l6: 0.001699
[epoch: 421/1000, batch:   400/ 1052, ite: 110560] train loss: 0.004487, tar: 0.000156 
l0: 0.000126, l1: 0.000129, l2: 0.000188, l3: 0.000375, l4: 0.001072, l5: 0.001947, l6: 0.002118
[epoch: 421/1000, batch:   480/ 1052, ite: 110580] train loss: 0.004489, tar: 0.000156 
l0: 0.000108, l1: 0.000113, l2: 0.000110, l3: 0.000174, l4: 0.000426, l5: 0.001215, l6: 0.002189
[epoch: 421/1000, batch:   560/ 1052, ite: 110600] train loss: 0.004491, tar: 0.000156 
l0: 0.000105, l1: 0.000102, l2: 0.000139, l3: 0.000192, l4: 0.000364, l5: 0.000891, l6: 0.002251
[epoch: 421/1000, batch:   640/ 1052, ite: 110620] train loss: 0.004491, tar: 0.000156 
l0: 0.000133, l1: 0.000134, l2: 0.000178, l3: 0.000223, l4: 0.000395, l5: 0.000872, l6: 0.001853
[epoch: 421/1000, batch:   720/ 1052, ite: 110640] train loss: 0.004493, tar: 0.000156 
l0: 0.000140, l1: 0.000138, l2: 0.000264, l3: 0.000432, l4: 0.000814, l5: 0.001546, l6: 0.001654
[epoch: 421/1000, batch:   800/ 1052, ite: 110660] train loss: 0.004491, tar: 0.000156 
l0: 0.000228, l1: 0.000229, l2: 0.000274, l3: 0.000376, l4: 0.000868, l5: 0.001622, l6: 0.002957
[epoch: 421/1000, batch:   880/ 1052, ite: 110680] train loss: 0.004491, tar: 0.000156 
l0: 0.000074, l1: 0.000074, l2: 0.000107, l3: 0.000116, l4: 0.000196, l5: 0.000769, l6: 0.000953
[epoch: 421/1000, batch:   960/ 1052, ite: 110700] train loss: 0.004489, tar: 0.000155 
l0: 0.000103, l1: 0.000113, l2: 0.000182, l3: 0.000244, l4: 0.000409, l5: 0.000674, l6: 0.001494
[epoch: 421/1000, batch:  1040/ 1052, ite: 110720] train loss: 0.004487, tar: 0.000155 
[Epoch 421/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000103, l1: 0.000102, l2: 0.000156, l3: 0.000243, l4: 0.000431, l5: 0.000988, l6: 0.002252
[epoch: 422/1000, batch:    68/ 1052, ite: 110740] train loss: 0.004485, tar: 0.000155 
l0: 0.000057, l1: 0.000061, l2: 0.000063, l3: 0.000105, l4: 0.000148, l5: 0.000683, l6: 0.001608
[epoch: 422/1000, batch:   148/ 1052, ite: 110760] train loss: 0.004483, tar: 0.000155 
l0: 0.000081, l1: 0.000083, l2: 0.000127, l3: 0.000218, l4: 0.000350, l5: 0.001199, l6: 0.001741
[epoch: 422/1000, batch:   228/ 1052, ite: 110780] train loss: 0.004482, tar: 0.000155 
l0: 0.000296, l1: 0.000300, l2: 0.000320, l3: 0.000446, l4: 0.000589, l5: 0.001657, l6: 0.002985
[epoch: 422/1000, batch:   308/ 1052, ite: 110800] train loss: 0.004482, tar: 0.000155 
l0: 0.000080, l1: 0.000083, l2: 0.000098, l3: 0.000124, l4: 0.000181, l5: 0.000644, l6: 0.001002
[epoch: 422/1000, batch:   388/ 1052, ite: 110820] train loss: 0.004483, tar: 0.000155 
l0: 0.000199, l1: 0.000202, l2: 0.000257, l3: 0.000425, l4: 0.000774, l5: 0.001462, l6: 0.002385
[epoch: 422/1000, batch:   468/ 1052, ite: 110840] train loss: 0.004484, tar: 0.000155 
l0: 0.000065, l1: 0.000068, l2: 0.000076, l3: 0.000096, l4: 0.000235, l5: 0.000823, l6: 0.001349
[epoch: 422/1000, batch:   548/ 1052, ite: 110860] train loss: 0.004485, tar: 0.000155 
l0: 0.000117, l1: 0.000121, l2: 0.000172, l3: 0.000239, l4: 0.000497, l5: 0.000801, l6: 0.001105
[epoch: 422/1000, batch:   628/ 1052, ite: 110880] train loss: 0.004485, tar: 0.000155 
l0: 0.000125, l1: 0.000126, l2: 0.000141, l3: 0.000238, l4: 0.000497, l5: 0.000636, l6: 0.001577
[epoch: 422/1000, batch:   708/ 1052, ite: 110900] train loss: 0.004485, tar: 0.000155 
l0: 0.000069, l1: 0.000065, l2: 0.000098, l3: 0.000184, l4: 0.000382, l5: 0.000876, l6: 0.001335
[epoch: 422/1000, batch:   788/ 1052, ite: 110920] train loss: 0.004483, tar: 0.000155 
l0: 0.000242, l1: 0.000250, l2: 0.000296, l3: 0.000388, l4: 0.000592, l5: 0.001894, l6: 0.003185
[epoch: 422/1000, batch:   868/ 1052, ite: 110940] train loss: 0.004484, tar: 0.000155 
l0: 0.000297, l1: 0.000294, l2: 0.000355, l3: 0.000506, l4: 0.000910, l5: 0.001513, l6: 0.002334
[epoch: 422/1000, batch:   948/ 1052, ite: 110960] train loss: 0.004485, tar: 0.000155 
l0: 0.000158, l1: 0.000172, l2: 0.000167, l3: 0.000318, l4: 0.000524, l5: 0.001372, l6: 0.002761
[epoch: 422/1000, batch:  1028/ 1052, ite: 110980] train loss: 0.004485, tar: 0.000155 
[Epoch 422/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000135, l1: 0.000138, l2: 0.000180, l3: 0.000298, l4: 0.000592, l5: 0.001301, l6: 0.001981
[epoch: 423/1000, batch:    56/ 1052, ite: 111000] train loss: 0.004485, tar: 0.000155 
l0: 0.000085, l1: 0.000083, l2: 0.000127, l3: 0.000225, l4: 0.000452, l5: 0.000827, l6: 0.001593
[epoch: 423/1000, batch:   136/ 1052, ite: 111020] train loss: 0.004484, tar: 0.000155 
l0: 0.000187, l1: 0.000185, l2: 0.000315, l3: 0.000513, l4: 0.000759, l5: 0.002261, l6: 0.003342
[epoch: 423/1000, batch:   216/ 1052, ite: 111040] train loss: 0.004486, tar: 0.000155 
l0: 0.000169, l1: 0.000173, l2: 0.000197, l3: 0.000360, l4: 0.001028, l5: 0.001311, l6: 0.002173
[epoch: 423/1000, batch:   296/ 1052, ite: 111060] train loss: 0.004485, tar: 0.000155 
l0: 0.000174, l1: 0.000176, l2: 0.000193, l3: 0.000320, l4: 0.000570, l5: 0.001506, l6: 0.002210
[epoch: 423/1000, batch:   376/ 1052, ite: 111080] train loss: 0.004484, tar: 0.000155 
l0: 0.000229, l1: 0.000222, l2: 0.000297, l3: 0.000577, l4: 0.000673, l5: 0.001523, l6: 0.003037
[epoch: 423/1000, batch:   456/ 1052, ite: 111100] train loss: 0.004485, tar: 0.000155 
l0: 0.000051, l1: 0.000050, l2: 0.000081, l3: 0.000174, l4: 0.000321, l5: 0.000653, l6: 0.000816
[epoch: 423/1000, batch:   536/ 1052, ite: 111120] train loss: 0.004485, tar: 0.000155 
l0: 0.000108, l1: 0.000112, l2: 0.000132, l3: 0.000185, l4: 0.000258, l5: 0.000543, l6: 0.001828
[epoch: 423/1000, batch:   616/ 1052, ite: 111140] train loss: 0.004485, tar: 0.000155 
l0: 0.000211, l1: 0.000215, l2: 0.000232, l3: 0.000315, l4: 0.000859, l5: 0.001729, l6: 0.002238
[epoch: 423/1000, batch:   696/ 1052, ite: 111160] train loss: 0.004485, tar: 0.000155 
l0: 0.000180, l1: 0.000176, l2: 0.000286, l3: 0.000448, l4: 0.000621, l5: 0.001292, l6: 0.002277
[epoch: 423/1000, batch:   776/ 1052, ite: 111180] train loss: 0.004483, tar: 0.000155 
l0: 0.000198, l1: 0.000202, l2: 0.000309, l3: 0.000439, l4: 0.001014, l5: 0.002624, l6: 0.003399
[epoch: 423/1000, batch:   856/ 1052, ite: 111200] train loss: 0.004482, tar: 0.000154 
l0: 0.000110, l1: 0.000112, l2: 0.000112, l3: 0.000176, l4: 0.000344, l5: 0.000921, l6: 0.001101
[epoch: 423/1000, batch:   936/ 1052, ite: 111220] train loss: 0.004481, tar: 0.000154 
l0: 0.000281, l1: 0.000287, l2: 0.000305, l3: 0.000416, l4: 0.000571, l5: 0.001301, l6: 0.002278
[epoch: 423/1000, batch:  1016/ 1052, ite: 111240] train loss: 0.004482, tar: 0.000154 
[Epoch 423/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000060, l1: 0.000060, l2: 0.000097, l3: 0.000150, l4: 0.000368, l5: 0.000576, l6: 0.001544
[epoch: 424/1000, batch:    44/ 1052, ite: 111260] train loss: 0.004482, tar: 0.000154 
l0: 0.000167, l1: 0.000172, l2: 0.000150, l3: 0.000168, l4: 0.000304, l5: 0.000616, l6: 0.001905
[epoch: 424/1000, batch:   124/ 1052, ite: 111280] train loss: 0.004479, tar: 0.000154 
l0: 0.000109, l1: 0.000111, l2: 0.000140, l3: 0.000208, l4: 0.000543, l5: 0.000877, l6: 0.001683
[epoch: 424/1000, batch:   204/ 1052, ite: 111300] train loss: 0.004478, tar: 0.000154 
l0: 0.000151, l1: 0.000152, l2: 0.000174, l3: 0.000218, l4: 0.000352, l5: 0.001237, l6: 0.001254
[epoch: 424/1000, batch:   284/ 1052, ite: 111320] train loss: 0.004478, tar: 0.000154 
l0: 0.000091, l1: 0.000087, l2: 0.000152, l3: 0.000301, l4: 0.000561, l5: 0.001191, l6: 0.001489
[epoch: 424/1000, batch:   364/ 1052, ite: 111340] train loss: 0.004478, tar: 0.000154 
l0: 0.000199, l1: 0.000204, l2: 0.000221, l3: 0.000284, l4: 0.000457, l5: 0.001173, l6: 0.002035
[epoch: 424/1000, batch:   444/ 1052, ite: 111360] train loss: 0.004478, tar: 0.000154 
l0: 0.000087, l1: 0.000089, l2: 0.000110, l3: 0.000161, l4: 0.000324, l5: 0.000546, l6: 0.001489
[epoch: 424/1000, batch:   524/ 1052, ite: 111380] train loss: 0.004477, tar: 0.000154 
l0: 0.000172, l1: 0.000174, l2: 0.000208, l3: 0.000345, l4: 0.000639, l5: 0.001263, l6: 0.002157
[epoch: 424/1000, batch:   604/ 1052, ite: 111400] train loss: 0.004479, tar: 0.000154 
l0: 0.000213, l1: 0.000208, l2: 0.000271, l3: 0.000352, l4: 0.000487, l5: 0.000988, l6: 0.003092
[epoch: 424/1000, batch:   684/ 1052, ite: 111420] train loss: 0.004479, tar: 0.000154 
l0: 0.000124, l1: 0.000122, l2: 0.000178, l3: 0.000441, l4: 0.001044, l5: 0.001434, l6: 0.003092
[epoch: 424/1000, batch:   764/ 1052, ite: 111440] train loss: 0.004480, tar: 0.000154 
l0: 0.000326, l1: 0.000332, l2: 0.000397, l3: 0.000593, l4: 0.001164, l5: 0.002043, l6: 0.004197
[epoch: 424/1000, batch:   844/ 1052, ite: 111460] train loss: 0.004481, tar: 0.000154 
l0: 0.000168, l1: 0.000170, l2: 0.000246, l3: 0.000513, l4: 0.001337, l5: 0.002508, l6: 0.003909
[epoch: 424/1000, batch:   924/ 1052, ite: 111480] train loss: 0.004480, tar: 0.000154 
l0: 0.000042, l1: 0.000042, l2: 0.000064, l3: 0.000163, l4: 0.000319, l5: 0.000720, l6: 0.000856
[epoch: 424/1000, batch:  1004/ 1052, ite: 111500] train loss: 0.004478, tar: 0.000154 
[Epoch 424/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000211, l1: 0.000209, l2: 0.000252, l3: 0.000322, l4: 0.000636, l5: 0.001124, l6: 0.001672
[epoch: 425/1000, batch:    32/ 1052, ite: 111520] train loss: 0.004479, tar: 0.000154 
l0: 0.000182, l1: 0.000186, l2: 0.000215, l3: 0.000304, l4: 0.000467, l5: 0.001380, l6: 0.002969
[epoch: 425/1000, batch:   112/ 1052, ite: 111540] train loss: 0.004478, tar: 0.000154 
l0: 0.000112, l1: 0.000109, l2: 0.000147, l3: 0.000266, l4: 0.000426, l5: 0.000905, l6: 0.001281
[epoch: 425/1000, batch:   192/ 1052, ite: 111560] train loss: 0.004479, tar: 0.000154 
l0: 0.000065, l1: 0.000066, l2: 0.000082, l3: 0.000113, l4: 0.000318, l5: 0.000621, l6: 0.001333
[epoch: 425/1000, batch:   272/ 1052, ite: 111580] train loss: 0.004479, tar: 0.000154 
l0: 0.000083, l1: 0.000085, l2: 0.000103, l3: 0.000147, l4: 0.000392, l5: 0.000741, l6: 0.001411
[epoch: 425/1000, batch:   352/ 1052, ite: 111600] train loss: 0.004480, tar: 0.000154 
l0: 0.000066, l1: 0.000065, l2: 0.000077, l3: 0.000146, l4: 0.000310, l5: 0.000561, l6: 0.001098
[epoch: 425/1000, batch:   432/ 1052, ite: 111620] train loss: 0.004478, tar: 0.000154 
l0: 0.000115, l1: 0.000115, l2: 0.000134, l3: 0.000226, l4: 0.000311, l5: 0.000797, l6: 0.001487
[epoch: 425/1000, batch:   512/ 1052, ite: 111640] train loss: 0.004479, tar: 0.000154 
l0: 0.000163, l1: 0.000161, l2: 0.000216, l3: 0.000225, l4: 0.000469, l5: 0.000819, l6: 0.001729
[epoch: 425/1000, batch:   592/ 1052, ite: 111660] train loss: 0.004478, tar: 0.000154 
l0: 0.000077, l1: 0.000078, l2: 0.000095, l3: 0.000155, l4: 0.000388, l5: 0.000852, l6: 0.001283
[epoch: 425/1000, batch:   672/ 1052, ite: 111680] train loss: 0.004477, tar: 0.000154 
l0: 0.000142, l1: 0.000137, l2: 0.000178, l3: 0.000340, l4: 0.000495, l5: 0.001135, l6: 0.000906
[epoch: 425/1000, batch:   752/ 1052, ite: 111700] train loss: 0.004477, tar: 0.000154 
l0: 0.000140, l1: 0.000137, l2: 0.000191, l3: 0.000256, l4: 0.000440, l5: 0.001046, l6: 0.001125
[epoch: 425/1000, batch:   832/ 1052, ite: 111720] train loss: 0.004477, tar: 0.000154 
l0: 0.000146, l1: 0.000148, l2: 0.000176, l3: 0.000224, l4: 0.000469, l5: 0.000791, l6: 0.001427
[epoch: 425/1000, batch:   912/ 1052, ite: 111740] train loss: 0.004477, tar: 0.000154 
l0: 0.000223, l1: 0.000220, l2: 0.000274, l3: 0.000412, l4: 0.000632, l5: 0.000938, l6: 0.001393
[epoch: 425/1000, batch:   992/ 1052, ite: 111760] train loss: 0.004477, tar: 0.000154 
[Epoch 425/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000066, l1: 0.000068, l2: 0.000088, l3: 0.000241, l4: 0.000544, l5: 0.000801, l6: 0.001481
[epoch: 426/1000, batch:    20/ 1052, ite: 111780] train loss: 0.004477, tar: 0.000154 
l0: 0.000125, l1: 0.000123, l2: 0.000192, l3: 0.000439, l4: 0.000590, l5: 0.001209, l6: 0.001378
[epoch: 426/1000, batch:   100/ 1052, ite: 111800] train loss: 0.004477, tar: 0.000154 
l0: 0.000101, l1: 0.000103, l2: 0.000122, l3: 0.000225, l4: 0.000466, l5: 0.000860, l6: 0.001481
[epoch: 426/1000, batch:   180/ 1052, ite: 111820] train loss: 0.004475, tar: 0.000153 
l0: 0.000229, l1: 0.000234, l2: 0.000266, l3: 0.000408, l4: 0.000880, l5: 0.001900, l6: 0.002224
[epoch: 426/1000, batch:   260/ 1052, ite: 111840] train loss: 0.004477, tar: 0.000154 
l0: 0.000197, l1: 0.000192, l2: 0.000253, l3: 0.000379, l4: 0.000687, l5: 0.000961, l6: 0.002078
[epoch: 426/1000, batch:   340/ 1052, ite: 111860] train loss: 0.004477, tar: 0.000154 
l0: 0.000071, l1: 0.000069, l2: 0.000111, l3: 0.000226, l4: 0.000675, l5: 0.001088, l6: 0.001504
[epoch: 426/1000, batch:   420/ 1052, ite: 111880] train loss: 0.004477, tar: 0.000153 
l0: 0.000170, l1: 0.000169, l2: 0.000235, l3: 0.000309, l4: 0.000595, l5: 0.001544, l6: 0.003726
[epoch: 426/1000, batch:   500/ 1052, ite: 111900] train loss: 0.004477, tar: 0.000153 
l0: 0.000201, l1: 0.000198, l2: 0.000222, l3: 0.000283, l4: 0.000445, l5: 0.001309, l6: 0.002466
[epoch: 426/1000, batch:   580/ 1052, ite: 111920] train loss: 0.004475, tar: 0.000153 
l0: 0.000198, l1: 0.000191, l2: 0.000291, l3: 0.000375, l4: 0.000634, l5: 0.001282, l6: 0.003521
[epoch: 426/1000, batch:   660/ 1052, ite: 111940] train loss: 0.004475, tar: 0.000153 
l0: 0.000087, l1: 0.000087, l2: 0.000118, l3: 0.000168, l4: 0.000330, l5: 0.000703, l6: 0.000949
[epoch: 426/1000, batch:   740/ 1052, ite: 111960] train loss: 0.004475, tar: 0.000153 
l0: 0.000161, l1: 0.000168, l2: 0.000222, l3: 0.000302, l4: 0.000864, l5: 0.001287, l6: 0.003495
[epoch: 426/1000, batch:   820/ 1052, ite: 111980] train loss: 0.004476, tar: 0.000153 
l0: 0.000161, l1: 0.000160, l2: 0.000252, l3: 0.000376, l4: 0.000878, l5: 0.001827, l6: 0.002885
[epoch: 426/1000, batch:   900/ 1052, ite: 112000] train loss: 0.004477, tar: 0.000153 
l0: 0.000079, l1: 0.000081, l2: 0.000105, l3: 0.000162, l4: 0.000342, l5: 0.000832, l6: 0.001470
[epoch: 426/1000, batch:   980/ 1052, ite: 112020] train loss: 0.004475, tar: 0.000153 
[Epoch 426/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000174, l1: 0.000167, l2: 0.000213, l3: 0.000296, l4: 0.000598, l5: 0.001361, l6: 0.002858
[epoch: 427/1000, batch:     8/ 1052, ite: 112040] train loss: 0.004475, tar: 0.000153 
l0: 0.000284, l1: 0.000295, l2: 0.000277, l3: 0.000332, l4: 0.000635, l5: 0.001830, l6: 0.002455
[epoch: 427/1000, batch:    88/ 1052, ite: 112060] train loss: 0.004475, tar: 0.000153 
l0: 0.000078, l1: 0.000077, l2: 0.000106, l3: 0.000159, l4: 0.000436, l5: 0.000847, l6: 0.002010
[epoch: 427/1000, batch:   168/ 1052, ite: 112080] train loss: 0.004474, tar: 0.000153 
l0: 0.000150, l1: 0.000146, l2: 0.000172, l3: 0.000294, l4: 0.000505, l5: 0.000878, l6: 0.001393
[epoch: 427/1000, batch:   248/ 1052, ite: 112100] train loss: 0.004474, tar: 0.000153 
l0: 0.000167, l1: 0.000169, l2: 0.000199, l3: 0.000271, l4: 0.000549, l5: 0.001197, l6: 0.001866
[epoch: 427/1000, batch:   328/ 1052, ite: 112120] train loss: 0.004472, tar: 0.000153 
l0: 0.000400, l1: 0.000403, l2: 0.000401, l3: 0.000583, l4: 0.000928, l5: 0.001581, l6: 0.003415
[epoch: 427/1000, batch:   408/ 1052, ite: 112140] train loss: 0.004473, tar: 0.000153 
l0: 0.000131, l1: 0.000131, l2: 0.000167, l3: 0.000240, l4: 0.000437, l5: 0.000764, l6: 0.001229
[epoch: 427/1000, batch:   488/ 1052, ite: 112160] train loss: 0.004473, tar: 0.000153 
l0: 0.000068, l1: 0.000068, l2: 0.000099, l3: 0.000138, l4: 0.000312, l5: 0.001165, l6: 0.001817
[epoch: 427/1000, batch:   568/ 1052, ite: 112180] train loss: 0.004472, tar: 0.000153 
l0: 0.000119, l1: 0.000123, l2: 0.000130, l3: 0.000184, l4: 0.000335, l5: 0.001478, l6: 0.001451
[epoch: 427/1000, batch:   648/ 1052, ite: 112200] train loss: 0.004472, tar: 0.000153 
l0: 0.000106, l1: 0.000106, l2: 0.000188, l3: 0.000349, l4: 0.000490, l5: 0.000978, l6: 0.001873
[epoch: 427/1000, batch:   728/ 1052, ite: 112220] train loss: 0.004472, tar: 0.000153 
l0: 0.000061, l1: 0.000060, l2: 0.000084, l3: 0.000143, l4: 0.000374, l5: 0.000936, l6: 0.001201
[epoch: 427/1000, batch:   808/ 1052, ite: 112240] train loss: 0.004471, tar: 0.000153 
l0: 0.000244, l1: 0.000240, l2: 0.000295, l3: 0.000344, l4: 0.000646, l5: 0.001389, l6: 0.002073
[epoch: 427/1000, batch:   888/ 1052, ite: 112260] train loss: 0.004471, tar: 0.000153 
l0: 0.000108, l1: 0.000105, l2: 0.000130, l3: 0.000232, l4: 0.000497, l5: 0.001191, l6: 0.001178
[epoch: 427/1000, batch:   968/ 1052, ite: 112280] train loss: 0.004472, tar: 0.000153 
l0: 0.000140, l1: 0.000143, l2: 0.000202, l3: 0.000306, l4: 0.000835, l5: 0.001393, l6: 0.002398
[epoch: 427/1000, batch:  1048/ 1052, ite: 112300] train loss: 0.004472, tar: 0.000153 
[Epoch 427/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000113, l1: 0.000115, l2: 0.000153, l3: 0.000245, l4: 0.000407, l5: 0.001518, l6: 0.003100
[epoch: 428/1000, batch:    76/ 1052, ite: 112320] train loss: 0.004473, tar: 0.000153 
l0: 0.000300, l1: 0.000307, l2: 0.000320, l3: 0.000470, l4: 0.000974, l5: 0.001759, l6: 0.003322
[epoch: 428/1000, batch:   156/ 1052, ite: 112340] train loss: 0.004473, tar: 0.000153 
l0: 0.000149, l1: 0.000150, l2: 0.000193, l3: 0.000299, l4: 0.000425, l5: 0.001040, l6: 0.001950
[epoch: 428/1000, batch:   236/ 1052, ite: 112360] train loss: 0.004472, tar: 0.000152 
l0: 0.000165, l1: 0.000164, l2: 0.000191, l3: 0.000260, l4: 0.000574, l5: 0.001388, l6: 0.002388
[epoch: 428/1000, batch:   316/ 1052, ite: 112380] train loss: 0.004472, tar: 0.000152 
l0: 0.000141, l1: 0.000145, l2: 0.000169, l3: 0.000228, l4: 0.000565, l5: 0.001181, l6: 0.001996
[epoch: 428/1000, batch:   396/ 1052, ite: 112400] train loss: 0.004472, tar: 0.000152 
l0: 0.000077, l1: 0.000079, l2: 0.000125, l3: 0.000200, l4: 0.000443, l5: 0.000754, l6: 0.001207
[epoch: 428/1000, batch:   476/ 1052, ite: 112420] train loss: 0.004472, tar: 0.000152 
l0: 0.000126, l1: 0.000120, l2: 0.000183, l3: 0.000295, l4: 0.000365, l5: 0.000815, l6: 0.001958
[epoch: 428/1000, batch:   556/ 1052, ite: 112440] train loss: 0.004471, tar: 0.000152 
l0: 0.000209, l1: 0.000212, l2: 0.000221, l3: 0.000199, l4: 0.000420, l5: 0.000675, l6: 0.001306
[epoch: 428/1000, batch:   636/ 1052, ite: 112460] train loss: 0.004470, tar: 0.000152 
l0: 0.000225, l1: 0.000227, l2: 0.000275, l3: 0.000337, l4: 0.000570, l5: 0.001587, l6: 0.002186
[epoch: 428/1000, batch:   716/ 1052, ite: 112480] train loss: 0.004470, tar: 0.000152 
l0: 0.000251, l1: 0.000254, l2: 0.000271, l3: 0.000385, l4: 0.000685, l5: 0.000959, l6: 0.003129
[epoch: 428/1000, batch:   796/ 1052, ite: 112500] train loss: 0.004469, tar: 0.000152 
l0: 0.000054, l1: 0.000053, l2: 0.000089, l3: 0.000102, l4: 0.000336, l5: 0.000552, l6: 0.001104
[epoch: 428/1000, batch:   876/ 1052, ite: 112520] train loss: 0.004469, tar: 0.000152 
l0: 0.000097, l1: 0.000101, l2: 0.000116, l3: 0.000144, l4: 0.000443, l5: 0.000873, l6: 0.001463
[epoch: 428/1000, batch:   956/ 1052, ite: 112540] train loss: 0.004469, tar: 0.000152 
l0: 0.000124, l1: 0.000126, l2: 0.000119, l3: 0.000214, l4: 0.000466, l5: 0.001353, l6: 0.001877
[epoch: 428/1000, batch:  1036/ 1052, ite: 112560] train loss: 0.004468, tar: 0.000152 
[Epoch 428/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000148, l1: 0.000157, l2: 0.000189, l3: 0.000347, l4: 0.000635, l5: 0.001249, l6: 0.003808
[epoch: 429/1000, batch:    64/ 1052, ite: 112580] train loss: 0.004470, tar: 0.000152 
l0: 0.000078, l1: 0.000076, l2: 0.000129, l3: 0.000182, l4: 0.000449, l5: 0.001004, l6: 0.002659
[epoch: 429/1000, batch:   144/ 1052, ite: 112600] train loss: 0.004471, tar: 0.000152 
l0: 0.000123, l1: 0.000128, l2: 0.000137, l3: 0.000186, l4: 0.000408, l5: 0.000934, l6: 0.001306
[epoch: 429/1000, batch:   224/ 1052, ite: 112620] train loss: 0.004470, tar: 0.000152 
l0: 0.000161, l1: 0.000165, l2: 0.000185, l3: 0.000284, l4: 0.000483, l5: 0.001220, l6: 0.001827
[epoch: 429/1000, batch:   304/ 1052, ite: 112640] train loss: 0.004470, tar: 0.000152 
l0: 0.000095, l1: 0.000098, l2: 0.000115, l3: 0.000183, l4: 0.000402, l5: 0.001224, l6: 0.001536
[epoch: 429/1000, batch:   384/ 1052, ite: 112660] train loss: 0.004470, tar: 0.000152 
l0: 0.000078, l1: 0.000079, l2: 0.000142, l3: 0.000178, l4: 0.000364, l5: 0.001295, l6: 0.001631
[epoch: 429/1000, batch:   464/ 1052, ite: 112680] train loss: 0.004470, tar: 0.000152 
l0: 0.000047, l1: 0.000047, l2: 0.000060, l3: 0.000169, l4: 0.000279, l5: 0.000481, l6: 0.000783
[epoch: 429/1000, batch:   544/ 1052, ite: 112700] train loss: 0.004469, tar: 0.000152 
l0: 0.000118, l1: 0.000126, l2: 0.000130, l3: 0.000201, l4: 0.000402, l5: 0.000633, l6: 0.001320
[epoch: 429/1000, batch:   624/ 1052, ite: 112720] train loss: 0.004470, tar: 0.000152 
l0: 0.000194, l1: 0.000199, l2: 0.000221, l3: 0.000365, l4: 0.000935, l5: 0.001675, l6: 0.003090
[epoch: 429/1000, batch:   704/ 1052, ite: 112740] train loss: 0.004470, tar: 0.000152 
l0: 0.000078, l1: 0.000079, l2: 0.000086, l3: 0.000142, l4: 0.000347, l5: 0.000770, l6: 0.001108
[epoch: 429/1000, batch:   784/ 1052, ite: 112760] train loss: 0.004469, tar: 0.000152 
l0: 0.000143, l1: 0.000147, l2: 0.000162, l3: 0.000242, l4: 0.000436, l5: 0.001374, l6: 0.001832
[epoch: 429/1000, batch:   864/ 1052, ite: 112780] train loss: 0.004470, tar: 0.000152 
l0: 0.000073, l1: 0.000073, l2: 0.000084, l3: 0.000187, l4: 0.000306, l5: 0.000409, l6: 0.001329
[epoch: 429/1000, batch:   944/ 1052, ite: 112800] train loss: 0.004468, tar: 0.000152 
l0: 0.000103, l1: 0.000104, l2: 0.000117, l3: 0.000191, l4: 0.000349, l5: 0.000996, l6: 0.001448
[epoch: 429/1000, batch:  1024/ 1052, ite: 112820] train loss: 0.004467, tar: 0.000152 
[Epoch 429/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000107, l1: 0.000108, l2: 0.000141, l3: 0.000208, l4: 0.000358, l5: 0.000991, l6: 0.001447
[epoch: 430/1000, batch:    52/ 1052, ite: 112840] train loss: 0.004467, tar: 0.000152 
l0: 0.000247, l1: 0.000252, l2: 0.000284, l3: 0.000317, l4: 0.000686, l5: 0.001559, l6: 0.002243
[epoch: 430/1000, batch:   132/ 1052, ite: 112860] train loss: 0.004467, tar: 0.000152 
l0: 0.000126, l1: 0.000124, l2: 0.000171, l3: 0.000254, l4: 0.000437, l5: 0.000898, l6: 0.001438
[epoch: 430/1000, batch:   212/ 1052, ite: 112880] train loss: 0.004469, tar: 0.000152 
l0: 0.000130, l1: 0.000129, l2: 0.000136, l3: 0.000245, l4: 0.000582, l5: 0.001179, l6: 0.001657
[epoch: 430/1000, batch:   292/ 1052, ite: 112900] train loss: 0.004469, tar: 0.000152 
l0: 0.000111, l1: 0.000112, l2: 0.000146, l3: 0.000228, l4: 0.000620, l5: 0.000868, l6: 0.001516
[epoch: 430/1000, batch:   372/ 1052, ite: 112920] train loss: 0.004468, tar: 0.000152 
l0: 0.000080, l1: 0.000082, l2: 0.000106, l3: 0.000169, l4: 0.000364, l5: 0.000651, l6: 0.001247
[epoch: 430/1000, batch:   452/ 1052, ite: 112940] train loss: 0.004467, tar: 0.000152 
l0: 0.000078, l1: 0.000076, l2: 0.000105, l3: 0.000153, l4: 0.000433, l5: 0.000553, l6: 0.001246
[epoch: 430/1000, batch:   532/ 1052, ite: 112960] train loss: 0.004467, tar: 0.000152 
l0: 0.000190, l1: 0.000189, l2: 0.000220, l3: 0.000219, l4: 0.000550, l5: 0.000910, l6: 0.001920
[epoch: 430/1000, batch:   612/ 1052, ite: 112980] train loss: 0.004469, tar: 0.000152 
l0: 0.000288, l1: 0.000296, l2: 0.000295, l3: 0.000417, l4: 0.000860, l5: 0.001236, l6: 0.002787
[epoch: 430/1000, batch:   692/ 1052, ite: 113000] train loss: 0.004470, tar: 0.000152 
l0: 0.000105, l1: 0.000102, l2: 0.000151, l3: 0.000209, l4: 0.000419, l5: 0.001249, l6: 0.001835
[epoch: 430/1000, batch:   772/ 1052, ite: 113020] train loss: 0.004470, tar: 0.000152 
l0: 0.000035, l1: 0.000036, l2: 0.000052, l3: 0.000099, l4: 0.000256, l5: 0.000650, l6: 0.001194
[epoch: 430/1000, batch:   852/ 1052, ite: 113040] train loss: 0.004468, tar: 0.000151 
l0: 0.000326, l1: 0.000331, l2: 0.000367, l3: 0.000529, l4: 0.000983, l5: 0.001569, l6: 0.002777
[epoch: 430/1000, batch:   932/ 1052, ite: 113060] train loss: 0.004468, tar: 0.000151 
l0: 0.000124, l1: 0.000117, l2: 0.000171, l3: 0.000309, l4: 0.000422, l5: 0.000909, l6: 0.002038
[epoch: 430/1000, batch:  1012/ 1052, ite: 113080] train loss: 0.004467, tar: 0.000151 
[Epoch 430/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000335, l1: 0.000337, l2: 0.000385, l3: 0.000495, l4: 0.001007, l5: 0.002185, l6: 0.003203
[epoch: 431/1000, batch:    40/ 1052, ite: 113100] train loss: 0.004469, tar: 0.000151 
l0: 0.000203, l1: 0.000200, l2: 0.000242, l3: 0.000494, l4: 0.000833, l5: 0.001673, l6: 0.001986
[epoch: 431/1000, batch:   120/ 1052, ite: 113120] train loss: 0.004469, tar: 0.000151 
l0: 0.000053, l1: 0.000055, l2: 0.000075, l3: 0.000118, l4: 0.000227, l5: 0.000534, l6: 0.000955
[epoch: 431/1000, batch:   200/ 1052, ite: 113140] train loss: 0.004468, tar: 0.000151 
l0: 0.000124, l1: 0.000125, l2: 0.000169, l3: 0.000216, l4: 0.000479, l5: 0.000620, l6: 0.001092
[epoch: 431/1000, batch:   280/ 1052, ite: 113160] train loss: 0.004466, tar: 0.000151 
l0: 0.000133, l1: 0.000134, l2: 0.000168, l3: 0.000399, l4: 0.000781, l5: 0.001602, l6: 0.002884
[epoch: 431/1000, batch:   360/ 1052, ite: 113180] train loss: 0.004466, tar: 0.000151 
l0: 0.000151, l1: 0.000152, l2: 0.000195, l3: 0.000279, l4: 0.000442, l5: 0.001161, l6: 0.001810
[epoch: 431/1000, batch:   440/ 1052, ite: 113200] train loss: 0.004464, tar: 0.000151 
l0: 0.000121, l1: 0.000125, l2: 0.000142, l3: 0.000198, l4: 0.000448, l5: 0.001417, l6: 0.002252
[epoch: 431/1000, batch:   520/ 1052, ite: 113220] train loss: 0.004465, tar: 0.000151 
l0: 0.000235, l1: 0.000232, l2: 0.000387, l3: 0.000598, l4: 0.000987, l5: 0.001714, l6: 0.003237
[epoch: 431/1000, batch:   600/ 1052, ite: 113240] train loss: 0.004465, tar: 0.000151 
l0: 0.000172, l1: 0.000178, l2: 0.000234, l3: 0.000329, l4: 0.000718, l5: 0.001672, l6: 0.002503
[epoch: 431/1000, batch:   680/ 1052, ite: 113260] train loss: 0.004465, tar: 0.000151 
l0: 0.000226, l1: 0.000229, l2: 0.000259, l3: 0.000340, l4: 0.000763, l5: 0.001455, l6: 0.003088
[epoch: 431/1000, batch:   760/ 1052, ite: 113280] train loss: 0.004465, tar: 0.000151 
l0: 0.000179, l1: 0.000175, l2: 0.000228, l3: 0.000391, l4: 0.000819, l5: 0.002164, l6: 0.002790
[epoch: 431/1000, batch:   840/ 1052, ite: 113300] train loss: 0.004466, tar: 0.000151 
l0: 0.000124, l1: 0.000123, l2: 0.000127, l3: 0.000212, l4: 0.000354, l5: 0.000938, l6: 0.002200
[epoch: 431/1000, batch:   920/ 1052, ite: 113320] train loss: 0.004465, tar: 0.000151 
l0: 0.000167, l1: 0.000169, l2: 0.000220, l3: 0.000268, l4: 0.000815, l5: 0.001485, l6: 0.002576
[epoch: 431/1000, batch:  1000/ 1052, ite: 113340] train loss: 0.004465, tar: 0.000151 
[Epoch 431/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000109, l1: 0.000109, l2: 0.000159, l3: 0.000272, l4: 0.000501, l5: 0.001144, l6: 0.002280
[epoch: 432/1000, batch:    28/ 1052, ite: 113360] train loss: 0.004464, tar: 0.000151 
l0: 0.000140, l1: 0.000141, l2: 0.000222, l3: 0.000288, l4: 0.000677, l5: 0.001411, l6: 0.001471
[epoch: 432/1000, batch:   108/ 1052, ite: 113380] train loss: 0.004464, tar: 0.000151 
l0: 0.000103, l1: 0.000105, l2: 0.000151, l3: 0.000387, l4: 0.000726, l5: 0.001538, l6: 0.003320
[epoch: 432/1000, batch:   188/ 1052, ite: 113400] train loss: 0.004465, tar: 0.000151 
l0: 0.000152, l1: 0.000157, l2: 0.000183, l3: 0.000271, l4: 0.000563, l5: 0.001082, l6: 0.001953
[epoch: 432/1000, batch:   268/ 1052, ite: 113420] train loss: 0.004465, tar: 0.000151 
l0: 0.000139, l1: 0.000139, l2: 0.000171, l3: 0.000257, l4: 0.000411, l5: 0.000746, l6: 0.001338
[epoch: 432/1000, batch:   348/ 1052, ite: 113440] train loss: 0.004464, tar: 0.000151 
l0: 0.000078, l1: 0.000076, l2: 0.000119, l3: 0.000123, l4: 0.000450, l5: 0.000516, l6: 0.001871
[epoch: 432/1000, batch:   428/ 1052, ite: 113460] train loss: 0.004464, tar: 0.000151 
l0: 0.000052, l1: 0.000054, l2: 0.000074, l3: 0.000100, l4: 0.000332, l5: 0.000711, l6: 0.001735
[epoch: 432/1000, batch:   508/ 1052, ite: 113480] train loss: 0.004463, tar: 0.000151 
l0: 0.000118, l1: 0.000120, l2: 0.000174, l3: 0.000278, l4: 0.000467, l5: 0.001058, l6: 0.001028
[epoch: 432/1000, batch:   588/ 1052, ite: 113500] train loss: 0.004464, tar: 0.000151 
l0: 0.000128, l1: 0.000131, l2: 0.000157, l3: 0.000223, l4: 0.000432, l5: 0.000686, l6: 0.001727
[epoch: 432/1000, batch:   668/ 1052, ite: 113520] train loss: 0.004462, tar: 0.000151 
l0: 0.000143, l1: 0.000140, l2: 0.000200, l3: 0.000289, l4: 0.000538, l5: 0.001054, l6: 0.001859
[epoch: 432/1000, batch:   748/ 1052, ite: 113540] train loss: 0.004461, tar: 0.000151 
l0: 0.000135, l1: 0.000135, l2: 0.000203, l3: 0.000304, l4: 0.000579, l5: 0.001595, l6: 0.004179
[epoch: 432/1000, batch:   828/ 1052, ite: 113560] train loss: 0.004461, tar: 0.000151 
l0: 0.000061, l1: 0.000061, l2: 0.000075, l3: 0.000125, l4: 0.000384, l5: 0.000975, l6: 0.001368
[epoch: 432/1000, batch:   908/ 1052, ite: 113580] train loss: 0.004462, tar: 0.000151 
l0: 0.000126, l1: 0.000127, l2: 0.000149, l3: 0.000222, l4: 0.000531, l5: 0.001046, l6: 0.002122
[epoch: 432/1000, batch:   988/ 1052, ite: 113600] train loss: 0.004462, tar: 0.000151 
[Epoch 432/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000032, l1: 0.000032, l2: 0.000073, l3: 0.000133, l4: 0.000315, l5: 0.000824, l6: 0.001412
[epoch: 433/1000, batch:    16/ 1052, ite: 113620] train loss: 0.004461, tar: 0.000151 
l0: 0.000056, l1: 0.000058, l2: 0.000066, l3: 0.000130, l4: 0.000275, l5: 0.000678, l6: 0.001073
[epoch: 433/1000, batch:    96/ 1052, ite: 113640] train loss: 0.004461, tar: 0.000151 
l0: 0.000130, l1: 0.000131, l2: 0.000152, l3: 0.000253, l4: 0.000350, l5: 0.000689, l6: 0.001707
[epoch: 433/1000, batch:   176/ 1052, ite: 113660] train loss: 0.004461, tar: 0.000151 
l0: 0.000094, l1: 0.000097, l2: 0.000130, l3: 0.000164, l4: 0.000415, l5: 0.001148, l6: 0.001202
[epoch: 433/1000, batch:   256/ 1052, ite: 113680] train loss: 0.004461, tar: 0.000151 
l0: 0.000048, l1: 0.000049, l2: 0.000077, l3: 0.000149, l4: 0.000324, l5: 0.000721, l6: 0.001743
[epoch: 433/1000, batch:   336/ 1052, ite: 113700] train loss: 0.004461, tar: 0.000151 
l0: 0.000140, l1: 0.000141, l2: 0.000189, l3: 0.000350, l4: 0.000590, l5: 0.001287, l6: 0.002208
[epoch: 433/1000, batch:   416/ 1052, ite: 113720] train loss: 0.004462, tar: 0.000151 
l0: 0.000213, l1: 0.000212, l2: 0.000301, l3: 0.000675, l4: 0.001075, l5: 0.001767, l6: 0.003429
[epoch: 433/1000, batch:   496/ 1052, ite: 113740] train loss: 0.004460, tar: 0.000150 
l0: 0.000138, l1: 0.000146, l2: 0.000135, l3: 0.000189, l4: 0.000364, l5: 0.000777, l6: 0.002191
[epoch: 433/1000, batch:   576/ 1052, ite: 113760] train loss: 0.004460, tar: 0.000150 
l0: 0.000194, l1: 0.000194, l2: 0.000263, l3: 0.000344, l4: 0.000616, l5: 0.001065, l6: 0.001879
[epoch: 433/1000, batch:   656/ 1052, ite: 113780] train loss: 0.004460, tar: 0.000150 
l0: 0.000186, l1: 0.000183, l2: 0.000230, l3: 0.000350, l4: 0.000806, l5: 0.001396, l6: 0.003096
[epoch: 433/1000, batch:   736/ 1052, ite: 113800] train loss: 0.004460, tar: 0.000150 
l0: 0.000301, l1: 0.000311, l2: 0.000291, l3: 0.000326, l4: 0.000755, l5: 0.001302, l6: 0.002878
[epoch: 433/1000, batch:   816/ 1052, ite: 113820] train loss: 0.004461, tar: 0.000150 
l0: 0.000134, l1: 0.000135, l2: 0.000212, l3: 0.000286, l4: 0.000576, l5: 0.000824, l6: 0.001486
[epoch: 433/1000, batch:   896/ 1052, ite: 113840] train loss: 0.004461, tar: 0.000150 
l0: 0.000116, l1: 0.000116, l2: 0.000144, l3: 0.000277, l4: 0.000248, l5: 0.000874, l6: 0.001720
[epoch: 433/1000, batch:   976/ 1052, ite: 113860] train loss: 0.004461, tar: 0.000150 
[Epoch 433/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000230, l1: 0.000225, l2: 0.000273, l3: 0.000349, l4: 0.000714, l5: 0.001166, l6: 0.002018
[epoch: 434/1000, batch:     4/ 1052, ite: 113880] train loss: 0.004461, tar: 0.000150 
l0: 0.000081, l1: 0.000079, l2: 0.000120, l3: 0.000142, l4: 0.000301, l5: 0.000840, l6: 0.001846
[epoch: 434/1000, batch:    84/ 1052, ite: 113900] train loss: 0.004460, tar: 0.000150 
l0: 0.000068, l1: 0.000065, l2: 0.000094, l3: 0.000165, l4: 0.000336, l5: 0.000712, l6: 0.001280
[epoch: 434/1000, batch:   164/ 1052, ite: 113920] train loss: 0.004460, tar: 0.000150 
l0: 0.000092, l1: 0.000088, l2: 0.000205, l3: 0.000517, l4: 0.000841, l5: 0.001199, l6: 0.002919
[epoch: 434/1000, batch:   244/ 1052, ite: 113940] train loss: 0.004459, tar: 0.000150 
l0: 0.000224, l1: 0.000220, l2: 0.000223, l3: 0.000264, l4: 0.000411, l5: 0.000639, l6: 0.001067
[epoch: 434/1000, batch:   324/ 1052, ite: 113960] train loss: 0.004459, tar: 0.000150 
l0: 0.000173, l1: 0.000175, l2: 0.000189, l3: 0.000296, l4: 0.000556, l5: 0.001679, l6: 0.002560
[epoch: 434/1000, batch:   404/ 1052, ite: 113980] train loss: 0.004460, tar: 0.000150 
l0: 0.000085, l1: 0.000087, l2: 0.000099, l3: 0.000272, l4: 0.000556, l5: 0.001064, l6: 0.002207
[epoch: 434/1000, batch:   484/ 1052, ite: 114000] train loss: 0.004460, tar: 0.000150 
l0: 0.000060, l1: 0.000061, l2: 0.000090, l3: 0.000128, l4: 0.000359, l5: 0.000865, l6: 0.002368
[epoch: 434/1000, batch:   564/ 1052, ite: 114020] train loss: 0.004460, tar: 0.000150 
l0: 0.000095, l1: 0.000098, l2: 0.000125, l3: 0.000184, l4: 0.000291, l5: 0.001066, l6: 0.001331
[epoch: 434/1000, batch:   644/ 1052, ite: 114040] train loss: 0.004460, tar: 0.000150 
l0: 0.000235, l1: 0.000236, l2: 0.000269, l3: 0.000369, l4: 0.000669, l5: 0.000992, l6: 0.002297
[epoch: 434/1000, batch:   724/ 1052, ite: 114060] train loss: 0.004461, tar: 0.000150 
l0: 0.000220, l1: 0.000229, l2: 0.000230, l3: 0.000283, l4: 0.000416, l5: 0.001265, l6: 0.002450
[epoch: 434/1000, batch:   804/ 1052, ite: 114080] train loss: 0.004461, tar: 0.000150 
l0: 0.000072, l1: 0.000072, l2: 0.000088, l3: 0.000111, l4: 0.000285, l5: 0.000625, l6: 0.000976
[epoch: 434/1000, batch:   884/ 1052, ite: 114100] train loss: 0.004461, tar: 0.000150 
l0: 0.000131, l1: 0.000135, l2: 0.000183, l3: 0.000256, l4: 0.000534, l5: 0.001067, l6: 0.002580
[epoch: 434/1000, batch:   964/ 1052, ite: 114120] train loss: 0.004461, tar: 0.000150 
l0: 0.000122, l1: 0.000125, l2: 0.000183, l3: 0.000444, l4: 0.001088, l5: 0.001296, l6: 0.002724
[epoch: 434/1000, batch:  1044/ 1052, ite: 114140] train loss: 0.004459, tar: 0.000150 
[Epoch 434/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000405, l1: 0.000409, l2: 0.000437, l3: 0.000505, l4: 0.000702, l5: 0.001537, l6: 0.002686
[epoch: 435/1000, batch:    72/ 1052, ite: 114160] train loss: 0.004459, tar: 0.000150 
l0: 0.000141, l1: 0.000141, l2: 0.000184, l3: 0.000277, l4: 0.000581, l5: 0.001575, l6: 0.002245
[epoch: 435/1000, batch:   152/ 1052, ite: 114180] train loss: 0.004459, tar: 0.000150 
l0: 0.000104, l1: 0.000102, l2: 0.000126, l3: 0.000196, l4: 0.000446, l5: 0.000694, l6: 0.001445
[epoch: 435/1000, batch:   232/ 1052, ite: 114200] train loss: 0.004460, tar: 0.000150 
l0: 0.000159, l1: 0.000158, l2: 0.000233, l3: 0.000320, l4: 0.000569, l5: 0.001034, l6: 0.001894
[epoch: 435/1000, batch:   312/ 1052, ite: 114220] train loss: 0.004459, tar: 0.000150 
l0: 0.000167, l1: 0.000168, l2: 0.000216, l3: 0.000384, l4: 0.000681, l5: 0.001804, l6: 0.002645
[epoch: 435/1000, batch:   392/ 1052, ite: 114240] train loss: 0.004459, tar: 0.000150 
l0: 0.000087, l1: 0.000086, l2: 0.000126, l3: 0.000235, l4: 0.000560, l5: 0.000771, l6: 0.001551
[epoch: 435/1000, batch:   472/ 1052, ite: 114260] train loss: 0.004460, tar: 0.000150 
l0: 0.000161, l1: 0.000162, l2: 0.000201, l3: 0.000235, l4: 0.000410, l5: 0.000739, l6: 0.001942
[epoch: 435/1000, batch:   552/ 1052, ite: 114280] train loss: 0.004460, tar: 0.000150 
l0: 0.000244, l1: 0.000248, l2: 0.000279, l3: 0.000338, l4: 0.000547, l5: 0.001186, l6: 0.002140
[epoch: 435/1000, batch:   632/ 1052, ite: 114300] train loss: 0.004460, tar: 0.000150 
l0: 0.000145, l1: 0.000149, l2: 0.000183, l3: 0.000303, l4: 0.000591, l5: 0.000945, l6: 0.001758
[epoch: 435/1000, batch:   712/ 1052, ite: 114320] train loss: 0.004460, tar: 0.000150 
l0: 0.000188, l1: 0.000187, l2: 0.000282, l3: 0.000361, l4: 0.000533, l5: 0.001292, l6: 0.002909
[epoch: 435/1000, batch:   792/ 1052, ite: 114340] train loss: 0.004459, tar: 0.000150 
l0: 0.000146, l1: 0.000157, l2: 0.000175, l3: 0.000317, l4: 0.000598, l5: 0.000945, l6: 0.002036
[epoch: 435/1000, batch:   872/ 1052, ite: 114360] train loss: 0.004459, tar: 0.000150 
l0: 0.000118, l1: 0.000121, l2: 0.000127, l3: 0.000224, l4: 0.000429, l5: 0.000703, l6: 0.001522
[epoch: 435/1000, batch:   952/ 1052, ite: 114380] train loss: 0.004459, tar: 0.000150 
l0: 0.000129, l1: 0.000130, l2: 0.000139, l3: 0.000196, l4: 0.000421, l5: 0.001105, l6: 0.001638
[epoch: 435/1000, batch:  1032/ 1052, ite: 114400] train loss: 0.004458, tar: 0.000150 
[Epoch 435/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000113, l1: 0.000112, l2: 0.000145, l3: 0.000203, l4: 0.000364, l5: 0.000772, l6: 0.001436
[epoch: 436/1000, batch:    60/ 1052, ite: 114420] train loss: 0.004459, tar: 0.000150 
l0: 0.000134, l1: 0.000131, l2: 0.000193, l3: 0.000280, l4: 0.000564, l5: 0.000943, l6: 0.001939
[epoch: 436/1000, batch:   140/ 1052, ite: 114440] train loss: 0.004459, tar: 0.000150 
l0: 0.000142, l1: 0.000144, l2: 0.000152, l3: 0.000250, l4: 0.000514, l5: 0.000545, l6: 0.002001
[epoch: 436/1000, batch:   220/ 1052, ite: 114460] train loss: 0.004459, tar: 0.000150 
l0: 0.000048, l1: 0.000049, l2: 0.000060, l3: 0.000115, l4: 0.000338, l5: 0.000738, l6: 0.001364
[epoch: 436/1000, batch:   300/ 1052, ite: 114480] train loss: 0.004459, tar: 0.000150 
l0: 0.000080, l1: 0.000079, l2: 0.000108, l3: 0.000167, l4: 0.000410, l5: 0.001223, l6: 0.001812
[epoch: 436/1000, batch:   380/ 1052, ite: 114500] train loss: 0.004459, tar: 0.000150 
l0: 0.000094, l1: 0.000094, l2: 0.000114, l3: 0.000161, l4: 0.000341, l5: 0.000757, l6: 0.001406
[epoch: 436/1000, batch:   460/ 1052, ite: 114520] train loss: 0.004458, tar: 0.000150 
l0: 0.000088, l1: 0.000088, l2: 0.000117, l3: 0.000241, l4: 0.000392, l5: 0.001216, l6: 0.002039
[epoch: 436/1000, batch:   540/ 1052, ite: 114540] train loss: 0.004457, tar: 0.000149 
l0: 0.000200, l1: 0.000203, l2: 0.000231, l3: 0.000416, l4: 0.001218, l5: 0.001999, l6: 0.002379
[epoch: 436/1000, batch:   620/ 1052, ite: 114560] train loss: 0.004457, tar: 0.000149 
l0: 0.000134, l1: 0.000137, l2: 0.000167, l3: 0.000256, l4: 0.000697, l5: 0.001228, l6: 0.001926
[epoch: 436/1000, batch:   700/ 1052, ite: 114580] train loss: 0.004457, tar: 0.000149 
l0: 0.000293, l1: 0.000302, l2: 0.000317, l3: 0.000353, l4: 0.000466, l5: 0.000795, l6: 0.001919
[epoch: 436/1000, batch:   780/ 1052, ite: 114600] train loss: 0.004456, tar: 0.000149 
l0: 0.000126, l1: 0.000128, l2: 0.000145, l3: 0.000211, l4: 0.000421, l5: 0.000844, l6: 0.001189
[epoch: 436/1000, batch:   860/ 1052, ite: 114620] train loss: 0.004456, tar: 0.000149 
l0: 0.000232, l1: 0.000237, l2: 0.000247, l3: 0.000288, l4: 0.000681, l5: 0.001283, l6: 0.002407
[epoch: 436/1000, batch:   940/ 1052, ite: 114640] train loss: 0.004456, tar: 0.000149 
l0: 0.000118, l1: 0.000121, l2: 0.000175, l3: 0.000279, l4: 0.000730, l5: 0.001099, l6: 0.002179
[epoch: 436/1000, batch:  1020/ 1052, ite: 114660] train loss: 0.004455, tar: 0.000149 
[Epoch 436/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000073, l1: 0.000071, l2: 0.000130, l3: 0.000403, l4: 0.000599, l5: 0.000954, l6: 0.002100
[epoch: 437/1000, batch:    48/ 1052, ite: 114680] train loss: 0.004455, tar: 0.000149 
l0: 0.000295, l1: 0.000300, l2: 0.000329, l3: 0.000393, l4: 0.000616, l5: 0.001142, l6: 0.001656
[epoch: 437/1000, batch:   128/ 1052, ite: 114700] train loss: 0.004454, tar: 0.000149 
l0: 0.000050, l1: 0.000052, l2: 0.000076, l3: 0.000144, l4: 0.000351, l5: 0.001214, l6: 0.000890
[epoch: 437/1000, batch:   208/ 1052, ite: 114720] train loss: 0.004456, tar: 0.000149 
l0: 0.000207, l1: 0.000211, l2: 0.000221, l3: 0.000239, l4: 0.000478, l5: 0.000866, l6: 0.002111
[epoch: 437/1000, batch:   288/ 1052, ite: 114740] train loss: 0.004454, tar: 0.000149 
l0: 0.000199, l1: 0.000190, l2: 0.000300, l3: 0.000586, l4: 0.000975, l5: 0.001830, l6: 0.002258
[epoch: 437/1000, batch:   368/ 1052, ite: 114760] train loss: 0.004453, tar: 0.000149 
l0: 0.000133, l1: 0.000136, l2: 0.000177, l3: 0.000269, l4: 0.000342, l5: 0.001057, l6: 0.002223
[epoch: 437/1000, batch:   448/ 1052, ite: 114780] train loss: 0.004452, tar: 0.000149 
l0: 0.000114, l1: 0.000112, l2: 0.000174, l3: 0.000304, l4: 0.000484, l5: 0.001001, l6: 0.001926
[epoch: 437/1000, batch:   528/ 1052, ite: 114800] train loss: 0.004452, tar: 0.000149 
l0: 0.000490, l1: 0.000501, l2: 0.000465, l3: 0.000511, l4: 0.000827, l5: 0.001908, l6: 0.003929
[epoch: 437/1000, batch:   608/ 1052, ite: 114820] train loss: 0.004453, tar: 0.000149 
l0: 0.000151, l1: 0.000153, l2: 0.000297, l3: 0.000547, l4: 0.000941, l5: 0.001669, l6: 0.003098
[epoch: 437/1000, batch:   688/ 1052, ite: 114840] train loss: 0.004453, tar: 0.000149 
l0: 0.000148, l1: 0.000150, l2: 0.000220, l3: 0.000338, l4: 0.000693, l5: 0.002406, l6: 0.003683
[epoch: 437/1000, batch:   768/ 1052, ite: 114860] train loss: 0.004455, tar: 0.000149 
l0: 0.000031, l1: 0.000033, l2: 0.000049, l3: 0.000111, l4: 0.000194, l5: 0.000549, l6: 0.001036
[epoch: 437/1000, batch:   848/ 1052, ite: 114880] train loss: 0.004454, tar: 0.000149 
l0: 0.000065, l1: 0.000064, l2: 0.000085, l3: 0.000121, l4: 0.000216, l5: 0.000562, l6: 0.001165
[epoch: 437/1000, batch:   928/ 1052, ite: 114900] train loss: 0.004453, tar: 0.000149 
l0: 0.000108, l1: 0.000118, l2: 0.000136, l3: 0.000147, l4: 0.000279, l5: 0.000624, l6: 0.001101
[epoch: 437/1000, batch:  1008/ 1052, ite: 114920] train loss: 0.004454, tar: 0.000149 
[Epoch 437/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000118, l1: 0.000120, l2: 0.000146, l3: 0.000197, l4: 0.000391, l5: 0.001034, l6: 0.001372
[epoch: 438/1000, batch:    36/ 1052, ite: 114940] train loss: 0.004453, tar: 0.000149 
l0: 0.000140, l1: 0.000143, l2: 0.000165, l3: 0.000247, l4: 0.000809, l5: 0.002369, l6: 0.004200
[epoch: 438/1000, batch:   116/ 1052, ite: 114960] train loss: 0.004454, tar: 0.000149 
l0: 0.000126, l1: 0.000128, l2: 0.000153, l3: 0.000262, l4: 0.000462, l5: 0.001013, l6: 0.002286
[epoch: 438/1000, batch:   196/ 1052, ite: 114980] train loss: 0.004453, tar: 0.000149 
l0: 0.000162, l1: 0.000162, l2: 0.000199, l3: 0.000267, l4: 0.000404, l5: 0.000746, l6: 0.002022
[epoch: 438/1000, batch:   276/ 1052, ite: 115000] train loss: 0.004452, tar: 0.000149 
l0: 0.000130, l1: 0.000130, l2: 0.000178, l3: 0.000305, l4: 0.000656, l5: 0.001213, l6: 0.001638
[epoch: 438/1000, batch:   356/ 1052, ite: 115020] train loss: 0.004452, tar: 0.000149 
l0: 0.000208, l1: 0.000207, l2: 0.000252, l3: 0.000354, l4: 0.000652, l5: 0.001089, l6: 0.002318
[epoch: 438/1000, batch:   436/ 1052, ite: 115040] train loss: 0.004451, tar: 0.000149 
l0: 0.000276, l1: 0.000277, l2: 0.000306, l3: 0.000509, l4: 0.000839, l5: 0.001313, l6: 0.002461
[epoch: 438/1000, batch:   516/ 1052, ite: 115060] train loss: 0.004453, tar: 0.000149 
l0: 0.000100, l1: 0.000100, l2: 0.000128, l3: 0.000170, l4: 0.000413, l5: 0.000905, l6: 0.001299
[epoch: 438/1000, batch:   596/ 1052, ite: 115080] train loss: 0.004453, tar: 0.000149 
l0: 0.000077, l1: 0.000077, l2: 0.000095, l3: 0.000208, l4: 0.000543, l5: 0.001221, l6: 0.001427
[epoch: 438/1000, batch:   676/ 1052, ite: 115100] train loss: 0.004452, tar: 0.000149 
l0: 0.000080, l1: 0.000078, l2: 0.000124, l3: 0.000150, l4: 0.000286, l5: 0.000860, l6: 0.001446
[epoch: 438/1000, batch:   756/ 1052, ite: 115120] train loss: 0.004451, tar: 0.000149 
l0: 0.000110, l1: 0.000109, l2: 0.000138, l3: 0.000290, l4: 0.000584, l5: 0.000714, l6: 0.001345
[epoch: 438/1000, batch:   836/ 1052, ite: 115140] train loss: 0.004452, tar: 0.000149 
l0: 0.000119, l1: 0.000116, l2: 0.000155, l3: 0.000252, l4: 0.000390, l5: 0.000639, l6: 0.001268
[epoch: 438/1000, batch:   916/ 1052, ite: 115160] train loss: 0.004452, tar: 0.000149 
l0: 0.000192, l1: 0.000193, l2: 0.000248, l3: 0.000408, l4: 0.000693, l5: 0.001209, l6: 0.002244
[epoch: 438/1000, batch:   996/ 1052, ite: 115180] train loss: 0.004452, tar: 0.000149 
[Epoch 438/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000580, l1: 0.000598, l2: 0.000608, l3: 0.000621, l4: 0.001027, l5: 0.001772, l6: 0.002924
[epoch: 439/1000, batch:    24/ 1052, ite: 115200] train loss: 0.004451, tar: 0.000149 
l0: 0.000167, l1: 0.000166, l2: 0.000231, l3: 0.000306, l4: 0.000437, l5: 0.000806, l6: 0.001511
[epoch: 439/1000, batch:   104/ 1052, ite: 115220] train loss: 0.004453, tar: 0.000149 
l0: 0.000207, l1: 0.000223, l2: 0.000163, l3: 0.000299, l4: 0.000699, l5: 0.001151, l6: 0.001565
[epoch: 439/1000, batch:   184/ 1052, ite: 115240] train loss: 0.004454, tar: 0.000149 
l0: 0.000277, l1: 0.000269, l2: 0.000391, l3: 0.000594, l4: 0.001087, l5: 0.001971, l6: 0.004244
[epoch: 439/1000, batch:   264/ 1052, ite: 115260] train loss: 0.004455, tar: 0.000149 
l0: 0.000111, l1: 0.000113, l2: 0.000124, l3: 0.000194, l4: 0.000320, l5: 0.000791, l6: 0.001494
[epoch: 439/1000, batch:   344/ 1052, ite: 115280] train loss: 0.004455, tar: 0.000149 
l0: 0.000089, l1: 0.000090, l2: 0.000092, l3: 0.000185, l4: 0.000306, l5: 0.000652, l6: 0.001229
[epoch: 439/1000, batch:   424/ 1052, ite: 115300] train loss: 0.004455, tar: 0.000149 
l0: 0.000275, l1: 0.000274, l2: 0.000338, l3: 0.000504, l4: 0.000708, l5: 0.002029, l6: 0.003650
[epoch: 439/1000, batch:   504/ 1052, ite: 115320] train loss: 0.004456, tar: 0.000149 
l0: 0.000137, l1: 0.000141, l2: 0.000158, l3: 0.000211, l4: 0.000568, l5: 0.001026, l6: 0.001248
[epoch: 439/1000, batch:   584/ 1052, ite: 115340] train loss: 0.004456, tar: 0.000149 
l0: 0.000290, l1: 0.000291, l2: 0.000356, l3: 0.000459, l4: 0.000569, l5: 0.001134, l6: 0.001969
[epoch: 439/1000, batch:   664/ 1052, ite: 115360] train loss: 0.004456, tar: 0.000149 
l0: 0.000092, l1: 0.000091, l2: 0.000129, l3: 0.000146, l4: 0.000359, l5: 0.001101, l6: 0.001394
[epoch: 439/1000, batch:   744/ 1052, ite: 115380] train loss: 0.004455, tar: 0.000149 
l0: 0.000259, l1: 0.000257, l2: 0.000349, l3: 0.000390, l4: 0.000619, l5: 0.001568, l6: 0.003545
[epoch: 439/1000, batch:   824/ 1052, ite: 115400] train loss: 0.004455, tar: 0.000149 
l0: 0.000122, l1: 0.000118, l2: 0.000139, l3: 0.000213, l4: 0.000447, l5: 0.000863, l6: 0.001191
[epoch: 439/1000, batch:   904/ 1052, ite: 115420] train loss: 0.004455, tar: 0.000149 
l0: 0.000343, l1: 0.000344, l2: 0.000347, l3: 0.000626, l4: 0.000854, l5: 0.001726, l6: 0.002220
[epoch: 439/1000, batch:   984/ 1052, ite: 115440] train loss: 0.004455, tar: 0.000149 
[Epoch 439/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000077, l1: 0.000074, l2: 0.000121, l3: 0.000204, l4: 0.000349, l5: 0.001085, l6: 0.001611
[epoch: 440/1000, batch:    12/ 1052, ite: 115460] train loss: 0.004454, tar: 0.000149 
l0: 0.000170, l1: 0.000168, l2: 0.000194, l3: 0.000244, l4: 0.000672, l5: 0.001308, l6: 0.001907
[epoch: 440/1000, batch:    92/ 1052, ite: 115480] train loss: 0.004454, tar: 0.000149 
l0: 0.000148, l1: 0.000151, l2: 0.000152, l3: 0.000328, l4: 0.000451, l5: 0.000855, l6: 0.001798
[epoch: 440/1000, batch:   172/ 1052, ite: 115500] train loss: 0.004454, tar: 0.000149 
l0: 0.000371, l1: 0.000405, l2: 0.000470, l3: 0.000534, l4: 0.000752, l5: 0.002185, l6: 0.003188
[epoch: 440/1000, batch:   252/ 1052, ite: 115520] train loss: 0.004455, tar: 0.000149 
l0: 0.000124, l1: 0.000127, l2: 0.000170, l3: 0.000250, l4: 0.000417, l5: 0.001159, l6: 0.001772
[epoch: 440/1000, batch:   332/ 1052, ite: 115540] train loss: 0.004456, tar: 0.000150 
l0: 0.000235, l1: 0.000254, l2: 0.000270, l3: 0.000354, l4: 0.000650, l5: 0.001102, l6: 0.002712
[epoch: 440/1000, batch:   412/ 1052, ite: 115560] train loss: 0.004458, tar: 0.000150 
l0: 0.000320, l1: 0.000318, l2: 0.000378, l3: 0.000402, l4: 0.000808, l5: 0.001941, l6: 0.002592
[epoch: 440/1000, batch:   492/ 1052, ite: 115580] train loss: 0.004460, tar: 0.000150 
l0: 0.000167, l1: 0.000153, l2: 0.000246, l3: 0.000352, l4: 0.000866, l5: 0.001420, l6: 0.003081
[epoch: 440/1000, batch:   572/ 1052, ite: 115600] train loss: 0.004461, tar: 0.000150 
l0: 0.000292, l1: 0.000283, l2: 0.000433, l3: 0.000576, l4: 0.000827, l5: 0.001338, l6: 0.002336
[epoch: 440/1000, batch:   652/ 1052, ite: 115620] train loss: 0.004461, tar: 0.000151 
l0: 0.000271, l1: 0.000275, l2: 0.000310, l3: 0.000391, l4: 0.000747, l5: 0.001607, l6: 0.002776
[epoch: 440/1000, batch:   732/ 1052, ite: 115640] train loss: 0.004461, tar: 0.000151 
l0: 0.000076, l1: 0.000084, l2: 0.000091, l3: 0.000114, l4: 0.000225, l5: 0.000471, l6: 0.001253
[epoch: 440/1000, batch:   812/ 1052, ite: 115660] train loss: 0.004462, tar: 0.000151 
l0: 0.000300, l1: 0.000291, l2: 0.000368, l3: 0.000383, l4: 0.000673, l5: 0.000990, l6: 0.001877
[epoch: 440/1000, batch:   892/ 1052, ite: 115680] train loss: 0.004463, tar: 0.000151 
l0: 0.000137, l1: 0.000142, l2: 0.000188, l3: 0.000268, l4: 0.000524, l5: 0.001058, l6: 0.001679
[epoch: 440/1000, batch:   972/ 1052, ite: 115700] train loss: 0.004462, tar: 0.000151 
l0: 0.000164, l1: 0.000162, l2: 0.000176, l3: 0.000268, l4: 0.000537, l5: 0.001033, l6: 0.001664
[epoch: 440/1000, batch:  1052/ 1052, ite: 115720] train loss: 0.004464, tar: 0.000151 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_115720.pkl
[Epoch 440/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000106, l1: 0.000105, l2: 0.000120, l3: 0.000222, l4: 0.000403, l5: 0.001249, l6: 0.002069
[epoch: 441/1000, batch:    80/ 1052, ite: 115740] train loss: 0.004814, tar: 0.000179 
l0: 0.000135, l1: 0.000137, l2: 0.000191, l3: 0.000382, l4: 0.000632, l5: 0.000972, l6: 0.002311
[epoch: 441/1000, batch:   160/ 1052, ite: 115760] train loss: 0.004785, tar: 0.000174 
l0: 0.000134, l1: 0.000138, l2: 0.000137, l3: 0.000252, l4: 0.000429, l5: 0.001176, l6: 0.001689
[epoch: 441/1000, batch:   240/ 1052, ite: 115780] train loss: 0.004668, tar: 0.000177 
l0: 0.000189, l1: 0.000188, l2: 0.000214, l3: 0.000253, l4: 0.000391, l5: 0.001206, l6: 0.002496
[epoch: 441/1000, batch:   320/ 1052, ite: 115800] train loss: 0.004779, tar: 0.000174 
l0: 0.000065, l1: 0.000062, l2: 0.000100, l3: 0.000173, l4: 0.000684, l5: 0.000851, l6: 0.002007
[epoch: 441/1000, batch:   400/ 1052, ite: 115820] train loss: 0.004848, tar: 0.000174 
l0: 0.000112, l1: 0.000112, l2: 0.000163, l3: 0.000203, l4: 0.000440, l5: 0.001075, l6: 0.001706
[epoch: 441/1000, batch:   480/ 1052, ite: 115840] train loss: 0.004802, tar: 0.000170 
l0: 0.000084, l1: 0.000088, l2: 0.000114, l3: 0.000162, l4: 0.000422, l5: 0.000833, l6: 0.001624
[epoch: 441/1000, batch:   560/ 1052, ite: 115860] train loss: 0.004811, tar: 0.000169 
l0: 0.000306, l1: 0.000310, l2: 0.000351, l3: 0.000460, l4: 0.000687, l5: 0.001523, l6: 0.002808
[epoch: 441/1000, batch:   640/ 1052, ite: 115880] train loss: 0.004766, tar: 0.000167 
l0: 0.000236, l1: 0.000240, l2: 0.000390, l3: 0.000638, l4: 0.001034, l5: 0.001622, l6: 0.003650
[epoch: 441/1000, batch:   720/ 1052, ite: 115900] train loss: 0.004723, tar: 0.000165 
l0: 0.000226, l1: 0.000229, l2: 0.000286, l3: 0.000430, l4: 0.000734, l5: 0.001519, l6: 0.002626
[epoch: 441/1000, batch:   800/ 1052, ite: 115920] train loss: 0.004688, tar: 0.000163 
l0: 0.000157, l1: 0.000154, l2: 0.000175, l3: 0.000201, l4: 0.000392, l5: 0.000766, l6: 0.001077
[epoch: 441/1000, batch:   880/ 1052, ite: 115940] train loss: 0.004608, tar: 0.000160 
l0: 0.000133, l1: 0.000139, l2: 0.000180, l3: 0.000245, l4: 0.000345, l5: 0.001024, l6: 0.002097
[epoch: 441/1000, batch:   960/ 1052, ite: 115960] train loss: 0.004590, tar: 0.000159 
l0: 0.000121, l1: 0.000114, l2: 0.000152, l3: 0.000170, l4: 0.000391, l5: 0.000616, l6: 0.001725
[epoch: 441/1000, batch:  1040/ 1052, ite: 115980] train loss: 0.004518, tar: 0.000157 
[Epoch 441/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000056, l1: 0.000056, l2: 0.000078, l3: 0.000169, l4: 0.000235, l5: 0.000582, l6: 0.001177
[epoch: 442/1000, batch:    68/ 1052, ite: 116000] train loss: 0.004488, tar: 0.000154 
l0: 0.000147, l1: 0.000146, l2: 0.000201, l3: 0.000276, l4: 0.000494, l5: 0.001179, l6: 0.002933
[epoch: 442/1000, batch:   148/ 1052, ite: 116020] train loss: 0.004478, tar: 0.000152 
l0: 0.000115, l1: 0.000119, l2: 0.000140, l3: 0.000246, l4: 0.000318, l5: 0.000738, l6: 0.001217
[epoch: 442/1000, batch:   228/ 1052, ite: 116040] train loss: 0.004452, tar: 0.000150 
l0: 0.000108, l1: 0.000107, l2: 0.000129, l3: 0.000181, l4: 0.000369, l5: 0.000544, l6: 0.002057
[epoch: 442/1000, batch:   308/ 1052, ite: 116060] train loss: 0.004464, tar: 0.000151 
l0: 0.000132, l1: 0.000126, l2: 0.000155, l3: 0.000237, l4: 0.000531, l5: 0.000891, l6: 0.001975
[epoch: 442/1000, batch:   388/ 1052, ite: 116080] train loss: 0.004459, tar: 0.000150 
l0: 0.000093, l1: 0.000094, l2: 0.000104, l3: 0.000202, l4: 0.000384, l5: 0.000915, l6: 0.001898
[epoch: 442/1000, batch:   468/ 1052, ite: 116100] train loss: 0.004434, tar: 0.000148 
l0: 0.000106, l1: 0.000102, l2: 0.000158, l3: 0.000358, l4: 0.000532, l5: 0.001541, l6: 0.002036
[epoch: 442/1000, batch:   548/ 1052, ite: 116120] train loss: 0.004452, tar: 0.000148 
l0: 0.000017, l1: 0.000018, l2: 0.000041, l3: 0.000078, l4: 0.000312, l5: 0.000484, l6: 0.000848
[epoch: 442/1000, batch:   628/ 1052, ite: 116140] train loss: 0.004464, tar: 0.000147 
l0: 0.000075, l1: 0.000085, l2: 0.000108, l3: 0.000156, l4: 0.000178, l5: 0.000875, l6: 0.001463
[epoch: 442/1000, batch:   708/ 1052, ite: 116160] train loss: 0.004445, tar: 0.000146 
l0: 0.000169, l1: 0.000181, l2: 0.000192, l3: 0.000292, l4: 0.000567, l5: 0.000805, l6: 0.001821
[epoch: 442/1000, batch:   788/ 1052, ite: 116180] train loss: 0.004435, tar: 0.000145 
l0: 0.000151, l1: 0.000153, l2: 0.000198, l3: 0.000317, l4: 0.000818, l5: 0.002880, l6: 0.004763
[epoch: 442/1000, batch:   868/ 1052, ite: 116200] train loss: 0.004458, tar: 0.000145 
l0: 0.000068, l1: 0.000064, l2: 0.000098, l3: 0.000204, l4: 0.000402, l5: 0.000933, l6: 0.001638
[epoch: 442/1000, batch:   948/ 1052, ite: 116220] train loss: 0.004446, tar: 0.000144 
l0: 0.000148, l1: 0.000152, l2: 0.000172, l3: 0.000267, l4: 0.000464, l5: 0.000727, l6: 0.001709
[epoch: 442/1000, batch:  1028/ 1052, ite: 116240] train loss: 0.004453, tar: 0.000144 
[Epoch 442/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000132, l1: 0.000131, l2: 0.000157, l3: 0.000260, l4: 0.000307, l5: 0.000679, l6: 0.001152
[epoch: 443/1000, batch:    56/ 1052, ite: 116260] train loss: 0.004456, tar: 0.000145 
l0: 0.000169, l1: 0.000176, l2: 0.000161, l3: 0.000215, l4: 0.000445, l5: 0.000602, l6: 0.001388
[epoch: 443/1000, batch:   136/ 1052, ite: 116280] train loss: 0.004474, tar: 0.000146 
l0: 0.000411, l1: 0.000403, l2: 0.000508, l3: 0.000724, l4: 0.001110, l5: 0.001708, l6: 0.005381
[epoch: 443/1000, batch:   216/ 1052, ite: 116300] train loss: 0.004496, tar: 0.000147 
l0: 0.000053, l1: 0.000049, l2: 0.000080, l3: 0.000148, l4: 0.000250, l5: 0.000682, l6: 0.001039
[epoch: 443/1000, batch:   296/ 1052, ite: 116320] train loss: 0.004492, tar: 0.000147 
l0: 0.000130, l1: 0.000124, l2: 0.000161, l3: 0.000309, l4: 0.000585, l5: 0.001112, l6: 0.001668
[epoch: 443/1000, batch:   376/ 1052, ite: 116340] train loss: 0.004489, tar: 0.000147 
l0: 0.000237, l1: 0.000240, l2: 0.000294, l3: 0.000499, l4: 0.000812, l5: 0.001650, l6: 0.004071
[epoch: 443/1000, batch:   456/ 1052, ite: 116360] train loss: 0.004489, tar: 0.000147 
l0: 0.000217, l1: 0.000243, l2: 0.000222, l3: 0.000317, l4: 0.000663, l5: 0.001147, l6: 0.002678
[epoch: 443/1000, batch:   536/ 1052, ite: 116380] train loss: 0.004483, tar: 0.000147 
l0: 0.000102, l1: 0.000096, l2: 0.000099, l3: 0.000156, l4: 0.000300, l5: 0.000559, l6: 0.001315
[epoch: 443/1000, batch:   616/ 1052, ite: 116400] train loss: 0.004480, tar: 0.000147 
l0: 0.000105, l1: 0.000101, l2: 0.000146, l3: 0.000264, l4: 0.000512, l5: 0.001765, l6: 0.001758
[epoch: 443/1000, batch:   696/ 1052, ite: 116420] train loss: 0.004476, tar: 0.000147 
l0: 0.000298, l1: 0.000294, l2: 0.000515, l3: 0.000612, l4: 0.001111, l5: 0.002090, l6: 0.004725
[epoch: 443/1000, batch:   776/ 1052, ite: 116440] train loss: 0.004465, tar: 0.000146 
l0: 0.000085, l1: 0.000087, l2: 0.000121, l3: 0.000271, l4: 0.000458, l5: 0.001301, l6: 0.001987
[epoch: 443/1000, batch:   856/ 1052, ite: 116460] train loss: 0.004463, tar: 0.000146 
l0: 0.000256, l1: 0.000270, l2: 0.000269, l3: 0.000293, l4: 0.000601, l5: 0.001340, l6: 0.001628
[epoch: 443/1000, batch:   936/ 1052, ite: 116480] train loss: 0.004458, tar: 0.000146 
l0: 0.000150, l1: 0.000146, l2: 0.000201, l3: 0.000268, l4: 0.000358, l5: 0.000927, l6: 0.001881
[epoch: 443/1000, batch:  1016/ 1052, ite: 116500] train loss: 0.004469, tar: 0.000146 
[Epoch 443/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000129, l1: 0.000128, l2: 0.000161, l3: 0.000252, l4: 0.000608, l5: 0.001447, l6: 0.002767
[epoch: 444/1000, batch:    44/ 1052, ite: 116520] train loss: 0.004480, tar: 0.000146 
l0: 0.000293, l1: 0.000296, l2: 0.000325, l3: 0.000402, l4: 0.000959, l5: 0.001615, l6: 0.002792
[epoch: 444/1000, batch:   124/ 1052, ite: 116540] train loss: 0.004490, tar: 0.000146 
l0: 0.000042, l1: 0.000043, l2: 0.000066, l3: 0.000096, l4: 0.000215, l5: 0.000518, l6: 0.000718
[epoch: 444/1000, batch:   204/ 1052, ite: 116560] train loss: 0.004490, tar: 0.000149 
l0: 0.000111, l1: 0.000123, l2: 0.000182, l3: 0.000241, l4: 0.000484, l5: 0.001177, l6: 0.001213
[epoch: 444/1000, batch:   284/ 1052, ite: 116580] train loss: 0.004469, tar: 0.000148 
l0: 0.000439, l1: 0.000457, l2: 0.000494, l3: 0.000654, l4: 0.001294, l5: 0.002649, l6: 0.004757
[epoch: 444/1000, batch:   364/ 1052, ite: 116600] train loss: 0.004473, tar: 0.000148 
l0: 0.000052, l1: 0.000051, l2: 0.000075, l3: 0.000104, l4: 0.000240, l5: 0.000917, l6: 0.001192
[epoch: 444/1000, batch:   444/ 1052, ite: 116620] train loss: 0.004471, tar: 0.000148 
l0: 0.000147, l1: 0.000148, l2: 0.000253, l3: 0.000409, l4: 0.000832, l5: 0.001207, l6: 0.001626
[epoch: 444/1000, batch:   524/ 1052, ite: 116640] train loss: 0.004467, tar: 0.000147 
l0: 0.000128, l1: 0.000138, l2: 0.000161, l3: 0.000251, l4: 0.000641, l5: 0.000879, l6: 0.002705
[epoch: 444/1000, batch:   604/ 1052, ite: 116660] train loss: 0.004465, tar: 0.000148 
l0: 0.000117, l1: 0.000117, l2: 0.000158, l3: 0.000225, l4: 0.000499, l5: 0.000845, l6: 0.001854
[epoch: 444/1000, batch:   684/ 1052, ite: 116680] train loss: 0.004471, tar: 0.000148 
l0: 0.000033, l1: 0.000034, l2: 0.000056, l3: 0.000101, l4: 0.000300, l5: 0.000958, l6: 0.001549
[epoch: 444/1000, batch:   764/ 1052, ite: 116700] train loss: 0.004464, tar: 0.000147 
l0: 0.000074, l1: 0.000069, l2: 0.000116, l3: 0.000223, l4: 0.000434, l5: 0.001042, l6: 0.001570
[epoch: 444/1000, batch:   844/ 1052, ite: 116720] train loss: 0.004458, tar: 0.000147 
l0: 0.000211, l1: 0.000209, l2: 0.000302, l3: 0.000443, l4: 0.000809, l5: 0.001488, l6: 0.002150
[epoch: 444/1000, batch:   924/ 1052, ite: 116740] train loss: 0.004466, tar: 0.000147 
l0: 0.000142, l1: 0.000140, l2: 0.000141, l3: 0.000215, l4: 0.000564, l5: 0.000777, l6: 0.001323
[epoch: 444/1000, batch:  1004/ 1052, ite: 116760] train loss: 0.004471, tar: 0.000147 
[Epoch 444/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000196, l1: 0.000195, l2: 0.000247, l3: 0.000577, l4: 0.000891, l5: 0.001506, l6: 0.002458
[epoch: 445/1000, batch:    32/ 1052, ite: 116780] train loss: 0.004496, tar: 0.000151 
l0: 0.000308, l1: 0.000311, l2: 0.000502, l3: 0.000893, l4: 0.001325, l5: 0.002134, l6: 0.004543
[epoch: 445/1000, batch:   112/ 1052, ite: 116800] train loss: 0.004500, tar: 0.000151 
l0: 0.000399, l1: 0.000411, l2: 0.000497, l3: 0.000597, l4: 0.001131, l5: 0.001810, l6: 0.003928
[epoch: 445/1000, batch:   192/ 1052, ite: 116820] train loss: 0.004501, tar: 0.000151 
l0: 0.000080, l1: 0.000079, l2: 0.000135, l3: 0.000262, l4: 0.000532, l5: 0.000900, l6: 0.001589
[epoch: 445/1000, batch:   272/ 1052, ite: 116840] train loss: 0.004491, tar: 0.000151 
l0: 0.000088, l1: 0.000087, l2: 0.000122, l3: 0.000216, l4: 0.000332, l5: 0.000603, l6: 0.001472
[epoch: 445/1000, batch:   352/ 1052, ite: 116860] train loss: 0.004486, tar: 0.000151 
l0: 0.000088, l1: 0.000085, l2: 0.000128, l3: 0.000210, l4: 0.000474, l5: 0.001063, l6: 0.001362
[epoch: 445/1000, batch:   432/ 1052, ite: 116880] train loss: 0.004497, tar: 0.000151 
l0: 0.000216, l1: 0.000223, l2: 0.000236, l3: 0.000284, l4: 0.000506, l5: 0.001256, l6: 0.002493
[epoch: 445/1000, batch:   512/ 1052, ite: 116900] train loss: 0.004504, tar: 0.000152 
l0: 0.000121, l1: 0.000123, l2: 0.000127, l3: 0.000197, l4: 0.000439, l5: 0.000692, l6: 0.001901
[epoch: 445/1000, batch:   592/ 1052, ite: 116920] train loss: 0.004505, tar: 0.000151 
l0: 0.000071, l1: 0.000071, l2: 0.000089, l3: 0.000142, l4: 0.000275, l5: 0.000813, l6: 0.001398
[epoch: 445/1000, batch:   672/ 1052, ite: 116940] train loss: 0.004498, tar: 0.000151 
l0: 0.000057, l1: 0.000056, l2: 0.000086, l3: 0.000181, l4: 0.000351, l5: 0.000863, l6: 0.001727
[epoch: 445/1000, batch:   752/ 1052, ite: 116960] train loss: 0.004491, tar: 0.000151 
l0: 0.000182, l1: 0.000190, l2: 0.000213, l3: 0.000258, l4: 0.000395, l5: 0.001388, l6: 0.002001
[epoch: 445/1000, batch:   832/ 1052, ite: 116980] train loss: 0.004502, tar: 0.000151 
l0: 0.000066, l1: 0.000065, l2: 0.000092, l3: 0.000185, l4: 0.000358, l5: 0.000647, l6: 0.001016
[epoch: 445/1000, batch:   912/ 1052, ite: 117000] train loss: 0.004496, tar: 0.000150 
l0: 0.000136, l1: 0.000130, l2: 0.000205, l3: 0.000339, l4: 0.000580, l5: 0.001234, l6: 0.001888
[epoch: 445/1000, batch:   992/ 1052, ite: 117020] train loss: 0.004488, tar: 0.000150 
[Epoch 445/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000167, l1: 0.000171, l2: 0.000208, l3: 0.000366, l4: 0.000620, l5: 0.001889, l6: 0.002238
[epoch: 446/1000, batch:    20/ 1052, ite: 117040] train loss: 0.004484, tar: 0.000150 
l0: 0.000157, l1: 0.000165, l2: 0.000248, l3: 0.000383, l4: 0.000689, l5: 0.001342, l6: 0.001778
[epoch: 446/1000, batch:   100/ 1052, ite: 117060] train loss: 0.004476, tar: 0.000149 
l0: 0.000243, l1: 0.000217, l2: 0.000206, l3: 0.000390, l4: 0.000534, l5: 0.000994, l6: 0.001391
[epoch: 446/1000, batch:   180/ 1052, ite: 117080] train loss: 0.004466, tar: 0.000148 
l0: 0.000058, l1: 0.000058, l2: 0.000075, l3: 0.000124, l4: 0.000256, l5: 0.000659, l6: 0.000959
[epoch: 446/1000, batch:   260/ 1052, ite: 117100] train loss: 0.004470, tar: 0.000148 
l0: 0.000085, l1: 0.000084, l2: 0.000115, l3: 0.000164, l4: 0.000381, l5: 0.000762, l6: 0.001624
[epoch: 446/1000, batch:   340/ 1052, ite: 117120] train loss: 0.004475, tar: 0.000148 
l0: 0.000083, l1: 0.000084, l2: 0.000134, l3: 0.000225, l4: 0.000446, l5: 0.001200, l6: 0.001345
[epoch: 446/1000, batch:   420/ 1052, ite: 117140] train loss: 0.004468, tar: 0.000148 
l0: 0.000106, l1: 0.000110, l2: 0.000139, l3: 0.000241, l4: 0.000514, l5: 0.000576, l6: 0.001382
[epoch: 446/1000, batch:   500/ 1052, ite: 117160] train loss: 0.004463, tar: 0.000147 
l0: 0.000087, l1: 0.000088, l2: 0.000102, l3: 0.000170, l4: 0.000280, l5: 0.000661, l6: 0.000988
[epoch: 446/1000, batch:   580/ 1052, ite: 117180] train loss: 0.004456, tar: 0.000147 
l0: 0.000101, l1: 0.000102, l2: 0.000160, l3: 0.000257, l4: 0.000605, l5: 0.001147, l6: 0.001782
[epoch: 446/1000, batch:   660/ 1052, ite: 117200] train loss: 0.004456, tar: 0.000147 
l0: 0.000399, l1: 0.000392, l2: 0.000458, l3: 0.000690, l4: 0.001121, l5: 0.001848, l6: 0.004995
[epoch: 446/1000, batch:   740/ 1052, ite: 117220] train loss: 0.004454, tar: 0.000146 
l0: 0.000182, l1: 0.000178, l2: 0.000217, l3: 0.000343, l4: 0.000664, l5: 0.001484, l6: 0.002966
[epoch: 446/1000, batch:   820/ 1052, ite: 117240] train loss: 0.004458, tar: 0.000146 
l0: 0.000253, l1: 0.000243, l2: 0.000303, l3: 0.000393, l4: 0.000593, l5: 0.001969, l6: 0.003764
[epoch: 446/1000, batch:   900/ 1052, ite: 117260] train loss: 0.004470, tar: 0.000147 
l0: 0.000086, l1: 0.000087, l2: 0.000156, l3: 0.000238, l4: 0.000510, l5: 0.001192, l6: 0.002098
[epoch: 446/1000, batch:   980/ 1052, ite: 117280] train loss: 0.004466, tar: 0.000146 
[Epoch 446/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000036, l1: 0.000036, l2: 0.000048, l3: 0.000074, l4: 0.000140, l5: 0.000698, l6: 0.000890
[epoch: 447/1000, batch:     8/ 1052, ite: 117300] train loss: 0.004465, tar: 0.000146 
l0: 0.000204, l1: 0.000196, l2: 0.000281, l3: 0.000480, l4: 0.000987, l5: 0.001908, l6: 0.003270
[epoch: 447/1000, batch:    88/ 1052, ite: 117320] train loss: 0.004467, tar: 0.000146 
l0: 0.000037, l1: 0.000036, l2: 0.000078, l3: 0.000167, l4: 0.000468, l5: 0.000996, l6: 0.001144
[epoch: 447/1000, batch:   168/ 1052, ite: 117340] train loss: 0.004462, tar: 0.000146 
l0: 0.000026, l1: 0.000024, l2: 0.000061, l3: 0.000125, l4: 0.000284, l5: 0.000581, l6: 0.001785
[epoch: 447/1000, batch:   248/ 1052, ite: 117360] train loss: 0.004458, tar: 0.000145 
l0: 0.000119, l1: 0.000122, l2: 0.000187, l3: 0.000336, l4: 0.000628, l5: 0.001211, l6: 0.001868
[epoch: 447/1000, batch:   328/ 1052, ite: 117380] train loss: 0.004457, tar: 0.000145 
l0: 0.000028, l1: 0.000027, l2: 0.000037, l3: 0.000093, l4: 0.000259, l5: 0.000492, l6: 0.000653
[epoch: 447/1000, batch:   408/ 1052, ite: 117400] train loss: 0.004449, tar: 0.000145 
l0: 0.000127, l1: 0.000124, l2: 0.000147, l3: 0.000198, l4: 0.000461, l5: 0.000756, l6: 0.001427
[epoch: 447/1000, batch:   488/ 1052, ite: 117420] train loss: 0.004461, tar: 0.000145 
l0: 0.000065, l1: 0.000071, l2: 0.000108, l3: 0.000189, l4: 0.000471, l5: 0.000676, l6: 0.001453
[epoch: 447/1000, batch:   568/ 1052, ite: 117440] train loss: 0.004461, tar: 0.000145 
l0: 0.000373, l1: 0.000372, l2: 0.000396, l3: 0.000431, l4: 0.001008, l5: 0.002096, l6: 0.003995
[epoch: 447/1000, batch:   648/ 1052, ite: 117460] train loss: 0.004470, tar: 0.000145 
l0: 0.000087, l1: 0.000088, l2: 0.000112, l3: 0.000197, l4: 0.000258, l5: 0.000572, l6: 0.000943
[epoch: 447/1000, batch:   728/ 1052, ite: 117480] train loss: 0.004468, tar: 0.000145 
l0: 0.000157, l1: 0.000161, l2: 0.000178, l3: 0.000238, l4: 0.000485, l5: 0.002074, l6: 0.002643
[epoch: 447/1000, batch:   808/ 1052, ite: 117500] train loss: 0.004467, tar: 0.000145 
l0: 0.000136, l1: 0.000140, l2: 0.000209, l3: 0.000286, l4: 0.000661, l5: 0.001284, l6: 0.002341
[epoch: 447/1000, batch:   888/ 1052, ite: 117520] train loss: 0.004468, tar: 0.000145 
l0: 0.000078, l1: 0.000081, l2: 0.000125, l3: 0.000162, l4: 0.000405, l5: 0.001001, l6: 0.001617
[epoch: 447/1000, batch:   968/ 1052, ite: 117540] train loss: 0.004462, tar: 0.000145 
l0: 0.000206, l1: 0.000213, l2: 0.000252, l3: 0.000413, l4: 0.000627, l5: 0.001152, l6: 0.001702
[epoch: 447/1000, batch:  1048/ 1052, ite: 117560] train loss: 0.004457, tar: 0.000145 
[Epoch 447/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000138, l1: 0.000139, l2: 0.000155, l3: 0.000243, l4: 0.000371, l5: 0.001310, l6: 0.002338
[epoch: 448/1000, batch:    76/ 1052, ite: 117580] train loss: 0.004457, tar: 0.000144 
l0: 0.000176, l1: 0.000188, l2: 0.000245, l3: 0.000386, l4: 0.000615, l5: 0.001301, l6: 0.001822
[epoch: 448/1000, batch:   156/ 1052, ite: 117600] train loss: 0.004457, tar: 0.000144 
l0: 0.000096, l1: 0.000098, l2: 0.000124, l3: 0.000168, l4: 0.000311, l5: 0.000426, l6: 0.001464
[epoch: 448/1000, batch:   236/ 1052, ite: 117620] train loss: 0.004456, tar: 0.000144 
l0: 0.000080, l1: 0.000084, l2: 0.000105, l3: 0.000145, l4: 0.000321, l5: 0.000757, l6: 0.001147
[epoch: 448/1000, batch:   316/ 1052, ite: 117640] train loss: 0.004449, tar: 0.000144 
l0: 0.000093, l1: 0.000093, l2: 0.000120, l3: 0.000177, l4: 0.000441, l5: 0.001276, l6: 0.002213
[epoch: 448/1000, batch:   396/ 1052, ite: 117660] train loss: 0.004455, tar: 0.000144 
l0: 0.000101, l1: 0.000101, l2: 0.000149, l3: 0.000242, l4: 0.000410, l5: 0.001109, l6: 0.002716
[epoch: 448/1000, batch:   476/ 1052, ite: 117680] train loss: 0.004459, tar: 0.000144 
l0: 0.000225, l1: 0.000212, l2: 0.000275, l3: 0.000474, l4: 0.000882, l5: 0.001852, l6: 0.002724
[epoch: 448/1000, batch:   556/ 1052, ite: 117700] train loss: 0.004455, tar: 0.000144 
l0: 0.000117, l1: 0.000121, l2: 0.000181, l3: 0.000232, l4: 0.000531, l5: 0.001404, l6: 0.002409
[epoch: 448/1000, batch:   636/ 1052, ite: 117720] train loss: 0.004457, tar: 0.000144 
l0: 0.000141, l1: 0.000141, l2: 0.000190, l3: 0.000242, l4: 0.000427, l5: 0.001042, l6: 0.001503
[epoch: 448/1000, batch:   716/ 1052, ite: 117740] train loss: 0.004456, tar: 0.000144 
l0: 0.000087, l1: 0.000087, l2: 0.000122, l3: 0.000157, l4: 0.000404, l5: 0.000600, l6: 0.001281
[epoch: 448/1000, batch:   796/ 1052, ite: 117760] train loss: 0.004453, tar: 0.000143 
l0: 0.000161, l1: 0.000164, l2: 0.000206, l3: 0.000313, l4: 0.000534, l5: 0.000803, l6: 0.002065
[epoch: 448/1000, batch:   876/ 1052, ite: 117780] train loss: 0.004454, tar: 0.000143 
l0: 0.000129, l1: 0.000128, l2: 0.000174, l3: 0.000269, l4: 0.000561, l5: 0.000880, l6: 0.001119
[epoch: 448/1000, batch:   956/ 1052, ite: 117800] train loss: 0.004454, tar: 0.000143 
l0: 0.000147, l1: 0.000153, l2: 0.000185, l3: 0.000195, l4: 0.000553, l5: 0.001279, l6: 0.002375
[epoch: 448/1000, batch:  1036/ 1052, ite: 117820] train loss: 0.004451, tar: 0.000143 
[Epoch 448/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000222, l1: 0.000223, l2: 0.000275, l3: 0.000314, l4: 0.000589, l5: 0.001097, l6: 0.002096
[epoch: 449/1000, batch:    64/ 1052, ite: 117840] train loss: 0.004448, tar: 0.000143 
l0: 0.000182, l1: 0.000184, l2: 0.000263, l3: 0.000498, l4: 0.000944, l5: 0.001559, l6: 0.002854
[epoch: 449/1000, batch:   144/ 1052, ite: 117860] train loss: 0.004453, tar: 0.000143 
l0: 0.000129, l1: 0.000134, l2: 0.000152, l3: 0.000243, l4: 0.000501, l5: 0.000947, l6: 0.002009
[epoch: 449/1000, batch:   224/ 1052, ite: 117880] train loss: 0.004452, tar: 0.000143 
l0: 0.000116, l1: 0.000114, l2: 0.000158, l3: 0.000254, l4: 0.000391, l5: 0.001162, l6: 0.001815
[epoch: 449/1000, batch:   304/ 1052, ite: 117900] train loss: 0.004453, tar: 0.000143 
l0: 0.000062, l1: 0.000062, l2: 0.000077, l3: 0.000146, l4: 0.000213, l5: 0.000666, l6: 0.001358
[epoch: 449/1000, batch:   384/ 1052, ite: 117920] train loss: 0.004453, tar: 0.000143 
l0: 0.000048, l1: 0.000045, l2: 0.000090, l3: 0.000116, l4: 0.000325, l5: 0.000878, l6: 0.001348
[epoch: 449/1000, batch:   464/ 1052, ite: 117940] train loss: 0.004446, tar: 0.000142 
l0: 0.000051, l1: 0.000053, l2: 0.000071, l3: 0.000123, l4: 0.000348, l5: 0.000913, l6: 0.001499
[epoch: 449/1000, batch:   544/ 1052, ite: 117960] train loss: 0.004440, tar: 0.000142 
l0: 0.000198, l1: 0.000202, l2: 0.000227, l3: 0.000304, l4: 0.000432, l5: 0.001377, l6: 0.002529
[epoch: 449/1000, batch:   624/ 1052, ite: 117980] train loss: 0.004438, tar: 0.000142 
l0: 0.000215, l1: 0.000225, l2: 0.000284, l3: 0.000438, l4: 0.001025, l5: 0.002439, l6: 0.003766
[epoch: 449/1000, batch:   704/ 1052, ite: 118000] train loss: 0.004437, tar: 0.000141 
l0: 0.000114, l1: 0.000118, l2: 0.000125, l3: 0.000231, l4: 0.000458, l5: 0.000925, l6: 0.002234
[epoch: 449/1000, batch:   784/ 1052, ite: 118020] train loss: 0.004434, tar: 0.000141 
l0: 0.000099, l1: 0.000101, l2: 0.000126, l3: 0.000195, l4: 0.000461, l5: 0.000914, l6: 0.001853
[epoch: 449/1000, batch:   864/ 1052, ite: 118040] train loss: 0.004432, tar: 0.000141 
l0: 0.000057, l1: 0.000058, l2: 0.000083, l3: 0.000161, l4: 0.000325, l5: 0.000832, l6: 0.001397
[epoch: 449/1000, batch:   944/ 1052, ite: 118060] train loss: 0.004434, tar: 0.000141 
l0: 0.000108, l1: 0.000112, l2: 0.000187, l3: 0.000256, l4: 0.000325, l5: 0.001143, l6: 0.001777
[epoch: 449/1000, batch:  1024/ 1052, ite: 118080] train loss: 0.004428, tar: 0.000141 
[Epoch 449/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000266, l1: 0.000265, l2: 0.000330, l3: 0.000450, l4: 0.000806, l5: 0.001940, l6: 0.003812
[epoch: 450/1000, batch:    52/ 1052, ite: 118100] train loss: 0.004427, tar: 0.000141 
l0: 0.000075, l1: 0.000076, l2: 0.000102, l3: 0.000183, l4: 0.000303, l5: 0.000855, l6: 0.001448
[epoch: 450/1000, batch:   132/ 1052, ite: 118120] train loss: 0.004423, tar: 0.000140 
l0: 0.000141, l1: 0.000139, l2: 0.000172, l3: 0.000280, l4: 0.000557, l5: 0.001070, l6: 0.003168
[epoch: 450/1000, batch:   212/ 1052, ite: 118140] train loss: 0.004422, tar: 0.000140 
l0: 0.000031, l1: 0.000033, l2: 0.000043, l3: 0.000107, l4: 0.000231, l5: 0.000898, l6: 0.001648
[epoch: 450/1000, batch:   292/ 1052, ite: 118160] train loss: 0.004421, tar: 0.000140 
l0: 0.000077, l1: 0.000078, l2: 0.000109, l3: 0.000183, l4: 0.000411, l5: 0.000545, l6: 0.001201
[epoch: 450/1000, batch:   372/ 1052, ite: 118180] train loss: 0.004418, tar: 0.000140 
l0: 0.000060, l1: 0.000060, l2: 0.000093, l3: 0.000120, l4: 0.000287, l5: 0.000551, l6: 0.000860
[epoch: 450/1000, batch:   452/ 1052, ite: 118200] train loss: 0.004419, tar: 0.000140 
l0: 0.000019, l1: 0.000017, l2: 0.000040, l3: 0.000086, l4: 0.000383, l5: 0.000876, l6: 0.001448
[epoch: 450/1000, batch:   532/ 1052, ite: 118220] train loss: 0.004423, tar: 0.000140 
l0: 0.000055, l1: 0.000056, l2: 0.000088, l3: 0.000140, l4: 0.000430, l5: 0.000824, l6: 0.001591
[epoch: 450/1000, batch:   612/ 1052, ite: 118240] train loss: 0.004419, tar: 0.000139 
l0: 0.000105, l1: 0.000105, l2: 0.000160, l3: 0.000244, l4: 0.000388, l5: 0.000601, l6: 0.002461
[epoch: 450/1000, batch:   692/ 1052, ite: 118260] train loss: 0.004416, tar: 0.000139 
l0: 0.000093, l1: 0.000093, l2: 0.000116, l3: 0.000250, l4: 0.000589, l5: 0.001306, l6: 0.002495
[epoch: 450/1000, batch:   772/ 1052, ite: 118280] train loss: 0.004413, tar: 0.000139 
l0: 0.000030, l1: 0.000028, l2: 0.000051, l3: 0.000119, l4: 0.000330, l5: 0.000916, l6: 0.001347
[epoch: 450/1000, batch:   852/ 1052, ite: 118300] train loss: 0.004414, tar: 0.000139 
l0: 0.000133, l1: 0.000135, l2: 0.000199, l3: 0.000402, l4: 0.001213, l5: 0.002026, l6: 0.003447
[epoch: 450/1000, batch:   932/ 1052, ite: 118320] train loss: 0.004414, tar: 0.000139 
l0: 0.000072, l1: 0.000076, l2: 0.000094, l3: 0.000207, l4: 0.000526, l5: 0.000743, l6: 0.001388
[epoch: 450/1000, batch:  1012/ 1052, ite: 118340] train loss: 0.004414, tar: 0.000139 
[Epoch 450/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000075, l1: 0.000077, l2: 0.000122, l3: 0.000252, l4: 0.000369, l5: 0.001167, l6: 0.003125
[epoch: 451/1000, batch:    40/ 1052, ite: 118360] train loss: 0.004413, tar: 0.000138 
l0: 0.000113, l1: 0.000118, l2: 0.000151, l3: 0.000273, l4: 0.000447, l5: 0.000666, l6: 0.002359
[epoch: 451/1000, batch:   120/ 1052, ite: 118380] train loss: 0.004412, tar: 0.000138 
l0: 0.000071, l1: 0.000075, l2: 0.000089, l3: 0.000174, l4: 0.000503, l5: 0.000950, l6: 0.001633
[epoch: 451/1000, batch:   200/ 1052, ite: 118400] train loss: 0.004410, tar: 0.000138 
l0: 0.000115, l1: 0.000122, l2: 0.000156, l3: 0.000243, l4: 0.000538, l5: 0.001258, l6: 0.002801
[epoch: 451/1000, batch:   280/ 1052, ite: 118420] train loss: 0.004413, tar: 0.000138 
l0: 0.000169, l1: 0.000172, l2: 0.000207, l3: 0.000385, l4: 0.000571, l5: 0.001039, l6: 0.001560
[epoch: 451/1000, batch:   360/ 1052, ite: 118440] train loss: 0.004412, tar: 0.000138 
l0: 0.000069, l1: 0.000068, l2: 0.000098, l3: 0.000147, l4: 0.000431, l5: 0.000735, l6: 0.001626
[epoch: 451/1000, batch:   440/ 1052, ite: 118460] train loss: 0.004405, tar: 0.000138 
l0: 0.000039, l1: 0.000040, l2: 0.000047, l3: 0.000083, l4: 0.000310, l5: 0.001158, l6: 0.001842
[epoch: 451/1000, batch:   520/ 1052, ite: 118480] train loss: 0.004403, tar: 0.000138 
l0: 0.000233, l1: 0.000226, l2: 0.000306, l3: 0.000338, l4: 0.000725, l5: 0.001133, l6: 0.002210
[epoch: 451/1000, batch:   600/ 1052, ite: 118500] train loss: 0.004403, tar: 0.000138 
l0: 0.000224, l1: 0.000225, l2: 0.000273, l3: 0.000382, l4: 0.000848, l5: 0.001809, l6: 0.003496
[epoch: 451/1000, batch:   680/ 1052, ite: 118520] train loss: 0.004408, tar: 0.000138 
l0: 0.000130, l1: 0.000131, l2: 0.000143, l3: 0.000189, l4: 0.000414, l5: 0.001005, l6: 0.002165
[epoch: 451/1000, batch:   760/ 1052, ite: 118540] train loss: 0.004410, tar: 0.000138 
l0: 0.000108, l1: 0.000113, l2: 0.000150, l3: 0.000282, l4: 0.000466, l5: 0.001192, l6: 0.002412
[epoch: 451/1000, batch:   840/ 1052, ite: 118560] train loss: 0.004410, tar: 0.000138 
l0: 0.000139, l1: 0.000144, l2: 0.000131, l3: 0.000200, l4: 0.000360, l5: 0.001339, l6: 0.002855
[epoch: 451/1000, batch:   920/ 1052, ite: 118580] train loss: 0.004413, tar: 0.000138 
l0: 0.000175, l1: 0.000176, l2: 0.000225, l3: 0.000277, l4: 0.000521, l5: 0.001483, l6: 0.002369
[epoch: 451/1000, batch:  1000/ 1052, ite: 118600] train loss: 0.004414, tar: 0.000138 
[Epoch 451/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000243, l1: 0.000234, l2: 0.000251, l3: 0.000270, l4: 0.000532, l5: 0.000971, l6: 0.002389
[epoch: 452/1000, batch:    28/ 1052, ite: 118620] train loss: 0.004414, tar: 0.000137 
l0: 0.000255, l1: 0.000277, l2: 0.000287, l3: 0.000382, l4: 0.000771, l5: 0.000937, l6: 0.002993
[epoch: 452/1000, batch:   108/ 1052, ite: 118640] train loss: 0.004416, tar: 0.000138 
l0: 0.000303, l1: 0.000309, l2: 0.000310, l3: 0.000414, l4: 0.000895, l5: 0.001748, l6: 0.003768
[epoch: 452/1000, batch:   188/ 1052, ite: 118660] train loss: 0.004419, tar: 0.000137 
l0: 0.000227, l1: 0.000222, l2: 0.000231, l3: 0.000311, l4: 0.000599, l5: 0.001455, l6: 0.002407
[epoch: 452/1000, batch:   268/ 1052, ite: 118680] train loss: 0.004415, tar: 0.000137 
l0: 0.000079, l1: 0.000076, l2: 0.000125, l3: 0.000221, l4: 0.000523, l5: 0.001069, l6: 0.002341
[epoch: 452/1000, batch:   348/ 1052, ite: 118700] train loss: 0.004420, tar: 0.000137 
l0: 0.000069, l1: 0.000067, l2: 0.000108, l3: 0.000167, l4: 0.000362, l5: 0.000695, l6: 0.000933
[epoch: 452/1000, batch:   428/ 1052, ite: 118720] train loss: 0.004413, tar: 0.000137 
l0: 0.000055, l1: 0.000056, l2: 0.000090, l3: 0.000155, l4: 0.000349, l5: 0.000617, l6: 0.001442
[epoch: 452/1000, batch:   508/ 1052, ite: 118740] train loss: 0.004415, tar: 0.000137 
l0: 0.000105, l1: 0.000107, l2: 0.000119, l3: 0.000220, l4: 0.000453, l5: 0.000787, l6: 0.001250
[epoch: 452/1000, batch:   588/ 1052, ite: 118760] train loss: 0.004414, tar: 0.000137 
l0: 0.000100, l1: 0.000100, l2: 0.000133, l3: 0.000209, l4: 0.000389, l5: 0.001017, l6: 0.002139
[epoch: 452/1000, batch:   668/ 1052, ite: 118780] train loss: 0.004408, tar: 0.000137 
l0: 0.000130, l1: 0.000131, l2: 0.000157, l3: 0.000238, l4: 0.000381, l5: 0.001011, l6: 0.002357
[epoch: 452/1000, batch:   748/ 1052, ite: 118800] train loss: 0.004411, tar: 0.000137 
l0: 0.000098, l1: 0.000098, l2: 0.000130, l3: 0.000226, l4: 0.000537, l5: 0.001014, l6: 0.001934
[epoch: 452/1000, batch:   828/ 1052, ite: 118820] train loss: 0.004414, tar: 0.000137 
l0: 0.000056, l1: 0.000055, l2: 0.000092, l3: 0.000211, l4: 0.000481, l5: 0.000722, l6: 0.001263
[epoch: 452/1000, batch:   908/ 1052, ite: 118840] train loss: 0.004408, tar: 0.000136 
l0: 0.000169, l1: 0.000172, l2: 0.000202, l3: 0.000279, l4: 0.000739, l5: 0.001928, l6: 0.003301
[epoch: 452/1000, batch:   988/ 1052, ite: 118860] train loss: 0.004408, tar: 0.000136 
[Epoch 452/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000190, l1: 0.000193, l2: 0.000224, l3: 0.000293, l4: 0.000492, l5: 0.000710, l6: 0.001641
[epoch: 453/1000, batch:    16/ 1052, ite: 118880] train loss: 0.004405, tar: 0.000136 
l0: 0.000101, l1: 0.000103, l2: 0.000118, l3: 0.000175, l4: 0.000369, l5: 0.000516, l6: 0.001842
[epoch: 453/1000, batch:    96/ 1052, ite: 118900] train loss: 0.004408, tar: 0.000136 
l0: 0.000058, l1: 0.000057, l2: 0.000101, l3: 0.000161, l4: 0.000220, l5: 0.000617, l6: 0.001202
[epoch: 453/1000, batch:   176/ 1052, ite: 118920] train loss: 0.004406, tar: 0.000136 
l0: 0.000076, l1: 0.000078, l2: 0.000114, l3: 0.000199, l4: 0.000339, l5: 0.001046, l6: 0.001073
[epoch: 453/1000, batch:   256/ 1052, ite: 118940] train loss: 0.004405, tar: 0.000136 
l0: 0.000154, l1: 0.000150, l2: 0.000177, l3: 0.000346, l4: 0.000661, l5: 0.001160, l6: 0.003299
[epoch: 453/1000, batch:   336/ 1052, ite: 118960] train loss: 0.004405, tar: 0.000136 
l0: 0.000132, l1: 0.000132, l2: 0.000187, l3: 0.000289, l4: 0.000431, l5: 0.001141, l6: 0.001739
[epoch: 453/1000, batch:   416/ 1052, ite: 118980] train loss: 0.004408, tar: 0.000136 
l0: 0.000122, l1: 0.000126, l2: 0.000198, l3: 0.000249, l4: 0.000497, l5: 0.000851, l6: 0.001742
[epoch: 453/1000, batch:   496/ 1052, ite: 119000] train loss: 0.004410, tar: 0.000136 
l0: 0.000069, l1: 0.000069, l2: 0.000104, l3: 0.000161, l4: 0.000463, l5: 0.000635, l6: 0.001588
[epoch: 453/1000, batch:   576/ 1052, ite: 119020] train loss: 0.004407, tar: 0.000136 
l0: 0.000176, l1: 0.000173, l2: 0.000244, l3: 0.000348, l4: 0.000678, l5: 0.001481, l6: 0.001918
[epoch: 453/1000, batch:   656/ 1052, ite: 119040] train loss: 0.004401, tar: 0.000135 
l0: 0.000083, l1: 0.000091, l2: 0.000115, l3: 0.000194, l4: 0.000504, l5: 0.001339, l6: 0.002142
[epoch: 453/1000, batch:   736/ 1052, ite: 119060] train loss: 0.004399, tar: 0.000135 
l0: 0.000087, l1: 0.000089, l2: 0.000121, l3: 0.000157, l4: 0.000322, l5: 0.000791, l6: 0.001175
[epoch: 453/1000, batch:   816/ 1052, ite: 119080] train loss: 0.004397, tar: 0.000135 
l0: 0.000062, l1: 0.000060, l2: 0.000092, l3: 0.000144, l4: 0.000345, l5: 0.000760, l6: 0.001704
[epoch: 453/1000, batch:   896/ 1052, ite: 119100] train loss: 0.004399, tar: 0.000135 
l0: 0.000097, l1: 0.000097, l2: 0.000124, l3: 0.000249, l4: 0.000469, l5: 0.001259, l6: 0.001344
[epoch: 453/1000, batch:   976/ 1052, ite: 119120] train loss: 0.004400, tar: 0.000135 
[Epoch 453/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000216, l1: 0.000219, l2: 0.000256, l3: 0.000453, l4: 0.000621, l5: 0.001295, l6: 0.003650
[epoch: 454/1000, batch:     4/ 1052, ite: 119140] train loss: 0.004400, tar: 0.000135 
l0: 0.000109, l1: 0.000109, l2: 0.000121, l3: 0.000189, l4: 0.000414, l5: 0.000845, l6: 0.001341
[epoch: 454/1000, batch:    84/ 1052, ite: 119160] train loss: 0.004399, tar: 0.000135 
l0: 0.000184, l1: 0.000188, l2: 0.000254, l3: 0.000409, l4: 0.000666, l5: 0.001663, l6: 0.002603
[epoch: 454/1000, batch:   164/ 1052, ite: 119180] train loss: 0.004403, tar: 0.000135 
l0: 0.000078, l1: 0.000078, l2: 0.000127, l3: 0.000172, l4: 0.000275, l5: 0.000506, l6: 0.001583
[epoch: 454/1000, batch:   244/ 1052, ite: 119200] train loss: 0.004406, tar: 0.000135 
l0: 0.000142, l1: 0.000146, l2: 0.000152, l3: 0.000245, l4: 0.000632, l5: 0.001443, l6: 0.004140
[epoch: 454/1000, batch:   324/ 1052, ite: 119220] train loss: 0.004404, tar: 0.000135 
l0: 0.000099, l1: 0.000099, l2: 0.000113, l3: 0.000174, l4: 0.000369, l5: 0.000663, l6: 0.001329
[epoch: 454/1000, batch:   404/ 1052, ite: 119240] train loss: 0.004401, tar: 0.000134 
l0: 0.000117, l1: 0.000116, l2: 0.000127, l3: 0.000255, l4: 0.000489, l5: 0.001177, l6: 0.001523
[epoch: 454/1000, batch:   484/ 1052, ite: 119260] train loss: 0.004399, tar: 0.000134 
l0: 0.000105, l1: 0.000104, l2: 0.000145, l3: 0.000192, l4: 0.000439, l5: 0.000916, l6: 0.001423
[epoch: 454/1000, batch:   564/ 1052, ite: 119280] train loss: 0.004397, tar: 0.000134 
l0: 0.000143, l1: 0.000145, l2: 0.000178, l3: 0.000281, l4: 0.000786, l5: 0.002975, l6: 0.004566
[epoch: 454/1000, batch:   644/ 1052, ite: 119300] train loss: 0.004399, tar: 0.000134 
l0: 0.000151, l1: 0.000149, l2: 0.000212, l3: 0.000322, l4: 0.000599, l5: 0.001553, l6: 0.002041
[epoch: 454/1000, batch:   724/ 1052, ite: 119320] train loss: 0.004395, tar: 0.000134 
l0: 0.000183, l1: 0.000188, l2: 0.000213, l3: 0.000267, l4: 0.000585, l5: 0.000966, l6: 0.003471
[epoch: 454/1000, batch:   804/ 1052, ite: 119340] train loss: 0.004399, tar: 0.000134 
l0: 0.000152, l1: 0.000153, l2: 0.000204, l3: 0.000492, l4: 0.000824, l5: 0.001431, l6: 0.002057
[epoch: 454/1000, batch:   884/ 1052, ite: 119360] train loss: 0.004399, tar: 0.000134 
l0: 0.000038, l1: 0.000039, l2: 0.000068, l3: 0.000136, l4: 0.000353, l5: 0.000539, l6: 0.001144
[epoch: 454/1000, batch:   964/ 1052, ite: 119380] train loss: 0.004396, tar: 0.000134 
l0: 0.000169, l1: 0.000176, l2: 0.000190, l3: 0.000225, l4: 0.000440, l5: 0.000782, l6: 0.002704
[epoch: 454/1000, batch:  1044/ 1052, ite: 119400] train loss: 0.004396, tar: 0.000134 
[Epoch 454/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000105, l1: 0.000105, l2: 0.000161, l3: 0.000255, l4: 0.000466, l5: 0.000922, l6: 0.001863
[epoch: 455/1000, batch:    72/ 1052, ite: 119420] train loss: 0.004394, tar: 0.000134 
l0: 0.000029, l1: 0.000030, l2: 0.000041, l3: 0.000106, l4: 0.000364, l5: 0.000935, l6: 0.001322
[epoch: 455/1000, batch:   152/ 1052, ite: 119440] train loss: 0.004391, tar: 0.000134 
l0: 0.000117, l1: 0.000119, l2: 0.000140, l3: 0.000182, l4: 0.000388, l5: 0.000737, l6: 0.001599
[epoch: 455/1000, batch:   232/ 1052, ite: 119460] train loss: 0.004391, tar: 0.000133 
l0: 0.000136, l1: 0.000140, l2: 0.000168, l3: 0.000343, l4: 0.000600, l5: 0.001174, l6: 0.001875
[epoch: 455/1000, batch:   312/ 1052, ite: 119480] train loss: 0.004398, tar: 0.000134 
l0: 0.000082, l1: 0.000082, l2: 0.000133, l3: 0.000170, l4: 0.000543, l5: 0.000611, l6: 0.001580
[epoch: 455/1000, batch:   392/ 1052, ite: 119500] train loss: 0.004396, tar: 0.000134 
l0: 0.000075, l1: 0.000075, l2: 0.000121, l3: 0.000215, l4: 0.000383, l5: 0.001022, l6: 0.002164
[epoch: 455/1000, batch:   472/ 1052, ite: 119520] train loss: 0.004394, tar: 0.000133 
l0: 0.000121, l1: 0.000123, l2: 0.000155, l3: 0.000363, l4: 0.000610, l5: 0.001169, l6: 0.001696
[epoch: 455/1000, batch:   552/ 1052, ite: 119540] train loss: 0.004393, tar: 0.000133 
l0: 0.000040, l1: 0.000040, l2: 0.000064, l3: 0.000130, l4: 0.000293, l5: 0.000837, l6: 0.001305
[epoch: 455/1000, batch:   632/ 1052, ite: 119560] train loss: 0.004392, tar: 0.000133 
l0: 0.000196, l1: 0.000198, l2: 0.000201, l3: 0.000294, l4: 0.000571, l5: 0.001088, l6: 0.002153
[epoch: 455/1000, batch:   712/ 1052, ite: 119580] train loss: 0.004390, tar: 0.000133 
l0: 0.000171, l1: 0.000170, l2: 0.000236, l3: 0.000345, l4: 0.000655, l5: 0.001226, l6: 0.002040
[epoch: 455/1000, batch:   792/ 1052, ite: 119600] train loss: 0.004392, tar: 0.000133 
l0: 0.000063, l1: 0.000063, l2: 0.000086, l3: 0.000108, l4: 0.000441, l5: 0.000676, l6: 0.001152
[epoch: 455/1000, batch:   872/ 1052, ite: 119620] train loss: 0.004390, tar: 0.000133 
l0: 0.000088, l1: 0.000087, l2: 0.000171, l3: 0.000247, l4: 0.000673, l5: 0.001020, l6: 0.001942
[epoch: 455/1000, batch:   952/ 1052, ite: 119640] train loss: 0.004390, tar: 0.000133 
l0: 0.000196, l1: 0.000199, l2: 0.000278, l3: 0.000393, l4: 0.000577, l5: 0.002076, l6: 0.003595
[epoch: 455/1000, batch:  1032/ 1052, ite: 119660] train loss: 0.004392, tar: 0.000133 
[Epoch 455/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000057, l1: 0.000056, l2: 0.000097, l3: 0.000136, l4: 0.000464, l5: 0.001300, l6: 0.001483
[epoch: 456/1000, batch:    60/ 1052, ite: 119680] train loss: 0.004389, tar: 0.000133 
l0: 0.000275, l1: 0.000271, l2: 0.000413, l3: 0.000533, l4: 0.000759, l5: 0.001238, l6: 0.003684
[epoch: 456/1000, batch:   140/ 1052, ite: 119700] train loss: 0.004390, tar: 0.000133 
l0: 0.000069, l1: 0.000071, l2: 0.000072, l3: 0.000145, l4: 0.000328, l5: 0.000852, l6: 0.001294
[epoch: 456/1000, batch:   220/ 1052, ite: 119720] train loss: 0.004387, tar: 0.000133 
l0: 0.000060, l1: 0.000059, l2: 0.000104, l3: 0.000237, l4: 0.000580, l5: 0.000878, l6: 0.002171
[epoch: 456/1000, batch:   300/ 1052, ite: 119740] train loss: 0.004386, tar: 0.000133 
l0: 0.000105, l1: 0.000106, l2: 0.000151, l3: 0.000410, l4: 0.000842, l5: 0.001548, l6: 0.002458
[epoch: 456/1000, batch:   380/ 1052, ite: 119760] train loss: 0.004387, tar: 0.000133 
l0: 0.000105, l1: 0.000108, l2: 0.000130, l3: 0.000257, l4: 0.000565, l5: 0.001444, l6: 0.002364
[epoch: 456/1000, batch:   460/ 1052, ite: 119780] train loss: 0.004388, tar: 0.000133 
l0: 0.000068, l1: 0.000069, l2: 0.000081, l3: 0.000126, l4: 0.000253, l5: 0.000417, l6: 0.000639
[epoch: 456/1000, batch:   540/ 1052, ite: 119800] train loss: 0.004389, tar: 0.000133 
l0: 0.000087, l1: 0.000093, l2: 0.000075, l3: 0.000107, l4: 0.000250, l5: 0.000450, l6: 0.001167
[epoch: 456/1000, batch:   620/ 1052, ite: 119820] train loss: 0.004387, tar: 0.000133 
l0: 0.000190, l1: 0.000187, l2: 0.000204, l3: 0.000336, l4: 0.000548, l5: 0.001284, l6: 0.002157
[epoch: 456/1000, batch:   700/ 1052, ite: 119840] train loss: 0.004384, tar: 0.000133 
l0: 0.000389, l1: 0.000405, l2: 0.000397, l3: 0.000592, l4: 0.001249, l5: 0.001930, l6: 0.004006
[epoch: 456/1000, batch:   780/ 1052, ite: 119860] train loss: 0.004387, tar: 0.000133 
l0: 0.000130, l1: 0.000128, l2: 0.000176, l3: 0.000269, l4: 0.000435, l5: 0.000971, l6: 0.002119
[epoch: 456/1000, batch:   860/ 1052, ite: 119880] train loss: 0.004385, tar: 0.000133 
l0: 0.000178, l1: 0.000179, l2: 0.000206, l3: 0.000335, l4: 0.000792, l5: 0.001472, l6: 0.002252
[epoch: 456/1000, batch:   940/ 1052, ite: 119900] train loss: 0.004385, tar: 0.000133 
l0: 0.000130, l1: 0.000134, l2: 0.000146, l3: 0.000250, l4: 0.000487, l5: 0.001152, l6: 0.001620
[epoch: 456/1000, batch:  1020/ 1052, ite: 119920] train loss: 0.004386, tar: 0.000133 
[Epoch 456/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000286, l1: 0.000289, l2: 0.000315, l3: 0.000363, l4: 0.000690, l5: 0.001104, l6: 0.002176
[epoch: 457/1000, batch:    48/ 1052, ite: 119940] train loss: 0.004385, tar: 0.000133 
l0: 0.000028, l1: 0.000027, l2: 0.000049, l3: 0.000112, l4: 0.000244, l5: 0.000501, l6: 0.000749
[epoch: 457/1000, batch:   128/ 1052, ite: 119960] train loss: 0.004384, tar: 0.000133 
l0: 0.000191, l1: 0.000200, l2: 0.000228, l3: 0.000333, l4: 0.000524, l5: 0.001263, l6: 0.003224
[epoch: 457/1000, batch:   208/ 1052, ite: 119980] train loss: 0.004384, tar: 0.000132 
l0: 0.000181, l1: 0.000184, l2: 0.000267, l3: 0.000450, l4: 0.000653, l5: 0.001483, l6: 0.003423
[epoch: 457/1000, batch:   288/ 1052, ite: 120000] train loss: 0.004383, tar: 0.000132 
l0: 0.000100, l1: 0.000100, l2: 0.000125, l3: 0.000193, l4: 0.000610, l5: 0.001184, l6: 0.002255
[epoch: 457/1000, batch:   368/ 1052, ite: 120020] train loss: 0.004385, tar: 0.000132 
l0: 0.000144, l1: 0.000146, l2: 0.000212, l3: 0.000367, l4: 0.000651, l5: 0.001198, l6: 0.001658
[epoch: 457/1000, batch:   448/ 1052, ite: 120040] train loss: 0.004386, tar: 0.000132 
l0: 0.000137, l1: 0.000137, l2: 0.000164, l3: 0.000297, l4: 0.000578, l5: 0.001202, l6: 0.002212
[epoch: 457/1000, batch:   528/ 1052, ite: 120060] train loss: 0.004385, tar: 0.000132 
l0: 0.000065, l1: 0.000067, l2: 0.000068, l3: 0.000123, l4: 0.000239, l5: 0.000850, l6: 0.001040
[epoch: 457/1000, batch:   608/ 1052, ite: 120080] train loss: 0.004382, tar: 0.000132 
l0: 0.000338, l1: 0.000336, l2: 0.000392, l3: 0.000438, l4: 0.000771, l5: 0.001511, l6: 0.002856
[epoch: 457/1000, batch:   688/ 1052, ite: 120100] train loss: 0.004383, tar: 0.000132 
l0: 0.000063, l1: 0.000062, l2: 0.000101, l3: 0.000241, l4: 0.000627, l5: 0.001391, l6: 0.002706
[epoch: 457/1000, batch:   768/ 1052, ite: 120120] train loss: 0.004381, tar: 0.000132 
l0: 0.000107, l1: 0.000107, l2: 0.000153, l3: 0.000267, l4: 0.000600, l5: 0.000810, l6: 0.001850
[epoch: 457/1000, batch:   848/ 1052, ite: 120140] train loss: 0.004382, tar: 0.000132 
l0: 0.000223, l1: 0.000221, l2: 0.000277, l3: 0.000359, l4: 0.000752, l5: 0.001451, l6: 0.002256
[epoch: 457/1000, batch:   928/ 1052, ite: 120160] train loss: 0.004380, tar: 0.000132 
l0: 0.000081, l1: 0.000079, l2: 0.000115, l3: 0.000173, l4: 0.000364, l5: 0.000711, l6: 0.001437
[epoch: 457/1000, batch:  1008/ 1052, ite: 120180] train loss: 0.004383, tar: 0.000132 
[Epoch 457/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000156, l1: 0.000160, l2: 0.000161, l3: 0.000214, l4: 0.000582, l5: 0.001528, l6: 0.002594
[epoch: 458/1000, batch:    36/ 1052, ite: 120200] train loss: 0.004382, tar: 0.000132 
l0: 0.000113, l1: 0.000115, l2: 0.000142, l3: 0.000208, l4: 0.000306, l5: 0.000778, l6: 0.001460
[epoch: 458/1000, batch:   116/ 1052, ite: 120220] train loss: 0.004381, tar: 0.000132 
l0: 0.000124, l1: 0.000120, l2: 0.000177, l3: 0.000333, l4: 0.000534, l5: 0.001432, l6: 0.002604
[epoch: 458/1000, batch:   196/ 1052, ite: 120240] train loss: 0.004379, tar: 0.000132 
l0: 0.000441, l1: 0.000446, l2: 0.000587, l3: 0.000740, l4: 0.001195, l5: 0.003028, l6: 0.006156
[epoch: 458/1000, batch:   276/ 1052, ite: 120260] train loss: 0.004382, tar: 0.000132 
l0: 0.000098, l1: 0.000095, l2: 0.000143, l3: 0.000233, l4: 0.000388, l5: 0.000941, l6: 0.002008
[epoch: 458/1000, batch:   356/ 1052, ite: 120280] train loss: 0.004382, tar: 0.000132 
l0: 0.000098, l1: 0.000100, l2: 0.000151, l3: 0.000199, l4: 0.000394, l5: 0.000930, l6: 0.002185
[epoch: 458/1000, batch:   436/ 1052, ite: 120300] train loss: 0.004383, tar: 0.000132 
l0: 0.000097, l1: 0.000099, l2: 0.000114, l3: 0.000192, l4: 0.000506, l5: 0.001427, l6: 0.002627
[epoch: 458/1000, batch:   516/ 1052, ite: 120320] train loss: 0.004384, tar: 0.000132 
l0: 0.000074, l1: 0.000076, l2: 0.000129, l3: 0.000198, l4: 0.000517, l5: 0.001237, l6: 0.002234
[epoch: 458/1000, batch:   596/ 1052, ite: 120340] train loss: 0.004384, tar: 0.000132 
l0: 0.000107, l1: 0.000109, l2: 0.000144, l3: 0.000234, l4: 0.000422, l5: 0.001449, l6: 0.001795
[epoch: 458/1000, batch:   676/ 1052, ite: 120360] train loss: 0.004384, tar: 0.000131 
l0: 0.000126, l1: 0.000123, l2: 0.000193, l3: 0.000370, l4: 0.000647, l5: 0.002150, l6: 0.004366
[epoch: 458/1000, batch:   756/ 1052, ite: 120380] train loss: 0.004385, tar: 0.000132 
l0: 0.000093, l1: 0.000095, l2: 0.000155, l3: 0.000253, l4: 0.000450, l5: 0.001284, l6: 0.001633
[epoch: 458/1000, batch:   836/ 1052, ite: 120400] train loss: 0.004384, tar: 0.000132 
l0: 0.000106, l1: 0.000108, l2: 0.000121, l3: 0.000211, l4: 0.000361, l5: 0.000985, l6: 0.001201
[epoch: 458/1000, batch:   916/ 1052, ite: 120420] train loss: 0.004383, tar: 0.000131 
l0: 0.000178, l1: 0.000178, l2: 0.000237, l3: 0.000370, l4: 0.000435, l5: 0.001134, l6: 0.001610
[epoch: 458/1000, batch:   996/ 1052, ite: 120440] train loss: 0.004382, tar: 0.000131 
[Epoch 458/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000205, l1: 0.000205, l2: 0.000269, l3: 0.000288, l4: 0.000546, l5: 0.000816, l6: 0.001707
[epoch: 459/1000, batch:    24/ 1052, ite: 120460] train loss: 0.004381, tar: 0.000131 
l0: 0.000091, l1: 0.000090, l2: 0.000145, l3: 0.000325, l4: 0.000609, l5: 0.001106, l6: 0.001779
[epoch: 459/1000, batch:   104/ 1052, ite: 120480] train loss: 0.004381, tar: 0.000131 
l0: 0.000082, l1: 0.000085, l2: 0.000102, l3: 0.000163, l4: 0.000271, l5: 0.000388, l6: 0.000756
[epoch: 459/1000, batch:   184/ 1052, ite: 120500] train loss: 0.004380, tar: 0.000131 
l0: 0.000173, l1: 0.000173, l2: 0.000213, l3: 0.000327, l4: 0.000480, l5: 0.001053, l6: 0.001753
[epoch: 459/1000, batch:   264/ 1052, ite: 120520] train loss: 0.004380, tar: 0.000131 
l0: 0.000106, l1: 0.000101, l2: 0.000164, l3: 0.000298, l4: 0.000681, l5: 0.001655, l6: 0.002024
[epoch: 459/1000, batch:   344/ 1052, ite: 120540] train loss: 0.004378, tar: 0.000131 
l0: 0.000148, l1: 0.000144, l2: 0.000228, l3: 0.000278, l4: 0.000463, l5: 0.001583, l6: 0.002282
[epoch: 459/1000, batch:   424/ 1052, ite: 120560] train loss: 0.004378, tar: 0.000131 
l0: 0.000108, l1: 0.000108, l2: 0.000124, l3: 0.000208, l4: 0.000587, l5: 0.001232, l6: 0.002640
[epoch: 459/1000, batch:   504/ 1052, ite: 120580] train loss: 0.004376, tar: 0.000131 
l0: 0.000086, l1: 0.000088, l2: 0.000137, l3: 0.000232, l4: 0.000364, l5: 0.001434, l6: 0.002213
[epoch: 459/1000, batch:   584/ 1052, ite: 120600] train loss: 0.004378, tar: 0.000131 
l0: 0.000070, l1: 0.000069, l2: 0.000127, l3: 0.000138, l4: 0.000448, l5: 0.000839, l6: 0.001402
[epoch: 459/1000, batch:   664/ 1052, ite: 120620] train loss: 0.004377, tar: 0.000131 
l0: 0.000107, l1: 0.000107, l2: 0.000128, l3: 0.000234, l4: 0.000379, l5: 0.001154, l6: 0.001425
[epoch: 459/1000, batch:   744/ 1052, ite: 120640] train loss: 0.004375, tar: 0.000131 
l0: 0.000107, l1: 0.000112, l2: 0.000112, l3: 0.000143, l4: 0.000280, l5: 0.001036, l6: 0.001548
[epoch: 459/1000, batch:   824/ 1052, ite: 120660] train loss: 0.004376, tar: 0.000131 
l0: 0.000102, l1: 0.000104, l2: 0.000099, l3: 0.000216, l4: 0.000707, l5: 0.001355, l6: 0.001992
[epoch: 459/1000, batch:   904/ 1052, ite: 120680] train loss: 0.004376, tar: 0.000131 
l0: 0.000159, l1: 0.000155, l2: 0.000175, l3: 0.000286, l4: 0.000584, l5: 0.001062, l6: 0.001730
[epoch: 459/1000, batch:   984/ 1052, ite: 120700] train loss: 0.004377, tar: 0.000131 
[Epoch 459/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000078, l1: 0.000078, l2: 0.000107, l3: 0.000126, l4: 0.000277, l5: 0.000759, l6: 0.000939
[epoch: 460/1000, batch:    12/ 1052, ite: 120720] train loss: 0.004376, tar: 0.000131 
l0: 0.000087, l1: 0.000088, l2: 0.000116, l3: 0.000203, l4: 0.000413, l5: 0.001239, l6: 0.002750
[epoch: 460/1000, batch:    92/ 1052, ite: 120740] train loss: 0.004376, tar: 0.000131 
l0: 0.000025, l1: 0.000026, l2: 0.000046, l3: 0.000127, l4: 0.000383, l5: 0.000460, l6: 0.001335
[epoch: 460/1000, batch:   172/ 1052, ite: 120760] train loss: 0.004375, tar: 0.000131 
l0: 0.000231, l1: 0.000222, l2: 0.000367, l3: 0.000553, l4: 0.001082, l5: 0.001938, l6: 0.003939
[epoch: 460/1000, batch:   252/ 1052, ite: 120780] train loss: 0.004376, tar: 0.000131 
l0: 0.000166, l1: 0.000174, l2: 0.000195, l3: 0.000304, l4: 0.000646, l5: 0.000835, l6: 0.002153
[epoch: 460/1000, batch:   332/ 1052, ite: 120800] train loss: 0.004374, tar: 0.000131 
l0: 0.000124, l1: 0.000128, l2: 0.000192, l3: 0.000267, l4: 0.000553, l5: 0.001292, l6: 0.001957
[epoch: 460/1000, batch:   412/ 1052, ite: 120820] train loss: 0.004374, tar: 0.000131 
l0: 0.000192, l1: 0.000192, l2: 0.000275, l3: 0.000359, l4: 0.000953, l5: 0.002026, l6: 0.003044
[epoch: 460/1000, batch:   492/ 1052, ite: 120840] train loss: 0.004373, tar: 0.000131 
l0: 0.000127, l1: 0.000125, l2: 0.000172, l3: 0.000218, l4: 0.000545, l5: 0.000895, l6: 0.001837
[epoch: 460/1000, batch:   572/ 1052, ite: 120860] train loss: 0.004374, tar: 0.000131 
l0: 0.000124, l1: 0.000125, l2: 0.000179, l3: 0.000403, l4: 0.000963, l5: 0.001668, l6: 0.002293
[epoch: 460/1000, batch:   652/ 1052, ite: 120880] train loss: 0.004375, tar: 0.000131 
l0: 0.000182, l1: 0.000187, l2: 0.000237, l3: 0.000293, l4: 0.000536, l5: 0.001089, l6: 0.001963
[epoch: 460/1000, batch:   732/ 1052, ite: 120900] train loss: 0.004375, tar: 0.000131 
l0: 0.000125, l1: 0.000125, l2: 0.000135, l3: 0.000211, l4: 0.000307, l5: 0.000851, l6: 0.001567
[epoch: 460/1000, batch:   812/ 1052, ite: 120920] train loss: 0.004375, tar: 0.000130 
l0: 0.000125, l1: 0.000126, l2: 0.000211, l3: 0.000276, l4: 0.000420, l5: 0.001134, l6: 0.001432
[epoch: 460/1000, batch:   892/ 1052, ite: 120940] train loss: 0.004375, tar: 0.000131 
l0: 0.000214, l1: 0.000220, l2: 0.000287, l3: 0.000471, l4: 0.000843, l5: 0.001733, l6: 0.004198
[epoch: 460/1000, batch:   972/ 1052, ite: 120960] train loss: 0.004376, tar: 0.000131 
l0: 0.000069, l1: 0.000068, l2: 0.000085, l3: 0.000191, l4: 0.000353, l5: 0.000859, l6: 0.001133
[epoch: 460/1000, batch:  1052/ 1052, ite: 120980] train loss: 0.004374, tar: 0.000131 
[Epoch 460/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000125, l1: 0.000125, l2: 0.000195, l3: 0.000273, l4: 0.000682, l5: 0.001235, l6: 0.002147
[epoch: 461/1000, batch:    80/ 1052, ite: 121000] train loss: 0.004373, tar: 0.000130 
l0: 0.000121, l1: 0.000124, l2: 0.000151, l3: 0.000246, l4: 0.000486, l5: 0.001476, l6: 0.003607
[epoch: 461/1000, batch:   160/ 1052, ite: 121020] train loss: 0.004372, tar: 0.000130 
l0: 0.000221, l1: 0.000224, l2: 0.000246, l3: 0.000340, l4: 0.000404, l5: 0.001101, l6: 0.002485
[epoch: 461/1000, batch:   240/ 1052, ite: 121040] train loss: 0.004370, tar: 0.000130 
l0: 0.000092, l1: 0.000094, l2: 0.000138, l3: 0.000274, l4: 0.000445, l5: 0.000920, l6: 0.002170
[epoch: 461/1000, batch:   320/ 1052, ite: 121060] train loss: 0.004370, tar: 0.000130 
l0: 0.000123, l1: 0.000130, l2: 0.000200, l3: 0.000334, l4: 0.000726, l5: 0.001750, l6: 0.002505
[epoch: 461/1000, batch:   400/ 1052, ite: 121080] train loss: 0.004370, tar: 0.000130 
l0: 0.000224, l1: 0.000225, l2: 0.000277, l3: 0.000305, l4: 0.000618, l5: 0.001411, l6: 0.002825
[epoch: 461/1000, batch:   480/ 1052, ite: 121100] train loss: 0.004373, tar: 0.000130 
l0: 0.000080, l1: 0.000083, l2: 0.000100, l3: 0.000215, l4: 0.000372, l5: 0.000919, l6: 0.002205
[epoch: 461/1000, batch:   560/ 1052, ite: 121120] train loss: 0.004374, tar: 0.000130 
l0: 0.000144, l1: 0.000146, l2: 0.000153, l3: 0.000251, l4: 0.000678, l5: 0.001590, l6: 0.002665
[epoch: 461/1000, batch:   640/ 1052, ite: 121140] train loss: 0.004376, tar: 0.000130 
l0: 0.000192, l1: 0.000198, l2: 0.000189, l3: 0.000314, l4: 0.000733, l5: 0.001250, l6: 0.002823
[epoch: 461/1000, batch:   720/ 1052, ite: 121160] train loss: 0.004375, tar: 0.000130 
l0: 0.000149, l1: 0.000142, l2: 0.000163, l3: 0.000224, l4: 0.000516, l5: 0.000802, l6: 0.001269
[epoch: 461/1000, batch:   800/ 1052, ite: 121180] train loss: 0.004374, tar: 0.000130 
l0: 0.000205, l1: 0.000204, l2: 0.000237, l3: 0.000287, l4: 0.000670, l5: 0.001281, l6: 0.003003
[epoch: 461/1000, batch:   880/ 1052, ite: 121200] train loss: 0.004375, tar: 0.000130 
l0: 0.000149, l1: 0.000152, l2: 0.000279, l3: 0.000525, l4: 0.001100, l5: 0.001881, l6: 0.003013
[epoch: 461/1000, batch:   960/ 1052, ite: 121220] train loss: 0.004374, tar: 0.000130 
l0: 0.000117, l1: 0.000120, l2: 0.000129, l3: 0.000142, l4: 0.000295, l5: 0.000535, l6: 0.000608
[epoch: 461/1000, batch:  1040/ 1052, ite: 121240] train loss: 0.004374, tar: 0.000130 
[Epoch 461/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000088, l1: 0.000090, l2: 0.000142, l3: 0.000258, l4: 0.000672, l5: 0.001354, l6: 0.002259
[epoch: 462/1000, batch:    68/ 1052, ite: 121260] train loss: 0.004376, tar: 0.000130 
l0: 0.000043, l1: 0.000043, l2: 0.000082, l3: 0.000111, l4: 0.000236, l5: 0.000613, l6: 0.000755
[epoch: 462/1000, batch:   148/ 1052, ite: 121280] train loss: 0.004377, tar: 0.000130 
l0: 0.000055, l1: 0.000054, l2: 0.000061, l3: 0.000107, l4: 0.000222, l5: 0.000398, l6: 0.000853
[epoch: 462/1000, batch:   228/ 1052, ite: 121300] train loss: 0.004377, tar: 0.000130 
l0: 0.000070, l1: 0.000073, l2: 0.000099, l3: 0.000155, l4: 0.000298, l5: 0.000633, l6: 0.001272
[epoch: 462/1000, batch:   308/ 1052, ite: 121320] train loss: 0.004377, tar: 0.000130 
l0: 0.000124, l1: 0.000125, l2: 0.000168, l3: 0.000215, l4: 0.000474, l5: 0.001050, l6: 0.001987
[epoch: 462/1000, batch:   388/ 1052, ite: 121340] train loss: 0.004377, tar: 0.000130 
l0: 0.000057, l1: 0.000056, l2: 0.000095, l3: 0.000117, l4: 0.000382, l5: 0.000664, l6: 0.001001
[epoch: 462/1000, batch:   468/ 1052, ite: 121360] train loss: 0.004376, tar: 0.000130 
l0: 0.000118, l1: 0.000119, l2: 0.000141, l3: 0.000222, l4: 0.000488, l5: 0.000957, l6: 0.001669
[epoch: 462/1000, batch:   548/ 1052, ite: 121380] train loss: 0.004374, tar: 0.000130 
l0: 0.000087, l1: 0.000087, l2: 0.000132, l3: 0.000271, l4: 0.000540, l5: 0.001396, l6: 0.002824
[epoch: 462/1000, batch:   628/ 1052, ite: 121400] train loss: 0.004375, tar: 0.000130 
l0: 0.000114, l1: 0.000116, l2: 0.000129, l3: 0.000183, l4: 0.000311, l5: 0.000827, l6: 0.000949
[epoch: 462/1000, batch:   708/ 1052, ite: 121420] train loss: 0.004372, tar: 0.000130 
l0: 0.000140, l1: 0.000141, l2: 0.000192, l3: 0.000252, l4: 0.000565, l5: 0.000846, l6: 0.001661
[epoch: 462/1000, batch:   788/ 1052, ite: 121440] train loss: 0.004373, tar: 0.000130 
l0: 0.000060, l1: 0.000061, l2: 0.000060, l3: 0.000084, l4: 0.000108, l5: 0.000394, l6: 0.000391
[epoch: 462/1000, batch:   868/ 1052, ite: 121460] train loss: 0.004372, tar: 0.000130 
l0: 0.000025, l1: 0.000026, l2: 0.000027, l3: 0.000064, l4: 0.000290, l5: 0.000385, l6: 0.000755
[epoch: 462/1000, batch:   948/ 1052, ite: 121480] train loss: 0.004372, tar: 0.000130 
l0: 0.000069, l1: 0.000069, l2: 0.000108, l3: 0.000172, l4: 0.000486, l5: 0.001331, l6: 0.001791
[epoch: 462/1000, batch:  1028/ 1052, ite: 121500] train loss: 0.004372, tar: 0.000130 
[Epoch 462/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000141, l1: 0.000140, l2: 0.000215, l3: 0.000286, l4: 0.000697, l5: 0.001076, l6: 0.002863
[epoch: 463/1000, batch:    56/ 1052, ite: 121520] train loss: 0.004373, tar: 0.000130 
l0: 0.000093, l1: 0.000092, l2: 0.000121, l3: 0.000164, l4: 0.000198, l5: 0.000755, l6: 0.000959
[epoch: 463/1000, batch:   136/ 1052, ite: 121540] train loss: 0.004371, tar: 0.000130 
l0: 0.000034, l1: 0.000032, l2: 0.000077, l3: 0.000187, l4: 0.000310, l5: 0.000699, l6: 0.001497
[epoch: 463/1000, batch:   216/ 1052, ite: 121560] train loss: 0.004369, tar: 0.000130 
l0: 0.000338, l1: 0.000340, l2: 0.000422, l3: 0.000617, l4: 0.001274, l5: 0.001767, l6: 0.003748
[epoch: 463/1000, batch:   296/ 1052, ite: 121580] train loss: 0.004372, tar: 0.000130 
l0: 0.000099, l1: 0.000104, l2: 0.000126, l3: 0.000223, l4: 0.000412, l5: 0.000624, l6: 0.001884
[epoch: 463/1000, batch:   376/ 1052, ite: 121600] train loss: 0.004374, tar: 0.000130 
l0: 0.000172, l1: 0.000174, l2: 0.000193, l3: 0.000281, l4: 0.000424, l5: 0.000779, l6: 0.002462
[epoch: 463/1000, batch:   456/ 1052, ite: 121620] train loss: 0.004372, tar: 0.000130 
l0: 0.000128, l1: 0.000126, l2: 0.000206, l3: 0.000340, l4: 0.000887, l5: 0.001563, l6: 0.002664
[epoch: 463/1000, batch:   536/ 1052, ite: 121640] train loss: 0.004371, tar: 0.000130 
l0: 0.000104, l1: 0.000104, l2: 0.000150, l3: 0.000240, l4: 0.000555, l5: 0.002126, l6: 0.002436
[epoch: 463/1000, batch:   616/ 1052, ite: 121660] train loss: 0.004371, tar: 0.000130 
l0: 0.000231, l1: 0.000233, l2: 0.000237, l3: 0.000283, l4: 0.000636, l5: 0.001394, l6: 0.002073
[epoch: 463/1000, batch:   696/ 1052, ite: 121680] train loss: 0.004370, tar: 0.000130 
l0: 0.000171, l1: 0.000167, l2: 0.000232, l3: 0.000360, l4: 0.000559, l5: 0.000986, l6: 0.001877
[epoch: 463/1000, batch:   776/ 1052, ite: 121700] train loss: 0.004369, tar: 0.000130 
l0: 0.000119, l1: 0.000122, l2: 0.000216, l3: 0.000308, l4: 0.000715, l5: 0.001206, l6: 0.003190
[epoch: 463/1000, batch:   856/ 1052, ite: 121720] train loss: 0.004369, tar: 0.000130 
l0: 0.000099, l1: 0.000099, l2: 0.000173, l3: 0.000299, l4: 0.000544, l5: 0.001364, l6: 0.002480
[epoch: 463/1000, batch:   936/ 1052, ite: 121740] train loss: 0.004370, tar: 0.000130 
l0: 0.000109, l1: 0.000112, l2: 0.000121, l3: 0.000155, l4: 0.000437, l5: 0.000843, l6: 0.001653
[epoch: 463/1000, batch:  1016/ 1052, ite: 121760] train loss: 0.004370, tar: 0.000130 
[Epoch 463/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000125, l1: 0.000127, l2: 0.000139, l3: 0.000184, l4: 0.000321, l5: 0.001049, l6: 0.001914
[epoch: 464/1000, batch:    44/ 1052, ite: 121780] train loss: 0.004370, tar: 0.000130 
l0: 0.000073, l1: 0.000071, l2: 0.000100, l3: 0.000178, l4: 0.000343, l5: 0.000704, l6: 0.001453
[epoch: 464/1000, batch:   124/ 1052, ite: 121800] train loss: 0.004372, tar: 0.000130 
l0: 0.000068, l1: 0.000068, l2: 0.000098, l3: 0.000148, l4: 0.000370, l5: 0.000735, l6: 0.001115
[epoch: 464/1000, batch:   204/ 1052, ite: 121820] train loss: 0.004372, tar: 0.000130 
l0: 0.000030, l1: 0.000029, l2: 0.000080, l3: 0.000166, l4: 0.000411, l5: 0.000885, l6: 0.002281
[epoch: 464/1000, batch:   284/ 1052, ite: 121840] train loss: 0.004371, tar: 0.000130 
l0: 0.000171, l1: 0.000178, l2: 0.000178, l3: 0.000245, l4: 0.000443, l5: 0.000613, l6: 0.000964
[epoch: 464/1000, batch:   364/ 1052, ite: 121860] train loss: 0.004371, tar: 0.000130 
l0: 0.000168, l1: 0.000170, l2: 0.000195, l3: 0.000225, l4: 0.000531, l5: 0.001163, l6: 0.002194
[epoch: 464/1000, batch:   444/ 1052, ite: 121880] train loss: 0.004371, tar: 0.000130 
l0: 0.000156, l1: 0.000158, l2: 0.000165, l3: 0.000265, l4: 0.000601, l5: 0.001320, l6: 0.001438
[epoch: 464/1000, batch:   524/ 1052, ite: 121900] train loss: 0.004372, tar: 0.000130 
l0: 0.000064, l1: 0.000062, l2: 0.000113, l3: 0.000165, l4: 0.000343, l5: 0.000994, l6: 0.001104
[epoch: 464/1000, batch:   604/ 1052, ite: 121920] train loss: 0.004368, tar: 0.000130 
l0: 0.000209, l1: 0.000209, l2: 0.000274, l3: 0.000409, l4: 0.000809, l5: 0.001571, l6: 0.002763
[epoch: 464/1000, batch:   684/ 1052, ite: 121940] train loss: 0.004368, tar: 0.000130 
l0: 0.000085, l1: 0.000085, l2: 0.000128, l3: 0.000201, l4: 0.000454, l5: 0.000799, l6: 0.001015
[epoch: 464/1000, batch:   764/ 1052, ite: 121960] train loss: 0.004369, tar: 0.000130 
l0: 0.000063, l1: 0.000070, l2: 0.000068, l3: 0.000127, l4: 0.000472, l5: 0.001193, l6: 0.001316
[epoch: 464/1000, batch:   844/ 1052, ite: 121980] train loss: 0.004368, tar: 0.000130 
l0: 0.000078, l1: 0.000077, l2: 0.000116, l3: 0.000189, l4: 0.000450, l5: 0.000851, l6: 0.001393
[epoch: 464/1000, batch:   924/ 1052, ite: 122000] train loss: 0.004367, tar: 0.000130 
l0: 0.000079, l1: 0.000077, l2: 0.000125, l3: 0.000171, l4: 0.000354, l5: 0.000542, l6: 0.000996
[epoch: 464/1000, batch:  1004/ 1052, ite: 122020] train loss: 0.004367, tar: 0.000129 
[Epoch 464/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000181, l1: 0.000185, l2: 0.000235, l3: 0.000336, l4: 0.000838, l5: 0.001849, l6: 0.002582
[epoch: 465/1000, batch:    32/ 1052, ite: 122040] train loss: 0.004365, tar: 0.000129 
l0: 0.000068, l1: 0.000068, l2: 0.000087, l3: 0.000171, l4: 0.000348, l5: 0.000776, l6: 0.001683
[epoch: 465/1000, batch:   112/ 1052, ite: 122060] train loss: 0.004366, tar: 0.000129 
l0: 0.000078, l1: 0.000077, l2: 0.000122, l3: 0.000293, l4: 0.000469, l5: 0.001067, l6: 0.001784
[epoch: 465/1000, batch:   192/ 1052, ite: 122080] train loss: 0.004368, tar: 0.000129 
l0: 0.000178, l1: 0.000184, l2: 0.000233, l3: 0.000347, l4: 0.000623, l5: 0.001334, l6: 0.002242
[epoch: 465/1000, batch:   272/ 1052, ite: 122100] train loss: 0.004368, tar: 0.000129 
l0: 0.000084, l1: 0.000083, l2: 0.000118, l3: 0.000203, l4: 0.000428, l5: 0.001563, l6: 0.002113
[epoch: 465/1000, batch:   352/ 1052, ite: 122120] train loss: 0.004368, tar: 0.000129 
l0: 0.000133, l1: 0.000133, l2: 0.000212, l3: 0.000255, l4: 0.000577, l5: 0.001750, l6: 0.001966
[epoch: 465/1000, batch:   432/ 1052, ite: 122140] train loss: 0.004368, tar: 0.000129 
l0: 0.000088, l1: 0.000087, l2: 0.000116, l3: 0.000178, l4: 0.000486, l5: 0.000814, l6: 0.001364
[epoch: 465/1000, batch:   512/ 1052, ite: 122160] train loss: 0.004365, tar: 0.000129 
l0: 0.000109, l1: 0.000108, l2: 0.000132, l3: 0.000242, l4: 0.000520, l5: 0.000850, l6: 0.002134
[epoch: 465/1000, batch:   592/ 1052, ite: 122180] train loss: 0.004366, tar: 0.000129 
l0: 0.000162, l1: 0.000166, l2: 0.000192, l3: 0.000247, l4: 0.000331, l5: 0.000850, l6: 0.002365
[epoch: 465/1000, batch:   672/ 1052, ite: 122200] train loss: 0.004366, tar: 0.000129 
l0: 0.000127, l1: 0.000123, l2: 0.000183, l3: 0.000336, l4: 0.000581, l5: 0.001364, l6: 0.001801
[epoch: 465/1000, batch:   752/ 1052, ite: 122220] train loss: 0.004367, tar: 0.000129 
l0: 0.000016, l1: 0.000015, l2: 0.000059, l3: 0.000091, l4: 0.000316, l5: 0.000742, l6: 0.001034
[epoch: 465/1000, batch:   832/ 1052, ite: 122240] train loss: 0.004365, tar: 0.000129 
l0: 0.000214, l1: 0.000214, l2: 0.000244, l3: 0.000278, l4: 0.000448, l5: 0.001397, l6: 0.002105
[epoch: 465/1000, batch:   912/ 1052, ite: 122260] train loss: 0.004365, tar: 0.000129 
l0: 0.000273, l1: 0.000283, l2: 0.000349, l3: 0.000506, l4: 0.000952, l5: 0.002200, l6: 0.003089
[epoch: 465/1000, batch:   992/ 1052, ite: 122280] train loss: 0.004367, tar: 0.000129 
[Epoch 465/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000242, l1: 0.000243, l2: 0.000244, l3: 0.000345, l4: 0.000552, l5: 0.001536, l6: 0.002057
[epoch: 466/1000, batch:    20/ 1052, ite: 122300] train loss: 0.004367, tar: 0.000129 
l0: 0.000039, l1: 0.000039, l2: 0.000049, l3: 0.000155, l4: 0.000333, l5: 0.000882, l6: 0.001022
[epoch: 466/1000, batch:   100/ 1052, ite: 122320] train loss: 0.004367, tar: 0.000129 
l0: 0.000086, l1: 0.000084, l2: 0.000116, l3: 0.000281, l4: 0.000726, l5: 0.001562, l6: 0.002484
[epoch: 466/1000, batch:   180/ 1052, ite: 122340] train loss: 0.004364, tar: 0.000129 
l0: 0.000232, l1: 0.000228, l2: 0.000306, l3: 0.000321, l4: 0.000437, l5: 0.001175, l6: 0.003111
[epoch: 466/1000, batch:   260/ 1052, ite: 122360] train loss: 0.004363, tar: 0.000129 
l0: 0.000136, l1: 0.000135, l2: 0.000202, l3: 0.000261, l4: 0.000502, l5: 0.000903, l6: 0.001407
[epoch: 466/1000, batch:   340/ 1052, ite: 122380] train loss: 0.004362, tar: 0.000129 
l0: 0.000060, l1: 0.000061, l2: 0.000090, l3: 0.000152, l4: 0.000317, l5: 0.000966, l6: 0.001527
[epoch: 466/1000, batch:   420/ 1052, ite: 122400] train loss: 0.004361, tar: 0.000129 
l0: 0.000106, l1: 0.000106, l2: 0.000137, l3: 0.000200, l4: 0.000386, l5: 0.001074, l6: 0.001744
[epoch: 466/1000, batch:   500/ 1052, ite: 122420] train loss: 0.004362, tar: 0.000129 
l0: 0.000034, l1: 0.000034, l2: 0.000054, l3: 0.000117, l4: 0.000430, l5: 0.000777, l6: 0.001510
[epoch: 466/1000, batch:   580/ 1052, ite: 122440] train loss: 0.004364, tar: 0.000129 
l0: 0.000252, l1: 0.000259, l2: 0.000314, l3: 0.000577, l4: 0.001204, l5: 0.001996, l6: 0.004753
[epoch: 466/1000, batch:   660/ 1052, ite: 122460] train loss: 0.004365, tar: 0.000129 
l0: 0.000052, l1: 0.000051, l2: 0.000089, l3: 0.000345, l4: 0.000839, l5: 0.001225, l6: 0.001521
[epoch: 466/1000, batch:   740/ 1052, ite: 122480] train loss: 0.004366, tar: 0.000129 
l0: 0.000032, l1: 0.000034, l2: 0.000046, l3: 0.000071, l4: 0.000231, l5: 0.000549, l6: 0.000679
[epoch: 466/1000, batch:   820/ 1052, ite: 122500] train loss: 0.004364, tar: 0.000129 
l0: 0.000046, l1: 0.000045, l2: 0.000080, l3: 0.000139, l4: 0.000308, l5: 0.000690, l6: 0.001066
[epoch: 466/1000, batch:   900/ 1052, ite: 122520] train loss: 0.004363, tar: 0.000129 
l0: 0.000180, l1: 0.000184, l2: 0.000202, l3: 0.000288, l4: 0.000583, l5: 0.001601, l6: 0.001866
[epoch: 466/1000, batch:   980/ 1052, ite: 122540] train loss: 0.004363, tar: 0.000129 
[Epoch 466/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000062, l1: 0.000060, l2: 0.000112, l3: 0.000189, l4: 0.000474, l5: 0.001049, l6: 0.000981
[epoch: 467/1000, batch:     8/ 1052, ite: 122560] train loss: 0.004363, tar: 0.000129 
l0: 0.000083, l1: 0.000085, l2: 0.000110, l3: 0.000161, l4: 0.000279, l5: 0.001179, l6: 0.002188
[epoch: 467/1000, batch:    88/ 1052, ite: 122580] train loss: 0.004362, tar: 0.000129 
l0: 0.000057, l1: 0.000057, l2: 0.000088, l3: 0.000204, l4: 0.000484, l5: 0.001481, l6: 0.001481
[epoch: 467/1000, batch:   168/ 1052, ite: 122600] train loss: 0.004361, tar: 0.000129 
l0: 0.000143, l1: 0.000141, l2: 0.000199, l3: 0.000343, l4: 0.000600, l5: 0.001130, l6: 0.001939
[epoch: 467/1000, batch:   248/ 1052, ite: 122620] train loss: 0.004361, tar: 0.000129 
l0: 0.000088, l1: 0.000084, l2: 0.000150, l3: 0.000468, l4: 0.001059, l5: 0.001813, l6: 0.003125
[epoch: 467/1000, batch:   328/ 1052, ite: 122640] train loss: 0.004361, tar: 0.000129 
l0: 0.000100, l1: 0.000100, l2: 0.000136, l3: 0.000163, l4: 0.000315, l5: 0.000650, l6: 0.001430
[epoch: 467/1000, batch:   408/ 1052, ite: 122660] train loss: 0.004359, tar: 0.000128 
l0: 0.000221, l1: 0.000218, l2: 0.000236, l3: 0.000268, l4: 0.000486, l5: 0.000744, l6: 0.002424
[epoch: 467/1000, batch:   488/ 1052, ite: 122680] train loss: 0.004358, tar: 0.000128 
l0: 0.000116, l1: 0.000118, l2: 0.000128, l3: 0.000231, l4: 0.000412, l5: 0.001146, l6: 0.002157
[epoch: 467/1000, batch:   568/ 1052, ite: 122700] train loss: 0.004357, tar: 0.000128 
l0: 0.000380, l1: 0.000387, l2: 0.000438, l3: 0.000541, l4: 0.000940, l5: 0.001963, l6: 0.003701
[epoch: 467/1000, batch:   648/ 1052, ite: 122720] train loss: 0.004359, tar: 0.000128 
l0: 0.000206, l1: 0.000204, l2: 0.000257, l3: 0.000313, l4: 0.000620, l5: 0.001640, l6: 0.002589
[epoch: 467/1000, batch:   728/ 1052, ite: 122740] train loss: 0.004360, tar: 0.000128 
l0: 0.000135, l1: 0.000135, l2: 0.000184, l3: 0.000273, l4: 0.000548, l5: 0.001109, l6: 0.001993
[epoch: 467/1000, batch:   808/ 1052, ite: 122760] train loss: 0.004359, tar: 0.000128 
l0: 0.000060, l1: 0.000062, l2: 0.000077, l3: 0.000143, l4: 0.000218, l5: 0.000886, l6: 0.000691
[epoch: 467/1000, batch:   888/ 1052, ite: 122780] train loss: 0.004359, tar: 0.000128 
l0: 0.000214, l1: 0.000212, l2: 0.000367, l3: 0.000483, l4: 0.000878, l5: 0.002093, l6: 0.003770
[epoch: 467/1000, batch:   968/ 1052, ite: 122800] train loss: 0.004361, tar: 0.000128 
l0: 0.000234, l1: 0.000238, l2: 0.000306, l3: 0.000430, l4: 0.000856, l5: 0.002188, l6: 0.003598
[epoch: 467/1000, batch:  1048/ 1052, ite: 122820] train loss: 0.004361, tar: 0.000128 
[Epoch 467/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000088, l1: 0.000090, l2: 0.000114, l3: 0.000131, l4: 0.000443, l5: 0.000546, l6: 0.001294
[epoch: 468/1000, batch:    76/ 1052, ite: 122840] train loss: 0.004360, tar: 0.000128 
l0: 0.000233, l1: 0.000236, l2: 0.000334, l3: 0.000589, l4: 0.001091, l5: 0.002936, l6: 0.004417
[epoch: 468/1000, batch:   156/ 1052, ite: 122860] train loss: 0.004361, tar: 0.000128 
l0: 0.000120, l1: 0.000122, l2: 0.000146, l3: 0.000184, l4: 0.000529, l5: 0.000988, l6: 0.002066
[epoch: 468/1000, batch:   236/ 1052, ite: 122880] train loss: 0.004360, tar: 0.000128 
l0: 0.000216, l1: 0.000213, l2: 0.000272, l3: 0.000369, l4: 0.000408, l5: 0.000818, l6: 0.001695
[epoch: 468/1000, batch:   316/ 1052, ite: 122900] train loss: 0.004359, tar: 0.000128 
l0: 0.000136, l1: 0.000141, l2: 0.000180, l3: 0.000333, l4: 0.000851, l5: 0.001548, l6: 0.002998
[epoch: 468/1000, batch:   396/ 1052, ite: 122920] train loss: 0.004357, tar: 0.000128 
l0: 0.000190, l1: 0.000195, l2: 0.000237, l3: 0.000338, l4: 0.000713, l5: 0.001702, l6: 0.002896
[epoch: 468/1000, batch:   476/ 1052, ite: 122940] train loss: 0.004358, tar: 0.000128 
l0: 0.000296, l1: 0.000295, l2: 0.000394, l3: 0.000499, l4: 0.000816, l5: 0.001925, l6: 0.003389
[epoch: 468/1000, batch:   556/ 1052, ite: 122960] train loss: 0.004357, tar: 0.000128 
l0: 0.000204, l1: 0.000206, l2: 0.000279, l3: 0.000360, l4: 0.000655, l5: 0.001705, l6: 0.003398
[epoch: 468/1000, batch:   636/ 1052, ite: 122980] train loss: 0.004358, tar: 0.000128 
l0: 0.000126, l1: 0.000132, l2: 0.000159, l3: 0.000252, l4: 0.000563, l5: 0.001695, l6: 0.001811
[epoch: 468/1000, batch:   716/ 1052, ite: 123000] train loss: 0.004357, tar: 0.000128 
l0: 0.000053, l1: 0.000052, l2: 0.000088, l3: 0.000182, l4: 0.000625, l5: 0.001329, l6: 0.001805
[epoch: 468/1000, batch:   796/ 1052, ite: 123020] train loss: 0.004358, tar: 0.000128 
l0: 0.000103, l1: 0.000104, l2: 0.000118, l3: 0.000246, l4: 0.000284, l5: 0.000859, l6: 0.001599
[epoch: 468/1000, batch:   876/ 1052, ite: 123040] train loss: 0.004357, tar: 0.000128 
l0: 0.000082, l1: 0.000081, l2: 0.000138, l3: 0.000175, l4: 0.000350, l5: 0.000838, l6: 0.001081
[epoch: 468/1000, batch:   956/ 1052, ite: 123060] train loss: 0.004358, tar: 0.000128 
l0: 0.000054, l1: 0.000052, l2: 0.000097, l3: 0.000148, l4: 0.000363, l5: 0.000650, l6: 0.001210
[epoch: 468/1000, batch:  1036/ 1052, ite: 123080] train loss: 0.004358, tar: 0.000128 
[Epoch 468/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000095, l1: 0.000097, l2: 0.000118, l3: 0.000193, l4: 0.000374, l5: 0.001305, l6: 0.001639
[epoch: 469/1000, batch:    64/ 1052, ite: 123100] train loss: 0.004357, tar: 0.000128 
l0: 0.000183, l1: 0.000177, l2: 0.000213, l3: 0.000241, l4: 0.000624, l5: 0.000986, l6: 0.001950
[epoch: 469/1000, batch:   144/ 1052, ite: 123120] train loss: 0.004357, tar: 0.000128 
l0: 0.000080, l1: 0.000080, l2: 0.000144, l3: 0.000208, l4: 0.000441, l5: 0.001069, l6: 0.001633
[epoch: 469/1000, batch:   224/ 1052, ite: 123140] train loss: 0.004356, tar: 0.000128 
l0: 0.000118, l1: 0.000119, l2: 0.000180, l3: 0.000390, l4: 0.000704, l5: 0.001470, l6: 0.002865
[epoch: 469/1000, batch:   304/ 1052, ite: 123160] train loss: 0.004357, tar: 0.000128 
l0: 0.000137, l1: 0.000140, l2: 0.000183, l3: 0.000288, l4: 0.000674, l5: 0.001049, l6: 0.002206
[epoch: 469/1000, batch:   384/ 1052, ite: 123180] train loss: 0.004357, tar: 0.000128 
l0: 0.000119, l1: 0.000121, l2: 0.000139, l3: 0.000236, l4: 0.000537, l5: 0.000895, l6: 0.002125
[epoch: 469/1000, batch:   464/ 1052, ite: 123200] train loss: 0.004358, tar: 0.000128 
l0: 0.000093, l1: 0.000090, l2: 0.000152, l3: 0.000199, l4: 0.000432, l5: 0.000935, l6: 0.002154
[epoch: 469/1000, batch:   544/ 1052, ite: 123220] train loss: 0.004357, tar: 0.000128 
l0: 0.000208, l1: 0.000204, l2: 0.000253, l3: 0.000406, l4: 0.000742, l5: 0.001760, l6: 0.003340
[epoch: 469/1000, batch:   624/ 1052, ite: 123240] train loss: 0.004358, tar: 0.000128 
l0: 0.000111, l1: 0.000109, l2: 0.000185, l3: 0.000394, l4: 0.000770, l5: 0.001292, l6: 0.002231
[epoch: 469/1000, batch:   704/ 1052, ite: 123260] train loss: 0.004358, tar: 0.000128 
l0: 0.000199, l1: 0.000201, l2: 0.000243, l3: 0.000353, l4: 0.000563, l5: 0.001045, l6: 0.002297
[epoch: 469/1000, batch:   784/ 1052, ite: 123280] train loss: 0.004358, tar: 0.000128 
l0: 0.000165, l1: 0.000163, l2: 0.000235, l3: 0.000359, l4: 0.000673, l5: 0.001935, l6: 0.002813
[epoch: 469/1000, batch:   864/ 1052, ite: 123300] train loss: 0.004357, tar: 0.000128 
l0: 0.000121, l1: 0.000116, l2: 0.000158, l3: 0.000243, l4: 0.000376, l5: 0.000708, l6: 0.001576
[epoch: 469/1000, batch:   944/ 1052, ite: 123320] train loss: 0.004358, tar: 0.000128 
l0: 0.000152, l1: 0.000151, l2: 0.000208, l3: 0.000278, l4: 0.000380, l5: 0.001214, l6: 0.001333
[epoch: 469/1000, batch:  1024/ 1052, ite: 123340] train loss: 0.004356, tar: 0.000128 
[Epoch 469/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000098, l1: 0.000100, l2: 0.000142, l3: 0.000177, l4: 0.000261, l5: 0.001227, l6: 0.001462
[epoch: 470/1000, batch:    52/ 1052, ite: 123360] train loss: 0.004357, tar: 0.000128 
l0: 0.000054, l1: 0.000054, l2: 0.000075, l3: 0.000133, l4: 0.000318, l5: 0.000937, l6: 0.001334
[epoch: 470/1000, batch:   132/ 1052, ite: 123380] train loss: 0.004355, tar: 0.000128 
l0: 0.000107, l1: 0.000109, l2: 0.000124, l3: 0.000185, l4: 0.000517, l5: 0.001277, l6: 0.002074
[epoch: 470/1000, batch:   212/ 1052, ite: 123400] train loss: 0.004357, tar: 0.000128 
l0: 0.000137, l1: 0.000138, l2: 0.000172, l3: 0.000252, l4: 0.000746, l5: 0.001251, l6: 0.002870
[epoch: 470/1000, batch:   292/ 1052, ite: 123420] train loss: 0.004358, tar: 0.000128 
l0: 0.000189, l1: 0.000191, l2: 0.000276, l3: 0.000402, l4: 0.001091, l5: 0.002350, l6: 0.003239
[epoch: 470/1000, batch:   372/ 1052, ite: 123440] train loss: 0.004358, tar: 0.000128 
l0: 0.000151, l1: 0.000152, l2: 0.000184, l3: 0.000209, l4: 0.000423, l5: 0.001070, l6: 0.002195
[epoch: 470/1000, batch:   452/ 1052, ite: 123460] train loss: 0.004357, tar: 0.000128 
l0: 0.000159, l1: 0.000162, l2: 0.000206, l3: 0.000296, l4: 0.000570, l5: 0.001321, l6: 0.001843
[epoch: 470/1000, batch:   532/ 1052, ite: 123480] train loss: 0.004356, tar: 0.000127 
l0: 0.000065, l1: 0.000067, l2: 0.000110, l3: 0.000258, l4: 0.000347, l5: 0.001274, l6: 0.001679
[epoch: 470/1000, batch:   612/ 1052, ite: 123500] train loss: 0.004356, tar: 0.000127 
l0: 0.000131, l1: 0.000130, l2: 0.000177, l3: 0.000279, l4: 0.000375, l5: 0.000752, l6: 0.001383
[epoch: 470/1000, batch:   692/ 1052, ite: 123520] train loss: 0.004355, tar: 0.000127 
l0: 0.000018, l1: 0.000017, l2: 0.000110, l3: 0.000345, l4: 0.000690, l5: 0.001567, l6: 0.003019
[epoch: 470/1000, batch:   772/ 1052, ite: 123540] train loss: 0.004353, tar: 0.000127 
l0: 0.000201, l1: 0.000200, l2: 0.000324, l3: 0.000435, l4: 0.000892, l5: 0.001948, l6: 0.004376
[epoch: 470/1000, batch:   852/ 1052, ite: 123560] train loss: 0.004353, tar: 0.000127 
l0: 0.000158, l1: 0.000155, l2: 0.000208, l3: 0.000270, l4: 0.000514, l5: 0.000972, l6: 0.001674
[epoch: 470/1000, batch:   932/ 1052, ite: 123580] train loss: 0.004353, tar: 0.000127 
l0: 0.000121, l1: 0.000136, l2: 0.000145, l3: 0.000146, l4: 0.000235, l5: 0.000758, l6: 0.000583
[epoch: 470/1000, batch:  1012/ 1052, ite: 123600] train loss: 0.004353, tar: 0.000127 
[Epoch 470/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000220, l1: 0.000217, l2: 0.000284, l3: 0.000429, l4: 0.000539, l5: 0.001408, l6: 0.001642
[epoch: 471/1000, batch:    40/ 1052, ite: 123620] train loss: 0.004352, tar: 0.000127 
l0: 0.000094, l1: 0.000091, l2: 0.000164, l3: 0.000266, l4: 0.000615, l5: 0.001064, l6: 0.002011
[epoch: 471/1000, batch:   120/ 1052, ite: 123640] train loss: 0.004351, tar: 0.000127 
l0: 0.000181, l1: 0.000182, l2: 0.000202, l3: 0.000290, l4: 0.000569, l5: 0.001162, l6: 0.002477
[epoch: 471/1000, batch:   200/ 1052, ite: 123660] train loss: 0.004349, tar: 0.000127 
l0: 0.000124, l1: 0.000123, l2: 0.000190, l3: 0.000394, l4: 0.000689, l5: 0.001278, l6: 0.001897
[epoch: 471/1000, batch:   280/ 1052, ite: 123680] train loss: 0.004348, tar: 0.000127 
l0: 0.000240, l1: 0.000244, l2: 0.000344, l3: 0.000418, l4: 0.000918, l5: 0.001946, l6: 0.004199
[epoch: 471/1000, batch:   360/ 1052, ite: 123700] train loss: 0.004348, tar: 0.000127 
l0: 0.000197, l1: 0.000200, l2: 0.000211, l3: 0.000262, l4: 0.000493, l5: 0.001207, l6: 0.002042
[epoch: 471/1000, batch:   440/ 1052, ite: 123720] train loss: 0.004347, tar: 0.000127 
l0: 0.000057, l1: 0.000057, l2: 0.000069, l3: 0.000130, l4: 0.000274, l5: 0.000669, l6: 0.001270
[epoch: 471/1000, batch:   520/ 1052, ite: 123740] train loss: 0.004348, tar: 0.000127 
l0: 0.000101, l1: 0.000109, l2: 0.000108, l3: 0.000122, l4: 0.000343, l5: 0.000612, l6: 0.001268
[epoch: 471/1000, batch:   600/ 1052, ite: 123760] train loss: 0.004348, tar: 0.000127 
l0: 0.000142, l1: 0.000144, l2: 0.000201, l3: 0.000263, l4: 0.000343, l5: 0.000819, l6: 0.001874
[epoch: 471/1000, batch:   680/ 1052, ite: 123780] train loss: 0.004348, tar: 0.000127 
l0: 0.000136, l1: 0.000137, l2: 0.000235, l3: 0.000336, l4: 0.000814, l5: 0.001729, l6: 0.003014
[epoch: 471/1000, batch:   760/ 1052, ite: 123800] train loss: 0.004350, tar: 0.000127 
l0: 0.000091, l1: 0.000091, l2: 0.000119, l3: 0.000236, l4: 0.000643, l5: 0.001299, l6: 0.001600
[epoch: 471/1000, batch:   840/ 1052, ite: 123820] train loss: 0.004351, tar: 0.000127 
l0: 0.000279, l1: 0.000280, l2: 0.000387, l3: 0.000431, l4: 0.000631, l5: 0.001430, l6: 0.001912
[epoch: 471/1000, batch:   920/ 1052, ite: 123840] train loss: 0.004353, tar: 0.000127 
l0: 0.000269, l1: 0.000274, l2: 0.000279, l3: 0.000362, l4: 0.000828, l5: 0.001356, l6: 0.002262
[epoch: 471/1000, batch:  1000/ 1052, ite: 123860] train loss: 0.004354, tar: 0.000127 
[Epoch 471/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000090, l1: 0.000095, l2: 0.000093, l3: 0.000153, l4: 0.000424, l5: 0.000593, l6: 0.001025
[epoch: 472/1000, batch:    28/ 1052, ite: 123880] train loss: 0.004355, tar: 0.000127 
l0: 0.000072, l1: 0.000070, l2: 0.000120, l3: 0.000187, l4: 0.000344, l5: 0.000955, l6: 0.002127
[epoch: 472/1000, batch:   108/ 1052, ite: 123900] train loss: 0.004354, tar: 0.000127 
l0: 0.000157, l1: 0.000158, l2: 0.000215, l3: 0.000243, l4: 0.000504, l5: 0.000750, l6: 0.001484
[epoch: 472/1000, batch:   188/ 1052, ite: 123920] train loss: 0.004354, tar: 0.000127 
l0: 0.000079, l1: 0.000077, l2: 0.000122, l3: 0.000181, l4: 0.000382, l5: 0.000830, l6: 0.001495
[epoch: 472/1000, batch:   268/ 1052, ite: 123940] train loss: 0.004354, tar: 0.000127 
l0: 0.000045, l1: 0.000045, l2: 0.000098, l3: 0.000120, l4: 0.000240, l5: 0.000725, l6: 0.001117
[epoch: 472/1000, batch:   348/ 1052, ite: 123960] train loss: 0.004353, tar: 0.000127 
l0: 0.000086, l1: 0.000087, l2: 0.000097, l3: 0.000168, l4: 0.000403, l5: 0.001214, l6: 0.001618
[epoch: 472/1000, batch:   428/ 1052, ite: 123980] train loss: 0.004352, tar: 0.000127 
l0: 0.000153, l1: 0.000150, l2: 0.000242, l3: 0.000521, l4: 0.000810, l5: 0.001394, l6: 0.004155
[epoch: 472/1000, batch:   508/ 1052, ite: 124000] train loss: 0.004354, tar: 0.000128 
l0: 0.000322, l1: 0.000506, l2: 0.000378, l3: 0.000355, l4: 0.000507, l5: 0.001166, l6: 0.001603
[epoch: 472/1000, batch:   588/ 1052, ite: 124020] train loss: 0.004356, tar: 0.000128 
l0: 0.000482, l1: 0.000516, l2: 0.000475, l3: 0.000673, l4: 0.001196, l5: 0.002206, l6: 0.003697
[epoch: 472/1000, batch:   668/ 1052, ite: 124040] train loss: 0.004361, tar: 0.000128 
l0: 0.000200, l1: 0.000212, l2: 0.000282, l3: 0.000285, l4: 0.000486, l5: 0.000977, l6: 0.002356
[epoch: 472/1000, batch:   748/ 1052, ite: 124060] train loss: 0.004362, tar: 0.000129 
l0: 0.000296, l1: 0.000284, l2: 0.000286, l3: 0.000475, l4: 0.000612, l5: 0.000910, l6: 0.002820
[epoch: 472/1000, batch:   828/ 1052, ite: 124080] train loss: 0.004364, tar: 0.000129 
l0: 0.000420, l1: 0.000413, l2: 0.000461, l3: 0.000939, l4: 0.001391, l5: 0.001932, l6: 0.004897
[epoch: 472/1000, batch:   908/ 1052, ite: 124100] train loss: 0.004366, tar: 0.000130 
l0: 0.000201, l1: 0.000202, l2: 0.000246, l3: 0.000406, l4: 0.000629, l5: 0.001443, l6: 0.001706
[epoch: 472/1000, batch:   988/ 1052, ite: 124120] train loss: 0.004368, tar: 0.000130 
[Epoch 472/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000148, l1: 0.000150, l2: 0.000196, l3: 0.000244, l4: 0.000574, l5: 0.000857, l6: 0.001771
[epoch: 473/1000, batch:    16/ 1052, ite: 124140] train loss: 0.004369, tar: 0.000130 
l0: 0.000155, l1: 0.000151, l2: 0.000225, l3: 0.000249, l4: 0.000419, l5: 0.000934, l6: 0.001863
[epoch: 473/1000, batch:    96/ 1052, ite: 124160] train loss: 0.004372, tar: 0.000130 
l0: 0.000165, l1: 0.000184, l2: 0.000205, l3: 0.000237, l4: 0.000466, l5: 0.001130, l6: 0.002068
[epoch: 473/1000, batch:   176/ 1052, ite: 124180] train loss: 0.004373, tar: 0.000131 
l0: 0.000105, l1: 0.000112, l2: 0.000139, l3: 0.000197, l4: 0.000372, l5: 0.000883, l6: 0.001620
[epoch: 473/1000, batch:   256/ 1052, ite: 124200] train loss: 0.004373, tar: 0.000131 
l0: 0.000285, l1: 0.000295, l2: 0.000296, l3: 0.000352, l4: 0.000526, l5: 0.001418, l6: 0.003008
[epoch: 473/1000, batch:   336/ 1052, ite: 124220] train loss: 0.004374, tar: 0.000131 
l0: 0.000208, l1: 0.000195, l2: 0.000265, l3: 0.000439, l4: 0.000897, l5: 0.001262, l6: 0.002194
[epoch: 473/1000, batch:   416/ 1052, ite: 124240] train loss: 0.004374, tar: 0.000131 
l0: 0.000350, l1: 0.000379, l2: 0.000390, l3: 0.000458, l4: 0.000743, l5: 0.001183, l6: 0.002064
[epoch: 473/1000, batch:   496/ 1052, ite: 124260] train loss: 0.004376, tar: 0.000131 
l0: 0.000105, l1: 0.000112, l2: 0.000137, l3: 0.000169, l4: 0.000267, l5: 0.000873, l6: 0.001298
[epoch: 473/1000, batch:   576/ 1052, ite: 124280] train loss: 0.004375, tar: 0.000131 
l0: 0.000092, l1: 0.000093, l2: 0.000119, l3: 0.000187, l4: 0.000298, l5: 0.000830, l6: 0.001699
[epoch: 473/1000, batch:   656/ 1052, ite: 124300] train loss: 0.004375, tar: 0.000131 
l0: 0.000073, l1: 0.000072, l2: 0.000153, l3: 0.000227, l4: 0.000538, l5: 0.000607, l6: 0.002481
[epoch: 473/1000, batch:   736/ 1052, ite: 124320] train loss: 0.004375, tar: 0.000131 
l0: 0.000080, l1: 0.000082, l2: 0.000109, l3: 0.000166, l4: 0.000469, l5: 0.001137, l6: 0.001891
[epoch: 473/1000, batch:   816/ 1052, ite: 124340] train loss: 0.004377, tar: 0.000131 
l0: 0.000136, l1: 0.000139, l2: 0.000194, l3: 0.000343, l4: 0.000641, l5: 0.001586, l6: 0.002358
[epoch: 473/1000, batch:   896/ 1052, ite: 124360] train loss: 0.004376, tar: 0.000131 
l0: 0.000080, l1: 0.000077, l2: 0.000101, l3: 0.000174, l4: 0.000378, l5: 0.000911, l6: 0.001833
[epoch: 473/1000, batch:   976/ 1052, ite: 124380] train loss: 0.004376, tar: 0.000132 
[Epoch 473/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000252, l1: 0.000245, l2: 0.000288, l3: 0.000413, l4: 0.000777, l5: 0.001317, l6: 0.001860
[epoch: 474/1000, batch:     4/ 1052, ite: 124400] train loss: 0.004377, tar: 0.000132 
l0: 0.000109, l1: 0.000108, l2: 0.000126, l3: 0.000198, l4: 0.000339, l5: 0.000914, l6: 0.001279
[epoch: 474/1000, batch:    84/ 1052, ite: 124420] train loss: 0.004378, tar: 0.000132 
l0: 0.000058, l1: 0.000057, l2: 0.000076, l3: 0.000127, l4: 0.000246, l5: 0.000817, l6: 0.001205
[epoch: 474/1000, batch:   164/ 1052, ite: 124440] train loss: 0.004379, tar: 0.000132 
l0: 0.000112, l1: 0.000113, l2: 0.000172, l3: 0.000240, l4: 0.000465, l5: 0.000866, l6: 0.002056
[epoch: 474/1000, batch:   244/ 1052, ite: 124460] train loss: 0.004378, tar: 0.000132 
l0: 0.000056, l1: 0.000055, l2: 0.000083, l3: 0.000139, l4: 0.000321, l5: 0.000737, l6: 0.001113
[epoch: 474/1000, batch:   324/ 1052, ite: 124480] train loss: 0.004377, tar: 0.000132 
l0: 0.000078, l1: 0.000078, l2: 0.000121, l3: 0.000207, l4: 0.000348, l5: 0.000796, l6: 0.001478
[epoch: 474/1000, batch:   404/ 1052, ite: 124500] train loss: 0.004376, tar: 0.000132 
l0: 0.000096, l1: 0.000094, l2: 0.000172, l3: 0.000254, l4: 0.000378, l5: 0.000890, l6: 0.001372
[epoch: 474/1000, batch:   484/ 1052, ite: 124520] train loss: 0.004376, tar: 0.000132 
l0: 0.000041, l1: 0.000040, l2: 0.000082, l3: 0.000214, l4: 0.000373, l5: 0.000908, l6: 0.000814
[epoch: 474/1000, batch:   564/ 1052, ite: 124540] train loss: 0.004375, tar: 0.000132 
l0: 0.000062, l1: 0.000062, l2: 0.000093, l3: 0.000169, l4: 0.000339, l5: 0.001131, l6: 0.001368
[epoch: 474/1000, batch:   644/ 1052, ite: 124560] train loss: 0.004375, tar: 0.000132 
l0: 0.000071, l1: 0.000069, l2: 0.000110, l3: 0.000194, l4: 0.000405, l5: 0.001140, l6: 0.001839
[epoch: 474/1000, batch:   724/ 1052, ite: 124580] train loss: 0.004377, tar: 0.000132 
l0: 0.000032, l1: 0.000034, l2: 0.000090, l3: 0.000201, l4: 0.000443, l5: 0.000577, l6: 0.001897
[epoch: 474/1000, batch:   804/ 1052, ite: 124600] train loss: 0.004377, tar: 0.000132 
l0: 0.000085, l1: 0.000089, l2: 0.000110, l3: 0.000164, l4: 0.000297, l5: 0.000848, l6: 0.000922
[epoch: 474/1000, batch:   884/ 1052, ite: 124620] train loss: 0.004376, tar: 0.000132 
l0: 0.000121, l1: 0.000129, l2: 0.000135, l3: 0.000156, l4: 0.000362, l5: 0.001173, l6: 0.001512
[epoch: 474/1000, batch:   964/ 1052, ite: 124640] train loss: 0.004377, tar: 0.000132 
l0: 0.000072, l1: 0.000070, l2: 0.000151, l3: 0.000261, l4: 0.000477, l5: 0.001026, l6: 0.002294
[epoch: 474/1000, batch:  1044/ 1052, ite: 124660] train loss: 0.004377, tar: 0.000132 
[Epoch 474/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000208, l1: 0.000201, l2: 0.000311, l3: 0.000538, l4: 0.001000, l5: 0.002070, l6: 0.003949
[epoch: 475/1000, batch:    72/ 1052, ite: 124680] train loss: 0.004377, tar: 0.000132 
l0: 0.000137, l1: 0.000129, l2: 0.000148, l3: 0.000212, l4: 0.000397, l5: 0.000653, l6: 0.001835
[epoch: 475/1000, batch:   152/ 1052, ite: 124700] train loss: 0.004378, tar: 0.000132 
l0: 0.000049, l1: 0.000048, l2: 0.000084, l3: 0.000130, l4: 0.000382, l5: 0.000929, l6: 0.001984
[epoch: 475/1000, batch:   232/ 1052, ite: 124720] train loss: 0.004377, tar: 0.000132 
l0: 0.000120, l1: 0.000122, l2: 0.000116, l3: 0.000183, l4: 0.000399, l5: 0.000642, l6: 0.001447
[epoch: 475/1000, batch:   312/ 1052, ite: 124740] train loss: 0.004377, tar: 0.000132 
l0: 0.000106, l1: 0.000104, l2: 0.000141, l3: 0.000271, l4: 0.000548, l5: 0.001352, l6: 0.001917
[epoch: 475/1000, batch:   392/ 1052, ite: 124760] train loss: 0.004377, tar: 0.000132 
l0: 0.000125, l1: 0.000121, l2: 0.000157, l3: 0.000256, l4: 0.000542, l5: 0.000931, l6: 0.001425
[epoch: 475/1000, batch:   472/ 1052, ite: 124780] train loss: 0.004376, tar: 0.000132 
l0: 0.000147, l1: 0.000147, l2: 0.000186, l3: 0.000315, l4: 0.000667, l5: 0.001250, l6: 0.001621
[epoch: 475/1000, batch:   552/ 1052, ite: 124800] train loss: 0.004377, tar: 0.000131 
l0: 0.000142, l1: 0.000138, l2: 0.000247, l3: 0.000524, l4: 0.000994, l5: 0.001394, l6: 0.002105
[epoch: 475/1000, batch:   632/ 1052, ite: 124820] train loss: 0.004376, tar: 0.000131 
l0: 0.000154, l1: 0.000154, l2: 0.000234, l3: 0.000353, l4: 0.000704, l5: 0.002320, l6: 0.003554
[epoch: 475/1000, batch:   712/ 1052, ite: 124840] train loss: 0.004377, tar: 0.000131 
l0: 0.000132, l1: 0.000137, l2: 0.000151, l3: 0.000268, l4: 0.000462, l5: 0.001485, l6: 0.001412
[epoch: 475/1000, batch:   792/ 1052, ite: 124860] train loss: 0.004377, tar: 0.000131 
l0: 0.000083, l1: 0.000078, l2: 0.000150, l3: 0.000244, l4: 0.000598, l5: 0.000980, l6: 0.001119
[epoch: 475/1000, batch:   872/ 1052, ite: 124880] train loss: 0.004377, tar: 0.000131 
l0: 0.000126, l1: 0.000129, l2: 0.000131, l3: 0.000173, l4: 0.000345, l5: 0.001146, l6: 0.001681
[epoch: 475/1000, batch:   952/ 1052, ite: 124900] train loss: 0.004377, tar: 0.000131 
l0: 0.000041, l1: 0.000041, l2: 0.000082, l3: 0.000151, l4: 0.000329, l5: 0.000688, l6: 0.002178
[epoch: 475/1000, batch:  1032/ 1052, ite: 124920] train loss: 0.004377, tar: 0.000131 
[Epoch 475/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000043, l1: 0.000046, l2: 0.000089, l3: 0.000208, l4: 0.000582, l5: 0.001099, l6: 0.001846
[epoch: 476/1000, batch:    60/ 1052, ite: 124940] train loss: 0.004379, tar: 0.000131 
l0: 0.000109, l1: 0.000118, l2: 0.000117, l3: 0.000133, l4: 0.000249, l5: 0.000520, l6: 0.000988
[epoch: 476/1000, batch:   140/ 1052, ite: 124960] train loss: 0.004377, tar: 0.000131 
l0: 0.000156, l1: 0.000155, l2: 0.000235, l3: 0.000333, l4: 0.000520, l5: 0.001184, l6: 0.002341
[epoch: 476/1000, batch:   220/ 1052, ite: 124980] train loss: 0.004376, tar: 0.000131 
l0: 0.000166, l1: 0.000165, l2: 0.000210, l3: 0.000318, l4: 0.000538, l5: 0.001000, l6: 0.003573
[epoch: 476/1000, batch:   300/ 1052, ite: 125000] train loss: 0.004377, tar: 0.000131 
l0: 0.000160, l1: 0.000160, l2: 0.000224, l3: 0.000314, l4: 0.000525, l5: 0.001176, l6: 0.001808
[epoch: 476/1000, batch:   380/ 1052, ite: 125020] train loss: 0.004376, tar: 0.000131 
l0: 0.000101, l1: 0.000104, l2: 0.000186, l3: 0.000390, l4: 0.000751, l5: 0.001362, l6: 0.003497
[epoch: 476/1000, batch:   460/ 1052, ite: 125040] train loss: 0.004376, tar: 0.000131 
l0: 0.000133, l1: 0.000125, l2: 0.000192, l3: 0.000258, l4: 0.000581, l5: 0.000927, l6: 0.001917
[epoch: 476/1000, batch:   540/ 1052, ite: 125060] train loss: 0.004377, tar: 0.000131 
l0: 0.000113, l1: 0.000110, l2: 0.000145, l3: 0.000236, l4: 0.000519, l5: 0.000926, l6: 0.001643
[epoch: 476/1000, batch:   620/ 1052, ite: 125080] train loss: 0.004376, tar: 0.000131 
l0: 0.000046, l1: 0.000046, l2: 0.000077, l3: 0.000198, l4: 0.000609, l5: 0.001064, l6: 0.001671
[epoch: 476/1000, batch:   700/ 1052, ite: 125100] train loss: 0.004374, tar: 0.000131 
l0: 0.000090, l1: 0.000091, l2: 0.000207, l3: 0.000331, l4: 0.000710, l5: 0.001110, l6: 0.002164
[epoch: 476/1000, batch:   780/ 1052, ite: 125120] train loss: 0.004374, tar: 0.000131 
l0: 0.000082, l1: 0.000081, l2: 0.000108, l3: 0.000173, l4: 0.000380, l5: 0.000799, l6: 0.000852
[epoch: 476/1000, batch:   860/ 1052, ite: 125140] train loss: 0.004374, tar: 0.000131 
l0: 0.000131, l1: 0.000133, l2: 0.000151, l3: 0.000238, l4: 0.000420, l5: 0.001867, l6: 0.002401
[epoch: 476/1000, batch:   940/ 1052, ite: 125160] train loss: 0.004375, tar: 0.000131 
l0: 0.000090, l1: 0.000085, l2: 0.000123, l3: 0.000170, l4: 0.000341, l5: 0.001063, l6: 0.001165
[epoch: 476/1000, batch:  1020/ 1052, ite: 125180] train loss: 0.004375, tar: 0.000131 
[Epoch 476/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000113, l1: 0.000109, l2: 0.000217, l3: 0.000565, l4: 0.001189, l5: 0.002734, l6: 0.002692
[epoch: 477/1000, batch:    48/ 1052, ite: 125200] train loss: 0.004375, tar: 0.000131 
l0: 0.000071, l1: 0.000074, l2: 0.000072, l3: 0.000152, l4: 0.000400, l5: 0.001011, l6: 0.001900
[epoch: 477/1000, batch:   128/ 1052, ite: 125220] train loss: 0.004375, tar: 0.000131 
l0: 0.000064, l1: 0.000060, l2: 0.000129, l3: 0.000335, l4: 0.000626, l5: 0.001444, l6: 0.002951
[epoch: 477/1000, batch:   208/ 1052, ite: 125240] train loss: 0.004374, tar: 0.000131 
l0: 0.000147, l1: 0.000150, l2: 0.000167, l3: 0.000240, l4: 0.000462, l5: 0.001138, l6: 0.002352
[epoch: 477/1000, batch:   288/ 1052, ite: 125260] train loss: 0.004374, tar: 0.000131 
l0: 0.000038, l1: 0.000037, l2: 0.000080, l3: 0.000194, l4: 0.000537, l5: 0.000986, l6: 0.001808
[epoch: 477/1000, batch:   368/ 1052, ite: 125280] train loss: 0.004373, tar: 0.000131 
l0: 0.000133, l1: 0.000131, l2: 0.000227, l3: 0.000283, l4: 0.000867, l5: 0.001861, l6: 0.002718
[epoch: 477/1000, batch:   448/ 1052, ite: 125300] train loss: 0.004372, tar: 0.000131 
l0: 0.000147, l1: 0.000144, l2: 0.000211, l3: 0.000292, l4: 0.000507, l5: 0.001550, l6: 0.001795
[epoch: 477/1000, batch:   528/ 1052, ite: 125320] train loss: 0.004370, tar: 0.000130 
l0: 0.000025, l1: 0.000027, l2: 0.000053, l3: 0.000128, l4: 0.000431, l5: 0.000657, l6: 0.001662
[epoch: 477/1000, batch:   608/ 1052, ite: 125340] train loss: 0.004371, tar: 0.000130 
l0: 0.000032, l1: 0.000032, l2: 0.000054, l3: 0.000112, l4: 0.000384, l5: 0.000618, l6: 0.000963
[epoch: 477/1000, batch:   688/ 1052, ite: 125360] train loss: 0.004371, tar: 0.000130 
l0: 0.000087, l1: 0.000093, l2: 0.000097, l3: 0.000135, l4: 0.000472, l5: 0.000855, l6: 0.001525
[epoch: 477/1000, batch:   768/ 1052, ite: 125380] train loss: 0.004370, tar: 0.000130 
l0: 0.000050, l1: 0.000050, l2: 0.000080, l3: 0.000205, l4: 0.000387, l5: 0.000981, l6: 0.001334
[epoch: 477/1000, batch:   848/ 1052, ite: 125400] train loss: 0.004372, tar: 0.000130 
l0: 0.000093, l1: 0.000095, l2: 0.000115, l3: 0.000242, l4: 0.000397, l5: 0.001174, l6: 0.002013
[epoch: 477/1000, batch:   928/ 1052, ite: 125420] train loss: 0.004374, tar: 0.000130 
l0: 0.000186, l1: 0.000187, l2: 0.000241, l3: 0.000356, l4: 0.001002, l5: 0.001982, l6: 0.001479
[epoch: 477/1000, batch:  1008/ 1052, ite: 125440] train loss: 0.004373, tar: 0.000130 
[Epoch 477/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000188, l1: 0.000188, l2: 0.000240, l3: 0.000306, l4: 0.000677, l5: 0.001494, l6: 0.003601
[epoch: 478/1000, batch:    36/ 1052, ite: 125460] train loss: 0.004372, tar: 0.000130 
l0: 0.000056, l1: 0.000054, l2: 0.000092, l3: 0.000167, l4: 0.000299, l5: 0.001260, l6: 0.001663
[epoch: 478/1000, batch:   116/ 1052, ite: 125480] train loss: 0.004373, tar: 0.000130 
l0: 0.000063, l1: 0.000061, l2: 0.000077, l3: 0.000124, l4: 0.000273, l5: 0.000737, l6: 0.000834
[epoch: 478/1000, batch:   196/ 1052, ite: 125500] train loss: 0.004372, tar: 0.000130 
l0: 0.000199, l1: 0.000209, l2: 0.000232, l3: 0.000389, l4: 0.000949, l5: 0.003483, l6: 0.005243
[epoch: 478/1000, batch:   276/ 1052, ite: 125520] train loss: 0.004371, tar: 0.000130 
l0: 0.000088, l1: 0.000087, l2: 0.000123, l3: 0.000216, l4: 0.000335, l5: 0.001323, l6: 0.002650
[epoch: 478/1000, batch:   356/ 1052, ite: 125540] train loss: 0.004371, tar: 0.000130 
l0: 0.000162, l1: 0.000162, l2: 0.000245, l3: 0.000461, l4: 0.000729, l5: 0.001327, l6: 0.002559
[epoch: 478/1000, batch:   436/ 1052, ite: 125560] train loss: 0.004371, tar: 0.000130 
l0: 0.000048, l1: 0.000051, l2: 0.000074, l3: 0.000215, l4: 0.000407, l5: 0.000868, l6: 0.001075
[epoch: 478/1000, batch:   516/ 1052, ite: 125580] train loss: 0.004372, tar: 0.000130 
l0: 0.000262, l1: 0.000260, l2: 0.000383, l3: 0.000637, l4: 0.000974, l5: 0.001634, l6: 0.002771
[epoch: 478/1000, batch:   596/ 1052, ite: 125600] train loss: 0.004372, tar: 0.000130 
l0: 0.000174, l1: 0.000170, l2: 0.000243, l3: 0.000427, l4: 0.000880, l5: 0.001855, l6: 0.003052
[epoch: 478/1000, batch:   676/ 1052, ite: 125620] train loss: 0.004371, tar: 0.000130 
l0: 0.000063, l1: 0.000063, l2: 0.000081, l3: 0.000163, l4: 0.000315, l5: 0.000579, l6: 0.000863
[epoch: 478/1000, batch:   756/ 1052, ite: 125640] train loss: 0.004371, tar: 0.000130 
l0: 0.000085, l1: 0.000086, l2: 0.000112, l3: 0.000149, l4: 0.000377, l5: 0.000692, l6: 0.001876
[epoch: 478/1000, batch:   836/ 1052, ite: 125660] train loss: 0.004372, tar: 0.000130 
l0: 0.000198, l1: 0.000201, l2: 0.000231, l3: 0.000334, l4: 0.000649, l5: 0.001434, l6: 0.003386
[epoch: 478/1000, batch:   916/ 1052, ite: 125680] train loss: 0.004371, tar: 0.000130 
l0: 0.000056, l1: 0.000058, l2: 0.000100, l3: 0.000196, l4: 0.000489, l5: 0.001117, l6: 0.001688
[epoch: 478/1000, batch:   996/ 1052, ite: 125700] train loss: 0.004371, tar: 0.000130 
[Epoch 478/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000099, l1: 0.000097, l2: 0.000131, l3: 0.000202, l4: 0.000402, l5: 0.001247, l6: 0.002124
[epoch: 479/1000, batch:    24/ 1052, ite: 125720] train loss: 0.004369, tar: 0.000130 
l0: 0.000131, l1: 0.000138, l2: 0.000165, l3: 0.000246, l4: 0.000510, l5: 0.001026, l6: 0.001468
[epoch: 479/1000, batch:   104/ 1052, ite: 125740] train loss: 0.004369, tar: 0.000130 
l0: 0.000161, l1: 0.000163, l2: 0.000190, l3: 0.000301, l4: 0.000508, l5: 0.001533, l6: 0.002762
[epoch: 479/1000, batch:   184/ 1052, ite: 125760] train loss: 0.004370, tar: 0.000130 
l0: 0.000102, l1: 0.000104, l2: 0.000112, l3: 0.000216, l4: 0.000502, l5: 0.000938, l6: 0.001366
[epoch: 479/1000, batch:   264/ 1052, ite: 125780] train loss: 0.004371, tar: 0.000130 
l0: 0.000192, l1: 0.000200, l2: 0.000297, l3: 0.000397, l4: 0.000643, l5: 0.002246, l6: 0.003691
[epoch: 479/1000, batch:   344/ 1052, ite: 125800] train loss: 0.004370, tar: 0.000130 
l0: 0.000015, l1: 0.000015, l2: 0.000045, l3: 0.000107, l4: 0.000289, l5: 0.000609, l6: 0.001309
[epoch: 479/1000, batch:   424/ 1052, ite: 125820] train loss: 0.004370, tar: 0.000129 
l0: 0.000072, l1: 0.000072, l2: 0.000112, l3: 0.000191, l4: 0.000351, l5: 0.000671, l6: 0.001432
[epoch: 479/1000, batch:   504/ 1052, ite: 125840] train loss: 0.004370, tar: 0.000129 
l0: 0.000166, l1: 0.000167, l2: 0.000213, l3: 0.000401, l4: 0.000903, l5: 0.001748, l6: 0.002598
[epoch: 479/1000, batch:   584/ 1052, ite: 125860] train loss: 0.004369, tar: 0.000129 
l0: 0.000067, l1: 0.000068, l2: 0.000086, l3: 0.000151, l4: 0.000315, l5: 0.001020, l6: 0.001346
[epoch: 479/1000, batch:   664/ 1052, ite: 125880] train loss: 0.004370, tar: 0.000129 
l0: 0.000163, l1: 0.000157, l2: 0.000224, l3: 0.000383, l4: 0.000629, l5: 0.001602, l6: 0.001444
[epoch: 479/1000, batch:   744/ 1052, ite: 125900] train loss: 0.004369, tar: 0.000129 
l0: 0.000102, l1: 0.000107, l2: 0.000135, l3: 0.000236, l4: 0.000503, l5: 0.001067, l6: 0.001833
[epoch: 479/1000, batch:   824/ 1052, ite: 125920] train loss: 0.004368, tar: 0.000129 
l0: 0.000083, l1: 0.000082, l2: 0.000123, l3: 0.000163, l4: 0.000371, l5: 0.000785, l6: 0.001654
[epoch: 479/1000, batch:   904/ 1052, ite: 125940] train loss: 0.004368, tar: 0.000129 
l0: 0.000031, l1: 0.000032, l2: 0.000049, l3: 0.000115, l4: 0.000449, l5: 0.000783, l6: 0.001190
[epoch: 479/1000, batch:   984/ 1052, ite: 125960] train loss: 0.004368, tar: 0.000129 
[Epoch 479/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000078, l1: 0.000078, l2: 0.000105, l3: 0.000164, l4: 0.000417, l5: 0.000796, l6: 0.001668
[epoch: 480/1000, batch:    12/ 1052, ite: 125980] train loss: 0.004367, tar: 0.000129 
l0: 0.000069, l1: 0.000066, l2: 0.000097, l3: 0.000147, l4: 0.000290, l5: 0.000753, l6: 0.001079
[epoch: 480/1000, batch:    92/ 1052, ite: 126000] train loss: 0.004367, tar: 0.000129 
l0: 0.000026, l1: 0.000026, l2: 0.000057, l3: 0.000085, l4: 0.000256, l5: 0.000619, l6: 0.001212
[epoch: 480/1000, batch:   172/ 1052, ite: 126020] train loss: 0.004366, tar: 0.000129 
l0: 0.000027, l1: 0.000026, l2: 0.000078, l3: 0.000133, l4: 0.000448, l5: 0.000956, l6: 0.001148
[epoch: 480/1000, batch:   252/ 1052, ite: 126040] train loss: 0.004366, tar: 0.000129 
l0: 0.000060, l1: 0.000060, l2: 0.000118, l3: 0.000189, l4: 0.000445, l5: 0.000861, l6: 0.001586
[epoch: 480/1000, batch:   332/ 1052, ite: 126060] train loss: 0.004366, tar: 0.000129 
l0: 0.000071, l1: 0.000071, l2: 0.000102, l3: 0.000144, l4: 0.000471, l5: 0.001090, l6: 0.001493
[epoch: 480/1000, batch:   412/ 1052, ite: 126080] train loss: 0.004366, tar: 0.000129 
l0: 0.000048, l1: 0.000048, l2: 0.000097, l3: 0.000145, l4: 0.000343, l5: 0.001121, l6: 0.002364
[epoch: 480/1000, batch:   492/ 1052, ite: 126100] train loss: 0.004365, tar: 0.000129 
l0: 0.000271, l1: 0.000279, l2: 0.000274, l3: 0.000518, l4: 0.000790, l5: 0.001707, l6: 0.003503
[epoch: 480/1000, batch:   572/ 1052, ite: 126120] train loss: 0.004367, tar: 0.000129 
l0: 0.000223, l1: 0.000227, l2: 0.000273, l3: 0.000315, l4: 0.000587, l5: 0.001116, l6: 0.003116
[epoch: 480/1000, batch:   652/ 1052, ite: 126140] train loss: 0.004367, tar: 0.000129 
l0: 0.000107, l1: 0.000105, l2: 0.000152, l3: 0.000234, l4: 0.000443, l5: 0.000904, l6: 0.001532
[epoch: 480/1000, batch:   732/ 1052, ite: 126160] train loss: 0.004368, tar: 0.000129 
l0: 0.000097, l1: 0.000094, l2: 0.000151, l3: 0.000214, l4: 0.000622, l5: 0.001541, l6: 0.002575
[epoch: 480/1000, batch:   812/ 1052, ite: 126180] train loss: 0.004367, tar: 0.000129 
l0: 0.000040, l1: 0.000044, l2: 0.000063, l3: 0.000133, l4: 0.000256, l5: 0.001267, l6: 0.002153
[epoch: 480/1000, batch:   892/ 1052, ite: 126200] train loss: 0.004366, tar: 0.000129 
l0: 0.000049, l1: 0.000049, l2: 0.000090, l3: 0.000170, l4: 0.000524, l5: 0.001505, l6: 0.002462
[epoch: 480/1000, batch:   972/ 1052, ite: 126220] train loss: 0.004366, tar: 0.000129 
l0: 0.000240, l1: 0.000247, l2: 0.000306, l3: 0.000447, l4: 0.000901, l5: 0.001544, l6: 0.002384
[epoch: 480/1000, batch:  1052/ 1052, ite: 126240] train loss: 0.004365, tar: 0.000129 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_126240.pkl
[Epoch 480/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000127, l1: 0.000130, l2: 0.000144, l3: 0.000257, l4: 0.000471, l5: 0.001307, l6: 0.002383
[epoch: 481/1000, batch:    80/ 1052, ite: 126260] train loss: 0.004224, tar: 0.000107 
l0: 0.000106, l1: 0.000106, l2: 0.000189, l3: 0.000360, l4: 0.000571, l5: 0.001402, l6: 0.002486
[epoch: 481/1000, batch:   160/ 1052, ite: 126280] train loss: 0.004258, tar: 0.000109 
l0: 0.000034, l1: 0.000034, l2: 0.000065, l3: 0.000111, l4: 0.000383, l5: 0.001055, l6: 0.002311
[epoch: 481/1000, batch:   240/ 1052, ite: 126300] train loss: 0.004450, tar: 0.000105 
l0: 0.000069, l1: 0.000069, l2: 0.000080, l3: 0.000121, l4: 0.000329, l5: 0.000649, l6: 0.001156
[epoch: 481/1000, batch:   320/ 1052, ite: 126320] train loss: 0.004421, tar: 0.000105 
l0: 0.000185, l1: 0.000186, l2: 0.000173, l3: 0.000381, l4: 0.000545, l5: 0.001206, l6: 0.002092
[epoch: 481/1000, batch:   400/ 1052, ite: 126340] train loss: 0.004332, tar: 0.000104 
l0: 0.000075, l1: 0.000072, l2: 0.000136, l3: 0.000249, l4: 0.000470, l5: 0.001297, l6: 0.002536
[epoch: 481/1000, batch:   480/ 1052, ite: 126360] train loss: 0.004371, tar: 0.000105 
l0: 0.000105, l1: 0.000108, l2: 0.000109, l3: 0.000111, l4: 0.000380, l5: 0.000686, l6: 0.001168
[epoch: 481/1000, batch:   560/ 1052, ite: 126380] train loss: 0.004309, tar: 0.000104 
l0: 0.000049, l1: 0.000054, l2: 0.000076, l3: 0.000110, l4: 0.000363, l5: 0.001193, l6: 0.001675
[epoch: 481/1000, batch:   640/ 1052, ite: 126400] train loss: 0.004282, tar: 0.000106 
l0: 0.000071, l1: 0.000071, l2: 0.000115, l3: 0.000176, l4: 0.000465, l5: 0.001095, l6: 0.002220
[epoch: 481/1000, batch:   720/ 1052, ite: 126420] train loss: 0.004273, tar: 0.000105 
l0: 0.000092, l1: 0.000090, l2: 0.000130, l3: 0.000210, l4: 0.000354, l5: 0.000768, l6: 0.001858
[epoch: 481/1000, batch:   800/ 1052, ite: 126440] train loss: 0.004316, tar: 0.000107 
l0: 0.000061, l1: 0.000060, l2: 0.000091, l3: 0.000150, l4: 0.000413, l5: 0.000739, l6: 0.001325
[epoch: 481/1000, batch:   880/ 1052, ite: 126460] train loss: 0.004283, tar: 0.000106 
l0: 0.000178, l1: 0.000180, l2: 0.000238, l3: 0.000397, l4: 0.000695, l5: 0.001337, l6: 0.002468
[epoch: 481/1000, batch:   960/ 1052, ite: 126480] train loss: 0.004233, tar: 0.000105 
l0: 0.000125, l1: 0.000123, l2: 0.000201, l3: 0.000360, l4: 0.000919, l5: 0.002724, l6: 0.004198
[epoch: 481/1000, batch:  1040/ 1052, ite: 126500] train loss: 0.004221, tar: 0.000105 
[Epoch 481/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000290, l1: 0.000292, l2: 0.000307, l3: 0.000337, l4: 0.000684, l5: 0.001473, l6: 0.002912
[epoch: 482/1000, batch:    68/ 1052, ite: 126520] train loss: 0.004238, tar: 0.000106 
l0: 0.000070, l1: 0.000070, l2: 0.000109, l3: 0.000128, l4: 0.000326, l5: 0.000532, l6: 0.001657
[epoch: 482/1000, batch:   148/ 1052, ite: 126540] train loss: 0.004209, tar: 0.000104 
l0: 0.000087, l1: 0.000087, l2: 0.000112, l3: 0.000192, l4: 0.000394, l5: 0.001511, l6: 0.002403
[epoch: 482/1000, batch:   228/ 1052, ite: 126560] train loss: 0.004226, tar: 0.000105 
l0: 0.000067, l1: 0.000068, l2: 0.000104, l3: 0.000219, l4: 0.000405, l5: 0.000873, l6: 0.001645
[epoch: 482/1000, batch:   308/ 1052, ite: 126580] train loss: 0.004236, tar: 0.000105 
l0: 0.000098, l1: 0.000102, l2: 0.000151, l3: 0.000216, l4: 0.000430, l5: 0.000790, l6: 0.001666
[epoch: 482/1000, batch:   388/ 1052, ite: 126600] train loss: 0.004257, tar: 0.000105 
l0: 0.000128, l1: 0.000131, l2: 0.000167, l3: 0.000234, l4: 0.000465, l5: 0.000893, l6: 0.001674
[epoch: 482/1000, batch:   468/ 1052, ite: 126620] train loss: 0.004235, tar: 0.000104 
l0: 0.000178, l1: 0.000191, l2: 0.000176, l3: 0.000230, l4: 0.000514, l5: 0.001146, l6: 0.003337
[epoch: 482/1000, batch:   548/ 1052, ite: 126640] train loss: 0.004226, tar: 0.000105 
l0: 0.000040, l1: 0.000040, l2: 0.000065, l3: 0.000104, l4: 0.000325, l5: 0.000452, l6: 0.000997
[epoch: 482/1000, batch:   628/ 1052, ite: 126660] train loss: 0.004204, tar: 0.000105 
l0: 0.000088, l1: 0.000093, l2: 0.000127, l3: 0.000293, l4: 0.000605, l5: 0.001140, l6: 0.002405
[epoch: 482/1000, batch:   708/ 1052, ite: 126680] train loss: 0.004202, tar: 0.000104 
l0: 0.000103, l1: 0.000103, l2: 0.000179, l3: 0.000398, l4: 0.000657, l5: 0.001404, l6: 0.002140
[epoch: 482/1000, batch:   788/ 1052, ite: 126700] train loss: 0.004207, tar: 0.000104 
l0: 0.000070, l1: 0.000071, l2: 0.000117, l3: 0.000184, l4: 0.000455, l5: 0.001121, l6: 0.001848
[epoch: 482/1000, batch:   868/ 1052, ite: 126720] train loss: 0.004201, tar: 0.000104 
l0: 0.000055, l1: 0.000055, l2: 0.000121, l3: 0.000212, l4: 0.000641, l5: 0.000743, l6: 0.002867
[epoch: 482/1000, batch:   948/ 1052, ite: 126740] train loss: 0.004221, tar: 0.000105 
l0: 0.000047, l1: 0.000050, l2: 0.000057, l3: 0.000113, l4: 0.000207, l5: 0.000716, l6: 0.001313
[epoch: 482/1000, batch:  1028/ 1052, ite: 126760] train loss: 0.004218, tar: 0.000104 
[Epoch 482/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000129, l1: 0.000131, l2: 0.000193, l3: 0.000300, l4: 0.000632, l5: 0.001600, l6: 0.001876
[epoch: 483/1000, batch:    56/ 1052, ite: 126780] train loss: 0.004224, tar: 0.000105 
l0: 0.000099, l1: 0.000098, l2: 0.000135, l3: 0.000235, l4: 0.000607, l5: 0.001337, l6: 0.001555
[epoch: 483/1000, batch:   136/ 1052, ite: 126800] train loss: 0.004207, tar: 0.000104 
l0: 0.000031, l1: 0.000032, l2: 0.000064, l3: 0.000113, l4: 0.000342, l5: 0.000729, l6: 0.000907
[epoch: 483/1000, batch:   216/ 1052, ite: 126820] train loss: 0.004200, tar: 0.000104 
l0: 0.000082, l1: 0.000078, l2: 0.000130, l3: 0.000192, l4: 0.000422, l5: 0.000851, l6: 0.001374
[epoch: 483/1000, batch:   296/ 1052, ite: 126840] train loss: 0.004214, tar: 0.000105 
l0: 0.000117, l1: 0.000112, l2: 0.000155, l3: 0.000177, l4: 0.000333, l5: 0.001063, l6: 0.001502
[epoch: 483/1000, batch:   376/ 1052, ite: 126860] train loss: 0.004196, tar: 0.000105 
l0: 0.000087, l1: 0.000085, l2: 0.000184, l3: 0.000398, l4: 0.001033, l5: 0.002111, l6: 0.003028
[epoch: 483/1000, batch:   456/ 1052, ite: 126880] train loss: 0.004204, tar: 0.000104 
l0: 0.000051, l1: 0.000050, l2: 0.000083, l3: 0.000198, l4: 0.000339, l5: 0.000972, l6: 0.001475
[epoch: 483/1000, batch:   536/ 1052, ite: 126900] train loss: 0.004198, tar: 0.000104 
l0: 0.000108, l1: 0.000109, l2: 0.000136, l3: 0.000229, l4: 0.000506, l5: 0.000977, l6: 0.001513
[epoch: 483/1000, batch:   616/ 1052, ite: 126920] train loss: 0.004216, tar: 0.000104 
l0: 0.000060, l1: 0.000065, l2: 0.000100, l3: 0.000178, l4: 0.000296, l5: 0.000923, l6: 0.001082
[epoch: 483/1000, batch:   696/ 1052, ite: 126940] train loss: 0.004220, tar: 0.000105 
l0: 0.000107, l1: 0.000106, l2: 0.000156, l3: 0.000269, l4: 0.000482, l5: 0.001303, l6: 0.001950
[epoch: 483/1000, batch:   776/ 1052, ite: 126960] train loss: 0.004213, tar: 0.000104 
l0: 0.000077, l1: 0.000077, l2: 0.000090, l3: 0.000135, l4: 0.000313, l5: 0.001006, l6: 0.001599
[epoch: 483/1000, batch:   856/ 1052, ite: 126980] train loss: 0.004198, tar: 0.000104 
l0: 0.000034, l1: 0.000035, l2: 0.000105, l3: 0.000261, l4: 0.000528, l5: 0.001039, l6: 0.001824
[epoch: 483/1000, batch:   936/ 1052, ite: 127000] train loss: 0.004214, tar: 0.000104 
l0: 0.000015, l1: 0.000016, l2: 0.000036, l3: 0.000115, l4: 0.000269, l5: 0.000731, l6: 0.001167
[epoch: 483/1000, batch:  1016/ 1052, ite: 127020] train loss: 0.004214, tar: 0.000104 
[Epoch 483/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000052, l1: 0.000055, l2: 0.000084, l3: 0.000143, l4: 0.000476, l5: 0.000939, l6: 0.001659
[epoch: 484/1000, batch:    44/ 1052, ite: 127040] train loss: 0.004212, tar: 0.000104 
l0: 0.000036, l1: 0.000035, l2: 0.000061, l3: 0.000141, l4: 0.000328, l5: 0.001029, l6: 0.001733
[epoch: 484/1000, batch:   124/ 1052, ite: 127060] train loss: 0.004216, tar: 0.000104 
l0: 0.000173, l1: 0.000175, l2: 0.000208, l3: 0.000311, l4: 0.000746, l5: 0.002166, l6: 0.003871
[epoch: 484/1000, batch:   204/ 1052, ite: 127080] train loss: 0.004219, tar: 0.000104 
l0: 0.000063, l1: 0.000062, l2: 0.000083, l3: 0.000132, l4: 0.000290, l5: 0.000781, l6: 0.001404
[epoch: 484/1000, batch:   284/ 1052, ite: 127100] train loss: 0.004215, tar: 0.000104 
l0: 0.000049, l1: 0.000051, l2: 0.000095, l3: 0.000345, l4: 0.000706, l5: 0.001396, l6: 0.002013
[epoch: 484/1000, batch:   364/ 1052, ite: 127120] train loss: 0.004216, tar: 0.000104 
l0: 0.000164, l1: 0.000164, l2: 0.000205, l3: 0.000353, l4: 0.000638, l5: 0.001086, l6: 0.001776
[epoch: 484/1000, batch:   444/ 1052, ite: 127140] train loss: 0.004218, tar: 0.000104 
l0: 0.000075, l1: 0.000075, l2: 0.000127, l3: 0.000297, l4: 0.000552, l5: 0.001349, l6: 0.002211
[epoch: 484/1000, batch:   524/ 1052, ite: 127160] train loss: 0.004223, tar: 0.000104 
l0: 0.000051, l1: 0.000048, l2: 0.000120, l3: 0.000266, l4: 0.000733, l5: 0.001798, l6: 0.002938
[epoch: 484/1000, batch:   604/ 1052, ite: 127180] train loss: 0.004222, tar: 0.000105 
l0: 0.000055, l1: 0.000053, l2: 0.000068, l3: 0.000128, l4: 0.000259, l5: 0.000816, l6: 0.001541
[epoch: 484/1000, batch:   684/ 1052, ite: 127200] train loss: 0.004229, tar: 0.000105 
l0: 0.000237, l1: 0.000234, l2: 0.000322, l3: 0.000471, l4: 0.000691, l5: 0.001358, l6: 0.003728
[epoch: 484/1000, batch:   764/ 1052, ite: 127220] train loss: 0.004229, tar: 0.000105 
l0: 0.000076, l1: 0.000076, l2: 0.000114, l3: 0.000204, l4: 0.000553, l5: 0.000932, l6: 0.001653
[epoch: 484/1000, batch:   844/ 1052, ite: 127240] train loss: 0.004229, tar: 0.000105 
l0: 0.000140, l1: 0.000140, l2: 0.000175, l3: 0.000306, l4: 0.000634, l5: 0.001361, l6: 0.002385
[epoch: 484/1000, batch:   924/ 1052, ite: 127260] train loss: 0.004237, tar: 0.000105 
l0: 0.000203, l1: 0.000204, l2: 0.000245, l3: 0.000342, l4: 0.000567, l5: 0.001272, l6: 0.002251
[epoch: 484/1000, batch:  1004/ 1052, ite: 127280] train loss: 0.004240, tar: 0.000105 
[Epoch 484/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000056, l1: 0.000055, l2: 0.000080, l3: 0.000237, l4: 0.000388, l5: 0.000656, l6: 0.001042
[epoch: 485/1000, batch:    32/ 1052, ite: 127300] train loss: 0.004228, tar: 0.000105 
l0: 0.000059, l1: 0.000059, l2: 0.000094, l3: 0.000191, l4: 0.000445, l5: 0.001042, l6: 0.001331
[epoch: 485/1000, batch:   112/ 1052, ite: 127320] train loss: 0.004230, tar: 0.000105 
l0: 0.000104, l1: 0.000101, l2: 0.000107, l3: 0.000142, l4: 0.000242, l5: 0.000483, l6: 0.000809
[epoch: 485/1000, batch:   192/ 1052, ite: 127340] train loss: 0.004235, tar: 0.000105 
l0: 0.000071, l1: 0.000070, l2: 0.000120, l3: 0.000248, l4: 0.000544, l5: 0.001134, l6: 0.002783
[epoch: 485/1000, batch:   272/ 1052, ite: 127360] train loss: 0.004228, tar: 0.000105 
l0: 0.000107, l1: 0.000109, l2: 0.000115, l3: 0.000240, l4: 0.000613, l5: 0.001422, l6: 0.002021
[epoch: 485/1000, batch:   352/ 1052, ite: 127380] train loss: 0.004236, tar: 0.000105 
l0: 0.000201, l1: 0.000207, l2: 0.000225, l3: 0.000252, l4: 0.000563, l5: 0.001869, l6: 0.003427
[epoch: 485/1000, batch:   432/ 1052, ite: 127400] train loss: 0.004236, tar: 0.000105 
l0: 0.000095, l1: 0.000097, l2: 0.000180, l3: 0.000463, l4: 0.001061, l5: 0.001921, l6: 0.003295
[epoch: 485/1000, batch:   512/ 1052, ite: 127420] train loss: 0.004231, tar: 0.000105 
l0: 0.000072, l1: 0.000073, l2: 0.000110, l3: 0.000225, l4: 0.000478, l5: 0.000994, l6: 0.001983
[epoch: 485/1000, batch:   592/ 1052, ite: 127440] train loss: 0.004242, tar: 0.000105 
l0: 0.000078, l1: 0.000079, l2: 0.000110, l3: 0.000177, l4: 0.000408, l5: 0.001081, l6: 0.001845
[epoch: 485/1000, batch:   672/ 1052, ite: 127460] train loss: 0.004232, tar: 0.000105 
l0: 0.000041, l1: 0.000043, l2: 0.000065, l3: 0.000112, l4: 0.000400, l5: 0.000949, l6: 0.002025
[epoch: 485/1000, batch:   752/ 1052, ite: 127480] train loss: 0.004229, tar: 0.000105 
l0: 0.000081, l1: 0.000080, l2: 0.000123, l3: 0.000253, l4: 0.000494, l5: 0.000596, l6: 0.001759
[epoch: 485/1000, batch:   832/ 1052, ite: 127500] train loss: 0.004224, tar: 0.000105 
l0: 0.000128, l1: 0.000122, l2: 0.000198, l3: 0.000287, l4: 0.000571, l5: 0.000963, l6: 0.001560
[epoch: 485/1000, batch:   912/ 1052, ite: 127520] train loss: 0.004228, tar: 0.000105 
l0: 0.000053, l1: 0.000055, l2: 0.000056, l3: 0.000109, l4: 0.000254, l5: 0.000643, l6: 0.001217
[epoch: 485/1000, batch:   992/ 1052, ite: 127540] train loss: 0.004226, tar: 0.000105 
[Epoch 485/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000100, l1: 0.000102, l2: 0.000120, l3: 0.000170, l4: 0.000382, l5: 0.000819, l6: 0.001638
[epoch: 486/1000, batch:    20/ 1052, ite: 127560] train loss: 0.004224, tar: 0.000105 
l0: 0.000084, l1: 0.000086, l2: 0.000120, l3: 0.000214, l4: 0.000533, l5: 0.000795, l6: 0.001316
[epoch: 486/1000, batch:   100/ 1052, ite: 127580] train loss: 0.004218, tar: 0.000105 
l0: 0.000079, l1: 0.000081, l2: 0.000114, l3: 0.000189, l4: 0.000461, l5: 0.001330, l6: 0.001551
[epoch: 486/1000, batch:   180/ 1052, ite: 127600] train loss: 0.004221, tar: 0.000105 
l0: 0.000164, l1: 0.000164, l2: 0.000186, l3: 0.000294, l4: 0.000589, l5: 0.001317, l6: 0.002819
[epoch: 486/1000, batch:   260/ 1052, ite: 127620] train loss: 0.004225, tar: 0.000105 
l0: 0.000207, l1: 0.000205, l2: 0.000343, l3: 0.000446, l4: 0.000728, l5: 0.001684, l6: 0.002557
[epoch: 486/1000, batch:   340/ 1052, ite: 127640] train loss: 0.004237, tar: 0.000106 
l0: 0.000083, l1: 0.000088, l2: 0.000116, l3: 0.000142, l4: 0.000319, l5: 0.000806, l6: 0.001239
[epoch: 486/1000, batch:   420/ 1052, ite: 127660] train loss: 0.004229, tar: 0.000106 
l0: 0.000073, l1: 0.000074, l2: 0.000109, l3: 0.000161, l4: 0.000375, l5: 0.000788, l6: 0.000987
[epoch: 486/1000, batch:   500/ 1052, ite: 127680] train loss: 0.004225, tar: 0.000106 
l0: 0.000720, l1: 0.000693, l2: 0.000592, l3: 0.000643, l4: 0.001155, l5: 0.001578, l6: 0.002656
[epoch: 486/1000, batch:   580/ 1052, ite: 127700] train loss: 0.004240, tar: 0.000106 
l0: 0.000069, l1: 0.000069, l2: 0.000109, l3: 0.000221, l4: 0.000381, l5: 0.001247, l6: 0.001783
[epoch: 486/1000, batch:   660/ 1052, ite: 127720] train loss: 0.004242, tar: 0.000107 
l0: 0.000107, l1: 0.000109, l2: 0.000169, l3: 0.000405, l4: 0.000883, l5: 0.001528, l6: 0.002851
[epoch: 486/1000, batch:   740/ 1052, ite: 127740] train loss: 0.004247, tar: 0.000107 
l0: 0.000097, l1: 0.000096, l2: 0.000112, l3: 0.000180, l4: 0.000405, l5: 0.000572, l6: 0.001307
[epoch: 486/1000, batch:   820/ 1052, ite: 127760] train loss: 0.004245, tar: 0.000107 
l0: 0.000099, l1: 0.000098, l2: 0.000161, l3: 0.000257, l4: 0.000446, l5: 0.001075, l6: 0.002341
[epoch: 486/1000, batch:   900/ 1052, ite: 127780] train loss: 0.004239, tar: 0.000107 
l0: 0.000120, l1: 0.000123, l2: 0.000160, l3: 0.000242, l4: 0.000534, l5: 0.001979, l6: 0.004087
[epoch: 486/1000, batch:   980/ 1052, ite: 127800] train loss: 0.004243, tar: 0.000107 
[Epoch 486/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000077, l1: 0.000079, l2: 0.000090, l3: 0.000144, l4: 0.000579, l5: 0.001027, l6: 0.001648
[epoch: 487/1000, batch:     8/ 1052, ite: 127820] train loss: 0.004250, tar: 0.000108 
l0: 0.000218, l1: 0.000217, l2: 0.000213, l3: 0.000366, l4: 0.000636, l5: 0.000893, l6: 0.002410
[epoch: 487/1000, batch:    88/ 1052, ite: 127840] train loss: 0.004252, tar: 0.000108 
l0: 0.000305, l1: 0.000314, l2: 0.000347, l3: 0.000539, l4: 0.000721, l5: 0.001853, l6: 0.002433
[epoch: 487/1000, batch:   168/ 1052, ite: 127860] train loss: 0.004256, tar: 0.000109 
l0: 0.000070, l1: 0.000066, l2: 0.000106, l3: 0.000244, l4: 0.000483, l5: 0.001123, l6: 0.000944
[epoch: 487/1000, batch:   248/ 1052, ite: 127880] train loss: 0.004249, tar: 0.000109 
l0: 0.000127, l1: 0.000151, l2: 0.000190, l3: 0.000203, l4: 0.000461, l5: 0.000919, l6: 0.001967
[epoch: 487/1000, batch:   328/ 1052, ite: 127900] train loss: 0.004246, tar: 0.000109 
l0: 0.000241, l1: 0.000247, l2: 0.000297, l3: 0.000515, l4: 0.001229, l5: 0.002639, l6: 0.004121
[epoch: 487/1000, batch:   408/ 1052, ite: 127920] train loss: 0.004246, tar: 0.000109 
l0: 0.000234, l1: 0.000232, l2: 0.000298, l3: 0.000457, l4: 0.000934, l5: 0.001719, l6: 0.002943
[epoch: 487/1000, batch:   488/ 1052, ite: 127940] train loss: 0.004253, tar: 0.000109 
l0: 0.000075, l1: 0.000073, l2: 0.000116, l3: 0.000173, l4: 0.000491, l5: 0.000733, l6: 0.002379
[epoch: 487/1000, batch:   568/ 1052, ite: 127960] train loss: 0.004257, tar: 0.000109 
l0: 0.000210, l1: 0.000213, l2: 0.000262, l3: 0.000374, l4: 0.000873, l5: 0.001732, l6: 0.003111
[epoch: 487/1000, batch:   648/ 1052, ite: 127980] train loss: 0.004253, tar: 0.000109 
l0: 0.000114, l1: 0.000114, l2: 0.000159, l3: 0.000326, l4: 0.000511, l5: 0.001184, l6: 0.001951
[epoch: 487/1000, batch:   728/ 1052, ite: 128000] train loss: 0.004254, tar: 0.000109 
l0: 0.000128, l1: 0.000132, l2: 0.000156, l3: 0.000223, l4: 0.000526, l5: 0.002006, l6: 0.003432
[epoch: 487/1000, batch:   808/ 1052, ite: 128020] train loss: 0.004257, tar: 0.000110 
l0: 0.000156, l1: 0.000159, l2: 0.000244, l3: 0.000285, l4: 0.000755, l5: 0.001379, l6: 0.002530
[epoch: 487/1000, batch:   888/ 1052, ite: 128040] train loss: 0.004256, tar: 0.000110 
l0: 0.000073, l1: 0.000072, l2: 0.000111, l3: 0.000210, l4: 0.000401, l5: 0.001028, l6: 0.001600
[epoch: 487/1000, batch:   968/ 1052, ite: 128060] train loss: 0.004254, tar: 0.000110 
l0: 0.000068, l1: 0.000068, l2: 0.000121, l3: 0.000140, l4: 0.000456, l5: 0.001130, l6: 0.001505
[epoch: 487/1000, batch:  1048/ 1052, ite: 128080] train loss: 0.004255, tar: 0.000110 
[Epoch 487/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000104, l1: 0.000110, l2: 0.000142, l3: 0.000203, l4: 0.000267, l5: 0.000911, l6: 0.001369
[epoch: 488/1000, batch:    76/ 1052, ite: 128100] train loss: 0.004260, tar: 0.000111 
l0: 0.000265, l1: 0.000265, l2: 0.000391, l3: 0.000498, l4: 0.000797, l5: 0.002647, l6: 0.004795
[epoch: 488/1000, batch:   156/ 1052, ite: 128120] train loss: 0.004261, tar: 0.000111 
l0: 0.000068, l1: 0.000071, l2: 0.000110, l3: 0.000174, l4: 0.000363, l5: 0.001123, l6: 0.001921
[epoch: 488/1000, batch:   236/ 1052, ite: 128140] train loss: 0.004268, tar: 0.000111 
l0: 0.000053, l1: 0.000052, l2: 0.000097, l3: 0.000146, l4: 0.000383, l5: 0.000741, l6: 0.001937
[epoch: 488/1000, batch:   316/ 1052, ite: 128160] train loss: 0.004265, tar: 0.000111 
l0: 0.000248, l1: 0.000250, l2: 0.000335, l3: 0.000545, l4: 0.000893, l5: 0.001556, l6: 0.002762
[epoch: 488/1000, batch:   396/ 1052, ite: 128180] train loss: 0.004267, tar: 0.000111 
l0: 0.000039, l1: 0.000037, l2: 0.000082, l3: 0.000169, l4: 0.000506, l5: 0.000871, l6: 0.002279
[epoch: 488/1000, batch:   476/ 1052, ite: 128200] train loss: 0.004261, tar: 0.000111 
l0: 0.000103, l1: 0.000105, l2: 0.000122, l3: 0.000201, l4: 0.000398, l5: 0.000956, l6: 0.001529
[epoch: 488/1000, batch:   556/ 1052, ite: 128220] train loss: 0.004256, tar: 0.000111 
l0: 0.000226, l1: 0.000230, l2: 0.000314, l3: 0.000407, l4: 0.000826, l5: 0.001801, l6: 0.003320
[epoch: 488/1000, batch:   636/ 1052, ite: 128240] train loss: 0.004264, tar: 0.000111 
l0: 0.000055, l1: 0.000059, l2: 0.000073, l3: 0.000143, l4: 0.000357, l5: 0.000687, l6: 0.001506
[epoch: 488/1000, batch:   716/ 1052, ite: 128260] train loss: 0.004264, tar: 0.000111 
l0: 0.000089, l1: 0.000089, l2: 0.000145, l3: 0.000240, l4: 0.000476, l5: 0.000959, l6: 0.001122
[epoch: 488/1000, batch:   796/ 1052, ite: 128280] train loss: 0.004266, tar: 0.000111 
l0: 0.000068, l1: 0.000070, l2: 0.000096, l3: 0.000126, l4: 0.000551, l5: 0.001191, l6: 0.001181
[epoch: 488/1000, batch:   876/ 1052, ite: 128300] train loss: 0.004263, tar: 0.000111 
l0: 0.000067, l1: 0.000070, l2: 0.000128, l3: 0.000255, l4: 0.000549, l5: 0.001157, l6: 0.003120
[epoch: 488/1000, batch:   956/ 1052, ite: 128320] train loss: 0.004260, tar: 0.000111 
l0: 0.000225, l1: 0.000225, l2: 0.000400, l3: 0.000499, l4: 0.000914, l5: 0.001508, l6: 0.005871
[epoch: 488/1000, batch:  1036/ 1052, ite: 128340] train loss: 0.004261, tar: 0.000111 
[Epoch 488/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000092, l1: 0.000093, l2: 0.000170, l3: 0.000257, l4: 0.000419, l5: 0.000931, l6: 0.001870
[epoch: 489/1000, batch:    64/ 1052, ite: 128360] train loss: 0.004262, tar: 0.000111 
l0: 0.000059, l1: 0.000058, l2: 0.000074, l3: 0.000159, l4: 0.000338, l5: 0.000478, l6: 0.000773
[epoch: 489/1000, batch:   144/ 1052, ite: 128380] train loss: 0.004260, tar: 0.000111 
l0: 0.000100, l1: 0.000107, l2: 0.000136, l3: 0.000274, l4: 0.000418, l5: 0.000869, l6: 0.001404
[epoch: 489/1000, batch:   224/ 1052, ite: 128400] train loss: 0.004258, tar: 0.000111 
l0: 0.000120, l1: 0.000114, l2: 0.000184, l3: 0.000364, l4: 0.000628, l5: 0.001087, l6: 0.001845
[epoch: 489/1000, batch:   304/ 1052, ite: 128420] train loss: 0.004260, tar: 0.000111 
l0: 0.000036, l1: 0.000034, l2: 0.000121, l3: 0.000267, l4: 0.000428, l5: 0.000729, l6: 0.002033
[epoch: 489/1000, batch:   384/ 1052, ite: 128440] train loss: 0.004266, tar: 0.000111 
l0: 0.000107, l1: 0.000108, l2: 0.000169, l3: 0.000276, l4: 0.000535, l5: 0.001059, l6: 0.002759
[epoch: 489/1000, batch:   464/ 1052, ite: 128460] train loss: 0.004261, tar: 0.000111 
l0: 0.000019, l1: 0.000017, l2: 0.000049, l3: 0.000136, l4: 0.000417, l5: 0.000694, l6: 0.001073
[epoch: 489/1000, batch:   544/ 1052, ite: 128480] train loss: 0.004262, tar: 0.000111 
l0: 0.000073, l1: 0.000074, l2: 0.000113, l3: 0.000228, l4: 0.000343, l5: 0.000886, l6: 0.001849
[epoch: 489/1000, batch:   624/ 1052, ite: 128500] train loss: 0.004262, tar: 0.000111 
l0: 0.000197, l1: 0.000203, l2: 0.000178, l3: 0.000274, l4: 0.000578, l5: 0.001270, l6: 0.001709
[epoch: 489/1000, batch:   704/ 1052, ite: 128520] train loss: 0.004260, tar: 0.000111 
l0: 0.000059, l1: 0.000060, l2: 0.000074, l3: 0.000100, l4: 0.000208, l5: 0.000639, l6: 0.000959
[epoch: 489/1000, batch:   784/ 1052, ite: 128540] train loss: 0.004264, tar: 0.000111 
l0: 0.000085, l1: 0.000085, l2: 0.000112, l3: 0.000157, l4: 0.000452, l5: 0.000849, l6: 0.001906
[epoch: 489/1000, batch:   864/ 1052, ite: 128560] train loss: 0.004264, tar: 0.000111 
l0: 0.000039, l1: 0.000041, l2: 0.000048, l3: 0.000097, l4: 0.000225, l5: 0.000574, l6: 0.000574
[epoch: 489/1000, batch:   944/ 1052, ite: 128580] train loss: 0.004262, tar: 0.000111 
l0: 0.000090, l1: 0.000089, l2: 0.000158, l3: 0.000225, l4: 0.000467, l5: 0.000997, l6: 0.001910
[epoch: 489/1000, batch:  1024/ 1052, ite: 128600] train loss: 0.004261, tar: 0.000111 
[Epoch 489/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000044, l1: 0.000045, l2: 0.000064, l3: 0.000118, l4: 0.000227, l5: 0.000833, l6: 0.001272
[epoch: 490/1000, batch:    52/ 1052, ite: 128620] train loss: 0.004262, tar: 0.000111 
l0: 0.000198, l1: 0.000200, l2: 0.000249, l3: 0.000367, l4: 0.000764, l5: 0.001890, l6: 0.002186
[epoch: 490/1000, batch:   132/ 1052, ite: 128640] train loss: 0.004259, tar: 0.000111 
l0: 0.000143, l1: 0.000147, l2: 0.000212, l3: 0.000288, l4: 0.000491, l5: 0.001131, l6: 0.002342
[epoch: 490/1000, batch:   212/ 1052, ite: 128660] train loss: 0.004264, tar: 0.000111 
l0: 0.000098, l1: 0.000097, l2: 0.000139, l3: 0.000277, l4: 0.000555, l5: 0.001394, l6: 0.002655
[epoch: 490/1000, batch:   292/ 1052, ite: 128680] train loss: 0.004262, tar: 0.000111 
l0: 0.000349, l1: 0.000368, l2: 0.000445, l3: 0.000604, l4: 0.001507, l5: 0.003619, l6: 0.007270
[epoch: 490/1000, batch:   372/ 1052, ite: 128700] train loss: 0.004264, tar: 0.000111 
l0: 0.000098, l1: 0.000104, l2: 0.000158, l3: 0.000159, l4: 0.000310, l5: 0.000840, l6: 0.001154
[epoch: 490/1000, batch:   452/ 1052, ite: 128720] train loss: 0.004262, tar: 0.000111 
l0: 0.000132, l1: 0.000133, l2: 0.000148, l3: 0.000236, l4: 0.000454, l5: 0.000937, l6: 0.001018
[epoch: 490/1000, batch:   532/ 1052, ite: 128740] train loss: 0.004261, tar: 0.000111 
l0: 0.000149, l1: 0.000149, l2: 0.000195, l3: 0.000249, l4: 0.000389, l5: 0.000919, l6: 0.002406
[epoch: 490/1000, batch:   612/ 1052, ite: 128760] train loss: 0.004259, tar: 0.000111 
l0: 0.000112, l1: 0.000110, l2: 0.000164, l3: 0.000365, l4: 0.000783, l5: 0.001266, l6: 0.002617
[epoch: 490/1000, batch:   692/ 1052, ite: 128780] train loss: 0.004261, tar: 0.000111 
l0: 0.000063, l1: 0.000064, l2: 0.000073, l3: 0.000120, l4: 0.000461, l5: 0.000903, l6: 0.001702
[epoch: 490/1000, batch:   772/ 1052, ite: 128800] train loss: 0.004260, tar: 0.000111 
l0: 0.000062, l1: 0.000060, l2: 0.000093, l3: 0.000166, l4: 0.000381, l5: 0.000873, l6: 0.001300
[epoch: 490/1000, batch:   852/ 1052, ite: 128820] train loss: 0.004261, tar: 0.000111 
l0: 0.000083, l1: 0.000086, l2: 0.000090, l3: 0.000105, l4: 0.000313, l5: 0.000692, l6: 0.001056
[epoch: 490/1000, batch:   932/ 1052, ite: 128840] train loss: 0.004260, tar: 0.000111 
l0: 0.000031, l1: 0.000029, l2: 0.000065, l3: 0.000139, l4: 0.000411, l5: 0.000961, l6: 0.001486
[epoch: 490/1000, batch:  1012/ 1052, ite: 128860] train loss: 0.004259, tar: 0.000111 
[Epoch 490/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000142, l1: 0.000143, l2: 0.000183, l3: 0.000270, l4: 0.000454, l5: 0.000919, l6: 0.002168
[epoch: 491/1000, batch:    40/ 1052, ite: 128880] train loss: 0.004261, tar: 0.000111 
l0: 0.000147, l1: 0.000147, l2: 0.000197, l3: 0.000234, l4: 0.000557, l5: 0.000716, l6: 0.002064
[epoch: 491/1000, batch:   120/ 1052, ite: 128900] train loss: 0.004263, tar: 0.000111 
l0: 0.000097, l1: 0.000099, l2: 0.000173, l3: 0.000332, l4: 0.000651, l5: 0.001432, l6: 0.002968
[epoch: 491/1000, batch:   200/ 1052, ite: 128920] train loss: 0.004262, tar: 0.000111 
l0: 0.000052, l1: 0.000051, l2: 0.000087, l3: 0.000231, l4: 0.000409, l5: 0.000693, l6: 0.001598
[epoch: 491/1000, batch:   280/ 1052, ite: 128940] train loss: 0.004259, tar: 0.000111 
l0: 0.000065, l1: 0.000064, l2: 0.000093, l3: 0.000149, l4: 0.000334, l5: 0.000894, l6: 0.002222
[epoch: 491/1000, batch:   360/ 1052, ite: 128960] train loss: 0.004259, tar: 0.000110 
l0: 0.000070, l1: 0.000072, l2: 0.000094, l3: 0.000146, l4: 0.000229, l5: 0.000755, l6: 0.001545
[epoch: 491/1000, batch:   440/ 1052, ite: 128980] train loss: 0.004257, tar: 0.000110 
l0: 0.000034, l1: 0.000034, l2: 0.000076, l3: 0.000169, l4: 0.000332, l5: 0.000600, l6: 0.000685
[epoch: 491/1000, batch:   520/ 1052, ite: 129000] train loss: 0.004257, tar: 0.000110 
l0: 0.000218, l1: 0.000224, l2: 0.000232, l3: 0.000293, l4: 0.000514, l5: 0.001272, l6: 0.002960
[epoch: 491/1000, batch:   600/ 1052, ite: 129020] train loss: 0.004254, tar: 0.000110 
l0: 0.000040, l1: 0.000040, l2: 0.000059, l3: 0.000177, l4: 0.000514, l5: 0.000959, l6: 0.001791
[epoch: 491/1000, batch:   680/ 1052, ite: 129040] train loss: 0.004255, tar: 0.000110 
l0: 0.000159, l1: 0.000159, l2: 0.000200, l3: 0.000275, l4: 0.000628, l5: 0.000983, l6: 0.001752
[epoch: 491/1000, batch:   760/ 1052, ite: 129060] train loss: 0.004253, tar: 0.000110 
l0: 0.000101, l1: 0.000104, l2: 0.000125, l3: 0.000258, l4: 0.000424, l5: 0.001467, l6: 0.002573
[epoch: 491/1000, batch:   840/ 1052, ite: 129080] train loss: 0.004255, tar: 0.000110 
l0: 0.000115, l1: 0.000114, l2: 0.000145, l3: 0.000187, l4: 0.000376, l5: 0.001386, l6: 0.002484
[epoch: 491/1000, batch:   920/ 1052, ite: 129100] train loss: 0.004255, tar: 0.000110 
l0: 0.000191, l1: 0.000191, l2: 0.000276, l3: 0.000499, l4: 0.000838, l5: 0.001593, l6: 0.002771
[epoch: 491/1000, batch:  1000/ 1052, ite: 129120] train loss: 0.004259, tar: 0.000110 
[Epoch 491/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000153, l1: 0.000157, l2: 0.000212, l3: 0.000237, l4: 0.000631, l5: 0.001938, l6: 0.003210
[epoch: 492/1000, batch:    28/ 1052, ite: 129140] train loss: 0.004260, tar: 0.000110 
l0: 0.000167, l1: 0.000168, l2: 0.000283, l3: 0.000470, l4: 0.000809, l5: 0.001341, l6: 0.004281
[epoch: 492/1000, batch:   108/ 1052, ite: 129160] train loss: 0.004262, tar: 0.000110 
l0: 0.000154, l1: 0.000149, l2: 0.000192, l3: 0.000355, l4: 0.000569, l5: 0.000900, l6: 0.001796
[epoch: 492/1000, batch:   188/ 1052, ite: 129180] train loss: 0.004258, tar: 0.000110 
l0: 0.000110, l1: 0.000112, l2: 0.000134, l3: 0.000270, l4: 0.000560, l5: 0.001326, l6: 0.002264
[epoch: 492/1000, batch:   268/ 1052, ite: 129200] train loss: 0.004258, tar: 0.000110 
l0: 0.000065, l1: 0.000069, l2: 0.000082, l3: 0.000134, l4: 0.000370, l5: 0.000566, l6: 0.001103
[epoch: 492/1000, batch:   348/ 1052, ite: 129220] train loss: 0.004256, tar: 0.000110 
l0: 0.000079, l1: 0.000076, l2: 0.000128, l3: 0.000229, l4: 0.000435, l5: 0.001151, l6: 0.002523
[epoch: 492/1000, batch:   428/ 1052, ite: 129240] train loss: 0.004253, tar: 0.000110 
l0: 0.000361, l1: 0.000368, l2: 0.000447, l3: 0.000588, l4: 0.001158, l5: 0.002227, l6: 0.005481
[epoch: 492/1000, batch:   508/ 1052, ite: 129260] train loss: 0.004254, tar: 0.000110 
l0: 0.000149, l1: 0.000153, l2: 0.000162, l3: 0.000217, l4: 0.000487, l5: 0.000988, l6: 0.001762
[epoch: 492/1000, batch:   588/ 1052, ite: 129280] train loss: 0.004255, tar: 0.000110 
l0: 0.000061, l1: 0.000060, l2: 0.000092, l3: 0.000151, l4: 0.000421, l5: 0.000667, l6: 0.001595
[epoch: 492/1000, batch:   668/ 1052, ite: 129300] train loss: 0.004255, tar: 0.000110 
l0: 0.000095, l1: 0.000094, l2: 0.000198, l3: 0.000465, l4: 0.000897, l5: 0.001264, l6: 0.001849
[epoch: 492/1000, batch:   748/ 1052, ite: 129320] train loss: 0.004252, tar: 0.000110 
l0: 0.000115, l1: 0.000116, l2: 0.000138, l3: 0.000191, l4: 0.000309, l5: 0.000855, l6: 0.001699
[epoch: 492/1000, batch:   828/ 1052, ite: 129340] train loss: 0.004252, tar: 0.000110 
l0: 0.000133, l1: 0.000133, l2: 0.000222, l3: 0.000490, l4: 0.000842, l5: 0.001650, l6: 0.003446
[epoch: 492/1000, batch:   908/ 1052, ite: 129360] train loss: 0.004255, tar: 0.000110 
l0: 0.000137, l1: 0.000132, l2: 0.000138, l3: 0.000202, l4: 0.000545, l5: 0.000873, l6: 0.001252
[epoch: 492/1000, batch:   988/ 1052, ite: 129380] train loss: 0.004253, tar: 0.000110 
[Epoch 492/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000086, l1: 0.000089, l2: 0.000127, l3: 0.000263, l4: 0.000587, l5: 0.001173, l6: 0.001532
[epoch: 493/1000, batch:    16/ 1052, ite: 129400] train loss: 0.004254, tar: 0.000110 
l0: 0.000141, l1: 0.000144, l2: 0.000174, l3: 0.000268, l4: 0.000552, l5: 0.001467, l6: 0.001609
[epoch: 493/1000, batch:    96/ 1052, ite: 129420] train loss: 0.004254, tar: 0.000110 
l0: 0.000195, l1: 0.000196, l2: 0.000256, l3: 0.000444, l4: 0.000599, l5: 0.001454, l6: 0.002730
[epoch: 493/1000, batch:   176/ 1052, ite: 129440] train loss: 0.004258, tar: 0.000110 
l0: 0.000032, l1: 0.000031, l2: 0.000098, l3: 0.000176, l4: 0.000325, l5: 0.000696, l6: 0.001545
[epoch: 493/1000, batch:   256/ 1052, ite: 129460] train loss: 0.004257, tar: 0.000110 
l0: 0.000117, l1: 0.000115, l2: 0.000210, l3: 0.000382, l4: 0.000721, l5: 0.001567, l6: 0.001986
[epoch: 493/1000, batch:   336/ 1052, ite: 129480] train loss: 0.004260, tar: 0.000110 
l0: 0.000132, l1: 0.000134, l2: 0.000158, l3: 0.000304, l4: 0.000627, l5: 0.001295, l6: 0.003117
[epoch: 493/1000, batch:   416/ 1052, ite: 129500] train loss: 0.004258, tar: 0.000110 
l0: 0.000120, l1: 0.000119, l2: 0.000136, l3: 0.000218, l4: 0.000465, l5: 0.000798, l6: 0.002264
[epoch: 493/1000, batch:   496/ 1052, ite: 129520] train loss: 0.004256, tar: 0.000110 
l0: 0.000112, l1: 0.000111, l2: 0.000150, l3: 0.000175, l4: 0.000421, l5: 0.000963, l6: 0.001246
[epoch: 493/1000, batch:   576/ 1052, ite: 129540] train loss: 0.004255, tar: 0.000110 
l0: 0.000109, l1: 0.000107, l2: 0.000159, l3: 0.000229, l4: 0.000392, l5: 0.001044, l6: 0.001801
[epoch: 493/1000, batch:   656/ 1052, ite: 129560] train loss: 0.004255, tar: 0.000110 
l0: 0.000147, l1: 0.000150, l2: 0.000176, l3: 0.000252, l4: 0.000377, l5: 0.001342, l6: 0.002453
[epoch: 493/1000, batch:   736/ 1052, ite: 129580] train loss: 0.004256, tar: 0.000110 
l0: 0.000080, l1: 0.000081, l2: 0.000133, l3: 0.000157, l4: 0.000289, l5: 0.000735, l6: 0.001345
[epoch: 493/1000, batch:   816/ 1052, ite: 129600] train loss: 0.004256, tar: 0.000110 
l0: 0.000155, l1: 0.000156, l2: 0.000176, l3: 0.000335, l4: 0.000531, l5: 0.001200, l6: 0.001950
[epoch: 493/1000, batch:   896/ 1052, ite: 129620] train loss: 0.004259, tar: 0.000110 
l0: 0.000023, l1: 0.000023, l2: 0.000047, l3: 0.000169, l4: 0.000500, l5: 0.001052, l6: 0.001312
[epoch: 493/1000, batch:   976/ 1052, ite: 129640] train loss: 0.004254, tar: 0.000110 
[Epoch 493/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000163, l1: 0.000163, l2: 0.000244, l3: 0.000408, l4: 0.000633, l5: 0.001308, l6: 0.002057
[epoch: 494/1000, batch:     4/ 1052, ite: 129660] train loss: 0.004251, tar: 0.000110 
l0: 0.000128, l1: 0.000128, l2: 0.000150, l3: 0.000276, l4: 0.000817, l5: 0.001619, l6: 0.002880
[epoch: 494/1000, batch:    84/ 1052, ite: 129680] train loss: 0.004253, tar: 0.000110 
l0: 0.000160, l1: 0.000162, l2: 0.000191, l3: 0.000231, l4: 0.000528, l5: 0.001070, l6: 0.003137
[epoch: 494/1000, batch:   164/ 1052, ite: 129700] train loss: 0.004253, tar: 0.000110 
l0: 0.000147, l1: 0.000145, l2: 0.000207, l3: 0.000452, l4: 0.000588, l5: 0.001716, l6: 0.002055
[epoch: 494/1000, batch:   244/ 1052, ite: 129720] train loss: 0.004251, tar: 0.000109 
l0: 0.000158, l1: 0.000161, l2: 0.000188, l3: 0.000318, l4: 0.000628, l5: 0.001815, l6: 0.002582
[epoch: 494/1000, batch:   324/ 1052, ite: 129740] train loss: 0.004252, tar: 0.000110 
l0: 0.000026, l1: 0.000026, l2: 0.000062, l3: 0.000151, l4: 0.000412, l5: 0.000669, l6: 0.001047
[epoch: 494/1000, batch:   404/ 1052, ite: 129760] train loss: 0.004252, tar: 0.000110 
l0: 0.000019, l1: 0.000018, l2: 0.000038, l3: 0.000072, l4: 0.000198, l5: 0.000479, l6: 0.000620
[epoch: 494/1000, batch:   484/ 1052, ite: 129780] train loss: 0.004251, tar: 0.000110 
l0: 0.000048, l1: 0.000049, l2: 0.000075, l3: 0.000149, l4: 0.000307, l5: 0.000766, l6: 0.000909
[epoch: 494/1000, batch:   564/ 1052, ite: 129800] train loss: 0.004252, tar: 0.000109 
l0: 0.000109, l1: 0.000110, l2: 0.000158, l3: 0.000209, l4: 0.000508, l5: 0.000642, l6: 0.002219
[epoch: 494/1000, batch:   644/ 1052, ite: 129820] train loss: 0.004254, tar: 0.000110 
l0: 0.000086, l1: 0.000081, l2: 0.000128, l3: 0.000201, l4: 0.000435, l5: 0.000802, l6: 0.001964
[epoch: 494/1000, batch:   724/ 1052, ite: 129840] train loss: 0.004252, tar: 0.000110 
l0: 0.000060, l1: 0.000060, l2: 0.000114, l3: 0.000237, l4: 0.000415, l5: 0.001415, l6: 0.002071
[epoch: 494/1000, batch:   804/ 1052, ite: 129860] train loss: 0.004250, tar: 0.000110 
l0: 0.000050, l1: 0.000051, l2: 0.000083, l3: 0.000133, l4: 0.000414, l5: 0.000755, l6: 0.001797
[epoch: 494/1000, batch:   884/ 1052, ite: 129880] train loss: 0.004251, tar: 0.000109 
l0: 0.000053, l1: 0.000055, l2: 0.000101, l3: 0.000142, l4: 0.000273, l5: 0.000956, l6: 0.001253
[epoch: 494/1000, batch:   964/ 1052, ite: 129900] train loss: 0.004249, tar: 0.000109 
l0: 0.000055, l1: 0.000061, l2: 0.000100, l3: 0.000221, l4: 0.000573, l5: 0.000801, l6: 0.003365
[epoch: 494/1000, batch:  1044/ 1052, ite: 129920] train loss: 0.004252, tar: 0.000109 
[Epoch 494/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000080, l1: 0.000082, l2: 0.000134, l3: 0.000203, l4: 0.000574, l5: 0.000885, l6: 0.001217
[epoch: 495/1000, batch:    72/ 1052, ite: 129940] train loss: 0.004252, tar: 0.000109 
l0: 0.000129, l1: 0.000129, l2: 0.000225, l3: 0.000368, l4: 0.000940, l5: 0.002072, l6: 0.003184
[epoch: 495/1000, batch:   152/ 1052, ite: 129960] train loss: 0.004254, tar: 0.000109 
l0: 0.000068, l1: 0.000069, l2: 0.000109, l3: 0.000211, l4: 0.000417, l5: 0.000887, l6: 0.002478
[epoch: 495/1000, batch:   232/ 1052, ite: 129980] train loss: 0.004254, tar: 0.000109 
l0: 0.000088, l1: 0.000088, l2: 0.000134, l3: 0.000293, l4: 0.000608, l5: 0.001389, l6: 0.002218
[epoch: 495/1000, batch:   312/ 1052, ite: 130000] train loss: 0.004253, tar: 0.000109 
l0: 0.000076, l1: 0.000075, l2: 0.000093, l3: 0.000150, l4: 0.000288, l5: 0.000585, l6: 0.000880
[epoch: 495/1000, batch:   392/ 1052, ite: 130020] train loss: 0.004250, tar: 0.000109 
l0: 0.000064, l1: 0.000064, l2: 0.000107, l3: 0.000162, l4: 0.000479, l5: 0.000661, l6: 0.001570
[epoch: 495/1000, batch:   472/ 1052, ite: 130040] train loss: 0.004252, tar: 0.000109 
l0: 0.000108, l1: 0.000110, l2: 0.000126, l3: 0.000178, l4: 0.000407, l5: 0.000952, l6: 0.001235
[epoch: 495/1000, batch:   552/ 1052, ite: 130060] train loss: 0.004253, tar: 0.000109 
l0: 0.000043, l1: 0.000043, l2: 0.000071, l3: 0.000128, l4: 0.000293, l5: 0.000835, l6: 0.000994
[epoch: 495/1000, batch:   632/ 1052, ite: 130080] train loss: 0.004253, tar: 0.000109 
l0: 0.000059, l1: 0.000061, l2: 0.000086, l3: 0.000214, l4: 0.000537, l5: 0.001223, l6: 0.002141
[epoch: 495/1000, batch:   712/ 1052, ite: 130100] train loss: 0.004259, tar: 0.000109 
l0: 0.000086, l1: 0.000086, l2: 0.000127, l3: 0.000223, l4: 0.000336, l5: 0.000778, l6: 0.001737
[epoch: 495/1000, batch:   792/ 1052, ite: 130120] train loss: 0.004259, tar: 0.000109 
l0: 0.000054, l1: 0.000053, l2: 0.000123, l3: 0.000196, l4: 0.000476, l5: 0.000873, l6: 0.001757
[epoch: 495/1000, batch:   872/ 1052, ite: 130140] train loss: 0.004257, tar: 0.000109 
l0: 0.000095, l1: 0.000095, l2: 0.000119, l3: 0.000238, l4: 0.000398, l5: 0.000797, l6: 0.001465
[epoch: 495/1000, batch:   952/ 1052, ite: 130160] train loss: 0.004252, tar: 0.000109 
l0: 0.000075, l1: 0.000076, l2: 0.000128, l3: 0.000176, l4: 0.000357, l5: 0.000785, l6: 0.001168
[epoch: 495/1000, batch:  1032/ 1052, ite: 130180] train loss: 0.004252, tar: 0.000109 
[Epoch 495/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000073, l1: 0.000072, l2: 0.000107, l3: 0.000161, l4: 0.000380, l5: 0.001116, l6: 0.001789
[epoch: 496/1000, batch:    60/ 1052, ite: 130200] train loss: 0.004250, tar: 0.000109 
l0: 0.000084, l1: 0.000082, l2: 0.000132, l3: 0.000220, l4: 0.000420, l5: 0.000971, l6: 0.002170
[epoch: 496/1000, batch:   140/ 1052, ite: 130220] train loss: 0.004251, tar: 0.000109 
l0: 0.000117, l1: 0.000120, l2: 0.000136, l3: 0.000148, l4: 0.000205, l5: 0.000909, l6: 0.001449
[epoch: 496/1000, batch:   220/ 1052, ite: 130240] train loss: 0.004254, tar: 0.000109 
l0: 0.000017, l1: 0.000016, l2: 0.000039, l3: 0.000124, l4: 0.000390, l5: 0.000693, l6: 0.001048
[epoch: 496/1000, batch:   300/ 1052, ite: 130260] train loss: 0.004251, tar: 0.000109 
l0: 0.000028, l1: 0.000028, l2: 0.000042, l3: 0.000092, l4: 0.000195, l5: 0.000469, l6: 0.001166
[epoch: 496/1000, batch:   380/ 1052, ite: 130280] train loss: 0.004250, tar: 0.000109 
l0: 0.000061, l1: 0.000060, l2: 0.000099, l3: 0.000193, l4: 0.000556, l5: 0.000867, l6: 0.001910
[epoch: 496/1000, batch:   460/ 1052, ite: 130300] train loss: 0.004253, tar: 0.000109 
l0: 0.000066, l1: 0.000069, l2: 0.000093, l3: 0.000164, l4: 0.000316, l5: 0.000640, l6: 0.001737
[epoch: 496/1000, batch:   540/ 1052, ite: 130320] train loss: 0.004253, tar: 0.000109 
l0: 0.000085, l1: 0.000087, l2: 0.000126, l3: 0.000312, l4: 0.000440, l5: 0.001196, l6: 0.001516
[epoch: 496/1000, batch:   620/ 1052, ite: 130340] train loss: 0.004254, tar: 0.000109 
l0: 0.000097, l1: 0.000100, l2: 0.000141, l3: 0.000255, l4: 0.000533, l5: 0.001157, l6: 0.001509
[epoch: 496/1000, batch:   700/ 1052, ite: 130360] train loss: 0.004252, tar: 0.000109 
l0: 0.000376, l1: 0.000376, l2: 0.000401, l3: 0.000380, l4: 0.000661, l5: 0.001452, l6: 0.002605
[epoch: 496/1000, batch:   780/ 1052, ite: 130380] train loss: 0.004254, tar: 0.000109 
l0: 0.000370, l1: 0.000370, l2: 0.000387, l3: 0.000520, l4: 0.000910, l5: 0.001545, l6: 0.002420
[epoch: 496/1000, batch:   860/ 1052, ite: 130400] train loss: 0.004255, tar: 0.000109 
l0: 0.000021, l1: 0.000021, l2: 0.000034, l3: 0.000127, l4: 0.000275, l5: 0.000554, l6: 0.000872
[epoch: 496/1000, batch:   940/ 1052, ite: 130420] train loss: 0.004252, tar: 0.000109 
l0: 0.000031, l1: 0.000032, l2: 0.000049, l3: 0.000082, l4: 0.000338, l5: 0.001150, l6: 0.001298
[epoch: 496/1000, batch:  1020/ 1052, ite: 130440] train loss: 0.004250, tar: 0.000109 
[Epoch 496/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000149, l1: 0.000175, l2: 0.000173, l3: 0.000208, l4: 0.000481, l5: 0.001072, l6: 0.001505
[epoch: 497/1000, batch:    48/ 1052, ite: 130460] train loss: 0.004251, tar: 0.000109 
l0: 0.000107, l1: 0.000106, l2: 0.000139, l3: 0.000229, l4: 0.000542, l5: 0.001298, l6: 0.002029
[epoch: 497/1000, batch:   128/ 1052, ite: 130480] train loss: 0.004250, tar: 0.000109 
l0: 0.000098, l1: 0.000100, l2: 0.000123, l3: 0.000233, l4: 0.000485, l5: 0.001202, l6: 0.002310
[epoch: 497/1000, batch:   208/ 1052, ite: 130500] train loss: 0.004249, tar: 0.000109 
l0: 0.000114, l1: 0.000117, l2: 0.000151, l3: 0.000239, l4: 0.000475, l5: 0.001031, l6: 0.001905
[epoch: 497/1000, batch:   288/ 1052, ite: 130520] train loss: 0.004249, tar: 0.000109 
l0: 0.000054, l1: 0.000055, l2: 0.000063, l3: 0.000087, l4: 0.000347, l5: 0.000731, l6: 0.001053
[epoch: 497/1000, batch:   368/ 1052, ite: 130540] train loss: 0.004248, tar: 0.000109 
l0: 0.000088, l1: 0.000088, l2: 0.000096, l3: 0.000211, l4: 0.000402, l5: 0.000926, l6: 0.001375
[epoch: 497/1000, batch:   448/ 1052, ite: 130560] train loss: 0.004249, tar: 0.000109 
l0: 0.000061, l1: 0.000065, l2: 0.000094, l3: 0.000209, l4: 0.000465, l5: 0.000737, l6: 0.001439
[epoch: 497/1000, batch:   528/ 1052, ite: 130580] train loss: 0.004246, tar: 0.000109 
l0: 0.000039, l1: 0.000038, l2: 0.000070, l3: 0.000138, l4: 0.000304, l5: 0.000950, l6: 0.001333
[epoch: 497/1000, batch:   608/ 1052, ite: 130600] train loss: 0.004247, tar: 0.000109 
l0: 0.000100, l1: 0.000103, l2: 0.000118, l3: 0.000238, l4: 0.000491, l5: 0.001109, l6: 0.002008
[epoch: 497/1000, batch:   688/ 1052, ite: 130620] train loss: 0.004249, tar: 0.000109 
l0: 0.000040, l1: 0.000043, l2: 0.000052, l3: 0.000135, l4: 0.000311, l5: 0.001406, l6: 0.001452
[epoch: 497/1000, batch:   768/ 1052, ite: 130640] train loss: 0.004248, tar: 0.000109 
l0: 0.000142, l1: 0.000139, l2: 0.000242, l3: 0.000403, l4: 0.000800, l5: 0.001501, l6: 0.002708
[epoch: 497/1000, batch:   848/ 1052, ite: 130660] train loss: 0.004249, tar: 0.000109 
l0: 0.000035, l1: 0.000032, l2: 0.000084, l3: 0.000192, l4: 0.000308, l5: 0.000918, l6: 0.001136
[epoch: 497/1000, batch:   928/ 1052, ite: 130680] train loss: 0.004249, tar: 0.000109 
l0: 0.000150, l1: 0.000151, l2: 0.000202, l3: 0.000305, l4: 0.000632, l5: 0.000966, l6: 0.001571
[epoch: 497/1000, batch:  1008/ 1052, ite: 130700] train loss: 0.004250, tar: 0.000109 
[Epoch 497/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000034, l1: 0.000033, l2: 0.000068, l3: 0.000082, l4: 0.000256, l5: 0.000728, l6: 0.000786
[epoch: 498/1000, batch:    36/ 1052, ite: 130720] train loss: 0.004248, tar: 0.000109 
l0: 0.000076, l1: 0.000076, l2: 0.000097, l3: 0.000159, l4: 0.000359, l5: 0.001090, l6: 0.001875
[epoch: 498/1000, batch:   116/ 1052, ite: 130740] train loss: 0.004249, tar: 0.000109 
l0: 0.000342, l1: 0.000339, l2: 0.000383, l3: 0.000577, l4: 0.000852, l5: 0.001581, l6: 0.002234
[epoch: 498/1000, batch:   196/ 1052, ite: 130760] train loss: 0.004246, tar: 0.000109 
l0: 0.000106, l1: 0.000108, l2: 0.000143, l3: 0.000301, l4: 0.000577, l5: 0.001187, l6: 0.001744
[epoch: 498/1000, batch:   276/ 1052, ite: 130780] train loss: 0.004247, tar: 0.000109 
l0: 0.000202, l1: 0.000205, l2: 0.000245, l3: 0.000345, l4: 0.000666, l5: 0.001328, l6: 0.002659
[epoch: 498/1000, batch:   356/ 1052, ite: 130800] train loss: 0.004250, tar: 0.000109 
l0: 0.000046, l1: 0.000047, l2: 0.000074, l3: 0.000142, l4: 0.000323, l5: 0.000897, l6: 0.001664
[epoch: 498/1000, batch:   436/ 1052, ite: 130820] train loss: 0.004252, tar: 0.000109 
l0: 0.000120, l1: 0.000118, l2: 0.000145, l3: 0.000225, l4: 0.000356, l5: 0.000775, l6: 0.001634
[epoch: 498/1000, batch:   516/ 1052, ite: 130840] train loss: 0.004253, tar: 0.000109 
l0: 0.000091, l1: 0.000091, l2: 0.000148, l3: 0.000277, l4: 0.000565, l5: 0.001263, l6: 0.002379
[epoch: 498/1000, batch:   596/ 1052, ite: 130860] train loss: 0.004253, tar: 0.000109 
l0: 0.000110, l1: 0.000111, l2: 0.000153, l3: 0.000244, l4: 0.000460, l5: 0.000759, l6: 0.001112
[epoch: 498/1000, batch:   676/ 1052, ite: 130880] train loss: 0.004253, tar: 0.000109 
l0: 0.000057, l1: 0.000057, l2: 0.000077, l3: 0.000154, l4: 0.000464, l5: 0.000840, l6: 0.001435
[epoch: 498/1000, batch:   756/ 1052, ite: 130900] train loss: 0.004253, tar: 0.000109 
l0: 0.000126, l1: 0.000124, l2: 0.000199, l3: 0.000284, l4: 0.000745, l5: 0.001090, l6: 0.003390
[epoch: 498/1000, batch:   836/ 1052, ite: 130920] train loss: 0.004254, tar: 0.000109 
l0: 0.000115, l1: 0.000118, l2: 0.000161, l3: 0.000359, l4: 0.000664, l5: 0.001478, l6: 0.002922
[epoch: 498/1000, batch:   916/ 1052, ite: 130940] train loss: 0.004252, tar: 0.000109 
l0: 0.000056, l1: 0.000056, l2: 0.000071, l3: 0.000154, l4: 0.000375, l5: 0.000733, l6: 0.001238
[epoch: 498/1000, batch:   996/ 1052, ite: 130960] train loss: 0.004250, tar: 0.000109 
[Epoch 498/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000090, l1: 0.000090, l2: 0.000135, l3: 0.000271, l4: 0.000666, l5: 0.001390, l6: 0.003587
[epoch: 499/1000, batch:    24/ 1052, ite: 130980] train loss: 0.004250, tar: 0.000109 
l0: 0.000080, l1: 0.000076, l2: 0.000109, l3: 0.000182, l4: 0.000325, l5: 0.000587, l6: 0.001268
[epoch: 499/1000, batch:   104/ 1052, ite: 131000] train loss: 0.004250, tar: 0.000109 
l0: 0.000066, l1: 0.000067, l2: 0.000101, l3: 0.000192, l4: 0.000413, l5: 0.000591, l6: 0.001536
[epoch: 499/1000, batch:   184/ 1052, ite: 131020] train loss: 0.004252, tar: 0.000109 
l0: 0.000188, l1: 0.000197, l2: 0.000213, l3: 0.000332, l4: 0.000638, l5: 0.001529, l6: 0.002848
[epoch: 499/1000, batch:   264/ 1052, ite: 131040] train loss: 0.004255, tar: 0.000109 
l0: 0.000084, l1: 0.000087, l2: 0.000121, l3: 0.000183, l4: 0.000419, l5: 0.000989, l6: 0.001604
[epoch: 499/1000, batch:   344/ 1052, ite: 131060] train loss: 0.004254, tar: 0.000109 
l0: 0.000036, l1: 0.000035, l2: 0.000064, l3: 0.000203, l4: 0.000385, l5: 0.000844, l6: 0.001298
[epoch: 499/1000, batch:   424/ 1052, ite: 131080] train loss: 0.004255, tar: 0.000109 
l0: 0.000067, l1: 0.000065, l2: 0.000093, l3: 0.000157, l4: 0.000332, l5: 0.000803, l6: 0.001903
[epoch: 499/1000, batch:   504/ 1052, ite: 131100] train loss: 0.004256, tar: 0.000109 
l0: 0.000039, l1: 0.000040, l2: 0.000064, l3: 0.000147, l4: 0.000282, l5: 0.000790, l6: 0.002238
[epoch: 499/1000, batch:   584/ 1052, ite: 131120] train loss: 0.004256, tar: 0.000109 
l0: 0.000148, l1: 0.000149, l2: 0.000228, l3: 0.000288, l4: 0.000447, l5: 0.001197, l6: 0.001817
[epoch: 499/1000, batch:   664/ 1052, ite: 131140] train loss: 0.004258, tar: 0.000109 
l0: 0.000032, l1: 0.000033, l2: 0.000035, l3: 0.000103, l4: 0.000336, l5: 0.000920, l6: 0.001678
[epoch: 499/1000, batch:   744/ 1052, ite: 131160] train loss: 0.004254, tar: 0.000109 
l0: 0.000044, l1: 0.000043, l2: 0.000080, l3: 0.000147, l4: 0.000279, l5: 0.000751, l6: 0.001990
[epoch: 499/1000, batch:   824/ 1052, ite: 131180] train loss: 0.004252, tar: 0.000109 
l0: 0.000083, l1: 0.000085, l2: 0.000113, l3: 0.000142, l4: 0.000345, l5: 0.000842, l6: 0.001042
[epoch: 499/1000, batch:   904/ 1052, ite: 131200] train loss: 0.004250, tar: 0.000109 
l0: 0.000106, l1: 0.000108, l2: 0.000137, l3: 0.000212, l4: 0.000414, l5: 0.000796, l6: 0.002054
[epoch: 499/1000, batch:   984/ 1052, ite: 131220] train loss: 0.004249, tar: 0.000109 
[Epoch 499/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000170, l1: 0.000173, l2: 0.000266, l3: 0.000329, l4: 0.000533, l5: 0.001092, l6: 0.002378
[epoch: 500/1000, batch:    12/ 1052, ite: 131240] train loss: 0.004249, tar: 0.000108 
l0: 0.000045, l1: 0.000045, l2: 0.000076, l3: 0.000126, l4: 0.000333, l5: 0.000642, l6: 0.000825
[epoch: 500/1000, batch:    92/ 1052, ite: 131260] train loss: 0.004249, tar: 0.000108 
l0: 0.000180, l1: 0.000186, l2: 0.000201, l3: 0.000379, l4: 0.000743, l5: 0.001942, l6: 0.003779
[epoch: 500/1000, batch:   172/ 1052, ite: 131280] train loss: 0.004250, tar: 0.000108 
l0: 0.000223, l1: 0.000228, l2: 0.000317, l3: 0.000478, l4: 0.000809, l5: 0.002604, l6: 0.003080
[epoch: 500/1000, batch:   252/ 1052, ite: 131300] train loss: 0.004251, tar: 0.000108 
l0: 0.000114, l1: 0.000123, l2: 0.000139, l3: 0.000152, l4: 0.000298, l5: 0.001161, l6: 0.001838
[epoch: 500/1000, batch:   332/ 1052, ite: 131320] train loss: 0.004248, tar: 0.000108 
l0: 0.000232, l1: 0.000228, l2: 0.000282, l3: 0.000442, l4: 0.000897, l5: 0.001917, l6: 0.003247
[epoch: 500/1000, batch:   412/ 1052, ite: 131340] train loss: 0.004248, tar: 0.000108 
l0: 0.000231, l1: 0.000231, l2: 0.000274, l3: 0.000446, l4: 0.000621, l5: 0.001876, l6: 0.005020
[epoch: 500/1000, batch:   492/ 1052, ite: 131360] train loss: 0.004250, tar: 0.000108 
l0: 0.000086, l1: 0.000077, l2: 0.000123, l3: 0.000296, l4: 0.000457, l5: 0.001005, l6: 0.002059
[epoch: 500/1000, batch:   572/ 1052, ite: 131380] train loss: 0.004250, tar: 0.000108 
l0: 0.000178, l1: 0.000178, l2: 0.000274, l3: 0.000455, l4: 0.000783, l5: 0.001535, l6: 0.005278
[epoch: 500/1000, batch:   652/ 1052, ite: 131400] train loss: 0.004250, tar: 0.000108 
l0: 0.000026, l1: 0.000026, l2: 0.000064, l3: 0.000165, l4: 0.000344, l5: 0.000695, l6: 0.001341
[epoch: 500/1000, batch:   732/ 1052, ite: 131420] train loss: 0.004251, tar: 0.000108 
l0: 0.000094, l1: 0.000090, l2: 0.000149, l3: 0.000212, l4: 0.000420, l5: 0.000933, l6: 0.001996
[epoch: 500/1000, batch:   812/ 1052, ite: 131440] train loss: 0.004251, tar: 0.000108 
l0: 0.000086, l1: 0.000088, l2: 0.000150, l3: 0.000298, l4: 0.000712, l5: 0.001166, l6: 0.002660
[epoch: 500/1000, batch:   892/ 1052, ite: 131460] train loss: 0.004252, tar: 0.000108 
l0: 0.000113, l1: 0.000113, l2: 0.000139, l3: 0.000222, l4: 0.000407, l5: 0.001365, l6: 0.001741
[epoch: 500/1000, batch:   972/ 1052, ite: 131480] train loss: 0.004252, tar: 0.000108 
l0: 0.000152, l1: 0.000150, l2: 0.000190, l3: 0.000302, l4: 0.000491, l5: 0.001126, l6: 0.001722
[epoch: 500/1000, batch:  1052/ 1052, ite: 131500] train loss: 0.004250, tar: 0.000108 
[Epoch 500/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000041, l1: 0.000041, l2: 0.000058, l3: 0.000116, l4: 0.000192, l5: 0.000595, l6: 0.000900
[epoch: 501/1000, batch:    80/ 1052, ite: 131520] train loss: 0.004251, tar: 0.000108 
l0: 0.000074, l1: 0.000078, l2: 0.000127, l3: 0.000205, l4: 0.000695, l5: 0.001424, l6: 0.002575
[epoch: 501/1000, batch:   160/ 1052, ite: 131540] train loss: 0.004252, tar: 0.000108 
l0: 0.000126, l1: 0.000128, l2: 0.000196, l3: 0.000376, l4: 0.000783, l5: 0.001157, l6: 0.002880
[epoch: 501/1000, batch:   240/ 1052, ite: 131560] train loss: 0.004252, tar: 0.000108 
l0: 0.000126, l1: 0.000129, l2: 0.000137, l3: 0.000180, l4: 0.000363, l5: 0.000880, l6: 0.001699
[epoch: 501/1000, batch:   320/ 1052, ite: 131580] train loss: 0.004255, tar: 0.000108 
l0: 0.000123, l1: 0.000125, l2: 0.000221, l3: 0.000372, l4: 0.000849, l5: 0.001532, l6: 0.002952
[epoch: 501/1000, batch:   400/ 1052, ite: 131600] train loss: 0.004255, tar: 0.000108 
l0: 0.000061, l1: 0.000062, l2: 0.000118, l3: 0.000245, l4: 0.000345, l5: 0.000713, l6: 0.002213
[epoch: 501/1000, batch:   480/ 1052, ite: 131620] train loss: 0.004254, tar: 0.000108 
l0: 0.000094, l1: 0.000094, l2: 0.000120, l3: 0.000222, l4: 0.000631, l5: 0.001590, l6: 0.002671
[epoch: 501/1000, batch:   560/ 1052, ite: 131640] train loss: 0.004251, tar: 0.000108 
l0: 0.000064, l1: 0.000063, l2: 0.000097, l3: 0.000198, l4: 0.000547, l5: 0.001651, l6: 0.002102
[epoch: 501/1000, batch:   640/ 1052, ite: 131660] train loss: 0.004250, tar: 0.000108 
l0: 0.000023, l1: 0.000023, l2: 0.000058, l3: 0.000105, l4: 0.000237, l5: 0.000785, l6: 0.000785
[epoch: 501/1000, batch:   720/ 1052, ite: 131680] train loss: 0.004250, tar: 0.000108 
l0: 0.000045, l1: 0.000043, l2: 0.000080, l3: 0.000121, l4: 0.000385, l5: 0.000903, l6: 0.001828
[epoch: 501/1000, batch:   800/ 1052, ite: 131700] train loss: 0.004249, tar: 0.000108 
l0: 0.000052, l1: 0.000051, l2: 0.000110, l3: 0.000177, l4: 0.000266, l5: 0.001249, l6: 0.001894
[epoch: 501/1000, batch:   880/ 1052, ite: 131720] train loss: 0.004249, tar: 0.000108 
l0: 0.000057, l1: 0.000056, l2: 0.000106, l3: 0.000347, l4: 0.000506, l5: 0.001268, l6: 0.001958
[epoch: 501/1000, batch:   960/ 1052, ite: 131740] train loss: 0.004248, tar: 0.000108 
l0: 0.000060, l1: 0.000064, l2: 0.000053, l3: 0.000120, l4: 0.000458, l5: 0.000904, l6: 0.001215
[epoch: 501/1000, batch:  1040/ 1052, ite: 131760] train loss: 0.004246, tar: 0.000108 
[Epoch 501/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000135, l1: 0.000137, l2: 0.000226, l3: 0.000350, l4: 0.000735, l5: 0.001664, l6: 0.002797
[epoch: 502/1000, batch:    68/ 1052, ite: 131780] train loss: 0.004247, tar: 0.000108 
l0: 0.000152, l1: 0.000153, l2: 0.000186, l3: 0.000324, l4: 0.000621, l5: 0.001628, l6: 0.001705
[epoch: 502/1000, batch:   148/ 1052, ite: 131800] train loss: 0.004249, tar: 0.000108 
l0: 0.000106, l1: 0.000110, l2: 0.000185, l3: 0.000389, l4: 0.000859, l5: 0.001809, l6: 0.002813
[epoch: 502/1000, batch:   228/ 1052, ite: 131820] train loss: 0.004247, tar: 0.000108 
l0: 0.000044, l1: 0.000045, l2: 0.000052, l3: 0.000086, l4: 0.000297, l5: 0.000924, l6: 0.001876
[epoch: 502/1000, batch:   308/ 1052, ite: 131840] train loss: 0.004246, tar: 0.000108 
l0: 0.000093, l1: 0.000091, l2: 0.000118, l3: 0.000192, l4: 0.000484, l5: 0.001244, l6: 0.001658
[epoch: 502/1000, batch:   388/ 1052, ite: 131860] train loss: 0.004245, tar: 0.000108 
l0: 0.000031, l1: 0.000032, l2: 0.000061, l3: 0.000130, l4: 0.000464, l5: 0.001451, l6: 0.002300
[epoch: 502/1000, batch:   468/ 1052, ite: 131880] train loss: 0.004245, tar: 0.000108 
l0: 0.000051, l1: 0.000049, l2: 0.000096, l3: 0.000132, l4: 0.000604, l5: 0.001308, l6: 0.001242
[epoch: 502/1000, batch:   548/ 1052, ite: 131900] train loss: 0.004245, tar: 0.000108 
l0: 0.000137, l1: 0.000135, l2: 0.000216, l3: 0.000339, l4: 0.000519, l5: 0.001491, l6: 0.002256
[epoch: 502/1000, batch:   628/ 1052, ite: 131920] train loss: 0.004246, tar: 0.000108 
l0: 0.000156, l1: 0.000157, l2: 0.000227, l3: 0.000295, l4: 0.000774, l5: 0.002237, l6: 0.004050
[epoch: 502/1000, batch:   708/ 1052, ite: 131940] train loss: 0.004247, tar: 0.000108 
l0: 0.000208, l1: 0.000210, l2: 0.000241, l3: 0.000357, l4: 0.000592, l5: 0.001722, l6: 0.003049
[epoch: 502/1000, batch:   788/ 1052, ite: 131960] train loss: 0.004247, tar: 0.000108 
l0: 0.000243, l1: 0.000247, l2: 0.000293, l3: 0.000330, l4: 0.000438, l5: 0.000875, l6: 0.001691
[epoch: 502/1000, batch:   868/ 1052, ite: 131980] train loss: 0.004248, tar: 0.000108 
l0: 0.000038, l1: 0.000037, l2: 0.000075, l3: 0.000151, l4: 0.000271, l5: 0.000632, l6: 0.001215
[epoch: 502/1000, batch:   948/ 1052, ite: 132000] train loss: 0.004248, tar: 0.000108 
l0: 0.000039, l1: 0.000038, l2: 0.000090, l3: 0.000243, l4: 0.000486, l5: 0.000763, l6: 0.001744
[epoch: 502/1000, batch:  1028/ 1052, ite: 132020] train loss: 0.004248, tar: 0.000108 
[Epoch 502/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000070, l1: 0.000074, l2: 0.000131, l3: 0.000287, l4: 0.000551, l5: 0.001477, l6: 0.002523
[epoch: 503/1000, batch:    56/ 1052, ite: 132040] train loss: 0.004249, tar: 0.000108 
l0: 0.000102, l1: 0.000106, l2: 0.000130, l3: 0.000216, l4: 0.000509, l5: 0.000721, l6: 0.001628
[epoch: 503/1000, batch:   136/ 1052, ite: 132060] train loss: 0.004249, tar: 0.000108 
l0: 0.000135, l1: 0.000135, l2: 0.000235, l3: 0.000444, l4: 0.000758, l5: 0.001587, l6: 0.002716
[epoch: 503/1000, batch:   216/ 1052, ite: 132080] train loss: 0.004249, tar: 0.000108 
l0: 0.000104, l1: 0.000111, l2: 0.000100, l3: 0.000196, l4: 0.000260, l5: 0.000879, l6: 0.001429
[epoch: 503/1000, batch:   296/ 1052, ite: 132100] train loss: 0.004251, tar: 0.000108 
l0: 0.000064, l1: 0.000069, l2: 0.000121, l3: 0.000263, l4: 0.000584, l5: 0.001236, l6: 0.002317
[epoch: 503/1000, batch:   376/ 1052, ite: 132120] train loss: 0.004250, tar: 0.000108 
l0: 0.000248, l1: 0.000247, l2: 0.000332, l3: 0.000455, l4: 0.000889, l5: 0.001594, l6: 0.002976
[epoch: 503/1000, batch:   456/ 1052, ite: 132140] train loss: 0.004251, tar: 0.000108 
l0: 0.000139, l1: 0.000138, l2: 0.000164, l3: 0.000207, l4: 0.000511, l5: 0.001033, l6: 0.002253
[epoch: 503/1000, batch:   536/ 1052, ite: 132160] train loss: 0.004249, tar: 0.000108 
l0: 0.000069, l1: 0.000070, l2: 0.000137, l3: 0.000157, l4: 0.000307, l5: 0.000829, l6: 0.001325
[epoch: 503/1000, batch:   616/ 1052, ite: 132180] train loss: 0.004248, tar: 0.000108 
l0: 0.000049, l1: 0.000049, l2: 0.000066, l3: 0.000102, l4: 0.000188, l5: 0.000740, l6: 0.001211
[epoch: 503/1000, batch:   696/ 1052, ite: 132200] train loss: 0.004249, tar: 0.000108 
l0: 0.000227, l1: 0.000234, l2: 0.000254, l3: 0.000288, l4: 0.000477, l5: 0.001724, l6: 0.002155
[epoch: 503/1000, batch:   776/ 1052, ite: 132220] train loss: 0.004249, tar: 0.000108 
l0: 0.000103, l1: 0.000101, l2: 0.000150, l3: 0.000289, l4: 0.000557, l5: 0.001095, l6: 0.001175
[epoch: 503/1000, batch:   856/ 1052, ite: 132240] train loss: 0.004248, tar: 0.000108 
l0: 0.000038, l1: 0.000038, l2: 0.000070, l3: 0.000121, l4: 0.000289, l5: 0.000711, l6: 0.001156
[epoch: 503/1000, batch:   936/ 1052, ite: 132260] train loss: 0.004246, tar: 0.000108 
l0: 0.000181, l1: 0.000179, l2: 0.000244, l3: 0.000374, l4: 0.000616, l5: 0.001008, l6: 0.002066
[epoch: 503/1000, batch:  1016/ 1052, ite: 132280] train loss: 0.004247, tar: 0.000108 
[Epoch 503/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000377, l1: 0.000388, l2: 0.000398, l3: 0.000673, l4: 0.000973, l5: 0.002490, l6: 0.004150
[epoch: 504/1000, batch:    44/ 1052, ite: 132300] train loss: 0.004248, tar: 0.000108 
l0: 0.000054, l1: 0.000054, l2: 0.000097, l3: 0.000247, l4: 0.000381, l5: 0.001606, l6: 0.001837
[epoch: 504/1000, batch:   124/ 1052, ite: 132320] train loss: 0.004247, tar: 0.000108 
l0: 0.000055, l1: 0.000056, l2: 0.000101, l3: 0.000251, l4: 0.000598, l5: 0.001020, l6: 0.001635
[epoch: 504/1000, batch:   204/ 1052, ite: 132340] train loss: 0.004246, tar: 0.000108 
l0: 0.000063, l1: 0.000063, l2: 0.000115, l3: 0.000150, l4: 0.000391, l5: 0.000799, l6: 0.001307
[epoch: 504/1000, batch:   284/ 1052, ite: 132360] train loss: 0.004246, tar: 0.000108 
l0: 0.000114, l1: 0.000119, l2: 0.000160, l3: 0.000168, l4: 0.000400, l5: 0.001228, l6: 0.001519
[epoch: 504/1000, batch:   364/ 1052, ite: 132380] train loss: 0.004246, tar: 0.000108 
l0: 0.000104, l1: 0.000100, l2: 0.000171, l3: 0.000212, l4: 0.000519, l5: 0.000820, l6: 0.001724
[epoch: 504/1000, batch:   444/ 1052, ite: 132400] train loss: 0.004246, tar: 0.000108 
l0: 0.000117, l1: 0.000115, l2: 0.000145, l3: 0.000220, l4: 0.000495, l5: 0.000897, l6: 0.001177
[epoch: 504/1000, batch:   524/ 1052, ite: 132420] train loss: 0.004245, tar: 0.000108 
l0: 0.000040, l1: 0.000040, l2: 0.000074, l3: 0.000138, l4: 0.000350, l5: 0.000771, l6: 0.001443
[epoch: 504/1000, batch:   604/ 1052, ite: 132440] train loss: 0.004245, tar: 0.000108 
l0: 0.000118, l1: 0.000120, l2: 0.000121, l3: 0.000192, l4: 0.000644, l5: 0.000927, l6: 0.002282
[epoch: 504/1000, batch:   684/ 1052, ite: 132460] train loss: 0.004245, tar: 0.000108 
l0: 0.000085, l1: 0.000083, l2: 0.000137, l3: 0.000243, l4: 0.000356, l5: 0.000887, l6: 0.001289
[epoch: 504/1000, batch:   764/ 1052, ite: 132480] train loss: 0.004246, tar: 0.000108 
l0: 0.000162, l1: 0.000168, l2: 0.000175, l3: 0.000255, l4: 0.000470, l5: 0.001113, l6: 0.002224
[epoch: 504/1000, batch:   844/ 1052, ite: 132500] train loss: 0.004246, tar: 0.000108 
l0: 0.000111, l1: 0.000109, l2: 0.000183, l3: 0.000272, l4: 0.000594, l5: 0.001470, l6: 0.003034
[epoch: 504/1000, batch:   924/ 1052, ite: 132520] train loss: 0.004246, tar: 0.000108 
l0: 0.000029, l1: 0.000029, l2: 0.000074, l3: 0.000135, l4: 0.000442, l5: 0.000426, l6: 0.001259
[epoch: 504/1000, batch:  1004/ 1052, ite: 132540] train loss: 0.004246, tar: 0.000108 
[Epoch 504/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000067, l1: 0.000065, l2: 0.000101, l3: 0.000198, l4: 0.000309, l5: 0.001222, l6: 0.001836
[epoch: 505/1000, batch:    32/ 1052, ite: 132560] train loss: 0.004246, tar: 0.000108 
l0: 0.000085, l1: 0.000083, l2: 0.000182, l3: 0.000285, l4: 0.000613, l5: 0.001189, l6: 0.002846
[epoch: 505/1000, batch:   112/ 1052, ite: 132580] train loss: 0.004245, tar: 0.000108 
l0: 0.000138, l1: 0.000133, l2: 0.000261, l3: 0.000558, l4: 0.000882, l5: 0.001699, l6: 0.002459
[epoch: 505/1000, batch:   192/ 1052, ite: 132600] train loss: 0.004245, tar: 0.000108 
l0: 0.000107, l1: 0.000109, l2: 0.000151, l3: 0.000211, l4: 0.000443, l5: 0.000801, l6: 0.001774
[epoch: 505/1000, batch:   272/ 1052, ite: 132620] train loss: 0.004244, tar: 0.000108 
l0: 0.000189, l1: 0.000194, l2: 0.000201, l3: 0.000271, l4: 0.000663, l5: 0.001407, l6: 0.002411
[epoch: 505/1000, batch:   352/ 1052, ite: 132640] train loss: 0.004245, tar: 0.000108 
l0: 0.000081, l1: 0.000077, l2: 0.000146, l3: 0.000254, l4: 0.000419, l5: 0.000935, l6: 0.001637
[epoch: 505/1000, batch:   432/ 1052, ite: 132660] train loss: 0.004244, tar: 0.000108 
l0: 0.000039, l1: 0.000038, l2: 0.000073, l3: 0.000159, l4: 0.000517, l5: 0.001022, l6: 0.001901
[epoch: 505/1000, batch:   512/ 1052, ite: 132680] train loss: 0.004243, tar: 0.000108 
l0: 0.000027, l1: 0.000027, l2: 0.000038, l3: 0.000073, l4: 0.000274, l5: 0.000772, l6: 0.000846
[epoch: 505/1000, batch:   592/ 1052, ite: 132700] train loss: 0.004244, tar: 0.000108 
l0: 0.000087, l1: 0.000086, l2: 0.000115, l3: 0.000182, l4: 0.000293, l5: 0.000929, l6: 0.001886
[epoch: 505/1000, batch:   672/ 1052, ite: 132720] train loss: 0.004244, tar: 0.000108 
l0: 0.000047, l1: 0.000047, l2: 0.000115, l3: 0.000217, l4: 0.000787, l5: 0.001466, l6: 0.002708
[epoch: 505/1000, batch:   752/ 1052, ite: 132740] train loss: 0.004246, tar: 0.000108 
l0: 0.000082, l1: 0.000084, l2: 0.000113, l3: 0.000130, l4: 0.000373, l5: 0.000650, l6: 0.000997
[epoch: 505/1000, batch:   832/ 1052, ite: 132760] train loss: 0.004245, tar: 0.000108 
l0: 0.000178, l1: 0.000181, l2: 0.000244, l3: 0.000403, l4: 0.000590, l5: 0.000802, l6: 0.001122
[epoch: 505/1000, batch:   912/ 1052, ite: 132780] train loss: 0.004245, tar: 0.000108 
l0: 0.000087, l1: 0.000089, l2: 0.000149, l3: 0.000280, l4: 0.000504, l5: 0.001054, l6: 0.001934
[epoch: 505/1000, batch:   992/ 1052, ite: 132800] train loss: 0.004245, tar: 0.000107 
[Epoch 505/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000063, l1: 0.000063, l2: 0.000121, l3: 0.000190, l4: 0.000456, l5: 0.001317, l6: 0.001956
[epoch: 506/1000, batch:    20/ 1052, ite: 132820] train loss: 0.004247, tar: 0.000107 
l0: 0.000039, l1: 0.000042, l2: 0.000075, l3: 0.000146, l4: 0.000295, l5: 0.000588, l6: 0.001386
[epoch: 506/1000, batch:   100/ 1052, ite: 132840] train loss: 0.004246, tar: 0.000107 
l0: 0.000097, l1: 0.000100, l2: 0.000135, l3: 0.000233, l4: 0.000541, l5: 0.001822, l6: 0.002757
[epoch: 506/1000, batch:   180/ 1052, ite: 132860] train loss: 0.004247, tar: 0.000107 
l0: 0.000094, l1: 0.000097, l2: 0.000126, l3: 0.000337, l4: 0.000913, l5: 0.001257, l6: 0.002294
[epoch: 506/1000, batch:   260/ 1052, ite: 132880] train loss: 0.004249, tar: 0.000107 
l0: 0.000064, l1: 0.000064, l2: 0.000105, l3: 0.000225, l4: 0.000436, l5: 0.001183, l6: 0.001611
[epoch: 506/1000, batch:   340/ 1052, ite: 132900] train loss: 0.004247, tar: 0.000107 
l0: 0.000049, l1: 0.000048, l2: 0.000089, l3: 0.000159, l4: 0.000293, l5: 0.000825, l6: 0.001340
[epoch: 506/1000, batch:   420/ 1052, ite: 132920] train loss: 0.004247, tar: 0.000107 
l0: 0.000031, l1: 0.000029, l2: 0.000102, l3: 0.000317, l4: 0.000555, l5: 0.000935, l6: 0.001657
[epoch: 506/1000, batch:   500/ 1052, ite: 132940] train loss: 0.004248, tar: 0.000107 
l0: 0.000211, l1: 0.000214, l2: 0.000238, l3: 0.000349, l4: 0.000636, l5: 0.000908, l6: 0.001914
[epoch: 506/1000, batch:   580/ 1052, ite: 132960] train loss: 0.004248, tar: 0.000107 
l0: 0.000126, l1: 0.000125, l2: 0.000167, l3: 0.000202, l4: 0.000559, l5: 0.000662, l6: 0.001514
[epoch: 506/1000, batch:   660/ 1052, ite: 132980] train loss: 0.004248, tar: 0.000107 
l0: 0.000074, l1: 0.000076, l2: 0.000098, l3: 0.000159, l4: 0.000456, l5: 0.000766, l6: 0.001103
[epoch: 506/1000, batch:   740/ 1052, ite: 133000] train loss: 0.004249, tar: 0.000107 
l0: 0.000118, l1: 0.000120, l2: 0.000154, l3: 0.000260, l4: 0.000579, l5: 0.001502, l6: 0.002541
[epoch: 506/1000, batch:   820/ 1052, ite: 133020] train loss: 0.004249, tar: 0.000107 
l0: 0.000072, l1: 0.000072, l2: 0.000107, l3: 0.000171, l4: 0.000448, l5: 0.000878, l6: 0.001727
[epoch: 506/1000, batch:   900/ 1052, ite: 133040] train loss: 0.004247, tar: 0.000107 
l0: 0.000134, l1: 0.000133, l2: 0.000135, l3: 0.000242, l4: 0.000353, l5: 0.000487, l6: 0.001756
[epoch: 506/1000, batch:   980/ 1052, ite: 133060] train loss: 0.004246, tar: 0.000107 
[Epoch 506/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000016, l1: 0.000017, l2: 0.000037, l3: 0.000110, l4: 0.000177, l5: 0.000743, l6: 0.001488
[epoch: 507/1000, batch:     8/ 1052, ite: 133080] train loss: 0.004247, tar: 0.000107 
l0: 0.000120, l1: 0.000124, l2: 0.000160, l3: 0.000396, l4: 0.001114, l5: 0.001620, l6: 0.003020
[epoch: 507/1000, batch:    88/ 1052, ite: 133100] train loss: 0.004247, tar: 0.000107 
l0: 0.000193, l1: 0.000199, l2: 0.000238, l3: 0.000314, l4: 0.000577, l5: 0.000652, l6: 0.002956
[epoch: 507/1000, batch:   168/ 1052, ite: 133120] train loss: 0.004246, tar: 0.000107 
l0: 0.000083, l1: 0.000083, l2: 0.000135, l3: 0.000200, l4: 0.000362, l5: 0.000716, l6: 0.001436
[epoch: 507/1000, batch:   248/ 1052, ite: 133140] train loss: 0.004247, tar: 0.000107 
l0: 0.000057, l1: 0.000056, l2: 0.000097, l3: 0.000224, l4: 0.000558, l5: 0.001553, l6: 0.001470
[epoch: 507/1000, batch:   328/ 1052, ite: 133160] train loss: 0.004247, tar: 0.000107 
l0: 0.000051, l1: 0.000050, l2: 0.000097, l3: 0.000184, l4: 0.000236, l5: 0.000646, l6: 0.001448
[epoch: 507/1000, batch:   408/ 1052, ite: 133180] train loss: 0.004246, tar: 0.000107 
l0: 0.000071, l1: 0.000071, l2: 0.000111, l3: 0.000160, l4: 0.000306, l5: 0.000645, l6: 0.001026
[epoch: 507/1000, batch:   488/ 1052, ite: 133200] train loss: 0.004245, tar: 0.000107 
l0: 0.000116, l1: 0.000119, l2: 0.000138, l3: 0.000213, l4: 0.000519, l5: 0.000705, l6: 0.001621
[epoch: 507/1000, batch:   568/ 1052, ite: 133220] train loss: 0.004247, tar: 0.000107 
l0: 0.000080, l1: 0.000086, l2: 0.000105, l3: 0.000179, l4: 0.000387, l5: 0.000685, l6: 0.001202
[epoch: 507/1000, batch:   648/ 1052, ite: 133240] train loss: 0.004247, tar: 0.000107 
l0: 0.000079, l1: 0.000078, l2: 0.000116, l3: 0.000171, l4: 0.000442, l5: 0.001073, l6: 0.001360
[epoch: 507/1000, batch:   728/ 1052, ite: 133260] train loss: 0.004246, tar: 0.000107 
l0: 0.000172, l1: 0.000172, l2: 0.000247, l3: 0.000371, l4: 0.000794, l5: 0.001467, l6: 0.004335
[epoch: 507/1000, batch:   808/ 1052, ite: 133280] train loss: 0.004245, tar: 0.000107 
l0: 0.000101, l1: 0.000102, l2: 0.000140, l3: 0.000238, l4: 0.000468, l5: 0.000698, l6: 0.001162
[epoch: 507/1000, batch:   888/ 1052, ite: 133300] train loss: 0.004243, tar: 0.000107 
l0: 0.000021, l1: 0.000021, l2: 0.000042, l3: 0.000097, l4: 0.000282, l5: 0.000870, l6: 0.001515
[epoch: 507/1000, batch:   968/ 1052, ite: 133320] train loss: 0.004242, tar: 0.000107 
l0: 0.000080, l1: 0.000079, l2: 0.000147, l3: 0.000322, l4: 0.000567, l5: 0.001368, l6: 0.002513
[epoch: 507/1000, batch:  1048/ 1052, ite: 133340] train loss: 0.004244, tar: 0.000107 
[Epoch 507/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000070, l1: 0.000070, l2: 0.000105, l3: 0.000153, l4: 0.000398, l5: 0.000680, l6: 0.002416
[epoch: 508/1000, batch:    76/ 1052, ite: 133360] train loss: 0.004243, tar: 0.000107 
l0: 0.000135, l1: 0.000131, l2: 0.000178, l3: 0.000287, l4: 0.000571, l5: 0.001130, l6: 0.001409
[epoch: 508/1000, batch:   156/ 1052, ite: 133380] train loss: 0.004244, tar: 0.000107 
l0: 0.000071, l1: 0.000067, l2: 0.000094, l3: 0.000194, l4: 0.000437, l5: 0.000817, l6: 0.001392
[epoch: 508/1000, batch:   236/ 1052, ite: 133400] train loss: 0.004245, tar: 0.000107 
l0: 0.000035, l1: 0.000034, l2: 0.000053, l3: 0.000105, l4: 0.000188, l5: 0.000679, l6: 0.000975
[epoch: 508/1000, batch:   316/ 1052, ite: 133420] train loss: 0.004247, tar: 0.000107 
l0: 0.000019, l1: 0.000018, l2: 0.000031, l3: 0.000073, l4: 0.000159, l5: 0.000270, l6: 0.000402
[epoch: 508/1000, batch:   396/ 1052, ite: 133440] train loss: 0.004244, tar: 0.000107 
l0: 0.000205, l1: 0.000206, l2: 0.000239, l3: 0.000358, l4: 0.000905, l5: 0.001923, l6: 0.002864
[epoch: 508/1000, batch:   476/ 1052, ite: 133460] train loss: 0.004244, tar: 0.000107 
l0: 0.000075, l1: 0.000072, l2: 0.000155, l3: 0.000243, l4: 0.000415, l5: 0.000965, l6: 0.002019
[epoch: 508/1000, batch:   556/ 1052, ite: 133480] train loss: 0.004243, tar: 0.000107 
l0: 0.000108, l1: 0.000107, l2: 0.000159, l3: 0.000333, l4: 0.000960, l5: 0.002069, l6: 0.003692
[epoch: 508/1000, batch:   636/ 1052, ite: 133500] train loss: 0.004244, tar: 0.000107 
l0: 0.000048, l1: 0.000051, l2: 0.000078, l3: 0.000156, l4: 0.000638, l5: 0.001204, l6: 0.002047
[epoch: 508/1000, batch:   716/ 1052, ite: 133520] train loss: 0.004245, tar: 0.000107 
l0: 0.000044, l1: 0.000044, l2: 0.000099, l3: 0.000261, l4: 0.000669, l5: 0.001681, l6: 0.002917
[epoch: 508/1000, batch:   796/ 1052, ite: 133540] train loss: 0.004244, tar: 0.000107 
l0: 0.000096, l1: 0.000097, l2: 0.000131, l3: 0.000242, l4: 0.000467, l5: 0.001177, l6: 0.001992
[epoch: 508/1000, batch:   876/ 1052, ite: 133560] train loss: 0.004244, tar: 0.000107 
l0: 0.000121, l1: 0.000126, l2: 0.000149, l3: 0.000271, l4: 0.000726, l5: 0.001075, l6: 0.002068
[epoch: 508/1000, batch:   956/ 1052, ite: 133580] train loss: 0.004244, tar: 0.000107 
l0: 0.000052, l1: 0.000051, l2: 0.000089, l3: 0.000123, l4: 0.000442, l5: 0.001046, l6: 0.001309
[epoch: 508/1000, batch:  1036/ 1052, ite: 133600] train loss: 0.004243, tar: 0.000107 
[Epoch 508/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000099, l1: 0.000100, l2: 0.000167, l3: 0.000484, l4: 0.001113, l5: 0.001681, l6: 0.002527
[epoch: 509/1000, batch:    64/ 1052, ite: 133620] train loss: 0.004242, tar: 0.000107 
l0: 0.000131, l1: 0.000130, l2: 0.000186, l3: 0.000401, l4: 0.000645, l5: 0.000740, l6: 0.001945
[epoch: 509/1000, batch:   144/ 1052, ite: 133640] train loss: 0.004242, tar: 0.000107 
l0: 0.000182, l1: 0.000184, l2: 0.000236, l3: 0.000364, l4: 0.000571, l5: 0.001600, l6: 0.002392
[epoch: 509/1000, batch:   224/ 1052, ite: 133660] train loss: 0.004242, tar: 0.000107 
l0: 0.000075, l1: 0.000077, l2: 0.000101, l3: 0.000217, l4: 0.000542, l5: 0.001408, l6: 0.002887
[epoch: 509/1000, batch:   304/ 1052, ite: 133680] train loss: 0.004241, tar: 0.000107 
l0: 0.000106, l1: 0.000106, l2: 0.000155, l3: 0.000266, l4: 0.000666, l5: 0.001117, l6: 0.003418
[epoch: 509/1000, batch:   384/ 1052, ite: 133700] train loss: 0.004241, tar: 0.000107 
l0: 0.000187, l1: 0.000187, l2: 0.000241, l3: 0.000294, l4: 0.000392, l5: 0.000797, l6: 0.001997
[epoch: 509/1000, batch:   464/ 1052, ite: 133720] train loss: 0.004240, tar: 0.000107 
l0: 0.000020, l1: 0.000020, l2: 0.000057, l3: 0.000104, l4: 0.000324, l5: 0.000543, l6: 0.001767
[epoch: 509/1000, batch:   544/ 1052, ite: 133740] train loss: 0.004239, tar: 0.000107 
l0: 0.000098, l1: 0.000099, l2: 0.000125, l3: 0.000168, l4: 0.000418, l5: 0.000669, l6: 0.001034
[epoch: 509/1000, batch:   624/ 1052, ite: 133760] train loss: 0.004240, tar: 0.000107 
l0: 0.000071, l1: 0.000072, l2: 0.000073, l3: 0.000101, l4: 0.000208, l5: 0.000355, l6: 0.000657
[epoch: 509/1000, batch:   704/ 1052, ite: 133780] train loss: 0.004241, tar: 0.000107 
l0: 0.000093, l1: 0.000097, l2: 0.000137, l3: 0.000226, l4: 0.000629, l5: 0.000867, l6: 0.002295
[epoch: 509/1000, batch:   784/ 1052, ite: 133800] train loss: 0.004240, tar: 0.000107 
l0: 0.000074, l1: 0.000077, l2: 0.000129, l3: 0.000130, l4: 0.000163, l5: 0.000706, l6: 0.001416
[epoch: 509/1000, batch:   864/ 1052, ite: 133820] train loss: 0.004240, tar: 0.000107 
l0: 0.000120, l1: 0.000121, l2: 0.000175, l3: 0.000294, l4: 0.000631, l5: 0.001501, l6: 0.001988
[epoch: 509/1000, batch:   944/ 1052, ite: 133840] train loss: 0.004240, tar: 0.000107 
l0: 0.000008, l1: 0.000008, l2: 0.000020, l3: 0.000054, l4: 0.000152, l5: 0.000245, l6: 0.000968
[epoch: 509/1000, batch:  1024/ 1052, ite: 133860] train loss: 0.004239, tar: 0.000107 
[Epoch 509/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000145, l1: 0.000141, l2: 0.000274, l3: 0.000408, l4: 0.000808, l5: 0.001518, l6: 0.002934
[epoch: 510/1000, batch:    52/ 1052, ite: 133880] train loss: 0.004242, tar: 0.000107 
l0: 0.000099, l1: 0.000099, l2: 0.000154, l3: 0.000249, l4: 0.000462, l5: 0.001163, l6: 0.001829
[epoch: 510/1000, batch:   132/ 1052, ite: 133900] train loss: 0.004241, tar: 0.000107 
l0: 0.000038, l1: 0.000041, l2: 0.000074, l3: 0.000103, l4: 0.000217, l5: 0.000480, l6: 0.000652
[epoch: 510/1000, batch:   212/ 1052, ite: 133920] train loss: 0.004241, tar: 0.000106 
l0: 0.000078, l1: 0.000079, l2: 0.000112, l3: 0.000168, l4: 0.000427, l5: 0.001125, l6: 0.001447
[epoch: 510/1000, batch:   292/ 1052, ite: 133940] train loss: 0.004239, tar: 0.000106 
l0: 0.000081, l1: 0.000081, l2: 0.000140, l3: 0.000193, l4: 0.000279, l5: 0.000846, l6: 0.001353
[epoch: 510/1000, batch:   372/ 1052, ite: 133960] train loss: 0.004239, tar: 0.000106 
l0: 0.000035, l1: 0.000034, l2: 0.000063, l3: 0.000127, l4: 0.000228, l5: 0.000617, l6: 0.001283
[epoch: 510/1000, batch:   452/ 1052, ite: 133980] train loss: 0.004239, tar: 0.000106 
l0: 0.000055, l1: 0.000055, l2: 0.000120, l3: 0.000195, l4: 0.000516, l5: 0.001439, l6: 0.002301
[epoch: 510/1000, batch:   532/ 1052, ite: 134000] train loss: 0.004239, tar: 0.000106 
l0: 0.000075, l1: 0.000075, l2: 0.000167, l3: 0.000304, l4: 0.000654, l5: 0.001548, l6: 0.002311
[epoch: 510/1000, batch:   612/ 1052, ite: 134020] train loss: 0.004240, tar: 0.000106 
l0: 0.000079, l1: 0.000076, l2: 0.000137, l3: 0.000272, l4: 0.000725, l5: 0.001315, l6: 0.001669
[epoch: 510/1000, batch:   692/ 1052, ite: 134040] train loss: 0.004240, tar: 0.000106 
l0: 0.000047, l1: 0.000047, l2: 0.000084, l3: 0.000162, l4: 0.000263, l5: 0.000655, l6: 0.000846
[epoch: 510/1000, batch:   772/ 1052, ite: 134060] train loss: 0.004238, tar: 0.000106 
l0: 0.000093, l1: 0.000098, l2: 0.000091, l3: 0.000174, l4: 0.000318, l5: 0.001050, l6: 0.001868
[epoch: 510/1000, batch:   852/ 1052, ite: 134080] train loss: 0.004238, tar: 0.000106 
l0: 0.000079, l1: 0.000078, l2: 0.000105, l3: 0.000181, l4: 0.000342, l5: 0.000475, l6: 0.000874
[epoch: 510/1000, batch:   932/ 1052, ite: 134100] train loss: 0.004239, tar: 0.000106 
l0: 0.000064, l1: 0.000064, l2: 0.000128, l3: 0.000235, l4: 0.000701, l5: 0.001658, l6: 0.002440
[epoch: 510/1000, batch:  1012/ 1052, ite: 134120] train loss: 0.004238, tar: 0.000106 
[Epoch 510/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000025, l1: 0.000025, l2: 0.000037, l3: 0.000116, l4: 0.000281, l5: 0.000634, l6: 0.001341
[epoch: 511/1000, batch:    40/ 1052, ite: 134140] train loss: 0.004239, tar: 0.000106 
l0: 0.000115, l1: 0.000114, l2: 0.000146, l3: 0.000201, l4: 0.000614, l5: 0.000901, l6: 0.001977
[epoch: 511/1000, batch:   120/ 1052, ite: 134160] train loss: 0.004238, tar: 0.000106 
l0: 0.000043, l1: 0.000041, l2: 0.000078, l3: 0.000159, l4: 0.000467, l5: 0.000645, l6: 0.001485
[epoch: 511/1000, batch:   200/ 1052, ite: 134180] train loss: 0.004237, tar: 0.000106 
l0: 0.000064, l1: 0.000066, l2: 0.000094, l3: 0.000394, l4: 0.000760, l5: 0.001526, l6: 0.002768
[epoch: 511/1000, batch:   280/ 1052, ite: 134200] train loss: 0.004238, tar: 0.000106 
l0: 0.000198, l1: 0.000205, l2: 0.000251, l3: 0.000435, l4: 0.000578, l5: 0.001174, l6: 0.002261
[epoch: 511/1000, batch:   360/ 1052, ite: 134220] train loss: 0.004237, tar: 0.000106 
l0: 0.000155, l1: 0.000154, l2: 0.000201, l3: 0.000372, l4: 0.000632, l5: 0.001668, l6: 0.002164
[epoch: 511/1000, batch:   440/ 1052, ite: 134240] train loss: 0.004236, tar: 0.000106 
l0: 0.000091, l1: 0.000096, l2: 0.000111, l3: 0.000212, l4: 0.000408, l5: 0.000652, l6: 0.001199
[epoch: 511/1000, batch:   520/ 1052, ite: 134260] train loss: 0.004235, tar: 0.000106 
l0: 0.000054, l1: 0.000054, l2: 0.000099, l3: 0.000253, l4: 0.000435, l5: 0.000833, l6: 0.002070
[epoch: 511/1000, batch:   600/ 1052, ite: 134280] train loss: 0.004235, tar: 0.000106 
l0: 0.000012, l1: 0.000012, l2: 0.000034, l3: 0.000082, l4: 0.000312, l5: 0.000560, l6: 0.001033
[epoch: 511/1000, batch:   680/ 1052, ite: 134300] train loss: 0.004235, tar: 0.000106 
l0: 0.000072, l1: 0.000074, l2: 0.000112, l3: 0.000218, l4: 0.000401, l5: 0.001022, l6: 0.001209
[epoch: 511/1000, batch:   760/ 1052, ite: 134320] train loss: 0.004234, tar: 0.000106 
l0: 0.000064, l1: 0.000063, l2: 0.000097, l3: 0.000167, l4: 0.000460, l5: 0.000676, l6: 0.001545
[epoch: 511/1000, batch:   840/ 1052, ite: 134340] train loss: 0.004234, tar: 0.000106 
l0: 0.000104, l1: 0.000103, l2: 0.000172, l3: 0.000280, l4: 0.000557, l5: 0.001370, l6: 0.001486
[epoch: 511/1000, batch:   920/ 1052, ite: 134360] train loss: 0.004234, tar: 0.000106 
l0: 0.000074, l1: 0.000075, l2: 0.000075, l3: 0.000132, l4: 0.000331, l5: 0.000940, l6: 0.001756
[epoch: 511/1000, batch:  1000/ 1052, ite: 134380] train loss: 0.004236, tar: 0.000106 
[Epoch 511/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000207, l1: 0.000209, l2: 0.000235, l3: 0.000362, l4: 0.000593, l5: 0.001319, l6: 0.003506
[epoch: 512/1000, batch:    28/ 1052, ite: 134400] train loss: 0.004237, tar: 0.000106 
l0: 0.000083, l1: 0.000080, l2: 0.000133, l3: 0.000240, l4: 0.000525, l5: 0.001097, l6: 0.001608
[epoch: 512/1000, batch:   108/ 1052, ite: 134420] train loss: 0.004238, tar: 0.000106 
l0: 0.000031, l1: 0.000030, l2: 0.000073, l3: 0.000119, l4: 0.000673, l5: 0.001001, l6: 0.001786
[epoch: 512/1000, batch:   188/ 1052, ite: 134440] train loss: 0.004237, tar: 0.000106 
l0: 0.000068, l1: 0.000069, l2: 0.000115, l3: 0.000217, l4: 0.000470, l5: 0.001494, l6: 0.002151
[epoch: 512/1000, batch:   268/ 1052, ite: 134460] train loss: 0.004237, tar: 0.000106 
l0: 0.000161, l1: 0.000164, l2: 0.000184, l3: 0.000306, l4: 0.000465, l5: 0.000869, l6: 0.002190
[epoch: 512/1000, batch:   348/ 1052, ite: 134480] train loss: 0.004237, tar: 0.000106 
l0: 0.000187, l1: 0.000188, l2: 0.000225, l3: 0.000333, l4: 0.000695, l5: 0.001367, l6: 0.003269
[epoch: 512/1000, batch:   428/ 1052, ite: 134500] train loss: 0.004237, tar: 0.000106 
l0: 0.000084, l1: 0.000083, l2: 0.000157, l3: 0.000276, l4: 0.000514, l5: 0.001174, l6: 0.002908
[epoch: 512/1000, batch:   508/ 1052, ite: 134520] train loss: 0.004237, tar: 0.000106 
l0: 0.000173, l1: 0.000173, l2: 0.000205, l3: 0.000364, l4: 0.000715, l5: 0.001971, l6: 0.003549
[epoch: 512/1000, batch:   588/ 1052, ite: 134540] train loss: 0.004236, tar: 0.000106 
l0: 0.000161, l1: 0.000165, l2: 0.000235, l3: 0.000364, l4: 0.000810, l5: 0.001758, l6: 0.002686
[epoch: 512/1000, batch:   668/ 1052, ite: 134560] train loss: 0.004237, tar: 0.000106 
l0: 0.000034, l1: 0.000035, l2: 0.000086, l3: 0.000289, l4: 0.000615, l5: 0.001371, l6: 0.001483
[epoch: 512/1000, batch:   748/ 1052, ite: 134580] train loss: 0.004236, tar: 0.000106 
l0: 0.000214, l1: 0.000212, l2: 0.000276, l3: 0.000443, l4: 0.000633, l5: 0.001244, l6: 0.002659
[epoch: 512/1000, batch:   828/ 1052, ite: 134600] train loss: 0.004237, tar: 0.000106 
l0: 0.000099, l1: 0.000101, l2: 0.000146, l3: 0.000182, l4: 0.000683, l5: 0.001070, l6: 0.001407
[epoch: 512/1000, batch:   908/ 1052, ite: 134620] train loss: 0.004236, tar: 0.000106 
l0: 0.000075, l1: 0.000076, l2: 0.000103, l3: 0.000155, l4: 0.000261, l5: 0.001006, l6: 0.001273
[epoch: 512/1000, batch:   988/ 1052, ite: 134640] train loss: 0.004236, tar: 0.000106 
[Epoch 512/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000062, l1: 0.000064, l2: 0.000147, l3: 0.000360, l4: 0.000787, l5: 0.001366, l6: 0.002591
[epoch: 513/1000, batch:    16/ 1052, ite: 134660] train loss: 0.004237, tar: 0.000106 
l0: 0.000043, l1: 0.000045, l2: 0.000100, l3: 0.000175, l4: 0.000334, l5: 0.000603, l6: 0.001818
[epoch: 513/1000, batch:    96/ 1052, ite: 134680] train loss: 0.004238, tar: 0.000106 
l0: 0.000059, l1: 0.000068, l2: 0.000132, l3: 0.000413, l4: 0.000679, l5: 0.001047, l6: 0.002182
[epoch: 513/1000, batch:   176/ 1052, ite: 134700] train loss: 0.004237, tar: 0.000106 
l0: 0.000179, l1: 0.000192, l2: 0.000188, l3: 0.000314, l4: 0.000847, l5: 0.002393, l6: 0.003489
[epoch: 513/1000, batch:   256/ 1052, ite: 134720] train loss: 0.004237, tar: 0.000106 
l0: 0.000183, l1: 0.000186, l2: 0.000243, l3: 0.000418, l4: 0.000712, l5: 0.001612, l6: 0.003033
[epoch: 513/1000, batch:   336/ 1052, ite: 134740] train loss: 0.004238, tar: 0.000106 
l0: 0.000055, l1: 0.000055, l2: 0.000118, l3: 0.000197, l4: 0.000567, l5: 0.001014, l6: 0.002523
[epoch: 513/1000, batch:   416/ 1052, ite: 134760] train loss: 0.004237, tar: 0.000106 
l0: 0.000077, l1: 0.000076, l2: 0.000096, l3: 0.000217, l4: 0.000387, l5: 0.000975, l6: 0.001574
[epoch: 513/1000, batch:   496/ 1052, ite: 134780] train loss: 0.004237, tar: 0.000106 
l0: 0.000045, l1: 0.000047, l2: 0.000055, l3: 0.000089, l4: 0.000156, l5: 0.000460, l6: 0.000670
[epoch: 513/1000, batch:   576/ 1052, ite: 134800] train loss: 0.004236, tar: 0.000106 
l0: 0.000123, l1: 0.000122, l2: 0.000142, l3: 0.000193, l4: 0.000381, l5: 0.001090, l6: 0.002572
[epoch: 513/1000, batch:   656/ 1052, ite: 134820] train loss: 0.004235, tar: 0.000106 
l0: 0.000085, l1: 0.000090, l2: 0.000150, l3: 0.000250, l4: 0.000514, l5: 0.001583, l6: 0.003499
[epoch: 513/1000, batch:   736/ 1052, ite: 134840] train loss: 0.004234, tar: 0.000106 
l0: 0.000182, l1: 0.000185, l2: 0.000221, l3: 0.000314, l4: 0.000615, l5: 0.001424, l6: 0.003139
[epoch: 513/1000, batch:   816/ 1052, ite: 134860] train loss: 0.004235, tar: 0.000106 
l0: 0.000244, l1: 0.000238, l2: 0.000255, l3: 0.000418, l4: 0.000738, l5: 0.001127, l6: 0.002781
[epoch: 513/1000, batch:   896/ 1052, ite: 134880] train loss: 0.004234, tar: 0.000106 
l0: 0.000095, l1: 0.000104, l2: 0.000112, l3: 0.000201, l4: 0.000345, l5: 0.000914, l6: 0.002155
[epoch: 513/1000, batch:   976/ 1052, ite: 134900] train loss: 0.004235, tar: 0.000106 
[Epoch 513/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000093, l1: 0.000095, l2: 0.000154, l3: 0.000285, l4: 0.000697, l5: 0.000931, l6: 0.002986
[epoch: 514/1000, batch:     4/ 1052, ite: 134920] train loss: 0.004236, tar: 0.000106 
l0: 0.000075, l1: 0.000075, l2: 0.000137, l3: 0.000226, l4: 0.000544, l5: 0.000790, l6: 0.001795
[epoch: 514/1000, batch:    84/ 1052, ite: 134940] train loss: 0.004236, tar: 0.000106 
l0: 0.000380, l1: 0.000385, l2: 0.000442, l3: 0.000590, l4: 0.000960, l5: 0.001683, l6: 0.002722
[epoch: 514/1000, batch:   164/ 1052, ite: 134960] train loss: 0.004238, tar: 0.000106 
l0: 0.000056, l1: 0.000056, l2: 0.000096, l3: 0.000170, l4: 0.000257, l5: 0.000857, l6: 0.001614
[epoch: 514/1000, batch:   244/ 1052, ite: 134980] train loss: 0.004237, tar: 0.000106 
l0: 0.000098, l1: 0.000098, l2: 0.000115, l3: 0.000148, l4: 0.000326, l5: 0.001051, l6: 0.001632
[epoch: 514/1000, batch:   324/ 1052, ite: 135000] train loss: 0.004237, tar: 0.000106 
l0: 0.000215, l1: 0.000215, l2: 0.000237, l3: 0.000311, l4: 0.000664, l5: 0.001501, l6: 0.003004
[epoch: 514/1000, batch:   404/ 1052, ite: 135020] train loss: 0.004236, tar: 0.000106 
l0: 0.000092, l1: 0.000092, l2: 0.000127, l3: 0.000316, l4: 0.000586, l5: 0.001884, l6: 0.003066
[epoch: 514/1000, batch:   484/ 1052, ite: 135040] train loss: 0.004237, tar: 0.000106 
l0: 0.000052, l1: 0.000053, l2: 0.000070, l3: 0.000143, l4: 0.000264, l5: 0.000937, l6: 0.001549
[epoch: 514/1000, batch:   564/ 1052, ite: 135060] train loss: 0.004237, tar: 0.000106 
l0: 0.000078, l1: 0.000078, l2: 0.000106, l3: 0.000194, l4: 0.000414, l5: 0.000576, l6: 0.001292
[epoch: 514/1000, batch:   644/ 1052, ite: 135080] train loss: 0.004236, tar: 0.000105 
l0: 0.000144, l1: 0.000149, l2: 0.000155, l3: 0.000250, l4: 0.000509, l5: 0.001513, l6: 0.001504
[epoch: 514/1000, batch:   724/ 1052, ite: 135100] train loss: 0.004235, tar: 0.000105 
l0: 0.000053, l1: 0.000056, l2: 0.000102, l3: 0.000209, l4: 0.000326, l5: 0.001142, l6: 0.002832
[epoch: 514/1000, batch:   804/ 1052, ite: 135120] train loss: 0.004235, tar: 0.000105 
l0: 0.000069, l1: 0.000067, l2: 0.000083, l3: 0.000189, l4: 0.000252, l5: 0.000775, l6: 0.001323
[epoch: 514/1000, batch:   884/ 1052, ite: 135140] train loss: 0.004235, tar: 0.000105 
l0: 0.000109, l1: 0.000114, l2: 0.000173, l3: 0.000250, l4: 0.000484, l5: 0.001173, l6: 0.001452
[epoch: 514/1000, batch:   964/ 1052, ite: 135160] train loss: 0.004235, tar: 0.000105 
l0: 0.000215, l1: 0.000219, l2: 0.000290, l3: 0.000542, l4: 0.001190, l5: 0.002510, l6: 0.004109
[epoch: 514/1000, batch:  1044/ 1052, ite: 135180] train loss: 0.004235, tar: 0.000105 
[Epoch 514/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000091, l1: 0.000090, l2: 0.000138, l3: 0.000207, l4: 0.000277, l5: 0.000573, l6: 0.001672
[epoch: 515/1000, batch:    72/ 1052, ite: 135200] train loss: 0.004235, tar: 0.000105 
l0: 0.000087, l1: 0.000087, l2: 0.000102, l3: 0.000156, l4: 0.000456, l5: 0.000981, l6: 0.002478
[epoch: 515/1000, batch:   152/ 1052, ite: 135220] train loss: 0.004236, tar: 0.000105 
l0: 0.000219, l1: 0.000225, l2: 0.000249, l3: 0.000294, l4: 0.000450, l5: 0.001085, l6: 0.002024
[epoch: 515/1000, batch:   232/ 1052, ite: 135240] train loss: 0.004237, tar: 0.000105 
l0: 0.000207, l1: 0.000207, l2: 0.000270, l3: 0.000436, l4: 0.000822, l5: 0.001351, l6: 0.002128
[epoch: 515/1000, batch:   312/ 1052, ite: 135260] train loss: 0.004236, tar: 0.000106 
l0: 0.000132, l1: 0.000136, l2: 0.000162, l3: 0.000274, l4: 0.000695, l5: 0.001607, l6: 0.001914
[epoch: 515/1000, batch:   392/ 1052, ite: 135280] train loss: 0.004237, tar: 0.000106 
l0: 0.000070, l1: 0.000068, l2: 0.000089, l3: 0.000105, l4: 0.000304, l5: 0.000585, l6: 0.000880
[epoch: 515/1000, batch:   472/ 1052, ite: 135300] train loss: 0.004236, tar: 0.000106 
l0: 0.000096, l1: 0.000095, l2: 0.000117, l3: 0.000220, l4: 0.000340, l5: 0.000907, l6: 0.001028
[epoch: 515/1000, batch:   552/ 1052, ite: 135320] train loss: 0.004235, tar: 0.000106 
l0: 0.000093, l1: 0.000095, l2: 0.000115, l3: 0.000154, l4: 0.000427, l5: 0.000918, l6: 0.001895
[epoch: 515/1000, batch:   632/ 1052, ite: 135340] train loss: 0.004235, tar: 0.000105 
l0: 0.000050, l1: 0.000048, l2: 0.000091, l3: 0.000249, l4: 0.000615, l5: 0.001241, l6: 0.001430
[epoch: 515/1000, batch:   712/ 1052, ite: 135360] train loss: 0.004235, tar: 0.000105 
l0: 0.000120, l1: 0.000123, l2: 0.000192, l3: 0.000289, l4: 0.000822, l5: 0.002306, l6: 0.002688
[epoch: 515/1000, batch:   792/ 1052, ite: 135380] train loss: 0.004237, tar: 0.000105 
l0: 0.000158, l1: 0.000158, l2: 0.000190, l3: 0.000297, l4: 0.000596, l5: 0.001202, l6: 0.002338
[epoch: 515/1000, batch:   872/ 1052, ite: 135400] train loss: 0.004237, tar: 0.000105 
l0: 0.000088, l1: 0.000085, l2: 0.000130, l3: 0.000216, l4: 0.000379, l5: 0.000884, l6: 0.001749
[epoch: 515/1000, batch:   952/ 1052, ite: 135420] train loss: 0.004237, tar: 0.000105 
l0: 0.000157, l1: 0.000160, l2: 0.000193, l3: 0.000268, l4: 0.000514, l5: 0.001343, l6: 0.001598
[epoch: 515/1000, batch:  1032/ 1052, ite: 135440] train loss: 0.004237, tar: 0.000105 
[Epoch 515/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000118, l1: 0.000116, l2: 0.000175, l3: 0.000270, l4: 0.000454, l5: 0.000902, l6: 0.001990
[epoch: 516/1000, batch:    60/ 1052, ite: 135460] train loss: 0.004238, tar: 0.000105 
l0: 0.000023, l1: 0.000023, l2: 0.000032, l3: 0.000086, l4: 0.000253, l5: 0.000588, l6: 0.000631
[epoch: 516/1000, batch:   140/ 1052, ite: 135480] train loss: 0.004238, tar: 0.000105 
l0: 0.000139, l1: 0.000137, l2: 0.000164, l3: 0.000277, l4: 0.000526, l5: 0.001185, l6: 0.001768
[epoch: 516/1000, batch:   220/ 1052, ite: 135500] train loss: 0.004238, tar: 0.000105 
l0: 0.000109, l1: 0.000106, l2: 0.000194, l3: 0.000311, l4: 0.000581, l5: 0.001510, l6: 0.003429
[epoch: 516/1000, batch:   300/ 1052, ite: 135520] train loss: 0.004238, tar: 0.000105 
l0: 0.000151, l1: 0.000152, l2: 0.000161, l3: 0.000233, l4: 0.000346, l5: 0.000803, l6: 0.001090
[epoch: 516/1000, batch:   380/ 1052, ite: 135540] train loss: 0.004237, tar: 0.000105 
l0: 0.000085, l1: 0.000086, l2: 0.000105, l3: 0.000179, l4: 0.000383, l5: 0.000627, l6: 0.000878
[epoch: 516/1000, batch:   460/ 1052, ite: 135560] train loss: 0.004236, tar: 0.000105 
l0: 0.000088, l1: 0.000089, l2: 0.000121, l3: 0.000229, l4: 0.000541, l5: 0.001513, l6: 0.002078
[epoch: 516/1000, batch:   540/ 1052, ite: 135580] train loss: 0.004237, tar: 0.000105 
l0: 0.000163, l1: 0.000160, l2: 0.000272, l3: 0.000348, l4: 0.000466, l5: 0.001196, l6: 0.002059
[epoch: 516/1000, batch:   620/ 1052, ite: 135600] train loss: 0.004236, tar: 0.000105 
l0: 0.000111, l1: 0.000109, l2: 0.000183, l3: 0.000238, l4: 0.000524, l5: 0.001175, l6: 0.002779
[epoch: 516/1000, batch:   700/ 1052, ite: 135620] train loss: 0.004235, tar: 0.000105 
l0: 0.000064, l1: 0.000064, l2: 0.000112, l3: 0.000182, l4: 0.000307, l5: 0.000926, l6: 0.001278
[epoch: 516/1000, batch:   780/ 1052, ite: 135640] train loss: 0.004235, tar: 0.000105 
l0: 0.000134, l1: 0.000140, l2: 0.000133, l3: 0.000206, l4: 0.000377, l5: 0.001037, l6: 0.001807
[epoch: 516/1000, batch:   860/ 1052, ite: 135660] train loss: 0.004236, tar: 0.000105 
l0: 0.000086, l1: 0.000087, l2: 0.000097, l3: 0.000131, l4: 0.000463, l5: 0.001242, l6: 0.001345
[epoch: 516/1000, batch:   940/ 1052, ite: 135680] train loss: 0.004235, tar: 0.000105 
l0: 0.000142, l1: 0.000141, l2: 0.000178, l3: 0.000303, l4: 0.000553, l5: 0.001188, l6: 0.001629
[epoch: 516/1000, batch:  1020/ 1052, ite: 135700] train loss: 0.004236, tar: 0.000105 
[Epoch 516/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000097, l1: 0.000098, l2: 0.000114, l3: 0.000169, l4: 0.000272, l5: 0.000541, l6: 0.001353
[epoch: 517/1000, batch:    48/ 1052, ite: 135720] train loss: 0.004236, tar: 0.000105 
l0: 0.000049, l1: 0.000049, l2: 0.000072, l3: 0.000123, l4: 0.000393, l5: 0.000972, l6: 0.001356
[epoch: 517/1000, batch:   128/ 1052, ite: 135740] train loss: 0.004236, tar: 0.000105 
l0: 0.000036, l1: 0.000035, l2: 0.000081, l3: 0.000196, l4: 0.000713, l5: 0.000923, l6: 0.001931
[epoch: 517/1000, batch:   208/ 1052, ite: 135760] train loss: 0.004236, tar: 0.000105 
l0: 0.000058, l1: 0.000060, l2: 0.000070, l3: 0.000148, l4: 0.000311, l5: 0.000892, l6: 0.001413
[epoch: 517/1000, batch:   288/ 1052, ite: 135780] train loss: 0.004238, tar: 0.000105 
l0: 0.000086, l1: 0.000086, l2: 0.000134, l3: 0.000297, l4: 0.000636, l5: 0.001256, l6: 0.002232
[epoch: 517/1000, batch:   368/ 1052, ite: 135800] train loss: 0.004237, tar: 0.000105 
l0: 0.000166, l1: 0.000170, l2: 0.000217, l3: 0.000338, l4: 0.000937, l5: 0.001498, l6: 0.002999
[epoch: 517/1000, batch:   448/ 1052, ite: 135820] train loss: 0.004237, tar: 0.000105 
l0: 0.000117, l1: 0.000118, l2: 0.000191, l3: 0.000328, l4: 0.000598, l5: 0.001161, l6: 0.001242
[epoch: 517/1000, batch:   528/ 1052, ite: 135840] train loss: 0.004238, tar: 0.000105 
l0: 0.000140, l1: 0.000141, l2: 0.000175, l3: 0.000297, l4: 0.000470, l5: 0.001147, l6: 0.002504
[epoch: 517/1000, batch:   608/ 1052, ite: 135860] train loss: 0.004238, tar: 0.000105 
l0: 0.000109, l1: 0.000110, l2: 0.000182, l3: 0.000293, l4: 0.000544, l5: 0.001100, l6: 0.002151
[epoch: 517/1000, batch:   688/ 1052, ite: 135880] train loss: 0.004237, tar: 0.000105 
l0: 0.000079, l1: 0.000079, l2: 0.000128, l3: 0.000187, l4: 0.000561, l5: 0.001196, l6: 0.001902
[epoch: 517/1000, batch:   768/ 1052, ite: 135900] train loss: 0.004237, tar: 0.000105 
l0: 0.000139, l1: 0.000142, l2: 0.000171, l3: 0.000338, l4: 0.000667, l5: 0.001207, l6: 0.002734
[epoch: 517/1000, batch:   848/ 1052, ite: 135920] train loss: 0.004238, tar: 0.000105 
l0: 0.000037, l1: 0.000037, l2: 0.000071, l3: 0.000195, l4: 0.000317, l5: 0.000945, l6: 0.001628
[epoch: 517/1000, batch:   928/ 1052, ite: 135940] train loss: 0.004237, tar: 0.000105 
l0: 0.000043, l1: 0.000043, l2: 0.000072, l3: 0.000135, l4: 0.000263, l5: 0.000902, l6: 0.001665
[epoch: 517/1000, batch:  1008/ 1052, ite: 135960] train loss: 0.004237, tar: 0.000105 
[Epoch 517/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000067, l1: 0.000068, l2: 0.000100, l3: 0.000168, l4: 0.000392, l5: 0.000707, l6: 0.001642
[epoch: 518/1000, batch:    36/ 1052, ite: 135980] train loss: 0.004237, tar: 0.000105 
l0: 0.000110, l1: 0.000111, l2: 0.000148, l3: 0.000174, l4: 0.000352, l5: 0.000681, l6: 0.001552
[epoch: 518/1000, batch:   116/ 1052, ite: 136000] train loss: 0.004236, tar: 0.000105 
l0: 0.000079, l1: 0.000078, l2: 0.000148, l3: 0.000349, l4: 0.000503, l5: 0.001173, l6: 0.001482
[epoch: 518/1000, batch:   196/ 1052, ite: 136020] train loss: 0.004235, tar: 0.000105 
l0: 0.000045, l1: 0.000045, l2: 0.000065, l3: 0.000121, l4: 0.000392, l5: 0.000645, l6: 0.001079
[epoch: 518/1000, batch:   276/ 1052, ite: 136040] train loss: 0.004234, tar: 0.000105 
l0: 0.000172, l1: 0.000175, l2: 0.000238, l3: 0.000358, l4: 0.000629, l5: 0.001064, l6: 0.002238
[epoch: 518/1000, batch:   356/ 1052, ite: 136060] train loss: 0.004234, tar: 0.000105 
l0: 0.000133, l1: 0.000135, l2: 0.000153, l3: 0.000219, l4: 0.000367, l5: 0.000504, l6: 0.001120
[epoch: 518/1000, batch:   436/ 1052, ite: 136080] train loss: 0.004233, tar: 0.000105 
l0: 0.000071, l1: 0.000070, l2: 0.000128, l3: 0.000196, l4: 0.000507, l5: 0.001119, l6: 0.002123
[epoch: 518/1000, batch:   516/ 1052, ite: 136100] train loss: 0.004234, tar: 0.000105 
l0: 0.000051, l1: 0.000052, l2: 0.000089, l3: 0.000136, l4: 0.000556, l5: 0.001004, l6: 0.002227
[epoch: 518/1000, batch:   596/ 1052, ite: 136120] train loss: 0.004234, tar: 0.000105 
l0: 0.000129, l1: 0.000134, l2: 0.000158, l3: 0.000225, l4: 0.000438, l5: 0.000895, l6: 0.001339
[epoch: 518/1000, batch:   676/ 1052, ite: 136140] train loss: 0.004235, tar: 0.000105 
l0: 0.000122, l1: 0.000119, l2: 0.000335, l3: 0.000488, l4: 0.000741, l5: 0.002440, l6: 0.003991
[epoch: 518/1000, batch:   756/ 1052, ite: 136160] train loss: 0.004236, tar: 0.000105 
l0: 0.000144, l1: 0.000146, l2: 0.000182, l3: 0.000243, l4: 0.000572, l5: 0.001134, l6: 0.002723
[epoch: 518/1000, batch:   836/ 1052, ite: 136180] train loss: 0.004235, tar: 0.000105 
l0: 0.000183, l1: 0.000186, l2: 0.000228, l3: 0.000425, l4: 0.000728, l5: 0.001374, l6: 0.002463
[epoch: 518/1000, batch:   916/ 1052, ite: 136200] train loss: 0.004235, tar: 0.000105 
l0: 0.000106, l1: 0.000108, l2: 0.000144, l3: 0.000253, l4: 0.000411, l5: 0.000810, l6: 0.001178
[epoch: 518/1000, batch:   996/ 1052, ite: 136220] train loss: 0.004234, tar: 0.000105 
[Epoch 518/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000048, l1: 0.000048, l2: 0.000092, l3: 0.000124, l4: 0.000346, l5: 0.000864, l6: 0.001532
[epoch: 519/1000, batch:    24/ 1052, ite: 136240] train loss: 0.004234, tar: 0.000105 
l0: 0.000034, l1: 0.000035, l2: 0.000091, l3: 0.000226, l4: 0.000874, l5: 0.001305, l6: 0.002440
[epoch: 519/1000, batch:   104/ 1052, ite: 136260] train loss: 0.004236, tar: 0.000105 
l0: 0.000054, l1: 0.000053, l2: 0.000084, l3: 0.000188, l4: 0.000762, l5: 0.001431, l6: 0.002057
[epoch: 519/1000, batch:   184/ 1052, ite: 136280] train loss: 0.004237, tar: 0.000105 
l0: 0.000091, l1: 0.000091, l2: 0.000121, l3: 0.000216, l4: 0.000551, l5: 0.001110, l6: 0.001626
[epoch: 519/1000, batch:   264/ 1052, ite: 136300] train loss: 0.004236, tar: 0.000105 
l0: 0.000104, l1: 0.000102, l2: 0.000156, l3: 0.000286, l4: 0.000492, l5: 0.000741, l6: 0.001877
[epoch: 519/1000, batch:   344/ 1052, ite: 136320] train loss: 0.004236, tar: 0.000105 
l0: 0.000074, l1: 0.000076, l2: 0.000119, l3: 0.000292, l4: 0.000496, l5: 0.001391, l6: 0.002857
[epoch: 519/1000, batch:   424/ 1052, ite: 136340] train loss: 0.004237, tar: 0.000105 
l0: 0.000134, l1: 0.000140, l2: 0.000187, l3: 0.000323, l4: 0.000870, l5: 0.001262, l6: 0.001748
[epoch: 519/1000, batch:   504/ 1052, ite: 136360] train loss: 0.004237, tar: 0.000105 
l0: 0.000084, l1: 0.000090, l2: 0.000150, l3: 0.000252, l4: 0.000416, l5: 0.001148, l6: 0.001572
[epoch: 519/1000, batch:   584/ 1052, ite: 136380] train loss: 0.004236, tar: 0.000105 
l0: 0.000016, l1: 0.000017, l2: 0.000022, l3: 0.000053, l4: 0.000348, l5: 0.000807, l6: 0.001414
[epoch: 519/1000, batch:   664/ 1052, ite: 136400] train loss: 0.004236, tar: 0.000105 
l0: 0.000094, l1: 0.000092, l2: 0.000212, l3: 0.000385, l4: 0.000778, l5: 0.001313, l6: 0.002852
[epoch: 519/1000, batch:   744/ 1052, ite: 136420] train loss: 0.004236, tar: 0.000105 
l0: 0.000006, l1: 0.000006, l2: 0.000020, l3: 0.000085, l4: 0.000162, l5: 0.000753, l6: 0.001491
[epoch: 519/1000, batch:   824/ 1052, ite: 136440] train loss: 0.004235, tar: 0.000105 
l0: 0.000006, l1: 0.000007, l2: 0.000023, l3: 0.000085, l4: 0.000302, l5: 0.000599, l6: 0.001287
[epoch: 519/1000, batch:   904/ 1052, ite: 136460] train loss: 0.004235, tar: 0.000105 
l0: 0.000109, l1: 0.000107, l2: 0.000175, l3: 0.000318, l4: 0.000724, l5: 0.001212, l6: 0.002222
[epoch: 519/1000, batch:   984/ 1052, ite: 136480] train loss: 0.004234, tar: 0.000104 
[Epoch 519/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000139, l1: 0.000141, l2: 0.000167, l3: 0.000225, l4: 0.000367, l5: 0.001029, l6: 0.002556
[epoch: 520/1000, batch:    12/ 1052, ite: 136500] train loss: 0.004233, tar: 0.000104 
l0: 0.000191, l1: 0.000190, l2: 0.000243, l3: 0.000452, l4: 0.000852, l5: 0.001745, l6: 0.002915
[epoch: 520/1000, batch:    92/ 1052, ite: 136520] train loss: 0.004233, tar: 0.000104 
l0: 0.000093, l1: 0.000093, l2: 0.000121, l3: 0.000245, l4: 0.000547, l5: 0.001061, l6: 0.001555
[epoch: 520/1000, batch:   172/ 1052, ite: 136540] train loss: 0.004232, tar: 0.000104 
l0: 0.000078, l1: 0.000083, l2: 0.000137, l3: 0.000272, l4: 0.000582, l5: 0.001471, l6: 0.001754
[epoch: 520/1000, batch:   252/ 1052, ite: 136560] train loss: 0.004232, tar: 0.000104 
l0: 0.000049, l1: 0.000047, l2: 0.000085, l3: 0.000132, l4: 0.000312, l5: 0.000479, l6: 0.001284
[epoch: 520/1000, batch:   332/ 1052, ite: 136580] train loss: 0.004233, tar: 0.000104 
l0: 0.000064, l1: 0.000065, l2: 0.000110, l3: 0.000208, l4: 0.000604, l5: 0.000925, l6: 0.001954
[epoch: 520/1000, batch:   412/ 1052, ite: 136600] train loss: 0.004232, tar: 0.000104 
l0: 0.000074, l1: 0.000071, l2: 0.000109, l3: 0.000186, l4: 0.000386, l5: 0.000743, l6: 0.001408
[epoch: 520/1000, batch:   492/ 1052, ite: 136620] train loss: 0.004232, tar: 0.000104 
l0: 0.000101, l1: 0.000098, l2: 0.000139, l3: 0.000203, l4: 0.000671, l5: 0.001479, l6: 0.002431
[epoch: 520/1000, batch:   572/ 1052, ite: 136640] train loss: 0.004233, tar: 0.000104 
l0: 0.000020, l1: 0.000020, l2: 0.000055, l3: 0.000135, l4: 0.000520, l5: 0.001160, l6: 0.002072
[epoch: 520/1000, batch:   652/ 1052, ite: 136660] train loss: 0.004233, tar: 0.000104 
l0: 0.000151, l1: 0.000148, l2: 0.000211, l3: 0.000348, l4: 0.000544, l5: 0.001262, l6: 0.002801
[epoch: 520/1000, batch:   732/ 1052, ite: 136680] train loss: 0.004234, tar: 0.000104 
l0: 0.000116, l1: 0.000116, l2: 0.000191, l3: 0.000305, l4: 0.000876, l5: 0.002158, l6: 0.003535
[epoch: 520/1000, batch:   812/ 1052, ite: 136700] train loss: 0.004234, tar: 0.000104 
l0: 0.000122, l1: 0.000119, l2: 0.000191, l3: 0.000333, l4: 0.000703, l5: 0.001609, l6: 0.002412
[epoch: 520/1000, batch:   892/ 1052, ite: 136720] train loss: 0.004234, tar: 0.000104 
l0: 0.000243, l1: 0.000249, l2: 0.000349, l3: 0.000424, l4: 0.000716, l5: 0.001467, l6: 0.003183
[epoch: 520/1000, batch:   972/ 1052, ite: 136740] train loss: 0.004233, tar: 0.000104 
l0: 0.000063, l1: 0.000062, l2: 0.000117, l3: 0.000210, l4: 0.000505, l5: 0.000713, l6: 0.001740
[epoch: 520/1000, batch:  1052/ 1052, ite: 136760] train loss: 0.004232, tar: 0.000104 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_136760.pkl
[Epoch 520/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000067, l1: 0.000068, l2: 0.000108, l3: 0.000197, l4: 0.000526, l5: 0.001513, l6: 0.002089
[epoch: 521/1000, batch:    80/ 1052, ite: 136780] train loss: 0.003752, tar: 0.000093 
l0: 0.000099, l1: 0.000096, l2: 0.000129, l3: 0.000153, l4: 0.000338, l5: 0.001034, l6: 0.001688
[epoch: 521/1000, batch:   160/ 1052, ite: 136800] train loss: 0.004089, tar: 0.000101 
l0: 0.000131, l1: 0.000135, l2: 0.000186, l3: 0.000336, l4: 0.000670, l5: 0.001315, l6: 0.003144
[epoch: 521/1000, batch:   240/ 1052, ite: 136820] train loss: 0.004057, tar: 0.000102 
l0: 0.000129, l1: 0.000135, l2: 0.000172, l3: 0.000328, l4: 0.000583, l5: 0.001945, l6: 0.003284
[epoch: 521/1000, batch:   320/ 1052, ite: 136840] train loss: 0.004144, tar: 0.000099 
l0: 0.000053, l1: 0.000055, l2: 0.000098, l3: 0.000137, l4: 0.000272, l5: 0.000748, l6: 0.001410
[epoch: 521/1000, batch:   400/ 1052, ite: 136860] train loss: 0.004043, tar: 0.000092 
l0: 0.000047, l1: 0.000049, l2: 0.000113, l3: 0.000197, l4: 0.000803, l5: 0.002043, l6: 0.002985
[epoch: 521/1000, batch:   480/ 1052, ite: 136880] train loss: 0.004190, tar: 0.000091 
l0: 0.000351, l1: 0.000348, l2: 0.000383, l3: 0.000603, l4: 0.000970, l5: 0.001665, l6: 0.004087
[epoch: 521/1000, batch:   560/ 1052, ite: 136900] train loss: 0.004226, tar: 0.000093 
l0: 0.000052, l1: 0.000052, l2: 0.000094, l3: 0.000167, l4: 0.000466, l5: 0.001004, l6: 0.001344
[epoch: 521/1000, batch:   640/ 1052, ite: 136920] train loss: 0.004147, tar: 0.000090 
l0: 0.000069, l1: 0.000066, l2: 0.000134, l3: 0.000227, l4: 0.000367, l5: 0.001418, l6: 0.002734
[epoch: 521/1000, batch:   720/ 1052, ite: 136940] train loss: 0.004136, tar: 0.000091 
l0: 0.000126, l1: 0.000125, l2: 0.000152, l3: 0.000188, l4: 0.000369, l5: 0.000805, l6: 0.001432
[epoch: 521/1000, batch:   800/ 1052, ite: 136960] train loss: 0.004181, tar: 0.000093 
l0: 0.000046, l1: 0.000047, l2: 0.000077, l3: 0.000127, l4: 0.000377, l5: 0.000645, l6: 0.001640
[epoch: 521/1000, batch:   880/ 1052, ite: 136980] train loss: 0.004189, tar: 0.000093 
l0: 0.000146, l1: 0.000147, l2: 0.000213, l3: 0.000362, l4: 0.000954, l5: 0.002214, l6: 0.003783
[epoch: 521/1000, batch:   960/ 1052, ite: 137000] train loss: 0.004172, tar: 0.000092 
l0: 0.000012, l1: 0.000012, l2: 0.000021, l3: 0.000054, l4: 0.000223, l5: 0.000497, l6: 0.001059
[epoch: 521/1000, batch:  1040/ 1052, ite: 137020] train loss: 0.004161, tar: 0.000094 
[Epoch 521/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000124, l1: 0.000122, l2: 0.000201, l3: 0.000329, l4: 0.000958, l5: 0.001894, l6: 0.003296
[epoch: 522/1000, batch:    68/ 1052, ite: 137040] train loss: 0.004224, tar: 0.000097 
l0: 0.000033, l1: 0.000031, l2: 0.000079, l3: 0.000186, l4: 0.000415, l5: 0.001051, l6: 0.002433
[epoch: 522/1000, batch:   148/ 1052, ite: 137060] train loss: 0.004238, tar: 0.000097 
l0: 0.000082, l1: 0.000083, l2: 0.000120, l3: 0.000138, l4: 0.000319, l5: 0.000976, l6: 0.002272
[epoch: 522/1000, batch:   228/ 1052, ite: 137080] train loss: 0.004253, tar: 0.000097 
l0: 0.000081, l1: 0.000083, l2: 0.000136, l3: 0.000287, l4: 0.000595, l5: 0.001545, l6: 0.002576
[epoch: 522/1000, batch:   308/ 1052, ite: 137100] train loss: 0.004288, tar: 0.000098 
l0: 0.000125, l1: 0.000125, l2: 0.000140, l3: 0.000225, l4: 0.000386, l5: 0.000749, l6: 0.001397
[epoch: 522/1000, batch:   388/ 1052, ite: 137120] train loss: 0.004278, tar: 0.000097 
l0: 0.000078, l1: 0.000079, l2: 0.000109, l3: 0.000168, l4: 0.000687, l5: 0.001337, l6: 0.001914
[epoch: 522/1000, batch:   468/ 1052, ite: 137140] train loss: 0.004268, tar: 0.000097 
l0: 0.000058, l1: 0.000059, l2: 0.000086, l3: 0.000155, l4: 0.000388, l5: 0.000903, l6: 0.001543
[epoch: 522/1000, batch:   548/ 1052, ite: 137160] train loss: 0.004257, tar: 0.000096 
l0: 0.000048, l1: 0.000047, l2: 0.000102, l3: 0.000230, l4: 0.000598, l5: 0.001263, l6: 0.001618
[epoch: 522/1000, batch:   628/ 1052, ite: 137180] train loss: 0.004208, tar: 0.000095 
l0: 0.000043, l1: 0.000043, l2: 0.000075, l3: 0.000158, l4: 0.000448, l5: 0.000651, l6: 0.001231
[epoch: 522/1000, batch:   708/ 1052, ite: 137200] train loss: 0.004198, tar: 0.000095 
l0: 0.000174, l1: 0.000173, l2: 0.000204, l3: 0.000249, l4: 0.000449, l5: 0.001025, l6: 0.002577
[epoch: 522/1000, batch:   788/ 1052, ite: 137220] train loss: 0.004197, tar: 0.000096 
l0: 0.000257, l1: 0.000261, l2: 0.000280, l3: 0.000439, l4: 0.000703, l5: 0.001562, l6: 0.003264
[epoch: 522/1000, batch:   868/ 1052, ite: 137240] train loss: 0.004217, tar: 0.000096 
l0: 0.000039, l1: 0.000040, l2: 0.000064, l3: 0.000097, l4: 0.000258, l5: 0.001021, l6: 0.001725
[epoch: 522/1000, batch:   948/ 1052, ite: 137260] train loss: 0.004214, tar: 0.000096 
l0: 0.000101, l1: 0.000098, l2: 0.000153, l3: 0.000348, l4: 0.000781, l5: 0.001846, l6: 0.002557
[epoch: 522/1000, batch:  1028/ 1052, ite: 137280] train loss: 0.004206, tar: 0.000095 
[Epoch 522/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000038, l1: 0.000038, l2: 0.000077, l3: 0.000240, l4: 0.000618, l5: 0.001016, l6: 0.002385
[epoch: 523/1000, batch:    56/ 1052, ite: 137300] train loss: 0.004202, tar: 0.000095 
l0: 0.000117, l1: 0.000120, l2: 0.000181, l3: 0.000237, l4: 0.000487, l5: 0.001316, l6: 0.001928
[epoch: 523/1000, batch:   136/ 1052, ite: 137320] train loss: 0.004204, tar: 0.000095 
l0: 0.000078, l1: 0.000079, l2: 0.000112, l3: 0.000163, l4: 0.000287, l5: 0.001013, l6: 0.001561
[epoch: 523/1000, batch:   216/ 1052, ite: 137340] train loss: 0.004184, tar: 0.000094 
l0: 0.000091, l1: 0.000090, l2: 0.000115, l3: 0.000228, l4: 0.000993, l5: 0.001968, l6: 0.002831
[epoch: 523/1000, batch:   296/ 1052, ite: 137360] train loss: 0.004186, tar: 0.000095 
l0: 0.000056, l1: 0.000057, l2: 0.000087, l3: 0.000147, l4: 0.000356, l5: 0.000497, l6: 0.001608
[epoch: 523/1000, batch:   376/ 1052, ite: 137380] train loss: 0.004199, tar: 0.000095 
l0: 0.000069, l1: 0.000066, l2: 0.000150, l3: 0.000354, l4: 0.000760, l5: 0.001308, l6: 0.002063
[epoch: 523/1000, batch:   456/ 1052, ite: 137400] train loss: 0.004192, tar: 0.000094 
l0: 0.000138, l1: 0.000136, l2: 0.000180, l3: 0.000273, l4: 0.000511, l5: 0.001036, l6: 0.002021
[epoch: 523/1000, batch:   536/ 1052, ite: 137420] train loss: 0.004185, tar: 0.000094 
l0: 0.000080, l1: 0.000081, l2: 0.000097, l3: 0.000107, l4: 0.000342, l5: 0.000908, l6: 0.000751
[epoch: 523/1000, batch:   616/ 1052, ite: 137440] train loss: 0.004191, tar: 0.000094 
l0: 0.000038, l1: 0.000039, l2: 0.000092, l3: 0.000245, l4: 0.000536, l5: 0.000939, l6: 0.002397
[epoch: 523/1000, batch:   696/ 1052, ite: 137460] train loss: 0.004202, tar: 0.000094 
l0: 0.000206, l1: 0.000203, l2: 0.000202, l3: 0.000273, l4: 0.000519, l5: 0.001306, l6: 0.002016
[epoch: 523/1000, batch:   776/ 1052, ite: 137480] train loss: 0.004203, tar: 0.000094 
l0: 0.000155, l1: 0.000157, l2: 0.000168, l3: 0.000228, l4: 0.000467, l5: 0.001464, l6: 0.002426
[epoch: 523/1000, batch:   856/ 1052, ite: 137500] train loss: 0.004208, tar: 0.000094 
l0: 0.000030, l1: 0.000028, l2: 0.000079, l3: 0.000156, l4: 0.000321, l5: 0.001512, l6: 0.002542
[epoch: 523/1000, batch:   936/ 1052, ite: 137520] train loss: 0.004204, tar: 0.000094 
l0: 0.000070, l1: 0.000071, l2: 0.000097, l3: 0.000232, l4: 0.000377, l5: 0.001083, l6: 0.001386
[epoch: 523/1000, batch:  1016/ 1052, ite: 137540] train loss: 0.004209, tar: 0.000094 
[Epoch 523/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000062, l1: 0.000060, l2: 0.000102, l3: 0.000228, l4: 0.000479, l5: 0.001087, l6: 0.001607
[epoch: 524/1000, batch:    44/ 1052, ite: 137560] train loss: 0.004209, tar: 0.000094 
l0: 0.000064, l1: 0.000063, l2: 0.000116, l3: 0.000150, l4: 0.000280, l5: 0.000795, l6: 0.001544
[epoch: 524/1000, batch:   124/ 1052, ite: 137580] train loss: 0.004202, tar: 0.000094 
l0: 0.000065, l1: 0.000065, l2: 0.000129, l3: 0.000211, l4: 0.000530, l5: 0.000970, l6: 0.001832
[epoch: 524/1000, batch:   204/ 1052, ite: 137600] train loss: 0.004198, tar: 0.000094 
l0: 0.000080, l1: 0.000083, l2: 0.000116, l3: 0.000219, l4: 0.000357, l5: 0.000595, l6: 0.001587
[epoch: 524/1000, batch:   284/ 1052, ite: 137620] train loss: 0.004203, tar: 0.000095 
l0: 0.000157, l1: 0.000159, l2: 0.000214, l3: 0.000353, l4: 0.000868, l5: 0.001578, l6: 0.002236
[epoch: 524/1000, batch:   364/ 1052, ite: 137640] train loss: 0.004199, tar: 0.000095 
l0: 0.000067, l1: 0.000067, l2: 0.000096, l3: 0.000162, l4: 0.000347, l5: 0.000986, l6: 0.001086
[epoch: 524/1000, batch:   444/ 1052, ite: 137660] train loss: 0.004201, tar: 0.000095 
l0: 0.000037, l1: 0.000035, l2: 0.000103, l3: 0.000349, l4: 0.000535, l5: 0.001306, l6: 0.002419
[epoch: 524/1000, batch:   524/ 1052, ite: 137680] train loss: 0.004210, tar: 0.000095 
l0: 0.000025, l1: 0.000025, l2: 0.000060, l3: 0.000122, l4: 0.000307, l5: 0.000821, l6: 0.002146
[epoch: 524/1000, batch:   604/ 1052, ite: 137700] train loss: 0.004205, tar: 0.000095 
l0: 0.000058, l1: 0.000060, l2: 0.000068, l3: 0.000146, l4: 0.000283, l5: 0.000807, l6: 0.001223
[epoch: 524/1000, batch:   684/ 1052, ite: 137720] train loss: 0.004205, tar: 0.000094 
l0: 0.000062, l1: 0.000060, l2: 0.000106, l3: 0.000329, l4: 0.000657, l5: 0.001278, l6: 0.002718
[epoch: 524/1000, batch:   764/ 1052, ite: 137740] train loss: 0.004206, tar: 0.000094 
l0: 0.000089, l1: 0.000086, l2: 0.000124, l3: 0.000235, l4: 0.000481, l5: 0.000858, l6: 0.001591
[epoch: 524/1000, batch:   844/ 1052, ite: 137760] train loss: 0.004205, tar: 0.000094 
l0: 0.000112, l1: 0.000111, l2: 0.000145, l3: 0.000155, l4: 0.000333, l5: 0.000536, l6: 0.000716
[epoch: 524/1000, batch:   924/ 1052, ite: 137780] train loss: 0.004203, tar: 0.000094 
l0: 0.000036, l1: 0.000037, l2: 0.000080, l3: 0.000167, l4: 0.000355, l5: 0.000958, l6: 0.001830
[epoch: 524/1000, batch:  1004/ 1052, ite: 137800] train loss: 0.004201, tar: 0.000094 
[Epoch 524/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000041, l1: 0.000040, l2: 0.000087, l3: 0.000175, l4: 0.000365, l5: 0.000684, l6: 0.001619
[epoch: 525/1000, batch:    32/ 1052, ite: 137820] train loss: 0.004197, tar: 0.000094 
l0: 0.000157, l1: 0.000161, l2: 0.000179, l3: 0.000272, l4: 0.000432, l5: 0.001040, l6: 0.002181
[epoch: 525/1000, batch:   112/ 1052, ite: 137840] train loss: 0.004188, tar: 0.000094 
l0: 0.000067, l1: 0.000066, l2: 0.000072, l3: 0.000213, l4: 0.000460, l5: 0.000633, l6: 0.001331
[epoch: 525/1000, batch:   192/ 1052, ite: 137860] train loss: 0.004189, tar: 0.000094 
l0: 0.000078, l1: 0.000076, l2: 0.000108, l3: 0.000175, l4: 0.000439, l5: 0.001056, l6: 0.001764
[epoch: 525/1000, batch:   272/ 1052, ite: 137880] train loss: 0.004189, tar: 0.000094 
l0: 0.000197, l1: 0.000199, l2: 0.000220, l3: 0.000344, l4: 0.000425, l5: 0.000763, l6: 0.001360
[epoch: 525/1000, batch:   352/ 1052, ite: 137900] train loss: 0.004192, tar: 0.000094 
l0: 0.000030, l1: 0.000031, l2: 0.000043, l3: 0.000071, l4: 0.000310, l5: 0.000544, l6: 0.001271
[epoch: 525/1000, batch:   432/ 1052, ite: 137920] train loss: 0.004197, tar: 0.000093 
l0: 0.000123, l1: 0.000120, l2: 0.000157, l3: 0.000274, l4: 0.000559, l5: 0.001011, l6: 0.002587
[epoch: 525/1000, batch:   512/ 1052, ite: 137940] train loss: 0.004199, tar: 0.000093 
l0: 0.000100, l1: 0.000101, l2: 0.000144, l3: 0.000203, l4: 0.000480, l5: 0.001165, l6: 0.002072
[epoch: 525/1000, batch:   592/ 1052, ite: 137960] train loss: 0.004190, tar: 0.000093 
l0: 0.000122, l1: 0.000124, l2: 0.000159, l3: 0.000224, l4: 0.000439, l5: 0.000734, l6: 0.002278
[epoch: 525/1000, batch:   672/ 1052, ite: 137980] train loss: 0.004191, tar: 0.000093 
l0: 0.000099, l1: 0.000096, l2: 0.000149, l3: 0.000311, l4: 0.000589, l5: 0.001624, l6: 0.002241
[epoch: 525/1000, batch:   752/ 1052, ite: 138000] train loss: 0.004193, tar: 0.000094 
l0: 0.000096, l1: 0.000095, l2: 0.000119, l3: 0.000224, l4: 0.000366, l5: 0.001106, l6: 0.001310
[epoch: 525/1000, batch:   832/ 1052, ite: 138020] train loss: 0.004193, tar: 0.000094 
l0: 0.000094, l1: 0.000096, l2: 0.000132, l3: 0.000297, l4: 0.000606, l5: 0.001109, l6: 0.002222
[epoch: 525/1000, batch:   912/ 1052, ite: 138040] train loss: 0.004193, tar: 0.000094 
l0: 0.000060, l1: 0.000059, l2: 0.000113, l3: 0.000217, l4: 0.000462, l5: 0.000877, l6: 0.002169
[epoch: 525/1000, batch:   992/ 1052, ite: 138060] train loss: 0.004183, tar: 0.000094 
[Epoch 525/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000030, l1: 0.000030, l2: 0.000048, l3: 0.000066, l4: 0.000234, l5: 0.000374, l6: 0.000849
[epoch: 526/1000, batch:    20/ 1052, ite: 138080] train loss: 0.004187, tar: 0.000094 
l0: 0.000126, l1: 0.000129, l2: 0.000155, l3: 0.000200, l4: 0.000409, l5: 0.001192, l6: 0.002440
[epoch: 526/1000, batch:   100/ 1052, ite: 138100] train loss: 0.004191, tar: 0.000094 
l0: 0.000076, l1: 0.000077, l2: 0.000109, l3: 0.000191, l4: 0.000474, l5: 0.000973, l6: 0.002154
[epoch: 526/1000, batch:   180/ 1052, ite: 138120] train loss: 0.004193, tar: 0.000094 
l0: 0.000162, l1: 0.000166, l2: 0.000185, l3: 0.000234, l4: 0.000489, l5: 0.000979, l6: 0.002063
[epoch: 526/1000, batch:   260/ 1052, ite: 138140] train loss: 0.004195, tar: 0.000094 
l0: 0.000188, l1: 0.000192, l2: 0.000197, l3: 0.000332, l4: 0.000735, l5: 0.001450, l6: 0.003166
[epoch: 526/1000, batch:   340/ 1052, ite: 138160] train loss: 0.004192, tar: 0.000094 
l0: 0.000043, l1: 0.000046, l2: 0.000049, l3: 0.000094, l4: 0.000310, l5: 0.000613, l6: 0.001167
[epoch: 526/1000, batch:   420/ 1052, ite: 138180] train loss: 0.004190, tar: 0.000094 
l0: 0.000046, l1: 0.000048, l2: 0.000084, l3: 0.000182, l4: 0.000630, l5: 0.001144, l6: 0.001642
[epoch: 526/1000, batch:   500/ 1052, ite: 138200] train loss: 0.004188, tar: 0.000094 
l0: 0.000050, l1: 0.000053, l2: 0.000088, l3: 0.000108, l4: 0.000233, l5: 0.000374, l6: 0.000854
[epoch: 526/1000, batch:   580/ 1052, ite: 138220] train loss: 0.004181, tar: 0.000094 
l0: 0.000146, l1: 0.000151, l2: 0.000170, l3: 0.000264, l4: 0.000336, l5: 0.000993, l6: 0.002443
[epoch: 526/1000, batch:   660/ 1052, ite: 138240] train loss: 0.004180, tar: 0.000094 
l0: 0.000026, l1: 0.000025, l2: 0.000081, l3: 0.000138, l4: 0.000328, l5: 0.000931, l6: 0.002024
[epoch: 526/1000, batch:   740/ 1052, ite: 138260] train loss: 0.004188, tar: 0.000094 
l0: 0.000058, l1: 0.000055, l2: 0.000090, l3: 0.000136, l4: 0.000288, l5: 0.000566, l6: 0.001406
[epoch: 526/1000, batch:   820/ 1052, ite: 138280] train loss: 0.004184, tar: 0.000094 
l0: 0.000071, l1: 0.000073, l2: 0.000138, l3: 0.000313, l4: 0.000508, l5: 0.001165, l6: 0.001851
[epoch: 526/1000, batch:   900/ 1052, ite: 138300] train loss: 0.004189, tar: 0.000094 
l0: 0.000061, l1: 0.000062, l2: 0.000071, l3: 0.000135, l4: 0.000197, l5: 0.000616, l6: 0.001120
[epoch: 526/1000, batch:   980/ 1052, ite: 138320] train loss: 0.004185, tar: 0.000093 
[Epoch 526/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000045, l1: 0.000044, l2: 0.000095, l3: 0.000402, l4: 0.000696, l5: 0.001337, l6: 0.002034
[epoch: 527/1000, batch:     8/ 1052, ite: 138340] train loss: 0.004182, tar: 0.000093 
l0: 0.000075, l1: 0.000076, l2: 0.000130, l3: 0.000232, l4: 0.000461, l5: 0.001595, l6: 0.002770
[epoch: 527/1000, batch:    88/ 1052, ite: 138360] train loss: 0.004179, tar: 0.000093 
l0: 0.000106, l1: 0.000107, l2: 0.000113, l3: 0.000150, l4: 0.000307, l5: 0.000836, l6: 0.001517
[epoch: 527/1000, batch:   168/ 1052, ite: 138380] train loss: 0.004173, tar: 0.000093 
l0: 0.000096, l1: 0.000098, l2: 0.000148, l3: 0.000224, l4: 0.000599, l5: 0.001436, l6: 0.002003
[epoch: 527/1000, batch:   248/ 1052, ite: 138400] train loss: 0.004167, tar: 0.000093 
l0: 0.000077, l1: 0.000077, l2: 0.000087, l3: 0.000191, l4: 0.000464, l5: 0.001272, l6: 0.001243
[epoch: 527/1000, batch:   328/ 1052, ite: 138420] train loss: 0.004170, tar: 0.000093 
l0: 0.000123, l1: 0.000124, l2: 0.000160, l3: 0.000276, l4: 0.000834, l5: 0.001827, l6: 0.002993
[epoch: 527/1000, batch:   408/ 1052, ite: 138440] train loss: 0.004173, tar: 0.000093 
l0: 0.000055, l1: 0.000054, l2: 0.000094, l3: 0.000194, l4: 0.000433, l5: 0.000780, l6: 0.001269
[epoch: 527/1000, batch:   488/ 1052, ite: 138460] train loss: 0.004176, tar: 0.000093 
l0: 0.000114, l1: 0.000114, l2: 0.000144, l3: 0.000238, l4: 0.000694, l5: 0.000996, l6: 0.002241
[epoch: 527/1000, batch:   568/ 1052, ite: 138480] train loss: 0.004176, tar: 0.000093 
l0: 0.000044, l1: 0.000044, l2: 0.000128, l3: 0.000209, l4: 0.000461, l5: 0.000777, l6: 0.001396
[epoch: 527/1000, batch:   648/ 1052, ite: 138500] train loss: 0.004177, tar: 0.000092 
l0: 0.000054, l1: 0.000054, l2: 0.000082, l3: 0.000143, l4: 0.000251, l5: 0.000886, l6: 0.001289
[epoch: 527/1000, batch:   728/ 1052, ite: 138520] train loss: 0.004177, tar: 0.000092 
l0: 0.000137, l1: 0.000143, l2: 0.000178, l3: 0.000261, l4: 0.000561, l5: 0.000689, l6: 0.000973
[epoch: 527/1000, batch:   808/ 1052, ite: 138540] train loss: 0.004179, tar: 0.000092 
l0: 0.000119, l1: 0.000120, l2: 0.000161, l3: 0.000318, l4: 0.000489, l5: 0.001818, l6: 0.003317
[epoch: 527/1000, batch:   888/ 1052, ite: 138560] train loss: 0.004183, tar: 0.000093 
l0: 0.000127, l1: 0.000131, l2: 0.000166, l3: 0.000236, l4: 0.000485, l5: 0.001193, l6: 0.002210
[epoch: 527/1000, batch:   968/ 1052, ite: 138580] train loss: 0.004181, tar: 0.000093 
l0: 0.000047, l1: 0.000044, l2: 0.000116, l3: 0.000239, l4: 0.000301, l5: 0.000859, l6: 0.001590
[epoch: 527/1000, batch:  1048/ 1052, ite: 138600] train loss: 0.004182, tar: 0.000093 
[Epoch 527/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000150, l1: 0.000150, l2: 0.000156, l3: 0.000215, l4: 0.000497, l5: 0.000928, l6: 0.001835
[epoch: 528/1000, batch:    76/ 1052, ite: 138620] train loss: 0.004173, tar: 0.000093 
l0: 0.000039, l1: 0.000039, l2: 0.000070, l3: 0.000219, l4: 0.000509, l5: 0.001191, l6: 0.002022
[epoch: 528/1000, batch:   156/ 1052, ite: 138640] train loss: 0.004166, tar: 0.000092 
l0: 0.000059, l1: 0.000059, l2: 0.000108, l3: 0.000162, l4: 0.000414, l5: 0.000805, l6: 0.001554
[epoch: 528/1000, batch:   236/ 1052, ite: 138660] train loss: 0.004169, tar: 0.000092 
l0: 0.000050, l1: 0.000049, l2: 0.000114, l3: 0.000163, l4: 0.000349, l5: 0.000539, l6: 0.002004
[epoch: 528/1000, batch:   316/ 1052, ite: 138680] train loss: 0.004170, tar: 0.000092 
l0: 0.000103, l1: 0.000101, l2: 0.000150, l3: 0.000248, l4: 0.000488, l5: 0.001363, l6: 0.002492
[epoch: 528/1000, batch:   396/ 1052, ite: 138700] train loss: 0.004172, tar: 0.000092 
l0: 0.000157, l1: 0.000159, l2: 0.000188, l3: 0.000306, l4: 0.000514, l5: 0.001042, l6: 0.001673
[epoch: 528/1000, batch:   476/ 1052, ite: 138720] train loss: 0.004180, tar: 0.000093 
l0: 0.000022, l1: 0.000021, l2: 0.000040, l3: 0.000092, l4: 0.000245, l5: 0.000330, l6: 0.000999
[epoch: 528/1000, batch:   556/ 1052, ite: 138740] train loss: 0.004185, tar: 0.000093 
l0: 0.000094, l1: 0.000096, l2: 0.000136, l3: 0.000246, l4: 0.000423, l5: 0.001065, l6: 0.001435
[epoch: 528/1000, batch:   636/ 1052, ite: 138760] train loss: 0.004189, tar: 0.000093 
l0: 0.000053, l1: 0.000059, l2: 0.000078, l3: 0.000174, l4: 0.000295, l5: 0.000648, l6: 0.001835
[epoch: 528/1000, batch:   716/ 1052, ite: 138780] train loss: 0.004188, tar: 0.000093 
l0: 0.000012, l1: 0.000012, l2: 0.000030, l3: 0.000070, l4: 0.000234, l5: 0.000645, l6: 0.001091
[epoch: 528/1000, batch:   796/ 1052, ite: 138800] train loss: 0.004183, tar: 0.000093 
l0: 0.000037, l1: 0.000038, l2: 0.000060, l3: 0.000113, l4: 0.000331, l5: 0.000822, l6: 0.001790
[epoch: 528/1000, batch:   876/ 1052, ite: 138820] train loss: 0.004182, tar: 0.000092 
l0: 0.000119, l1: 0.000120, l2: 0.000130, l3: 0.000228, l4: 0.000354, l5: 0.001263, l6: 0.002130
[epoch: 528/1000, batch:   956/ 1052, ite: 138840] train loss: 0.004186, tar: 0.000093 
l0: 0.000105, l1: 0.000109, l2: 0.000125, l3: 0.000210, l4: 0.000402, l5: 0.001303, l6: 0.001701
[epoch: 528/1000, batch:  1036/ 1052, ite: 138860] train loss: 0.004184, tar: 0.000093 
[Epoch 528/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000013, l1: 0.000013, l2: 0.000032, l3: 0.000083, l4: 0.000350, l5: 0.000906, l6: 0.001165
[epoch: 529/1000, batch:    64/ 1052, ite: 138880] train loss: 0.004181, tar: 0.000092 
l0: 0.000038, l1: 0.000041, l2: 0.000058, l3: 0.000126, l4: 0.000543, l5: 0.001239, l6: 0.001982
[epoch: 529/1000, batch:   144/ 1052, ite: 138900] train loss: 0.004177, tar: 0.000092 
l0: 0.000095, l1: 0.000093, l2: 0.000153, l3: 0.000186, l4: 0.000496, l5: 0.001032, l6: 0.001836
[epoch: 529/1000, batch:   224/ 1052, ite: 138920] train loss: 0.004175, tar: 0.000092 
l0: 0.000156, l1: 0.000157, l2: 0.000181, l3: 0.000265, l4: 0.000512, l5: 0.001175, l6: 0.002496
[epoch: 529/1000, batch:   304/ 1052, ite: 138940] train loss: 0.004176, tar: 0.000092 
l0: 0.000135, l1: 0.000132, l2: 0.000173, l3: 0.000247, l4: 0.000529, l5: 0.001072, l6: 0.001777
[epoch: 529/1000, batch:   384/ 1052, ite: 138960] train loss: 0.004174, tar: 0.000092 
l0: 0.000126, l1: 0.000124, l2: 0.000205, l3: 0.000363, l4: 0.000866, l5: 0.001185, l6: 0.002674
[epoch: 529/1000, batch:   464/ 1052, ite: 138980] train loss: 0.004175, tar: 0.000092 
l0: 0.000174, l1: 0.000177, l2: 0.000281, l3: 0.000400, l4: 0.000633, l5: 0.001035, l6: 0.002947
[epoch: 529/1000, batch:   544/ 1052, ite: 139000] train loss: 0.004180, tar: 0.000092 
l0: 0.000028, l1: 0.000027, l2: 0.000067, l3: 0.000106, l4: 0.000307, l5: 0.000851, l6: 0.000911
[epoch: 529/1000, batch:   624/ 1052, ite: 139020] train loss: 0.004179, tar: 0.000092 
l0: 0.000069, l1: 0.000069, l2: 0.000085, l3: 0.000131, l4: 0.000430, l5: 0.000819, l6: 0.001418
[epoch: 529/1000, batch:   704/ 1052, ite: 139040] train loss: 0.004177, tar: 0.000092 
l0: 0.000019, l1: 0.000017, l2: 0.000041, l3: 0.000087, l4: 0.000325, l5: 0.000540, l6: 0.000644
[epoch: 529/1000, batch:   784/ 1052, ite: 139060] train loss: 0.004177, tar: 0.000092 
l0: 0.000056, l1: 0.000056, l2: 0.000115, l3: 0.000192, l4: 0.000345, l5: 0.000491, l6: 0.001538
[epoch: 529/1000, batch:   864/ 1052, ite: 139080] train loss: 0.004176, tar: 0.000092 
l0: 0.000038, l1: 0.000037, l2: 0.000107, l3: 0.000240, l4: 0.000616, l5: 0.001111, l6: 0.001720
[epoch: 529/1000, batch:   944/ 1052, ite: 139100] train loss: 0.004180, tar: 0.000092 
l0: 0.000132, l1: 0.000129, l2: 0.000197, l3: 0.000322, l4: 0.000706, l5: 0.001514, l6: 0.002605
[epoch: 529/1000, batch:  1024/ 1052, ite: 139120] train loss: 0.004183, tar: 0.000092 
[Epoch 529/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000163, l1: 0.000164, l2: 0.000230, l3: 0.000444, l4: 0.000649, l5: 0.001265, l6: 0.002623
[epoch: 530/1000, batch:    52/ 1052, ite: 139140] train loss: 0.004182, tar: 0.000092 
l0: 0.000312, l1: 0.000312, l2: 0.000340, l3: 0.000504, l4: 0.001045, l5: 0.002656, l6: 0.003662
[epoch: 530/1000, batch:   132/ 1052, ite: 139160] train loss: 0.004185, tar: 0.000093 
l0: 0.000102, l1: 0.000101, l2: 0.000139, l3: 0.000211, l4: 0.000556, l5: 0.000637, l6: 0.001679
[epoch: 530/1000, batch:   212/ 1052, ite: 139180] train loss: 0.004184, tar: 0.000093 
l0: 0.000087, l1: 0.000079, l2: 0.000137, l3: 0.000178, l4: 0.000452, l5: 0.001076, l6: 0.001366
[epoch: 530/1000, batch:   292/ 1052, ite: 139200] train loss: 0.004183, tar: 0.000093 
l0: 0.000122, l1: 0.000126, l2: 0.000161, l3: 0.000375, l4: 0.000661, l5: 0.001628, l6: 0.001736
[epoch: 530/1000, batch:   372/ 1052, ite: 139220] train loss: 0.004183, tar: 0.000093 
l0: 0.000981, l1: 0.000984, l2: 0.000815, l3: 0.000835, l4: 0.001008, l5: 0.001503, l6: 0.002537
[epoch: 530/1000, batch:   452/ 1052, ite: 139240] train loss: 0.004189, tar: 0.000095 
l0: 0.000131, l1: 0.000147, l2: 0.000144, l3: 0.000159, l4: 0.000322, l5: 0.001028, l6: 0.001844
[epoch: 530/1000, batch:   532/ 1052, ite: 139260] train loss: 0.004194, tar: 0.000096 
l0: 0.000210, l1: 0.000217, l2: 0.000246, l3: 0.000251, l4: 0.000443, l5: 0.000909, l6: 0.002345
[epoch: 530/1000, batch:   612/ 1052, ite: 139280] train loss: 0.004197, tar: 0.000096 
l0: 0.000082, l1: 0.000073, l2: 0.000140, l3: 0.000137, l4: 0.000446, l5: 0.000783, l6: 0.001498
[epoch: 530/1000, batch:   692/ 1052, ite: 139300] train loss: 0.004204, tar: 0.000098 
l0: 0.000149, l1: 0.000151, l2: 0.000169, l3: 0.000190, l4: 0.000353, l5: 0.000745, l6: 0.001228
[epoch: 530/1000, batch:   772/ 1052, ite: 139320] train loss: 0.004205, tar: 0.000099 
l0: 0.000162, l1: 0.000163, l2: 0.000215, l3: 0.000327, l4: 0.000478, l5: 0.001160, l6: 0.002374
[epoch: 530/1000, batch:   852/ 1052, ite: 139340] train loss: 0.004209, tar: 0.000099 
l0: 0.000339, l1: 0.000314, l2: 0.000348, l3: 0.000651, l4: 0.000737, l5: 0.001796, l6: 0.002205
[epoch: 530/1000, batch:   932/ 1052, ite: 139360] train loss: 0.004216, tar: 0.000100 
l0: 0.000160, l1: 0.000171, l2: 0.000202, l3: 0.000274, l4: 0.000680, l5: 0.001545, l6: 0.002297
[epoch: 530/1000, batch:  1012/ 1052, ite: 139380] train loss: 0.004220, tar: 0.000101 
[Epoch 530/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000089, l1: 0.000131, l2: 0.000160, l3: 0.000219, l4: 0.000606, l5: 0.000660, l6: 0.001786
[epoch: 531/1000, batch:    40/ 1052, ite: 139400] train loss: 0.004223, tar: 0.000101 
l0: 0.000169, l1: 0.000176, l2: 0.000208, l3: 0.000288, l4: 0.000537, l5: 0.001759, l6: 0.002607
[epoch: 531/1000, batch:   120/ 1052, ite: 139420] train loss: 0.004224, tar: 0.000102 
l0: 0.000251, l1: 0.000257, l2: 0.000288, l3: 0.000321, l4: 0.000650, l5: 0.002390, l6: 0.002749
[epoch: 531/1000, batch:   200/ 1052, ite: 139440] train loss: 0.004228, tar: 0.000102 
l0: 0.000143, l1: 0.000151, l2: 0.000192, l3: 0.000337, l4: 0.000684, l5: 0.001332, l6: 0.002732
[epoch: 531/1000, batch:   280/ 1052, ite: 139460] train loss: 0.004226, tar: 0.000102 
l0: 0.000097, l1: 0.000093, l2: 0.000160, l3: 0.000295, l4: 0.000755, l5: 0.001579, l6: 0.003283
[epoch: 531/1000, batch:   360/ 1052, ite: 139480] train loss: 0.004231, tar: 0.000103 
l0: 0.000427, l1: 0.000421, l2: 0.000446, l3: 0.000611, l4: 0.000546, l5: 0.001248, l6: 0.001730
[epoch: 531/1000, batch:   440/ 1052, ite: 139500] train loss: 0.004231, tar: 0.000103 
l0: 0.000037, l1: 0.000039, l2: 0.000076, l3: 0.000201, l4: 0.000261, l5: 0.000592, l6: 0.001283
[epoch: 531/1000, batch:   520/ 1052, ite: 139520] train loss: 0.004230, tar: 0.000103 
l0: 0.000132, l1: 0.000138, l2: 0.000146, l3: 0.000208, l4: 0.000519, l5: 0.000992, l6: 0.001598
[epoch: 531/1000, batch:   600/ 1052, ite: 139540] train loss: 0.004230, tar: 0.000103 
l0: 0.000375, l1: 0.000399, l2: 0.000435, l3: 0.000559, l4: 0.001065, l5: 0.002047, l6: 0.003507
[epoch: 531/1000, batch:   680/ 1052, ite: 139560] train loss: 0.004232, tar: 0.000103 
l0: 0.000209, l1: 0.000212, l2: 0.000262, l3: 0.000357, l4: 0.000694, l5: 0.001370, l6: 0.002603
[epoch: 531/1000, batch:   760/ 1052, ite: 139580] train loss: 0.004232, tar: 0.000103 
l0: 0.000077, l1: 0.000078, l2: 0.000099, l3: 0.000124, l4: 0.000250, l5: 0.000373, l6: 0.000765
[epoch: 531/1000, batch:   840/ 1052, ite: 139600] train loss: 0.004231, tar: 0.000103 
l0: 0.000165, l1: 0.000162, l2: 0.000194, l3: 0.000323, l4: 0.000653, l5: 0.001437, l6: 0.002029
[epoch: 531/1000, batch:   920/ 1052, ite: 139620] train loss: 0.004235, tar: 0.000104 
l0: 0.000070, l1: 0.000070, l2: 0.000098, l3: 0.000159, l4: 0.000439, l5: 0.001193, l6: 0.002284
[epoch: 531/1000, batch:  1000/ 1052, ite: 139640] train loss: 0.004238, tar: 0.000104 
[Epoch 531/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000164, l1: 0.000173, l2: 0.000188, l3: 0.000259, l4: 0.000487, l5: 0.001310, l6: 0.002689
[epoch: 532/1000, batch:    28/ 1052, ite: 139660] train loss: 0.004236, tar: 0.000104 
l0: 0.000132, l1: 0.000136, l2: 0.000170, l3: 0.000278, l4: 0.000570, l5: 0.001572, l6: 0.002551
[epoch: 532/1000, batch:   108/ 1052, ite: 139680] train loss: 0.004238, tar: 0.000104 
l0: 0.000055, l1: 0.000052, l2: 0.000085, l3: 0.000119, l4: 0.000491, l5: 0.000986, l6: 0.001062
[epoch: 532/1000, batch:   188/ 1052, ite: 139700] train loss: 0.004240, tar: 0.000104 
l0: 0.000032, l1: 0.000033, l2: 0.000052, l3: 0.000110, l4: 0.000373, l5: 0.000695, l6: 0.001919
[epoch: 532/1000, batch:   268/ 1052, ite: 139720] train loss: 0.004239, tar: 0.000104 
l0: 0.000112, l1: 0.000112, l2: 0.000174, l3: 0.000291, l4: 0.000947, l5: 0.001651, l6: 0.002842
[epoch: 532/1000, batch:   348/ 1052, ite: 139740] train loss: 0.004239, tar: 0.000104 
l0: 0.000146, l1: 0.000149, l2: 0.000172, l3: 0.000265, l4: 0.000494, l5: 0.001127, l6: 0.001653
[epoch: 532/1000, batch:   428/ 1052, ite: 139760] train loss: 0.004236, tar: 0.000104 
l0: 0.000150, l1: 0.000156, l2: 0.000200, l3: 0.000282, l4: 0.000321, l5: 0.000880, l6: 0.001776
[epoch: 532/1000, batch:   508/ 1052, ite: 139780] train loss: 0.004238, tar: 0.000104 
l0: 0.000062, l1: 0.000065, l2: 0.000090, l3: 0.000152, l4: 0.000371, l5: 0.000739, l6: 0.001272
[epoch: 532/1000, batch:   588/ 1052, ite: 139800] train loss: 0.004245, tar: 0.000105 
l0: 0.000068, l1: 0.000063, l2: 0.000145, l3: 0.000327, l4: 0.000771, l5: 0.001453, l6: 0.002688
[epoch: 532/1000, batch:   668/ 1052, ite: 139820] train loss: 0.004247, tar: 0.000105 
l0: 0.000155, l1: 0.000158, l2: 0.000149, l3: 0.000203, l4: 0.000284, l5: 0.000777, l6: 0.001762
[epoch: 532/1000, batch:   748/ 1052, ite: 139840] train loss: 0.004249, tar: 0.000106 
l0: 0.000129, l1: 0.000129, l2: 0.000198, l3: 0.000338, l4: 0.000610, l5: 0.001300, l6: 0.001500
[epoch: 532/1000, batch:   828/ 1052, ite: 139860] train loss: 0.004251, tar: 0.000106 
l0: 0.000162, l1: 0.000210, l2: 0.000213, l3: 0.000307, l4: 0.000365, l5: 0.000694, l6: 0.000954
[epoch: 532/1000, batch:   908/ 1052, ite: 139880] train loss: 0.004254, tar: 0.000107 
l0: 0.000038, l1: 0.000039, l2: 0.000070, l3: 0.000181, l4: 0.000337, l5: 0.000629, l6: 0.000894
[epoch: 532/1000, batch:   988/ 1052, ite: 139900] train loss: 0.004258, tar: 0.000107 
[Epoch 532/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000093, l1: 0.000092, l2: 0.000140, l3: 0.000177, l4: 0.000309, l5: 0.000930, l6: 0.001623
[epoch: 533/1000, batch:    16/ 1052, ite: 139920] train loss: 0.004260, tar: 0.000107 
l0: 0.000140, l1: 0.000146, l2: 0.000162, l3: 0.000244, l4: 0.000335, l5: 0.000813, l6: 0.001553
[epoch: 533/1000, batch:    96/ 1052, ite: 139940] train loss: 0.004267, tar: 0.000108 
l0: 0.000119, l1: 0.000123, l2: 0.000135, l3: 0.000164, l4: 0.000276, l5: 0.000883, l6: 0.001849
[epoch: 533/1000, batch:   176/ 1052, ite: 139960] train loss: 0.004264, tar: 0.000108 
l0: 0.000135, l1: 0.000133, l2: 0.000171, l3: 0.000248, l4: 0.000590, l5: 0.001335, l6: 0.001335
[epoch: 533/1000, batch:   256/ 1052, ite: 139980] train loss: 0.004269, tar: 0.000108 
l0: 0.000092, l1: 0.000088, l2: 0.000158, l3: 0.000368, l4: 0.000741, l5: 0.001212, l6: 0.002161
[epoch: 533/1000, batch:   336/ 1052, ite: 140000] train loss: 0.004266, tar: 0.000108 
l0: 0.000046, l1: 0.000044, l2: 0.000076, l3: 0.000135, l4: 0.000279, l5: 0.000880, l6: 0.001948
[epoch: 533/1000, batch:   416/ 1052, ite: 140020] train loss: 0.004265, tar: 0.000108 
l0: 0.000058, l1: 0.000054, l2: 0.000078, l3: 0.000173, l4: 0.000372, l5: 0.000964, l6: 0.001378
[epoch: 533/1000, batch:   496/ 1052, ite: 140040] train loss: 0.004265, tar: 0.000108 
l0: 0.000070, l1: 0.000064, l2: 0.000107, l3: 0.000174, l4: 0.000304, l5: 0.000965, l6: 0.001398
[epoch: 533/1000, batch:   576/ 1052, ite: 140060] train loss: 0.004262, tar: 0.000108 
l0: 0.000267, l1: 0.000273, l2: 0.000338, l3: 0.000489, l4: 0.000858, l5: 0.002207, l6: 0.003077
[epoch: 533/1000, batch:   656/ 1052, ite: 140080] train loss: 0.004263, tar: 0.000108 
l0: 0.000051, l1: 0.000047, l2: 0.000103, l3: 0.000205, l4: 0.000426, l5: 0.000759, l6: 0.001300
[epoch: 533/1000, batch:   736/ 1052, ite: 140100] train loss: 0.004261, tar: 0.000108 
l0: 0.000099, l1: 0.000094, l2: 0.000146, l3: 0.000482, l4: 0.001279, l5: 0.002252, l6: 0.003465
[epoch: 533/1000, batch:   816/ 1052, ite: 140120] train loss: 0.004259, tar: 0.000108 
l0: 0.000088, l1: 0.000091, l2: 0.000142, l3: 0.000236, l4: 0.000641, l5: 0.001574, l6: 0.002381
[epoch: 533/1000, batch:   896/ 1052, ite: 140140] train loss: 0.004262, tar: 0.000108 
l0: 0.000145, l1: 0.000152, l2: 0.000194, l3: 0.000237, l4: 0.000351, l5: 0.001228, l6: 0.002922
[epoch: 533/1000, batch:   976/ 1052, ite: 140160] train loss: 0.004263, tar: 0.000108 
[Epoch 533/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000026, l1: 0.000024, l2: 0.000044, l3: 0.000093, l4: 0.000411, l5: 0.000523, l6: 0.001433
[epoch: 534/1000, batch:     4/ 1052, ite: 140180] train loss: 0.004264, tar: 0.000108 
l0: 0.000082, l1: 0.000089, l2: 0.000119, l3: 0.000183, l4: 0.000463, l5: 0.001177, l6: 0.001854
[epoch: 534/1000, batch:    84/ 1052, ite: 140200] train loss: 0.004264, tar: 0.000108 
l0: 0.000050, l1: 0.000053, l2: 0.000067, l3: 0.000080, l4: 0.000250, l5: 0.000786, l6: 0.001542
[epoch: 534/1000, batch:   164/ 1052, ite: 140220] train loss: 0.004260, tar: 0.000108 
l0: 0.000088, l1: 0.000087, l2: 0.000097, l3: 0.000124, l4: 0.000332, l5: 0.000588, l6: 0.001029
[epoch: 534/1000, batch:   244/ 1052, ite: 140240] train loss: 0.004262, tar: 0.000108 
l0: 0.000156, l1: 0.000157, l2: 0.000195, l3: 0.000293, l4: 0.000613, l5: 0.000970, l6: 0.002402
[epoch: 534/1000, batch:   324/ 1052, ite: 140260] train loss: 0.004263, tar: 0.000108 
l0: 0.000075, l1: 0.000071, l2: 0.000111, l3: 0.000284, l4: 0.000566, l5: 0.001189, l6: 0.001883
[epoch: 534/1000, batch:   404/ 1052, ite: 140280] train loss: 0.004263, tar: 0.000108 
l0: 0.000153, l1: 0.000153, l2: 0.000218, l3: 0.000323, l4: 0.000696, l5: 0.001511, l6: 0.002993
[epoch: 534/1000, batch:   484/ 1052, ite: 140300] train loss: 0.004264, tar: 0.000108 
l0: 0.000061, l1: 0.000060, l2: 0.000142, l3: 0.000305, l4: 0.001091, l5: 0.001615, l6: 0.002527
[epoch: 534/1000, batch:   564/ 1052, ite: 140320] train loss: 0.004261, tar: 0.000108 
l0: 0.000110, l1: 0.000112, l2: 0.000160, l3: 0.000279, l4: 0.000710, l5: 0.001345, l6: 0.002091
[epoch: 534/1000, batch:   644/ 1052, ite: 140340] train loss: 0.004263, tar: 0.000108 
l0: 0.000114, l1: 0.000113, l2: 0.000166, l3: 0.000259, l4: 0.000578, l5: 0.001692, l6: 0.003275
[epoch: 534/1000, batch:   724/ 1052, ite: 140360] train loss: 0.004263, tar: 0.000108 
l0: 0.000173, l1: 0.000169, l2: 0.000289, l3: 0.000321, l4: 0.000705, l5: 0.001685, l6: 0.001995
[epoch: 534/1000, batch:   804/ 1052, ite: 140380] train loss: 0.004268, tar: 0.000108 
l0: 0.000044, l1: 0.000045, l2: 0.000062, l3: 0.000105, l4: 0.000240, l5: 0.000644, l6: 0.001328
[epoch: 534/1000, batch:   884/ 1052, ite: 140400] train loss: 0.004267, tar: 0.000108 
l0: 0.000080, l1: 0.000079, l2: 0.000110, l3: 0.000243, l4: 0.000420, l5: 0.001187, l6: 0.001872
[epoch: 534/1000, batch:   964/ 1052, ite: 140420] train loss: 0.004263, tar: 0.000108 
l0: 0.000011, l1: 0.000009, l2: 0.000026, l3: 0.000084, l4: 0.000229, l5: 0.000363, l6: 0.000548
[epoch: 534/1000, batch:  1044/ 1052, ite: 140440] train loss: 0.004260, tar: 0.000108 
[Epoch 534/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000069, l1: 0.000069, l2: 0.000093, l3: 0.000172, l4: 0.000341, l5: 0.000835, l6: 0.001538
[epoch: 535/1000, batch:    72/ 1052, ite: 140460] train loss: 0.004259, tar: 0.000108 
l0: 0.000058, l1: 0.000057, l2: 0.000111, l3: 0.000215, l4: 0.000471, l5: 0.000983, l6: 0.001207
[epoch: 535/1000, batch:   152/ 1052, ite: 140480] train loss: 0.004256, tar: 0.000107 
l0: 0.000042, l1: 0.000044, l2: 0.000061, l3: 0.000145, l4: 0.000271, l5: 0.000884, l6: 0.001398
[epoch: 535/1000, batch:   232/ 1052, ite: 140500] train loss: 0.004254, tar: 0.000107 
l0: 0.000114, l1: 0.000114, l2: 0.000172, l3: 0.000404, l4: 0.000557, l5: 0.001329, l6: 0.001822
[epoch: 535/1000, batch:   312/ 1052, ite: 140520] train loss: 0.004257, tar: 0.000107 
l0: 0.000036, l1: 0.000036, l2: 0.000058, l3: 0.000135, l4: 0.000554, l5: 0.000939, l6: 0.000999
[epoch: 535/1000, batch:   392/ 1052, ite: 140540] train loss: 0.004254, tar: 0.000107 
l0: 0.000089, l1: 0.000087, l2: 0.000130, l3: 0.000226, l4: 0.000457, l5: 0.001171, l6: 0.001048
[epoch: 535/1000, batch:   472/ 1052, ite: 140560] train loss: 0.004253, tar: 0.000107 
l0: 0.000023, l1: 0.000022, l2: 0.000072, l3: 0.000155, l4: 0.000416, l5: 0.000742, l6: 0.001365
[epoch: 535/1000, batch:   552/ 1052, ite: 140580] train loss: 0.004253, tar: 0.000107 
l0: 0.000075, l1: 0.000072, l2: 0.000108, l3: 0.000127, l4: 0.000316, l5: 0.000718, l6: 0.000963
[epoch: 535/1000, batch:   632/ 1052, ite: 140600] train loss: 0.004255, tar: 0.000107 
l0: 0.000033, l1: 0.000033, l2: 0.000067, l3: 0.000191, l4: 0.000397, l5: 0.000987, l6: 0.002705
[epoch: 535/1000, batch:   712/ 1052, ite: 140620] train loss: 0.004257, tar: 0.000107 
l0: 0.000084, l1: 0.000086, l2: 0.000122, l3: 0.000184, l4: 0.000230, l5: 0.000509, l6: 0.001119
[epoch: 535/1000, batch:   792/ 1052, ite: 140640] train loss: 0.004259, tar: 0.000107 
l0: 0.000027, l1: 0.000027, l2: 0.000067, l3: 0.000144, l4: 0.000372, l5: 0.000886, l6: 0.001538
[epoch: 535/1000, batch:   872/ 1052, ite: 140660] train loss: 0.004258, tar: 0.000107 
l0: 0.000100, l1: 0.000115, l2: 0.000120, l3: 0.000212, l4: 0.000458, l5: 0.000726, l6: 0.001672
[epoch: 535/1000, batch:   952/ 1052, ite: 140680] train loss: 0.004259, tar: 0.000107 
l0: 0.000042, l1: 0.000041, l2: 0.000070, l3: 0.000118, l4: 0.000299, l5: 0.000932, l6: 0.000926
[epoch: 535/1000, batch:  1032/ 1052, ite: 140700] train loss: 0.004257, tar: 0.000107 
[Epoch 535/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000095, l1: 0.000102, l2: 0.000100, l3: 0.000191, l4: 0.000302, l5: 0.000670, l6: 0.000929
[epoch: 536/1000, batch:    60/ 1052, ite: 140720] train loss: 0.004254, tar: 0.000107 
l0: 0.000107, l1: 0.000110, l2: 0.000151, l3: 0.000277, l4: 0.000704, l5: 0.001099, l6: 0.001967
[epoch: 536/1000, batch:   140/ 1052, ite: 140740] train loss: 0.004257, tar: 0.000107 
l0: 0.000118, l1: 0.000115, l2: 0.000136, l3: 0.000279, l4: 0.000403, l5: 0.000771, l6: 0.002633
[epoch: 536/1000, batch:   220/ 1052, ite: 140760] train loss: 0.004259, tar: 0.000107 
l0: 0.000065, l1: 0.000067, l2: 0.000115, l3: 0.000207, l4: 0.000459, l5: 0.001003, l6: 0.002343
[epoch: 536/1000, batch:   300/ 1052, ite: 140780] train loss: 0.004258, tar: 0.000107 
l0: 0.000101, l1: 0.000100, l2: 0.000124, l3: 0.000182, l4: 0.000357, l5: 0.001031, l6: 0.001457
[epoch: 536/1000, batch:   380/ 1052, ite: 140800] train loss: 0.004258, tar: 0.000107 
l0: 0.000059, l1: 0.000058, l2: 0.000074, l3: 0.000145, l4: 0.000296, l5: 0.000671, l6: 0.001727
[epoch: 536/1000, batch:   460/ 1052, ite: 140820] train loss: 0.004258, tar: 0.000107 
l0: 0.000093, l1: 0.000090, l2: 0.000133, l3: 0.000200, l4: 0.000494, l5: 0.000943, l6: 0.001795
[epoch: 536/1000, batch:   540/ 1052, ite: 140840] train loss: 0.004256, tar: 0.000107 
l0: 0.000081, l1: 0.000082, l2: 0.000117, l3: 0.000189, l4: 0.000513, l5: 0.000996, l6: 0.002027
[epoch: 536/1000, batch:   620/ 1052, ite: 140860] train loss: 0.004256, tar: 0.000107 
l0: 0.000225, l1: 0.000225, l2: 0.000286, l3: 0.000299, l4: 0.000565, l5: 0.001036, l6: 0.002117
[epoch: 536/1000, batch:   700/ 1052, ite: 140880] train loss: 0.004258, tar: 0.000107 
l0: 0.000066, l1: 0.000067, l2: 0.000097, l3: 0.000219, l4: 0.000540, l5: 0.001250, l6: 0.002619
[epoch: 536/1000, batch:   780/ 1052, ite: 140900] train loss: 0.004257, tar: 0.000106 
l0: 0.000066, l1: 0.000066, l2: 0.000087, l3: 0.000197, l4: 0.000493, l5: 0.000512, l6: 0.001372
[epoch: 536/1000, batch:   860/ 1052, ite: 140920] train loss: 0.004255, tar: 0.000106 
l0: 0.000141, l1: 0.000147, l2: 0.000152, l3: 0.000325, l4: 0.000848, l5: 0.001353, l6: 0.002001
[epoch: 536/1000, batch:   940/ 1052, ite: 140940] train loss: 0.004256, tar: 0.000106 
l0: 0.000041, l1: 0.000044, l2: 0.000094, l3: 0.000197, l4: 0.000518, l5: 0.000979, l6: 0.003161
[epoch: 536/1000, batch:  1020/ 1052, ite: 140960] train loss: 0.004256, tar: 0.000106 
[Epoch 536/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000027, l1: 0.000026, l2: 0.000047, l3: 0.000096, l4: 0.000271, l5: 0.000832, l6: 0.001470
[epoch: 537/1000, batch:    48/ 1052, ite: 140980] train loss: 0.004254, tar: 0.000106 
l0: 0.000458, l1: 0.000442, l2: 0.000578, l3: 0.000539, l4: 0.000713, l5: 0.001272, l6: 0.001804
[epoch: 537/1000, batch:   128/ 1052, ite: 141000] train loss: 0.004253, tar: 0.000106 
l0: 0.000123, l1: 0.000122, l2: 0.000162, l3: 0.000275, l4: 0.000748, l5: 0.001610, l6: 0.002214
[epoch: 537/1000, batch:   208/ 1052, ite: 141020] train loss: 0.004252, tar: 0.000106 
l0: 0.000076, l1: 0.000072, l2: 0.000110, l3: 0.000250, l4: 0.000337, l5: 0.001194, l6: 0.002172
[epoch: 537/1000, batch:   288/ 1052, ite: 141040] train loss: 0.004251, tar: 0.000106 
l0: 0.000083, l1: 0.000080, l2: 0.000147, l3: 0.000402, l4: 0.000584, l5: 0.001184, l6: 0.002266
[epoch: 537/1000, batch:   368/ 1052, ite: 141060] train loss: 0.004252, tar: 0.000106 
l0: 0.000120, l1: 0.000115, l2: 0.000168, l3: 0.000235, l4: 0.000305, l5: 0.000590, l6: 0.000857
[epoch: 537/1000, batch:   448/ 1052, ite: 141080] train loss: 0.004251, tar: 0.000106 
l0: 0.000157, l1: 0.000159, l2: 0.000215, l3: 0.000341, l4: 0.000682, l5: 0.001360, l6: 0.002367
[epoch: 537/1000, batch:   528/ 1052, ite: 141100] train loss: 0.004251, tar: 0.000106 
l0: 0.000107, l1: 0.000110, l2: 0.000154, l3: 0.000227, l4: 0.000673, l5: 0.001389, l6: 0.002820
[epoch: 537/1000, batch:   608/ 1052, ite: 141120] train loss: 0.004254, tar: 0.000106 
l0: 0.000103, l1: 0.000103, l2: 0.000161, l3: 0.000263, l4: 0.000530, l5: 0.001095, l6: 0.001640
[epoch: 537/1000, batch:   688/ 1052, ite: 141140] train loss: 0.004252, tar: 0.000106 
l0: 0.000085, l1: 0.000087, l2: 0.000139, l3: 0.000232, l4: 0.000494, l5: 0.000839, l6: 0.002145
[epoch: 537/1000, batch:   768/ 1052, ite: 141160] train loss: 0.004254, tar: 0.000106 
l0: 0.000079, l1: 0.000078, l2: 0.000115, l3: 0.000218, l4: 0.000376, l5: 0.000698, l6: 0.001153
[epoch: 537/1000, batch:   848/ 1052, ite: 141180] train loss: 0.004252, tar: 0.000105 
l0: 0.000064, l1: 0.000063, l2: 0.000095, l3: 0.000187, l4: 0.000402, l5: 0.000687, l6: 0.001214
[epoch: 537/1000, batch:   928/ 1052, ite: 141200] train loss: 0.004251, tar: 0.000105 
l0: 0.000050, l1: 0.000050, l2: 0.000097, l3: 0.000190, l4: 0.000521, l5: 0.001099, l6: 0.001539
[epoch: 537/1000, batch:  1008/ 1052, ite: 141220] train loss: 0.004249, tar: 0.000105 
[Epoch 537/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000097, l1: 0.000101, l2: 0.000149, l3: 0.000331, l4: 0.000608, l5: 0.001303, l6: 0.001909
[epoch: 538/1000, batch:    36/ 1052, ite: 141240] train loss: 0.004248, tar: 0.000105 
l0: 0.000094, l1: 0.000091, l2: 0.000118, l3: 0.000195, l4: 0.000412, l5: 0.000974, l6: 0.002007
[epoch: 538/1000, batch:   116/ 1052, ite: 141260] train loss: 0.004249, tar: 0.000105 
l0: 0.000122, l1: 0.000128, l2: 0.000149, l3: 0.000207, l4: 0.000493, l5: 0.001020, l6: 0.002079
[epoch: 538/1000, batch:   196/ 1052, ite: 141280] train loss: 0.004247, tar: 0.000105 
l0: 0.000097, l1: 0.000104, l2: 0.000128, l3: 0.000149, l4: 0.000383, l5: 0.000942, l6: 0.002401
[epoch: 538/1000, batch:   276/ 1052, ite: 141300] train loss: 0.004245, tar: 0.000105 
l0: 0.000028, l1: 0.000025, l2: 0.000073, l3: 0.000087, l4: 0.000281, l5: 0.000532, l6: 0.000942
[epoch: 538/1000, batch:   356/ 1052, ite: 141320] train loss: 0.004247, tar: 0.000105 
l0: 0.000032, l1: 0.000035, l2: 0.000033, l3: 0.000098, l4: 0.000299, l5: 0.000322, l6: 0.000927
[epoch: 538/1000, batch:   436/ 1052, ite: 141340] train loss: 0.004245, tar: 0.000105 
l0: 0.000142, l1: 0.000145, l2: 0.000198, l3: 0.000354, l4: 0.000977, l5: 0.001841, l6: 0.003931
[epoch: 538/1000, batch:   516/ 1052, ite: 141360] train loss: 0.004247, tar: 0.000105 
l0: 0.000018, l1: 0.000019, l2: 0.000046, l3: 0.000165, l4: 0.000434, l5: 0.000880, l6: 0.001377
[epoch: 538/1000, batch:   596/ 1052, ite: 141380] train loss: 0.004247, tar: 0.000104 
l0: 0.000115, l1: 0.000117, l2: 0.000132, l3: 0.000165, l4: 0.000410, l5: 0.001092, l6: 0.002263
[epoch: 538/1000, batch:   676/ 1052, ite: 141400] train loss: 0.004249, tar: 0.000105 
l0: 0.000070, l1: 0.000071, l2: 0.000107, l3: 0.000222, l4: 0.000581, l5: 0.001089, l6: 0.002504
[epoch: 538/1000, batch:   756/ 1052, ite: 141420] train loss: 0.004248, tar: 0.000105 
l0: 0.000020, l1: 0.000020, l2: 0.000075, l3: 0.000155, l4: 0.000393, l5: 0.000637, l6: 0.001818
[epoch: 538/1000, batch:   836/ 1052, ite: 141440] train loss: 0.004246, tar: 0.000104 
l0: 0.000042, l1: 0.000044, l2: 0.000076, l3: 0.000138, l4: 0.000398, l5: 0.000796, l6: 0.001382
[epoch: 538/1000, batch:   916/ 1052, ite: 141460] train loss: 0.004247, tar: 0.000104 
l0: 0.000043, l1: 0.000047, l2: 0.000071, l3: 0.000087, l4: 0.000266, l5: 0.000916, l6: 0.001086
[epoch: 538/1000, batch:   996/ 1052, ite: 141480] train loss: 0.004245, tar: 0.000104 
[Epoch 538/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000187, l1: 0.000186, l2: 0.000255, l3: 0.000476, l4: 0.001086, l5: 0.001978, l6: 0.003968
[epoch: 539/1000, batch:    24/ 1052, ite: 141500] train loss: 0.004245, tar: 0.000104 
l0: 0.000066, l1: 0.000066, l2: 0.000111, l3: 0.000187, l4: 0.000430, l5: 0.001112, l6: 0.002375
[epoch: 539/1000, batch:   104/ 1052, ite: 141520] train loss: 0.004246, tar: 0.000104 
l0: 0.000004, l1: 0.000004, l2: 0.000011, l3: 0.000061, l4: 0.000162, l5: 0.000823, l6: 0.001443
[epoch: 539/1000, batch:   184/ 1052, ite: 141540] train loss: 0.004245, tar: 0.000104 
l0: 0.000059, l1: 0.000062, l2: 0.000088, l3: 0.000258, l4: 0.000426, l5: 0.000979, l6: 0.001689
[epoch: 539/1000, batch:   264/ 1052, ite: 141560] train loss: 0.004245, tar: 0.000104 
l0: 0.000116, l1: 0.000118, l2: 0.000167, l3: 0.000255, l4: 0.000495, l5: 0.001603, l6: 0.003465
[epoch: 539/1000, batch:   344/ 1052, ite: 141580] train loss: 0.004246, tar: 0.000104 
l0: 0.000084, l1: 0.000085, l2: 0.000115, l3: 0.000280, l4: 0.000993, l5: 0.002291, l6: 0.002960
[epoch: 539/1000, batch:   424/ 1052, ite: 141600] train loss: 0.004247, tar: 0.000104 
l0: 0.000153, l1: 0.000150, l2: 0.000229, l3: 0.000289, l4: 0.000528, l5: 0.002024, l6: 0.002971
[epoch: 539/1000, batch:   504/ 1052, ite: 141620] train loss: 0.004245, tar: 0.000104 
l0: 0.000249, l1: 0.000250, l2: 0.000321, l3: 0.000524, l4: 0.001177, l5: 0.002775, l6: 0.004596
[epoch: 539/1000, batch:   584/ 1052, ite: 141640] train loss: 0.004247, tar: 0.000104 
l0: 0.000226, l1: 0.000234, l2: 0.000277, l3: 0.000447, l4: 0.001209, l5: 0.002669, l6: 0.003941
[epoch: 539/1000, batch:   664/ 1052, ite: 141660] train loss: 0.004246, tar: 0.000104 
l0: 0.000107, l1: 0.000107, l2: 0.000202, l3: 0.000360, l4: 0.000688, l5: 0.001189, l6: 0.001797
[epoch: 539/1000, batch:   744/ 1052, ite: 141680] train loss: 0.004244, tar: 0.000104 
l0: 0.000025, l1: 0.000026, l2: 0.000060, l3: 0.000115, l4: 0.000213, l5: 0.000974, l6: 0.001480
[epoch: 539/1000, batch:   824/ 1052, ite: 141700] train loss: 0.004242, tar: 0.000104 
l0: 0.000089, l1: 0.000090, l2: 0.000136, l3: 0.000180, l4: 0.000256, l5: 0.000956, l6: 0.001423
[epoch: 539/1000, batch:   904/ 1052, ite: 141720] train loss: 0.004241, tar: 0.000104 
l0: 0.000058, l1: 0.000062, l2: 0.000083, l3: 0.000154, l4: 0.000368, l5: 0.000775, l6: 0.001114
[epoch: 539/1000, batch:   984/ 1052, ite: 141740] train loss: 0.004241, tar: 0.000103 
[Epoch 539/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000133, l1: 0.000132, l2: 0.000201, l3: 0.000323, l4: 0.000666, l5: 0.001855, l6: 0.003149
[epoch: 540/1000, batch:    12/ 1052, ite: 141760] train loss: 0.004242, tar: 0.000103 
l0: 0.000066, l1: 0.000066, l2: 0.000124, l3: 0.000189, l4: 0.000551, l5: 0.001245, l6: 0.002842
[epoch: 540/1000, batch:    92/ 1052, ite: 141780] train loss: 0.004241, tar: 0.000103 
l0: 0.000062, l1: 0.000063, l2: 0.000102, l3: 0.000174, l4: 0.000502, l5: 0.000800, l6: 0.001473
[epoch: 540/1000, batch:   172/ 1052, ite: 141800] train loss: 0.004241, tar: 0.000103 
l0: 0.000020, l1: 0.000021, l2: 0.000050, l3: 0.000153, l4: 0.000338, l5: 0.001062, l6: 0.001483
[epoch: 540/1000, batch:   252/ 1052, ite: 141820] train loss: 0.004239, tar: 0.000103 
l0: 0.000073, l1: 0.000075, l2: 0.000115, l3: 0.000163, l4: 0.000542, l5: 0.000619, l6: 0.001520
[epoch: 540/1000, batch:   332/ 1052, ite: 141840] train loss: 0.004240, tar: 0.000103 
l0: 0.000130, l1: 0.000129, l2: 0.000169, l3: 0.000366, l4: 0.000758, l5: 0.001981, l6: 0.003107
[epoch: 540/1000, batch:   412/ 1052, ite: 141860] train loss: 0.004241, tar: 0.000103 
l0: 0.000237, l1: 0.000234, l2: 0.000244, l3: 0.000283, l4: 0.000359, l5: 0.000810, l6: 0.002039
[epoch: 540/1000, batch:   492/ 1052, ite: 141880] train loss: 0.004242, tar: 0.000103 
l0: 0.000067, l1: 0.000068, l2: 0.000117, l3: 0.000231, l4: 0.000952, l5: 0.001787, l6: 0.002669
[epoch: 540/1000, batch:   572/ 1052, ite: 141900] train loss: 0.004242, tar: 0.000103 
l0: 0.000044, l1: 0.000043, l2: 0.000088, l3: 0.000193, l4: 0.000436, l5: 0.001100, l6: 0.002002
[epoch: 540/1000, batch:   652/ 1052, ite: 141920] train loss: 0.004240, tar: 0.000103 
l0: 0.000065, l1: 0.000066, l2: 0.000096, l3: 0.000204, l4: 0.000366, l5: 0.001448, l6: 0.002207
[epoch: 540/1000, batch:   732/ 1052, ite: 141940] train loss: 0.004241, tar: 0.000103 
l0: 0.000044, l1: 0.000046, l2: 0.000100, l3: 0.000199, l4: 0.000758, l5: 0.001306, l6: 0.001521
[epoch: 540/1000, batch:   812/ 1052, ite: 141960] train loss: 0.004241, tar: 0.000103 
l0: 0.000244, l1: 0.000246, l2: 0.000266, l3: 0.000410, l4: 0.001069, l5: 0.002050, l6: 0.003667
[epoch: 540/1000, batch:   892/ 1052, ite: 141980] train loss: 0.004239, tar: 0.000103 
l0: 0.000052, l1: 0.000053, l2: 0.000092, l3: 0.000193, l4: 0.000334, l5: 0.000637, l6: 0.001907
[epoch: 540/1000, batch:   972/ 1052, ite: 142000] train loss: 0.004240, tar: 0.000103 
l0: 0.000064, l1: 0.000066, l2: 0.000111, l3: 0.000237, l4: 0.000655, l5: 0.001483, l6: 0.001962
[epoch: 540/1000, batch:  1052/ 1052, ite: 142020] train loss: 0.004238, tar: 0.000103 
[Epoch 540/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000144, l1: 0.000152, l2: 0.000185, l3: 0.000401, l4: 0.001035, l5: 0.001306, l6: 0.002399
[epoch: 541/1000, batch:    80/ 1052, ite: 142040] train loss: 0.004237, tar: 0.000103 
l0: 0.000136, l1: 0.000136, l2: 0.000171, l3: 0.000295, l4: 0.000598, l5: 0.001020, l6: 0.002017
[epoch: 541/1000, batch:   160/ 1052, ite: 142060] train loss: 0.004238, tar: 0.000103 
l0: 0.000026, l1: 0.000023, l2: 0.000051, l3: 0.000169, l4: 0.000229, l5: 0.000680, l6: 0.001459
[epoch: 541/1000, batch:   240/ 1052, ite: 142080] train loss: 0.004237, tar: 0.000103 
l0: 0.000051, l1: 0.000050, l2: 0.000092, l3: 0.000175, l4: 0.000437, l5: 0.001642, l6: 0.002509
[epoch: 541/1000, batch:   320/ 1052, ite: 142100] train loss: 0.004238, tar: 0.000103 
l0: 0.000061, l1: 0.000066, l2: 0.000091, l3: 0.000183, l4: 0.000377, l5: 0.000936, l6: 0.001390
[epoch: 541/1000, batch:   400/ 1052, ite: 142120] train loss: 0.004237, tar: 0.000102 
l0: 0.000111, l1: 0.000116, l2: 0.000144, l3: 0.000165, l4: 0.000417, l5: 0.000891, l6: 0.001247
[epoch: 541/1000, batch:   480/ 1052, ite: 142140] train loss: 0.004236, tar: 0.000102 
l0: 0.000102, l1: 0.000105, l2: 0.000116, l3: 0.000174, l4: 0.000475, l5: 0.000709, l6: 0.002496
[epoch: 541/1000, batch:   560/ 1052, ite: 142160] train loss: 0.004235, tar: 0.000102 
l0: 0.000201, l1: 0.000203, l2: 0.000210, l3: 0.000296, l4: 0.000570, l5: 0.001220, l6: 0.002909
[epoch: 541/1000, batch:   640/ 1052, ite: 142180] train loss: 0.004234, tar: 0.000102 
l0: 0.000041, l1: 0.000041, l2: 0.000064, l3: 0.000214, l4: 0.000483, l5: 0.001206, l6: 0.001957
[epoch: 541/1000, batch:   720/ 1052, ite: 142200] train loss: 0.004234, tar: 0.000102 
l0: 0.000034, l1: 0.000034, l2: 0.000066, l3: 0.000166, l4: 0.000729, l5: 0.001689, l6: 0.003236
[epoch: 541/1000, batch:   800/ 1052, ite: 142220] train loss: 0.004234, tar: 0.000102 
l0: 0.000040, l1: 0.000039, l2: 0.000087, l3: 0.000186, l4: 0.000457, l5: 0.000920, l6: 0.001413
[epoch: 541/1000, batch:   880/ 1052, ite: 142240] train loss: 0.004233, tar: 0.000102 
l0: 0.000079, l1: 0.000080, l2: 0.000104, l3: 0.000111, l4: 0.000363, l5: 0.000709, l6: 0.001363
[epoch: 541/1000, batch:   960/ 1052, ite: 142260] train loss: 0.004232, tar: 0.000102 
l0: 0.000013, l1: 0.000013, l2: 0.000045, l3: 0.000103, l4: 0.000417, l5: 0.000787, l6: 0.001371
[epoch: 541/1000, batch:  1040/ 1052, ite: 142280] train loss: 0.004233, tar: 0.000102 
[Epoch 541/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000121, l1: 0.000120, l2: 0.000162, l3: 0.000219, l4: 0.000403, l5: 0.000968, l6: 0.001869
[epoch: 542/1000, batch:    68/ 1052, ite: 142300] train loss: 0.004235, tar: 0.000102 
l0: 0.000076, l1: 0.000077, l2: 0.000081, l3: 0.000131, l4: 0.000415, l5: 0.000854, l6: 0.001464
[epoch: 542/1000, batch:   148/ 1052, ite: 142320] train loss: 0.004236, tar: 0.000102 
l0: 0.000038, l1: 0.000038, l2: 0.000052, l3: 0.000112, l4: 0.000336, l5: 0.001175, l6: 0.001554
[epoch: 542/1000, batch:   228/ 1052, ite: 142340] train loss: 0.004235, tar: 0.000102 
l0: 0.000064, l1: 0.000067, l2: 0.000074, l3: 0.000191, l4: 0.000340, l5: 0.000795, l6: 0.001692
[epoch: 542/1000, batch:   308/ 1052, ite: 142360] train loss: 0.004234, tar: 0.000102 
l0: 0.000062, l1: 0.000057, l2: 0.000162, l3: 0.000376, l4: 0.000703, l5: 0.001315, l6: 0.002316
[epoch: 542/1000, batch:   388/ 1052, ite: 142380] train loss: 0.004232, tar: 0.000102 
l0: 0.000088, l1: 0.000090, l2: 0.000149, l3: 0.000301, l4: 0.000860, l5: 0.001677, l6: 0.002349
[epoch: 542/1000, batch:   468/ 1052, ite: 142400] train loss: 0.004234, tar: 0.000101 
l0: 0.000076, l1: 0.000077, l2: 0.000104, l3: 0.000151, l4: 0.000248, l5: 0.000636, l6: 0.001579
[epoch: 542/1000, batch:   548/ 1052, ite: 142420] train loss: 0.004232, tar: 0.000101 
l0: 0.000137, l1: 0.000141, l2: 0.000216, l3: 0.000413, l4: 0.001150, l5: 0.002011, l6: 0.002770
[epoch: 542/1000, batch:   628/ 1052, ite: 142440] train loss: 0.004233, tar: 0.000101 
l0: 0.000086, l1: 0.000090, l2: 0.000101, l3: 0.000141, l4: 0.000290, l5: 0.000909, l6: 0.000902
[epoch: 542/1000, batch:   708/ 1052, ite: 142460] train loss: 0.004230, tar: 0.000101 
l0: 0.000256, l1: 0.000263, l2: 0.000295, l3: 0.000564, l4: 0.000964, l5: 0.001831, l6: 0.003759
[epoch: 542/1000, batch:   788/ 1052, ite: 142480] train loss: 0.004230, tar: 0.000101 
l0: 0.000249, l1: 0.000252, l2: 0.000270, l3: 0.000340, l4: 0.000702, l5: 0.001656, l6: 0.002382
[epoch: 542/1000, batch:   868/ 1052, ite: 142500] train loss: 0.004229, tar: 0.000101 
l0: 0.000029, l1: 0.000028, l2: 0.000060, l3: 0.000155, l4: 0.000355, l5: 0.000919, l6: 0.001310
[epoch: 542/1000, batch:   948/ 1052, ite: 142520] train loss: 0.004230, tar: 0.000101 
l0: 0.000152, l1: 0.000152, l2: 0.000184, l3: 0.000213, l4: 0.000550, l5: 0.000977, l6: 0.001946
[epoch: 542/1000, batch:  1028/ 1052, ite: 142540] train loss: 0.004228, tar: 0.000101 
[Epoch 542/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000033, l1: 0.000034, l2: 0.000055, l3: 0.000169, l4: 0.000441, l5: 0.001038, l6: 0.001805
[epoch: 543/1000, batch:    56/ 1052, ite: 142560] train loss: 0.004229, tar: 0.000101 
l0: 0.000054, l1: 0.000054, l2: 0.000094, l3: 0.000193, l4: 0.000334, l5: 0.000594, l6: 0.000814
[epoch: 543/1000, batch:   136/ 1052, ite: 142580] train loss: 0.004227, tar: 0.000101 
l0: 0.000218, l1: 0.000221, l2: 0.000261, l3: 0.000399, l4: 0.000761, l5: 0.001980, l6: 0.003266
[epoch: 543/1000, batch:   216/ 1052, ite: 142600] train loss: 0.004226, tar: 0.000101 
l0: 0.000115, l1: 0.000122, l2: 0.000117, l3: 0.000292, l4: 0.000379, l5: 0.001038, l6: 0.003589
[epoch: 543/1000, batch:   296/ 1052, ite: 142620] train loss: 0.004226, tar: 0.000101 
l0: 0.000150, l1: 0.000148, l2: 0.000223, l3: 0.000403, l4: 0.001126, l5: 0.002378, l6: 0.003915
[epoch: 543/1000, batch:   376/ 1052, ite: 142640] train loss: 0.004226, tar: 0.000101 
l0: 0.000069, l1: 0.000067, l2: 0.000147, l3: 0.000234, l4: 0.000665, l5: 0.001204, l6: 0.002394
[epoch: 543/1000, batch:   456/ 1052, ite: 142660] train loss: 0.004224, tar: 0.000101 
l0: 0.000057, l1: 0.000057, l2: 0.000077, l3: 0.000133, l4: 0.000330, l5: 0.000916, l6: 0.001396
[epoch: 543/1000, batch:   536/ 1052, ite: 142680] train loss: 0.004225, tar: 0.000101 
l0: 0.000052, l1: 0.000049, l2: 0.000092, l3: 0.000155, l4: 0.000321, l5: 0.001262, l6: 0.002175
[epoch: 543/1000, batch:   616/ 1052, ite: 142700] train loss: 0.004224, tar: 0.000101 
l0: 0.000053, l1: 0.000051, l2: 0.000117, l3: 0.000172, l4: 0.000406, l5: 0.001422, l6: 0.003044
[epoch: 543/1000, batch:   696/ 1052, ite: 142720] train loss: 0.004223, tar: 0.000101 
l0: 0.000143, l1: 0.000142, l2: 0.000214, l3: 0.000320, l4: 0.000687, l5: 0.001187, l6: 0.003280
[epoch: 543/1000, batch:   776/ 1052, ite: 142740] train loss: 0.004224, tar: 0.000101 
l0: 0.000055, l1: 0.000055, l2: 0.000123, l3: 0.000230, l4: 0.000501, l5: 0.000773, l6: 0.002653
[epoch: 543/1000, batch:   856/ 1052, ite: 142760] train loss: 0.004225, tar: 0.000100 
l0: 0.000189, l1: 0.000197, l2: 0.000258, l3: 0.000379, l4: 0.000703, l5: 0.001447, l6: 0.003206
[epoch: 543/1000, batch:   936/ 1052, ite: 142780] train loss: 0.004225, tar: 0.000100 
l0: 0.000097, l1: 0.000095, l2: 0.000128, l3: 0.000222, l4: 0.000496, l5: 0.001082, l6: 0.002218
[epoch: 543/1000, batch:  1016/ 1052, ite: 142800] train loss: 0.004225, tar: 0.000100 
[Epoch 543/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000104, l1: 0.000105, l2: 0.000135, l3: 0.000151, l4: 0.000417, l5: 0.001286, l6: 0.002076
[epoch: 544/1000, batch:    44/ 1052, ite: 142820] train loss: 0.004223, tar: 0.000100 
l0: 0.000051, l1: 0.000050, l2: 0.000089, l3: 0.000152, l4: 0.000297, l5: 0.000824, l6: 0.001142
[epoch: 544/1000, batch:   124/ 1052, ite: 142840] train loss: 0.004222, tar: 0.000100 
l0: 0.000074, l1: 0.000073, l2: 0.000084, l3: 0.000171, l4: 0.000380, l5: 0.001101, l6: 0.001360
[epoch: 544/1000, batch:   204/ 1052, ite: 142860] train loss: 0.004223, tar: 0.000100 
l0: 0.000078, l1: 0.000081, l2: 0.000111, l3: 0.000253, l4: 0.000669, l5: 0.001000, l6: 0.002370
[epoch: 544/1000, batch:   284/ 1052, ite: 142880] train loss: 0.004224, tar: 0.000100 
l0: 0.000082, l1: 0.000084, l2: 0.000119, l3: 0.000185, l4: 0.000336, l5: 0.000934, l6: 0.001988
[epoch: 544/1000, batch:   364/ 1052, ite: 142900] train loss: 0.004224, tar: 0.000100 
l0: 0.000163, l1: 0.000165, l2: 0.000239, l3: 0.000501, l4: 0.001116, l5: 0.002482, l6: 0.004719
[epoch: 544/1000, batch:   444/ 1052, ite: 142920] train loss: 0.004225, tar: 0.000100 
l0: 0.000041, l1: 0.000041, l2: 0.000073, l3: 0.000174, l4: 0.000369, l5: 0.000803, l6: 0.001497
[epoch: 544/1000, batch:   524/ 1052, ite: 142940] train loss: 0.004225, tar: 0.000100 
l0: 0.000039, l1: 0.000039, l2: 0.000057, l3: 0.000093, l4: 0.000408, l5: 0.000856, l6: 0.001199
[epoch: 544/1000, batch:   604/ 1052, ite: 142960] train loss: 0.004224, tar: 0.000100 
l0: 0.000094, l1: 0.000099, l2: 0.000133, l3: 0.000299, l4: 0.000691, l5: 0.001103, l6: 0.003161
[epoch: 544/1000, batch:   684/ 1052, ite: 142980] train loss: 0.004223, tar: 0.000100 
l0: 0.000053, l1: 0.000053, l2: 0.000073, l3: 0.000155, l4: 0.000444, l5: 0.000987, l6: 0.001743
[epoch: 544/1000, batch:   764/ 1052, ite: 143000] train loss: 0.004221, tar: 0.000100 
l0: 0.000069, l1: 0.000070, l2: 0.000083, l3: 0.000198, l4: 0.000438, l5: 0.000951, l6: 0.001292
[epoch: 544/1000, batch:   844/ 1052, ite: 143020] train loss: 0.004221, tar: 0.000100 
l0: 0.000260, l1: 0.000263, l2: 0.000323, l3: 0.000431, l4: 0.000723, l5: 0.001768, l6: 0.003054
[epoch: 544/1000, batch:   924/ 1052, ite: 143040] train loss: 0.004221, tar: 0.000100 
l0: 0.000121, l1: 0.000122, l2: 0.000162, l3: 0.000344, l4: 0.000602, l5: 0.000879, l6: 0.002525
[epoch: 544/1000, batch:  1004/ 1052, ite: 143060] train loss: 0.004220, tar: 0.000100 
[Epoch 544/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000192, l1: 0.000193, l2: 0.000221, l3: 0.000303, l4: 0.000492, l5: 0.000901, l6: 0.003076
[epoch: 545/1000, batch:    32/ 1052, ite: 143080] train loss: 0.004220, tar: 0.000100 
l0: 0.000184, l1: 0.000188, l2: 0.000204, l3: 0.000321, l4: 0.000561, l5: 0.001345, l6: 0.002258
[epoch: 545/1000, batch:   112/ 1052, ite: 143100] train loss: 0.004219, tar: 0.000100 
l0: 0.000078, l1: 0.000078, l2: 0.000138, l3: 0.000338, l4: 0.000792, l5: 0.002807, l6: 0.003472
[epoch: 545/1000, batch:   192/ 1052, ite: 143120] train loss: 0.004218, tar: 0.000100 
l0: 0.000157, l1: 0.000151, l2: 0.000200, l3: 0.000358, l4: 0.000594, l5: 0.001140, l6: 0.001820
[epoch: 545/1000, batch:   272/ 1052, ite: 143140] train loss: 0.004218, tar: 0.000100 
l0: 0.000025, l1: 0.000026, l2: 0.000049, l3: 0.000069, l4: 0.000301, l5: 0.000507, l6: 0.001424
[epoch: 545/1000, batch:   352/ 1052, ite: 143160] train loss: 0.004219, tar: 0.000099 
l0: 0.000121, l1: 0.000120, l2: 0.000155, l3: 0.000230, l4: 0.000626, l5: 0.001335, l6: 0.002693
[epoch: 545/1000, batch:   432/ 1052, ite: 143180] train loss: 0.004219, tar: 0.000099 
l0: 0.000090, l1: 0.000088, l2: 0.000135, l3: 0.000304, l4: 0.000404, l5: 0.000975, l6: 0.001920
[epoch: 545/1000, batch:   512/ 1052, ite: 143200] train loss: 0.004219, tar: 0.000099 
l0: 0.000052, l1: 0.000050, l2: 0.000111, l3: 0.000194, l4: 0.000502, l5: 0.000970, l6: 0.001905
[epoch: 545/1000, batch:   592/ 1052, ite: 143220] train loss: 0.004217, tar: 0.000099 
l0: 0.000169, l1: 0.000170, l2: 0.000182, l3: 0.000258, l4: 0.000504, l5: 0.000835, l6: 0.001473
[epoch: 545/1000, batch:   672/ 1052, ite: 143240] train loss: 0.004215, tar: 0.000099 
l0: 0.000221, l1: 0.000220, l2: 0.000288, l3: 0.000431, l4: 0.000866, l5: 0.001809, l6: 0.004286
[epoch: 545/1000, batch:   752/ 1052, ite: 143260] train loss: 0.004215, tar: 0.000099 
l0: 0.000027, l1: 0.000029, l2: 0.000050, l3: 0.000070, l4: 0.000287, l5: 0.000452, l6: 0.000916
[epoch: 545/1000, batch:   832/ 1052, ite: 143280] train loss: 0.004215, tar: 0.000099 
l0: 0.000087, l1: 0.000088, l2: 0.000094, l3: 0.000173, l4: 0.000284, l5: 0.000881, l6: 0.001259
[epoch: 545/1000, batch:   912/ 1052, ite: 143300] train loss: 0.004214, tar: 0.000099 
l0: 0.000188, l1: 0.000190, l2: 0.000214, l3: 0.000281, l4: 0.000520, l5: 0.001220, l6: 0.003088
[epoch: 545/1000, batch:   992/ 1052, ite: 143320] train loss: 0.004216, tar: 0.000099 
[Epoch 545/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000079, l1: 0.000076, l2: 0.000131, l3: 0.000157, l4: 0.000404, l5: 0.000916, l6: 0.001490
[epoch: 546/1000, batch:    20/ 1052, ite: 143340] train loss: 0.004216, tar: 0.000099 
l0: 0.000031, l1: 0.000032, l2: 0.000038, l3: 0.000111, l4: 0.000407, l5: 0.000988, l6: 0.001264
[epoch: 546/1000, batch:   100/ 1052, ite: 143360] train loss: 0.004214, tar: 0.000099 
l0: 0.000063, l1: 0.000063, l2: 0.000094, l3: 0.000167, l4: 0.000400, l5: 0.001098, l6: 0.001928
[epoch: 546/1000, batch:   180/ 1052, ite: 143380] train loss: 0.004213, tar: 0.000099 
l0: 0.000010, l1: 0.000010, l2: 0.000023, l3: 0.000077, l4: 0.000308, l5: 0.000469, l6: 0.001486
[epoch: 546/1000, batch:   260/ 1052, ite: 143400] train loss: 0.004212, tar: 0.000099 
l0: 0.000187, l1: 0.000193, l2: 0.000226, l3: 0.000298, l4: 0.000576, l5: 0.001646, l6: 0.002919
[epoch: 546/1000, batch:   340/ 1052, ite: 143420] train loss: 0.004213, tar: 0.000099 
l0: 0.000081, l1: 0.000082, l2: 0.000103, l3: 0.000192, l4: 0.000374, l5: 0.001030, l6: 0.001892
[epoch: 546/1000, batch:   420/ 1052, ite: 143440] train loss: 0.004212, tar: 0.000099 
l0: 0.000050, l1: 0.000048, l2: 0.000102, l3: 0.000184, l4: 0.000746, l5: 0.001116, l6: 0.002741
[epoch: 546/1000, batch:   500/ 1052, ite: 143460] train loss: 0.004212, tar: 0.000099 
l0: 0.000158, l1: 0.000159, l2: 0.000221, l3: 0.000367, l4: 0.001030, l5: 0.001541, l6: 0.002610
[epoch: 546/1000, batch:   580/ 1052, ite: 143480] train loss: 0.004211, tar: 0.000099 
l0: 0.000077, l1: 0.000078, l2: 0.000117, l3: 0.000245, l4: 0.000467, l5: 0.001365, l6: 0.002035
[epoch: 546/1000, batch:   660/ 1052, ite: 143500] train loss: 0.004212, tar: 0.000099 
l0: 0.000023, l1: 0.000024, l2: 0.000049, l3: 0.000114, l4: 0.000277, l5: 0.000852, l6: 0.001298
[epoch: 546/1000, batch:   740/ 1052, ite: 143520] train loss: 0.004212, tar: 0.000099 
l0: 0.000043, l1: 0.000043, l2: 0.000103, l3: 0.000151, l4: 0.000505, l5: 0.000853, l6: 0.001543
[epoch: 546/1000, batch:   820/ 1052, ite: 143540] train loss: 0.004211, tar: 0.000098 
l0: 0.000033, l1: 0.000036, l2: 0.000072, l3: 0.000194, l4: 0.000463, l5: 0.001103, l6: 0.001761
[epoch: 546/1000, batch:   900/ 1052, ite: 143560] train loss: 0.004211, tar: 0.000098 
l0: 0.000135, l1: 0.000133, l2: 0.000238, l3: 0.000343, l4: 0.000626, l5: 0.001436, l6: 0.003624
[epoch: 546/1000, batch:   980/ 1052, ite: 143580] train loss: 0.004212, tar: 0.000098 
[Epoch 546/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000241, l1: 0.000244, l2: 0.000306, l3: 0.000484, l4: 0.000680, l5: 0.000713, l6: 0.002061
[epoch: 547/1000, batch:     8/ 1052, ite: 143600] train loss: 0.004213, tar: 0.000098 
l0: 0.000116, l1: 0.000116, l2: 0.000194, l3: 0.000268, l4: 0.000467, l5: 0.001157, l6: 0.002379
[epoch: 547/1000, batch:    88/ 1052, ite: 143620] train loss: 0.004214, tar: 0.000098 
l0: 0.000169, l1: 0.000169, l2: 0.000194, l3: 0.000247, l4: 0.000596, l5: 0.001441, l6: 0.002673
[epoch: 547/1000, batch:   168/ 1052, ite: 143640] train loss: 0.004214, tar: 0.000098 
l0: 0.000031, l1: 0.000031, l2: 0.000037, l3: 0.000072, l4: 0.000359, l5: 0.000559, l6: 0.001237
[epoch: 547/1000, batch:   248/ 1052, ite: 143660] train loss: 0.004213, tar: 0.000098 
l0: 0.000060, l1: 0.000056, l2: 0.000122, l3: 0.000210, l4: 0.000370, l5: 0.001218, l6: 0.003358
[epoch: 547/1000, batch:   328/ 1052, ite: 143680] train loss: 0.004213, tar: 0.000098 
l0: 0.000031, l1: 0.000030, l2: 0.000065, l3: 0.000170, l4: 0.000487, l5: 0.000897, l6: 0.002366
[epoch: 547/1000, batch:   408/ 1052, ite: 143700] train loss: 0.004213, tar: 0.000098 
l0: 0.000064, l1: 0.000064, l2: 0.000103, l3: 0.000198, l4: 0.000353, l5: 0.000806, l6: 0.001510
[epoch: 547/1000, batch:   488/ 1052, ite: 143720] train loss: 0.004213, tar: 0.000098 
l0: 0.000067, l1: 0.000067, l2: 0.000101, l3: 0.000179, l4: 0.000284, l5: 0.001137, l6: 0.001245
[epoch: 547/1000, batch:   568/ 1052, ite: 143740] train loss: 0.004212, tar: 0.000098 
l0: 0.000090, l1: 0.000090, l2: 0.000126, l3: 0.000191, l4: 0.000412, l5: 0.001225, l6: 0.002898
[epoch: 547/1000, batch:   648/ 1052, ite: 143760] train loss: 0.004212, tar: 0.000098 
l0: 0.000046, l1: 0.000047, l2: 0.000051, l3: 0.000069, l4: 0.000314, l5: 0.000746, l6: 0.000506
[epoch: 547/1000, batch:   728/ 1052, ite: 143780] train loss: 0.004211, tar: 0.000098 
l0: 0.000174, l1: 0.000177, l2: 0.000202, l3: 0.000306, l4: 0.000710, l5: 0.000989, l6: 0.002611
[epoch: 547/1000, batch:   808/ 1052, ite: 143800] train loss: 0.004212, tar: 0.000098 
l0: 0.000017, l1: 0.000017, l2: 0.000032, l3: 0.000070, l4: 0.000225, l5: 0.000540, l6: 0.000929
[epoch: 547/1000, batch:   888/ 1052, ite: 143820] train loss: 0.004211, tar: 0.000098 
l0: 0.000095, l1: 0.000096, l2: 0.000127, l3: 0.000226, l4: 0.000518, l5: 0.001127, l6: 0.001658
[epoch: 547/1000, batch:   968/ 1052, ite: 143840] train loss: 0.004210, tar: 0.000098 
l0: 0.000117, l1: 0.000116, l2: 0.000153, l3: 0.000280, l4: 0.000705, l5: 0.001686, l6: 0.002858
[epoch: 547/1000, batch:  1048/ 1052, ite: 143860] train loss: 0.004210, tar: 0.000098 
[Epoch 547/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000086, l1: 0.000089, l2: 0.000114, l3: 0.000266, l4: 0.000481, l5: 0.001080, l6: 0.001296
[epoch: 548/1000, batch:    76/ 1052, ite: 143880] train loss: 0.004210, tar: 0.000098 
l0: 0.000128, l1: 0.000128, l2: 0.000208, l3: 0.000268, l4: 0.000455, l5: 0.001104, l6: 0.001861
[epoch: 548/1000, batch:   156/ 1052, ite: 143900] train loss: 0.004210, tar: 0.000098 
l0: 0.000116, l1: 0.000113, l2: 0.000146, l3: 0.000201, l4: 0.000369, l5: 0.001166, l6: 0.001987
[epoch: 548/1000, batch:   236/ 1052, ite: 143920] train loss: 0.004210, tar: 0.000098 
l0: 0.000087, l1: 0.000088, l2: 0.000099, l3: 0.000181, l4: 0.000392, l5: 0.000900, l6: 0.001802
[epoch: 548/1000, batch:   316/ 1052, ite: 143940] train loss: 0.004209, tar: 0.000098 
l0: 0.000032, l1: 0.000033, l2: 0.000062, l3: 0.000094, l4: 0.000345, l5: 0.000760, l6: 0.001636
[epoch: 548/1000, batch:   396/ 1052, ite: 143960] train loss: 0.004208, tar: 0.000098 
l0: 0.000056, l1: 0.000056, l2: 0.000074, l3: 0.000130, l4: 0.000371, l5: 0.000777, l6: 0.001262
[epoch: 548/1000, batch:   476/ 1052, ite: 143980] train loss: 0.004209, tar: 0.000098 
l0: 0.000064, l1: 0.000062, l2: 0.000095, l3: 0.000103, l4: 0.000445, l5: 0.001110, l6: 0.001654
[epoch: 548/1000, batch:   556/ 1052, ite: 144000] train loss: 0.004208, tar: 0.000098 
l0: 0.000109, l1: 0.000108, l2: 0.000171, l3: 0.000320, l4: 0.000696, l5: 0.001329, l6: 0.001980
[epoch: 548/1000, batch:   636/ 1052, ite: 144020] train loss: 0.004207, tar: 0.000098 
l0: 0.000173, l1: 0.000181, l2: 0.000255, l3: 0.000427, l4: 0.001005, l5: 0.002613, l6: 0.003388
[epoch: 548/1000, batch:   716/ 1052, ite: 144040] train loss: 0.004208, tar: 0.000098 
l0: 0.000089, l1: 0.000088, l2: 0.000134, l3: 0.000280, l4: 0.000527, l5: 0.000769, l6: 0.002162
[epoch: 548/1000, batch:   796/ 1052, ite: 144060] train loss: 0.004206, tar: 0.000098 
l0: 0.000099, l1: 0.000100, l2: 0.000155, l3: 0.000284, l4: 0.000506, l5: 0.000813, l6: 0.003151
[epoch: 548/1000, batch:   876/ 1052, ite: 144080] train loss: 0.004208, tar: 0.000098 
l0: 0.000091, l1: 0.000091, l2: 0.000149, l3: 0.000281, l4: 0.000442, l5: 0.000667, l6: 0.001539
[epoch: 548/1000, batch:   956/ 1052, ite: 144100] train loss: 0.004207, tar: 0.000098 
l0: 0.000074, l1: 0.000074, l2: 0.000105, l3: 0.000194, l4: 0.000651, l5: 0.001205, l6: 0.002089
[epoch: 548/1000, batch:  1036/ 1052, ite: 144120] train loss: 0.004207, tar: 0.000098 
[Epoch 548/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000119, l1: 0.000120, l2: 0.000174, l3: 0.000340, l4: 0.000793, l5: 0.001790, l6: 0.002422
[epoch: 549/1000, batch:    64/ 1052, ite: 144140] train loss: 0.004207, tar: 0.000097 
l0: 0.000057, l1: 0.000057, l2: 0.000106, l3: 0.000275, l4: 0.001008, l5: 0.001386, l6: 0.002410
[epoch: 549/1000, batch:   144/ 1052, ite: 144160] train loss: 0.004208, tar: 0.000097 
l0: 0.000124, l1: 0.000123, l2: 0.000164, l3: 0.000282, l4: 0.000487, l5: 0.001214, l6: 0.002489
[epoch: 549/1000, batch:   224/ 1052, ite: 144180] train loss: 0.004208, tar: 0.000097 
l0: 0.000171, l1: 0.000170, l2: 0.000244, l3: 0.000460, l4: 0.001065, l5: 0.002775, l6: 0.005522
[epoch: 549/1000, batch:   304/ 1052, ite: 144200] train loss: 0.004208, tar: 0.000097 
l0: 0.000031, l1: 0.000031, l2: 0.000062, l3: 0.000145, l4: 0.000302, l5: 0.000642, l6: 0.000903
[epoch: 549/1000, batch:   384/ 1052, ite: 144220] train loss: 0.004207, tar: 0.000097 
l0: 0.000056, l1: 0.000058, l2: 0.000100, l3: 0.000122, l4: 0.000339, l5: 0.000932, l6: 0.001579
[epoch: 549/1000, batch:   464/ 1052, ite: 144240] train loss: 0.004209, tar: 0.000097 
l0: 0.000103, l1: 0.000100, l2: 0.000174, l3: 0.000328, l4: 0.001008, l5: 0.001999, l6: 0.004466
[epoch: 549/1000, batch:   544/ 1052, ite: 144260] train loss: 0.004209, tar: 0.000097 
l0: 0.000134, l1: 0.000138, l2: 0.000198, l3: 0.000291, l4: 0.000650, l5: 0.001603, l6: 0.002136
[epoch: 549/1000, batch:   624/ 1052, ite: 144280] train loss: 0.004210, tar: 0.000098 
l0: 0.000128, l1: 0.000127, l2: 0.000209, l3: 0.000345, l4: 0.000508, l5: 0.001216, l6: 0.001954
[epoch: 549/1000, batch:   704/ 1052, ite: 144300] train loss: 0.004210, tar: 0.000097 
l0: 0.000025, l1: 0.000024, l2: 0.000057, l3: 0.000207, l4: 0.000557, l5: 0.000894, l6: 0.001833
[epoch: 549/1000, batch:   784/ 1052, ite: 144320] train loss: 0.004208, tar: 0.000097 
l0: 0.000104, l1: 0.000107, l2: 0.000134, l3: 0.000212, l4: 0.000470, l5: 0.001039, l6: 0.001412
[epoch: 549/1000, batch:   864/ 1052, ite: 144340] train loss: 0.004208, tar: 0.000097 
l0: 0.000007, l1: 0.000007, l2: 0.000020, l3: 0.000074, l4: 0.000118, l5: 0.000708, l6: 0.001175
[epoch: 549/1000, batch:   944/ 1052, ite: 144360] train loss: 0.004206, tar: 0.000097 
l0: 0.000043, l1: 0.000043, l2: 0.000071, l3: 0.000089, l4: 0.000298, l5: 0.001218, l6: 0.000801
[epoch: 549/1000, batch:  1024/ 1052, ite: 144380] train loss: 0.004206, tar: 0.000097 
[Epoch 549/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000078, l1: 0.000079, l2: 0.000109, l3: 0.000264, l4: 0.000432, l5: 0.001005, l6: 0.001601
[epoch: 550/1000, batch:    52/ 1052, ite: 144400] train loss: 0.004205, tar: 0.000097 
l0: 0.000050, l1: 0.000056, l2: 0.000115, l3: 0.000356, l4: 0.000759, l5: 0.001113, l6: 0.002356
[epoch: 550/1000, batch:   132/ 1052, ite: 144420] train loss: 0.004205, tar: 0.000097 
l0: 0.000076, l1: 0.000079, l2: 0.000116, l3: 0.000136, l4: 0.000296, l5: 0.000857, l6: 0.001313
[epoch: 550/1000, batch:   212/ 1052, ite: 144440] train loss: 0.004206, tar: 0.000097 
l0: 0.000059, l1: 0.000059, l2: 0.000100, l3: 0.000177, l4: 0.000420, l5: 0.000699, l6: 0.001595
[epoch: 550/1000, batch:   292/ 1052, ite: 144460] train loss: 0.004206, tar: 0.000097 
l0: 0.000023, l1: 0.000022, l2: 0.000075, l3: 0.000149, l4: 0.000357, l5: 0.000805, l6: 0.001700
[epoch: 550/1000, batch:   372/ 1052, ite: 144480] train loss: 0.004207, tar: 0.000097 
l0: 0.000101, l1: 0.000103, l2: 0.000145, l3: 0.000366, l4: 0.001048, l5: 0.001393, l6: 0.003099
[epoch: 550/1000, batch:   452/ 1052, ite: 144500] train loss: 0.004205, tar: 0.000097 
l0: 0.000087, l1: 0.000089, l2: 0.000105, l3: 0.000200, l4: 0.000479, l5: 0.001058, l6: 0.002299
[epoch: 550/1000, batch:   532/ 1052, ite: 144520] train loss: 0.004205, tar: 0.000097 
l0: 0.000001, l1: 0.000001, l2: 0.000008, l3: 0.000030, l4: 0.000092, l5: 0.000396, l6: 0.000603
[epoch: 550/1000, batch:   612/ 1052, ite: 144540] train loss: 0.004205, tar: 0.000097 
l0: 0.000048, l1: 0.000045, l2: 0.000120, l3: 0.000308, l4: 0.000991, l5: 0.001331, l6: 0.002031
[epoch: 550/1000, batch:   692/ 1052, ite: 144560] train loss: 0.004204, tar: 0.000097 
l0: 0.000259, l1: 0.000265, l2: 0.000290, l3: 0.000533, l4: 0.000769, l5: 0.001484, l6: 0.003652
[epoch: 550/1000, batch:   772/ 1052, ite: 144580] train loss: 0.004204, tar: 0.000097 
l0: 0.000092, l1: 0.000092, l2: 0.000131, l3: 0.000163, l4: 0.000343, l5: 0.000781, l6: 0.000939
[epoch: 550/1000, batch:   852/ 1052, ite: 144600] train loss: 0.004202, tar: 0.000097 
l0: 0.000042, l1: 0.000040, l2: 0.000090, l3: 0.000161, l4: 0.000519, l5: 0.000781, l6: 0.001581
[epoch: 550/1000, batch:   932/ 1052, ite: 144620] train loss: 0.004202, tar: 0.000097 
l0: 0.000106, l1: 0.000105, l2: 0.000175, l3: 0.000276, l4: 0.000625, l5: 0.001488, l6: 0.002987
[epoch: 550/1000, batch:  1012/ 1052, ite: 144640] train loss: 0.004202, tar: 0.000097 
[Epoch 550/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000124, l1: 0.000129, l2: 0.000152, l3: 0.000188, l4: 0.000405, l5: 0.001144, l6: 0.002173
[epoch: 551/1000, batch:    40/ 1052, ite: 144660] train loss: 0.004203, tar: 0.000097 
l0: 0.000015, l1: 0.000015, l2: 0.000046, l3: 0.000113, l4: 0.000333, l5: 0.000604, l6: 0.001392
[epoch: 551/1000, batch:   120/ 1052, ite: 144680] train loss: 0.004203, tar: 0.000097 
l0: 0.000128, l1: 0.000135, l2: 0.000146, l3: 0.000150, l4: 0.000225, l5: 0.000790, l6: 0.001785
[epoch: 551/1000, batch:   200/ 1052, ite: 144700] train loss: 0.004201, tar: 0.000097 
l0: 0.000030, l1: 0.000030, l2: 0.000060, l3: 0.000109, l4: 0.000321, l5: 0.000507, l6: 0.001347
[epoch: 551/1000, batch:   280/ 1052, ite: 144720] train loss: 0.004202, tar: 0.000097 
l0: 0.000054, l1: 0.000053, l2: 0.000073, l3: 0.000143, l4: 0.000421, l5: 0.000790, l6: 0.001657
[epoch: 551/1000, batch:   360/ 1052, ite: 144740] train loss: 0.004201, tar: 0.000097 
l0: 0.000059, l1: 0.000058, l2: 0.000091, l3: 0.000167, l4: 0.000516, l5: 0.000740, l6: 0.001893
[epoch: 551/1000, batch:   440/ 1052, ite: 144760] train loss: 0.004201, tar: 0.000097 
l0: 0.000130, l1: 0.000131, l2: 0.000288, l3: 0.000456, l4: 0.000725, l5: 0.001726, l6: 0.004174
[epoch: 551/1000, batch:   520/ 1052, ite: 144780] train loss: 0.004200, tar: 0.000097 
l0: 0.000068, l1: 0.000066, l2: 0.000107, l3: 0.000223, l4: 0.000555, l5: 0.000910, l6: 0.001639
[epoch: 551/1000, batch:   600/ 1052, ite: 144800] train loss: 0.004201, tar: 0.000097 
l0: 0.000039, l1: 0.000040, l2: 0.000062, l3: 0.000134, l4: 0.000333, l5: 0.000750, l6: 0.001784
[epoch: 551/1000, batch:   680/ 1052, ite: 144820] train loss: 0.004200, tar: 0.000096 
l0: 0.000100, l1: 0.000097, l2: 0.000161, l3: 0.000267, l4: 0.000377, l5: 0.001091, l6: 0.002684
[epoch: 551/1000, batch:   760/ 1052, ite: 144840] train loss: 0.004201, tar: 0.000096 
l0: 0.000020, l1: 0.000021, l2: 0.000030, l3: 0.000061, l4: 0.000175, l5: 0.000466, l6: 0.001011
[epoch: 551/1000, batch:   840/ 1052, ite: 144860] train loss: 0.004200, tar: 0.000096 
l0: 0.000078, l1: 0.000079, l2: 0.000160, l3: 0.000193, l4: 0.000549, l5: 0.001089, l6: 0.002005
[epoch: 551/1000, batch:   920/ 1052, ite: 144880] train loss: 0.004201, tar: 0.000096 
l0: 0.000133, l1: 0.000132, l2: 0.000164, l3: 0.000304, l4: 0.000596, l5: 0.001354, l6: 0.001319
[epoch: 551/1000, batch:  1000/ 1052, ite: 144900] train loss: 0.004202, tar: 0.000096 
[Epoch 551/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000093, l1: 0.000091, l2: 0.000111, l3: 0.000174, l4: 0.000533, l5: 0.001167, l6: 0.002373
[epoch: 552/1000, batch:    28/ 1052, ite: 144920] train loss: 0.004201, tar: 0.000096 
l0: 0.000016, l1: 0.000016, l2: 0.000046, l3: 0.000143, l4: 0.000448, l5: 0.000871, l6: 0.001474
[epoch: 552/1000, batch:   108/ 1052, ite: 144940] train loss: 0.004201, tar: 0.000096 
l0: 0.000051, l1: 0.000056, l2: 0.000095, l3: 0.000143, l4: 0.001035, l5: 0.001471, l6: 0.002149
[epoch: 552/1000, batch:   188/ 1052, ite: 144960] train loss: 0.004200, tar: 0.000096 
l0: 0.000109, l1: 0.000110, l2: 0.000135, l3: 0.000244, l4: 0.000588, l5: 0.001016, l6: 0.002425
[epoch: 552/1000, batch:   268/ 1052, ite: 144980] train loss: 0.004200, tar: 0.000096 
l0: 0.000134, l1: 0.000133, l2: 0.000156, l3: 0.000250, l4: 0.000425, l5: 0.001269, l6: 0.002502
[epoch: 552/1000, batch:   348/ 1052, ite: 145000] train loss: 0.004201, tar: 0.000096 
l0: 0.000105, l1: 0.000103, l2: 0.000224, l3: 0.000540, l4: 0.001003, l5: 0.001673, l6: 0.002009
[epoch: 552/1000, batch:   428/ 1052, ite: 145020] train loss: 0.004200, tar: 0.000096 
l0: 0.000026, l1: 0.000026, l2: 0.000051, l3: 0.000192, l4: 0.000519, l5: 0.000932, l6: 0.001392
[epoch: 552/1000, batch:   508/ 1052, ite: 145040] train loss: 0.004199, tar: 0.000096 
l0: 0.000069, l1: 0.000073, l2: 0.000081, l3: 0.000195, l4: 0.000507, l5: 0.000982, l6: 0.001819
[epoch: 552/1000, batch:   588/ 1052, ite: 145060] train loss: 0.004199, tar: 0.000096 
l0: 0.000128, l1: 0.000129, l2: 0.000247, l3: 0.000357, l4: 0.000718, l5: 0.001531, l6: 0.004429
[epoch: 552/1000, batch:   668/ 1052, ite: 145080] train loss: 0.004199, tar: 0.000096 
l0: 0.000036, l1: 0.000037, l2: 0.000067, l3: 0.000128, l4: 0.000263, l5: 0.000872, l6: 0.001067
[epoch: 552/1000, batch:   748/ 1052, ite: 145100] train loss: 0.004199, tar: 0.000096 
l0: 0.000054, l1: 0.000053, l2: 0.000057, l3: 0.000118, l4: 0.000354, l5: 0.000704, l6: 0.001402
[epoch: 552/1000, batch:   828/ 1052, ite: 145120] train loss: 0.004199, tar: 0.000096 
l0: 0.000039, l1: 0.000040, l2: 0.000087, l3: 0.000133, l4: 0.000390, l5: 0.000502, l6: 0.001137
[epoch: 552/1000, batch:   908/ 1052, ite: 145140] train loss: 0.004199, tar: 0.000096 
l0: 0.000052, l1: 0.000053, l2: 0.000073, l3: 0.000144, l4: 0.000311, l5: 0.000641, l6: 0.000821
[epoch: 552/1000, batch:   988/ 1052, ite: 145160] train loss: 0.004198, tar: 0.000096 
[Epoch 552/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000129, l1: 0.000128, l2: 0.000202, l3: 0.000361, l4: 0.000595, l5: 0.001198, l6: 0.002071
[epoch: 553/1000, batch:    16/ 1052, ite: 145180] train loss: 0.004198, tar: 0.000096 
l0: 0.000087, l1: 0.000089, l2: 0.000136, l3: 0.000389, l4: 0.000760, l5: 0.001779, l6: 0.003195
[epoch: 553/1000, batch:    96/ 1052, ite: 145200] train loss: 0.004197, tar: 0.000096 
l0: 0.000104, l1: 0.000103, l2: 0.000147, l3: 0.000257, l4: 0.000522, l5: 0.001047, l6: 0.001636
[epoch: 553/1000, batch:   176/ 1052, ite: 145220] train loss: 0.004197, tar: 0.000096 
l0: 0.000123, l1: 0.000124, l2: 0.000205, l3: 0.000348, l4: 0.000760, l5: 0.001611, l6: 0.001935
[epoch: 553/1000, batch:   256/ 1052, ite: 145240] train loss: 0.004196, tar: 0.000096 
l0: 0.000128, l1: 0.000127, l2: 0.000171, l3: 0.000261, l4: 0.000445, l5: 0.000951, l6: 0.001348
[epoch: 553/1000, batch:   336/ 1052, ite: 145260] train loss: 0.004197, tar: 0.000096 
l0: 0.000033, l1: 0.000034, l2: 0.000070, l3: 0.000144, l4: 0.000307, l5: 0.001003, l6: 0.000986
[epoch: 553/1000, batch:   416/ 1052, ite: 145280] train loss: 0.004197, tar: 0.000096 
l0: 0.000116, l1: 0.000121, l2: 0.000170, l3: 0.000268, l4: 0.000527, l5: 0.001621, l6: 0.002942
[epoch: 553/1000, batch:   496/ 1052, ite: 145300] train loss: 0.004198, tar: 0.000096 
l0: 0.000077, l1: 0.000077, l2: 0.000102, l3: 0.000183, l4: 0.000415, l5: 0.000733, l6: 0.001417
[epoch: 553/1000, batch:   576/ 1052, ite: 145320] train loss: 0.004198, tar: 0.000096 
l0: 0.000172, l1: 0.000174, l2: 0.000221, l3: 0.000394, l4: 0.000729, l5: 0.001000, l6: 0.003388
[epoch: 553/1000, batch:   656/ 1052, ite: 145340] train loss: 0.004197, tar: 0.000096 
l0: 0.000121, l1: 0.000126, l2: 0.000191, l3: 0.000208, l4: 0.000440, l5: 0.000652, l6: 0.001325
[epoch: 553/1000, batch:   736/ 1052, ite: 145360] train loss: 0.004196, tar: 0.000096 
l0: 0.000060, l1: 0.000058, l2: 0.000124, l3: 0.000221, l4: 0.000566, l5: 0.001256, l6: 0.002297
[epoch: 553/1000, batch:   816/ 1052, ite: 145380] train loss: 0.004195, tar: 0.000096 
l0: 0.000053, l1: 0.000055, l2: 0.000087, l3: 0.000188, l4: 0.000394, l5: 0.001143, l6: 0.001642
[epoch: 553/1000, batch:   896/ 1052, ite: 145400] train loss: 0.004196, tar: 0.000096 
l0: 0.000101, l1: 0.000102, l2: 0.000122, l3: 0.000307, l4: 0.000710, l5: 0.001282, l6: 0.002277
[epoch: 553/1000, batch:   976/ 1052, ite: 145420] train loss: 0.004196, tar: 0.000096 
[Epoch 553/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000061, l1: 0.000064, l2: 0.000114, l3: 0.000133, l4: 0.000306, l5: 0.000717, l6: 0.000966
[epoch: 554/1000, batch:     4/ 1052, ite: 145440] train loss: 0.004197, tar: 0.000096 
l0: 0.000036, l1: 0.000036, l2: 0.000069, l3: 0.000105, l4: 0.000334, l5: 0.000868, l6: 0.001829
[epoch: 554/1000, batch:    84/ 1052, ite: 145460] train loss: 0.004197, tar: 0.000096 
l0: 0.000088, l1: 0.000092, l2: 0.000094, l3: 0.000180, l4: 0.000368, l5: 0.000744, l6: 0.001283
[epoch: 554/1000, batch:   164/ 1052, ite: 145480] train loss: 0.004197, tar: 0.000096 
l0: 0.000039, l1: 0.000042, l2: 0.000100, l3: 0.000157, l4: 0.000351, l5: 0.001211, l6: 0.001628
[epoch: 554/1000, batch:   244/ 1052, ite: 145500] train loss: 0.004197, tar: 0.000096 
l0: 0.000308, l1: 0.000318, l2: 0.000389, l3: 0.000455, l4: 0.000779, l5: 0.001144, l6: 0.002695
[epoch: 554/1000, batch:   324/ 1052, ite: 145520] train loss: 0.004198, tar: 0.000096 
l0: 0.000065, l1: 0.000066, l2: 0.000098, l3: 0.000230, l4: 0.000307, l5: 0.000875, l6: 0.001148
[epoch: 554/1000, batch:   404/ 1052, ite: 145540] train loss: 0.004197, tar: 0.000096 
l0: 0.000093, l1: 0.000095, l2: 0.000091, l3: 0.000216, l4: 0.000619, l5: 0.001660, l6: 0.003096
[epoch: 554/1000, batch:   484/ 1052, ite: 145560] train loss: 0.004198, tar: 0.000096 
l0: 0.000100, l1: 0.000100, l2: 0.000158, l3: 0.000219, l4: 0.000416, l5: 0.000872, l6: 0.001272
[epoch: 554/1000, batch:   564/ 1052, ite: 145580] train loss: 0.004197, tar: 0.000096 
l0: 0.000013, l1: 0.000012, l2: 0.000038, l3: 0.000121, l4: 0.000349, l5: 0.001228, l6: 0.001799
[epoch: 554/1000, batch:   644/ 1052, ite: 145600] train loss: 0.004197, tar: 0.000096 
l0: 0.000227, l1: 0.000231, l2: 0.000218, l3: 0.000315, l4: 0.000820, l5: 0.001461, l6: 0.001930
[epoch: 554/1000, batch:   724/ 1052, ite: 145620] train loss: 0.004198, tar: 0.000096 
l0: 0.000112, l1: 0.000113, l2: 0.000132, l3: 0.000183, l4: 0.000418, l5: 0.000991, l6: 0.002000
[epoch: 554/1000, batch:   804/ 1052, ite: 145640] train loss: 0.004196, tar: 0.000096 
l0: 0.000038, l1: 0.000037, l2: 0.000077, l3: 0.000104, l4: 0.000308, l5: 0.000849, l6: 0.001325
[epoch: 554/1000, batch:   884/ 1052, ite: 145660] train loss: 0.004196, tar: 0.000096 
l0: 0.000096, l1: 0.000098, l2: 0.000116, l3: 0.000250, l4: 0.000543, l5: 0.001440, l6: 0.002578
[epoch: 554/1000, batch:   964/ 1052, ite: 145680] train loss: 0.004195, tar: 0.000096 
l0: 0.000077, l1: 0.000078, l2: 0.000125, l3: 0.000266, l4: 0.000609, l5: 0.001496, l6: 0.002345
[epoch: 554/1000, batch:  1044/ 1052, ite: 145700] train loss: 0.004196, tar: 0.000096 
[Epoch 554/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000046, l1: 0.000046, l2: 0.000086, l3: 0.000207, l4: 0.000514, l5: 0.001474, l6: 0.003079
[epoch: 555/1000, batch:    72/ 1052, ite: 145720] train loss: 0.004197, tar: 0.000096 
l0: 0.000114, l1: 0.000115, l2: 0.000184, l3: 0.000282, l4: 0.000695, l5: 0.001385, l6: 0.002310
[epoch: 555/1000, batch:   152/ 1052, ite: 145740] train loss: 0.004197, tar: 0.000096 
l0: 0.000152, l1: 0.000154, l2: 0.000162, l3: 0.000217, l4: 0.000541, l5: 0.000726, l6: 0.001778
[epoch: 555/1000, batch:   232/ 1052, ite: 145760] train loss: 0.004197, tar: 0.000096 
l0: 0.000050, l1: 0.000050, l2: 0.000083, l3: 0.000142, l4: 0.000310, l5: 0.000667, l6: 0.000860
[epoch: 555/1000, batch:   312/ 1052, ite: 145780] train loss: 0.004196, tar: 0.000096 
l0: 0.000031, l1: 0.000033, l2: 0.000057, l3: 0.000176, l4: 0.000562, l5: 0.001327, l6: 0.001919
[epoch: 555/1000, batch:   392/ 1052, ite: 145800] train loss: 0.004196, tar: 0.000096 
l0: 0.000033, l1: 0.000033, l2: 0.000049, l3: 0.000192, l4: 0.000239, l5: 0.000588, l6: 0.000835
[epoch: 555/1000, batch:   472/ 1052, ite: 145820] train loss: 0.004195, tar: 0.000096 
l0: 0.000158, l1: 0.000162, l2: 0.000225, l3: 0.000423, l4: 0.001087, l5: 0.002083, l6: 0.002625
[epoch: 555/1000, batch:   552/ 1052, ite: 145840] train loss: 0.004196, tar: 0.000096 
l0: 0.000072, l1: 0.000073, l2: 0.000109, l3: 0.000150, l4: 0.000509, l5: 0.001126, l6: 0.001517
[epoch: 555/1000, batch:   632/ 1052, ite: 145860] train loss: 0.004196, tar: 0.000095 
l0: 0.000131, l1: 0.000132, l2: 0.000139, l3: 0.000184, l4: 0.000505, l5: 0.001169, l6: 0.002175
[epoch: 555/1000, batch:   712/ 1052, ite: 145880] train loss: 0.004195, tar: 0.000095 
l0: 0.000056, l1: 0.000056, l2: 0.000105, l3: 0.000164, l4: 0.000437, l5: 0.000978, l6: 0.001252
[epoch: 555/1000, batch:   792/ 1052, ite: 145900] train loss: 0.004195, tar: 0.000095 
l0: 0.000089, l1: 0.000089, l2: 0.000121, l3: 0.000279, l4: 0.000678, l5: 0.001582, l6: 0.002861
[epoch: 555/1000, batch:   872/ 1052, ite: 145920] train loss: 0.004194, tar: 0.000095 
l0: 0.000071, l1: 0.000069, l2: 0.000135, l3: 0.000187, l4: 0.000499, l5: 0.000922, l6: 0.002068
[epoch: 555/1000, batch:   952/ 1052, ite: 145940] train loss: 0.004193, tar: 0.000095 
l0: 0.000075, l1: 0.000073, l2: 0.000113, l3: 0.000166, l4: 0.000427, l5: 0.001277, l6: 0.002523
[epoch: 555/1000, batch:  1032/ 1052, ite: 145960] train loss: 0.004194, tar: 0.000095 
[Epoch 555/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000033, l1: 0.000033, l2: 0.000055, l3: 0.000110, l4: 0.000160, l5: 0.000589, l6: 0.000992
[epoch: 556/1000, batch:    60/ 1052, ite: 145980] train loss: 0.004194, tar: 0.000095 
l0: 0.000115, l1: 0.000112, l2: 0.000195, l3: 0.000305, l4: 0.000706, l5: 0.001119, l6: 0.002201
[epoch: 556/1000, batch:   140/ 1052, ite: 146000] train loss: 0.004194, tar: 0.000095 
l0: 0.000220, l1: 0.000222, l2: 0.000288, l3: 0.000523, l4: 0.000926, l5: 0.001914, l6: 0.003951
[epoch: 556/1000, batch:   220/ 1052, ite: 146020] train loss: 0.004194, tar: 0.000095 
l0: 0.000083, l1: 0.000084, l2: 0.000104, l3: 0.000170, l4: 0.000243, l5: 0.000928, l6: 0.001570
[epoch: 556/1000, batch:   300/ 1052, ite: 146040] train loss: 0.004194, tar: 0.000095 
l0: 0.000065, l1: 0.000064, l2: 0.000092, l3: 0.000179, l4: 0.000305, l5: 0.000968, l6: 0.001434
[epoch: 556/1000, batch:   380/ 1052, ite: 146060] train loss: 0.004193, tar: 0.000095 
l0: 0.000025, l1: 0.000025, l2: 0.000071, l3: 0.000259, l4: 0.000634, l5: 0.001447, l6: 0.001792
[epoch: 556/1000, batch:   460/ 1052, ite: 146080] train loss: 0.004194, tar: 0.000095 
l0: 0.000058, l1: 0.000058, l2: 0.000094, l3: 0.000227, l4: 0.000415, l5: 0.000916, l6: 0.001378
[epoch: 556/1000, batch:   540/ 1052, ite: 146100] train loss: 0.004193, tar: 0.000095 
l0: 0.000035, l1: 0.000036, l2: 0.000053, l3: 0.000087, l4: 0.000308, l5: 0.001154, l6: 0.001647
[epoch: 556/1000, batch:   620/ 1052, ite: 146120] train loss: 0.004192, tar: 0.000095 
l0: 0.000060, l1: 0.000062, l2: 0.000096, l3: 0.000308, l4: 0.000793, l5: 0.002043, l6: 0.002933
[epoch: 556/1000, batch:   700/ 1052, ite: 146140] train loss: 0.004193, tar: 0.000095 
l0: 0.000062, l1: 0.000063, l2: 0.000109, l3: 0.000196, l4: 0.000463, l5: 0.001051, l6: 0.002781
[epoch: 556/1000, batch:   780/ 1052, ite: 146160] train loss: 0.004194, tar: 0.000095 
l0: 0.000169, l1: 0.000169, l2: 0.000239, l3: 0.000356, l4: 0.001195, l5: 0.001884, l6: 0.002715
[epoch: 556/1000, batch:   860/ 1052, ite: 146180] train loss: 0.004194, tar: 0.000095 
l0: 0.000061, l1: 0.000062, l2: 0.000132, l3: 0.000242, l4: 0.000708, l5: 0.001291, l6: 0.003087
[epoch: 556/1000, batch:   940/ 1052, ite: 146200] train loss: 0.004194, tar: 0.000095 
l0: 0.000060, l1: 0.000061, l2: 0.000085, l3: 0.000136, l4: 0.000503, l5: 0.000817, l6: 0.001567
[epoch: 556/1000, batch:  1020/ 1052, ite: 146220] train loss: 0.004193, tar: 0.000095 
[Epoch 556/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000201, l1: 0.000204, l2: 0.000212, l3: 0.000201, l4: 0.000505, l5: 0.000911, l6: 0.001642
[epoch: 557/1000, batch:    48/ 1052, ite: 146240] train loss: 0.004193, tar: 0.000095 
l0: 0.000097, l1: 0.000098, l2: 0.000116, l3: 0.000261, l4: 0.000498, l5: 0.000722, l6: 0.002230
[epoch: 557/1000, batch:   128/ 1052, ite: 146260] train loss: 0.004191, tar: 0.000095 
l0: 0.000050, l1: 0.000051, l2: 0.000088, l3: 0.000169, l4: 0.000365, l5: 0.000627, l6: 0.001129
[epoch: 557/1000, batch:   208/ 1052, ite: 146280] train loss: 0.004191, tar: 0.000095 
l0: 0.000057, l1: 0.000057, l2: 0.000097, l3: 0.000319, l4: 0.000667, l5: 0.001088, l6: 0.001536
[epoch: 557/1000, batch:   288/ 1052, ite: 146300] train loss: 0.004192, tar: 0.000095 
l0: 0.000023, l1: 0.000022, l2: 0.000051, l3: 0.000162, l4: 0.000317, l5: 0.000799, l6: 0.001168
[epoch: 557/1000, batch:   368/ 1052, ite: 146320] train loss: 0.004192, tar: 0.000095 
l0: 0.000042, l1: 0.000045, l2: 0.000054, l3: 0.000113, l4: 0.000320, l5: 0.000594, l6: 0.001982
[epoch: 557/1000, batch:   448/ 1052, ite: 146340] train loss: 0.004192, tar: 0.000095 
l0: 0.000054, l1: 0.000055, l2: 0.000099, l3: 0.000193, l4: 0.000386, l5: 0.000839, l6: 0.001479
[epoch: 557/1000, batch:   528/ 1052, ite: 146360] train loss: 0.004192, tar: 0.000095 
l0: 0.000087, l1: 0.000087, l2: 0.000141, l3: 0.000357, l4: 0.000761, l5: 0.001045, l6: 0.002066
[epoch: 557/1000, batch:   608/ 1052, ite: 146380] train loss: 0.004191, tar: 0.000095 
l0: 0.000039, l1: 0.000038, l2: 0.000069, l3: 0.000199, l4: 0.000444, l5: 0.000950, l6: 0.002025
[epoch: 557/1000, batch:   688/ 1052, ite: 146400] train loss: 0.004191, tar: 0.000095 
l0: 0.000119, l1: 0.000122, l2: 0.000191, l3: 0.000298, l4: 0.000600, l5: 0.002411, l6: 0.003687
[epoch: 557/1000, batch:   768/ 1052, ite: 146420] train loss: 0.004193, tar: 0.000095 
l0: 0.000052, l1: 0.000053, l2: 0.000071, l3: 0.000122, l4: 0.000307, l5: 0.000647, l6: 0.000554
[epoch: 557/1000, batch:   848/ 1052, ite: 146440] train loss: 0.004193, tar: 0.000095 
l0: 0.000051, l1: 0.000049, l2: 0.000111, l3: 0.000212, l4: 0.000425, l5: 0.000682, l6: 0.001278
[epoch: 557/1000, batch:   928/ 1052, ite: 146460] train loss: 0.004194, tar: 0.000095 
l0: 0.000062, l1: 0.000065, l2: 0.000094, l3: 0.000172, l4: 0.000559, l5: 0.000981, l6: 0.001991
[epoch: 557/1000, batch:  1008/ 1052, ite: 146480] train loss: 0.004193, tar: 0.000095 
[Epoch 557/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000028, l1: 0.000028, l2: 0.000054, l3: 0.000142, l4: 0.000359, l5: 0.000771, l6: 0.001421
[epoch: 558/1000, batch:    36/ 1052, ite: 146500] train loss: 0.004193, tar: 0.000095 
l0: 0.000045, l1: 0.000047, l2: 0.000083, l3: 0.000160, l4: 0.000581, l5: 0.001015, l6: 0.001902
[epoch: 558/1000, batch:   116/ 1052, ite: 146520] train loss: 0.004194, tar: 0.000095 
l0: 0.000042, l1: 0.000043, l2: 0.000119, l3: 0.000241, l4: 0.000378, l5: 0.000874, l6: 0.001920
[epoch: 558/1000, batch:   196/ 1052, ite: 146540] train loss: 0.004193, tar: 0.000095 
l0: 0.000037, l1: 0.000036, l2: 0.000067, l3: 0.000186, l4: 0.000362, l5: 0.000830, l6: 0.001630
[epoch: 558/1000, batch:   276/ 1052, ite: 146560] train loss: 0.004193, tar: 0.000095 
l0: 0.000108, l1: 0.000107, l2: 0.000117, l3: 0.000178, l4: 0.000646, l5: 0.000849, l6: 0.001805
[epoch: 558/1000, batch:   356/ 1052, ite: 146580] train loss: 0.004194, tar: 0.000095 
l0: 0.000051, l1: 0.000054, l2: 0.000069, l3: 0.000121, l4: 0.000369, l5: 0.000924, l6: 0.001787
[epoch: 558/1000, batch:   436/ 1052, ite: 146600] train loss: 0.004194, tar: 0.000095 
l0: 0.000147, l1: 0.000146, l2: 0.000213, l3: 0.000287, l4: 0.000537, l5: 0.000788, l6: 0.002099
[epoch: 558/1000, batch:   516/ 1052, ite: 146620] train loss: 0.004194, tar: 0.000094 
l0: 0.000033, l1: 0.000032, l2: 0.000099, l3: 0.000361, l4: 0.000670, l5: 0.001710, l6: 0.004037
[epoch: 558/1000, batch:   596/ 1052, ite: 146640] train loss: 0.004195, tar: 0.000095 
l0: 0.000062, l1: 0.000061, l2: 0.000097, l3: 0.000203, l4: 0.000400, l5: 0.001019, l6: 0.001950
[epoch: 558/1000, batch:   676/ 1052, ite: 146660] train loss: 0.004192, tar: 0.000094 
l0: 0.000078, l1: 0.000076, l2: 0.000117, l3: 0.000202, l4: 0.000508, l5: 0.001254, l6: 0.002311
[epoch: 558/1000, batch:   756/ 1052, ite: 146680] train loss: 0.004191, tar: 0.000094 
l0: 0.000044, l1: 0.000041, l2: 0.000096, l3: 0.000222, l4: 0.000659, l5: 0.001409, l6: 0.002121
[epoch: 558/1000, batch:   836/ 1052, ite: 146700] train loss: 0.004191, tar: 0.000094 
l0: 0.000055, l1: 0.000055, l2: 0.000074, l3: 0.000183, l4: 0.000343, l5: 0.001528, l6: 0.002679
[epoch: 558/1000, batch:   916/ 1052, ite: 146720] train loss: 0.004192, tar: 0.000094 
l0: 0.000177, l1: 0.000171, l2: 0.000242, l3: 0.000416, l4: 0.000786, l5: 0.002921, l6: 0.004297
[epoch: 558/1000, batch:   996/ 1052, ite: 146740] train loss: 0.004192, tar: 0.000094 
[Epoch 558/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000097, l1: 0.000099, l2: 0.000116, l3: 0.000156, l4: 0.000328, l5: 0.001010, l6: 0.001190
[epoch: 559/1000, batch:    24/ 1052, ite: 146760] train loss: 0.004191, tar: 0.000094 
l0: 0.000127, l1: 0.000126, l2: 0.000163, l3: 0.000253, l4: 0.000659, l5: 0.001054, l6: 0.002435
[epoch: 559/1000, batch:   104/ 1052, ite: 146780] train loss: 0.004192, tar: 0.000094 
l0: 0.000015, l1: 0.000015, l2: 0.000041, l3: 0.000089, l4: 0.000197, l5: 0.000957, l6: 0.001303
[epoch: 559/1000, batch:   184/ 1052, ite: 146800] train loss: 0.004191, tar: 0.000094 
l0: 0.000125, l1: 0.000108, l2: 0.000207, l3: 0.000285, l4: 0.000395, l5: 0.001188, l6: 0.002789
[epoch: 559/1000, batch:   264/ 1052, ite: 146820] train loss: 0.004191, tar: 0.000094 
l0: 0.000109, l1: 0.000110, l2: 0.000131, l3: 0.000200, l4: 0.000470, l5: 0.001124, l6: 0.003138
[epoch: 559/1000, batch:   344/ 1052, ite: 146840] train loss: 0.004191, tar: 0.000094 
l0: 0.000076, l1: 0.000075, l2: 0.000088, l3: 0.000164, l4: 0.000323, l5: 0.001017, l6: 0.001580
[epoch: 559/1000, batch:   424/ 1052, ite: 146860] train loss: 0.004190, tar: 0.000094 
l0: 0.000041, l1: 0.000042, l2: 0.000078, l3: 0.000161, l4: 0.000399, l5: 0.000715, l6: 0.001500
[epoch: 559/1000, batch:   504/ 1052, ite: 146880] train loss: 0.004189, tar: 0.000094 
l0: 0.000127, l1: 0.000130, l2: 0.000182, l3: 0.000263, l4: 0.000491, l5: 0.001315, l6: 0.001759
[epoch: 559/1000, batch:   584/ 1052, ite: 146900] train loss: 0.004189, tar: 0.000094 
l0: 0.000085, l1: 0.000086, l2: 0.000130, l3: 0.000212, l4: 0.000457, l5: 0.001440, l6: 0.002147
[epoch: 559/1000, batch:   664/ 1052, ite: 146920] train loss: 0.004189, tar: 0.000094 
l0: 0.000057, l1: 0.000059, l2: 0.000060, l3: 0.000107, l4: 0.000226, l5: 0.000620, l6: 0.001170
[epoch: 559/1000, batch:   744/ 1052, ite: 146940] train loss: 0.004190, tar: 0.000094 
l0: 0.000045, l1: 0.000044, l2: 0.000070, l3: 0.000175, l4: 0.000381, l5: 0.000638, l6: 0.001445
[epoch: 559/1000, batch:   824/ 1052, ite: 146960] train loss: 0.004189, tar: 0.000094 
l0: 0.000117, l1: 0.000117, l2: 0.000206, l3: 0.000378, l4: 0.000912, l5: 0.001428, l6: 0.004363
[epoch: 559/1000, batch:   904/ 1052, ite: 146980] train loss: 0.004190, tar: 0.000094 
l0: 0.000081, l1: 0.000081, l2: 0.000115, l3: 0.000186, l4: 0.000415, l5: 0.001259, l6: 0.001188
[epoch: 559/1000, batch:   984/ 1052, ite: 147000] train loss: 0.004190, tar: 0.000094 
[Epoch 559/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000080, l1: 0.000078, l2: 0.000112, l3: 0.000196, l4: 0.000403, l5: 0.000823, l6: 0.001220
[epoch: 560/1000, batch:    12/ 1052, ite: 147020] train loss: 0.004189, tar: 0.000094 
l0: 0.000036, l1: 0.000040, l2: 0.000073, l3: 0.000157, l4: 0.000205, l5: 0.000591, l6: 0.001530
[epoch: 560/1000, batch:    92/ 1052, ite: 147040] train loss: 0.004188, tar: 0.000094 
l0: 0.000092, l1: 0.000095, l2: 0.000108, l3: 0.000225, l4: 0.000456, l5: 0.000826, l6: 0.001468
[epoch: 560/1000, batch:   172/ 1052, ite: 147060] train loss: 0.004188, tar: 0.000094 
l0: 0.000046, l1: 0.000048, l2: 0.000052, l3: 0.000114, l4: 0.000205, l5: 0.000933, l6: 0.001318
[epoch: 560/1000, batch:   252/ 1052, ite: 147080] train loss: 0.004188, tar: 0.000094 
l0: 0.000072, l1: 0.000073, l2: 0.000094, l3: 0.000130, l4: 0.000359, l5: 0.001367, l6: 0.002062
[epoch: 560/1000, batch:   332/ 1052, ite: 147100] train loss: 0.004188, tar: 0.000094 
l0: 0.000245, l1: 0.000247, l2: 0.000315, l3: 0.000428, l4: 0.000780, l5: 0.001190, l6: 0.002739
[epoch: 560/1000, batch:   412/ 1052, ite: 147120] train loss: 0.004189, tar: 0.000094 
l0: 0.000077, l1: 0.000077, l2: 0.000121, l3: 0.000250, l4: 0.000541, l5: 0.001491, l6: 0.003189
[epoch: 560/1000, batch:   492/ 1052, ite: 147140] train loss: 0.004190, tar: 0.000094 
l0: 0.000145, l1: 0.000145, l2: 0.000163, l3: 0.000222, l4: 0.000421, l5: 0.001030, l6: 0.002480
[epoch: 560/1000, batch:   572/ 1052, ite: 147160] train loss: 0.004189, tar: 0.000094 
l0: 0.000228, l1: 0.000234, l2: 0.000315, l3: 0.000357, l4: 0.000546, l5: 0.001509, l6: 0.003061
[epoch: 560/1000, batch:   652/ 1052, ite: 147180] train loss: 0.004190, tar: 0.000094 
l0: 0.000066, l1: 0.000066, l2: 0.000094, l3: 0.000184, l4: 0.000237, l5: 0.000882, l6: 0.001535
[epoch: 560/1000, batch:   732/ 1052, ite: 147200] train loss: 0.004190, tar: 0.000094 
l0: 0.000060, l1: 0.000061, l2: 0.000097, l3: 0.000180, l4: 0.000500, l5: 0.000979, l6: 0.001848
[epoch: 560/1000, batch:   812/ 1052, ite: 147220] train loss: 0.004189, tar: 0.000094 
l0: 0.000095, l1: 0.000093, l2: 0.000112, l3: 0.000236, l4: 0.000349, l5: 0.000526, l6: 0.001260
[epoch: 560/1000, batch:   892/ 1052, ite: 147240] train loss: 0.004189, tar: 0.000094 
l0: 0.000078, l1: 0.000078, l2: 0.000107, l3: 0.000206, l4: 0.000416, l5: 0.000663, l6: 0.002094
[epoch: 560/1000, batch:   972/ 1052, ite: 147260] train loss: 0.004189, tar: 0.000094 
l0: 0.000061, l1: 0.000059, l2: 0.000103, l3: 0.000310, l4: 0.000853, l5: 0.001515, l6: 0.003538
[epoch: 560/1000, batch:  1052/ 1052, ite: 147280] train loss: 0.004189, tar: 0.000094 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_147280.pkl
[Epoch 560/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000061, l1: 0.000063, l2: 0.000085, l3: 0.000151, l4: 0.000501, l5: 0.001054, l6: 0.001493
[epoch: 561/1000, batch:    80/ 1052, ite: 147300] train loss: 0.003539, tar: 0.000072 
l0: 0.000074, l1: 0.000074, l2: 0.000108, l3: 0.000213, l4: 0.000331, l5: 0.001260, l6: 0.001760
[epoch: 561/1000, batch:   160/ 1052, ite: 147320] train loss: 0.004028, tar: 0.000083 
l0: 0.000083, l1: 0.000080, l2: 0.000161, l3: 0.000321, l4: 0.000700, l5: 0.002224, l6: 0.003417
[epoch: 561/1000, batch:   240/ 1052, ite: 147340] train loss: 0.004309, tar: 0.000088 
l0: 0.000066, l1: 0.000066, l2: 0.000129, l3: 0.000353, l4: 0.000769, l5: 0.002208, l6: 0.003868
[epoch: 561/1000, batch:   320/ 1052, ite: 147360] train loss: 0.004214, tar: 0.000083 
l0: 0.000082, l1: 0.000083, l2: 0.000108, l3: 0.000206, l4: 0.000355, l5: 0.001132, l6: 0.001259
[epoch: 561/1000, batch:   400/ 1052, ite: 147380] train loss: 0.004053, tar: 0.000079 
l0: 0.000023, l1: 0.000022, l2: 0.000052, l3: 0.000108, l4: 0.000326, l5: 0.000867, l6: 0.000785
[epoch: 561/1000, batch:   480/ 1052, ite: 147400] train loss: 0.004016, tar: 0.000077 
l0: 0.000056, l1: 0.000060, l2: 0.000086, l3: 0.000109, l4: 0.000207, l5: 0.000871, l6: 0.000925
[epoch: 561/1000, batch:   560/ 1052, ite: 147420] train loss: 0.003988, tar: 0.000079 
l0: 0.000102, l1: 0.000103, l2: 0.000136, l3: 0.000206, l4: 0.000609, l5: 0.001160, l6: 0.001766
[epoch: 561/1000, batch:   640/ 1052, ite: 147440] train loss: 0.004081, tar: 0.000083 
l0: 0.000051, l1: 0.000051, l2: 0.000092, l3: 0.000109, l4: 0.000389, l5: 0.000614, l6: 0.001216
[epoch: 561/1000, batch:   720/ 1052, ite: 147460] train loss: 0.004098, tar: 0.000085 
l0: 0.000164, l1: 0.000165, l2: 0.000252, l3: 0.000343, l4: 0.000635, l5: 0.002000, l6: 0.003151
[epoch: 561/1000, batch:   800/ 1052, ite: 147480] train loss: 0.004049, tar: 0.000084 
l0: 0.000045, l1: 0.000046, l2: 0.000055, l3: 0.000122, l4: 0.000218, l5: 0.000867, l6: 0.001090
[epoch: 561/1000, batch:   880/ 1052, ite: 147500] train loss: 0.004115, tar: 0.000085 
l0: 0.000086, l1: 0.000084, l2: 0.000141, l3: 0.000268, l4: 0.000695, l5: 0.001024, l6: 0.001640
[epoch: 561/1000, batch:   960/ 1052, ite: 147520] train loss: 0.004155, tar: 0.000086 
l0: 0.000151, l1: 0.000151, l2: 0.000195, l3: 0.000310, l4: 0.000491, l5: 0.001119, l6: 0.001621
[epoch: 561/1000, batch:  1040/ 1052, ite: 147540] train loss: 0.004149, tar: 0.000087 
[Epoch 561/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000080, l1: 0.000077, l2: 0.000131, l3: 0.000251, l4: 0.000497, l5: 0.001107, l6: 0.001677
[epoch: 562/1000, batch:    68/ 1052, ite: 147560] train loss: 0.004168, tar: 0.000088 
l0: 0.000073, l1: 0.000074, l2: 0.000088, l3: 0.000185, l4: 0.000424, l5: 0.000994, l6: 0.001352
[epoch: 562/1000, batch:   148/ 1052, ite: 147580] train loss: 0.004127, tar: 0.000086 
l0: 0.000052, l1: 0.000052, l2: 0.000102, l3: 0.000177, l4: 0.000363, l5: 0.000755, l6: 0.001303
[epoch: 562/1000, batch:   228/ 1052, ite: 147600] train loss: 0.004102, tar: 0.000085 
l0: 0.000072, l1: 0.000071, l2: 0.000101, l3: 0.000148, l4: 0.000502, l5: 0.001472, l6: 0.001732
[epoch: 562/1000, batch:   308/ 1052, ite: 147620] train loss: 0.004092, tar: 0.000085 
l0: 0.000262, l1: 0.000261, l2: 0.000411, l3: 0.000718, l4: 0.001000, l5: 0.001850, l6: 0.005746
[epoch: 562/1000, batch:   388/ 1052, ite: 147640] train loss: 0.004129, tar: 0.000086 
l0: 0.000139, l1: 0.000138, l2: 0.000215, l3: 0.000318, l4: 0.000537, l5: 0.001266, l6: 0.002468
[epoch: 562/1000, batch:   468/ 1052, ite: 147660] train loss: 0.004146, tar: 0.000086 
l0: 0.000059, l1: 0.000061, l2: 0.000111, l3: 0.000130, l4: 0.000386, l5: 0.000987, l6: 0.002358
[epoch: 562/1000, batch:   548/ 1052, ite: 147680] train loss: 0.004132, tar: 0.000086 
l0: 0.000035, l1: 0.000032, l2: 0.000097, l3: 0.000397, l4: 0.000687, l5: 0.001549, l6: 0.002313
[epoch: 562/1000, batch:   628/ 1052, ite: 147700] train loss: 0.004142, tar: 0.000086 
l0: 0.000117, l1: 0.000119, l2: 0.000119, l3: 0.000210, l4: 0.000405, l5: 0.001029, l6: 0.001100
[epoch: 562/1000, batch:   708/ 1052, ite: 147720] train loss: 0.004146, tar: 0.000086 
l0: 0.000050, l1: 0.000049, l2: 0.000085, l3: 0.000188, l4: 0.000488, l5: 0.001105, l6: 0.001582
[epoch: 562/1000, batch:   788/ 1052, ite: 147740] train loss: 0.004147, tar: 0.000087 
l0: 0.000025, l1: 0.000027, l2: 0.000050, l3: 0.000096, l4: 0.000335, l5: 0.000762, l6: 0.000974
[epoch: 562/1000, batch:   868/ 1052, ite: 147760] train loss: 0.004148, tar: 0.000086 
l0: 0.000051, l1: 0.000053, l2: 0.000082, l3: 0.000177, l4: 0.000574, l5: 0.001141, l6: 0.001882
[epoch: 562/1000, batch:   948/ 1052, ite: 147780] train loss: 0.004149, tar: 0.000086 
l0: 0.000053, l1: 0.000055, l2: 0.000088, l3: 0.000221, l4: 0.000383, l5: 0.001105, l6: 0.001773
[epoch: 562/1000, batch:  1028/ 1052, ite: 147800] train loss: 0.004153, tar: 0.000087 
[Epoch 562/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000039, l1: 0.000038, l2: 0.000068, l3: 0.000120, l4: 0.000394, l5: 0.000787, l6: 0.001235
[epoch: 563/1000, batch:    56/ 1052, ite: 147820] train loss: 0.004135, tar: 0.000086 
l0: 0.000089, l1: 0.000090, l2: 0.000134, l3: 0.000216, l4: 0.000455, l5: 0.000980, l6: 0.002502
[epoch: 563/1000, batch:   136/ 1052, ite: 147840] train loss: 0.004126, tar: 0.000087 
l0: 0.000064, l1: 0.000066, l2: 0.000118, l3: 0.000129, l4: 0.000270, l5: 0.000897, l6: 0.001336
[epoch: 563/1000, batch:   216/ 1052, ite: 147860] train loss: 0.004117, tar: 0.000087 
l0: 0.000071, l1: 0.000070, l2: 0.000192, l3: 0.000377, l4: 0.000645, l5: 0.002030, l6: 0.003482
[epoch: 563/1000, batch:   296/ 1052, ite: 147880] train loss: 0.004133, tar: 0.000087 
l0: 0.000144, l1: 0.000143, l2: 0.000202, l3: 0.000303, l4: 0.000626, l5: 0.001211, l6: 0.001286
[epoch: 563/1000, batch:   376/ 1052, ite: 147900] train loss: 0.004143, tar: 0.000087 
l0: 0.000116, l1: 0.000118, l2: 0.000173, l3: 0.000283, l4: 0.000557, l5: 0.001564, l6: 0.003029
[epoch: 563/1000, batch:   456/ 1052, ite: 147920] train loss: 0.004143, tar: 0.000087 
l0: 0.000111, l1: 0.000114, l2: 0.000154, l3: 0.000225, l4: 0.000333, l5: 0.000757, l6: 0.001716
[epoch: 563/1000, batch:   536/ 1052, ite: 147940] train loss: 0.004133, tar: 0.000086 
l0: 0.000099, l1: 0.000103, l2: 0.000119, l3: 0.000146, l4: 0.000484, l5: 0.001136, l6: 0.001884
[epoch: 563/1000, batch:   616/ 1052, ite: 147960] train loss: 0.004128, tar: 0.000086 
l0: 0.000171, l1: 0.000172, l2: 0.000193, l3: 0.000276, l4: 0.000375, l5: 0.001738, l6: 0.002745
[epoch: 563/1000, batch:   696/ 1052, ite: 147980] train loss: 0.004131, tar: 0.000086 
l0: 0.000047, l1: 0.000049, l2: 0.000075, l3: 0.000148, l4: 0.000509, l5: 0.000535, l6: 0.001999
[epoch: 563/1000, batch:   776/ 1052, ite: 148000] train loss: 0.004144, tar: 0.000086 
l0: 0.000114, l1: 0.000115, l2: 0.000126, l3: 0.000240, l4: 0.000406, l5: 0.000680, l6: 0.001537
[epoch: 563/1000, batch:   856/ 1052, ite: 148020] train loss: 0.004150, tar: 0.000086 
l0: 0.000030, l1: 0.000030, l2: 0.000041, l3: 0.000087, l4: 0.000408, l5: 0.000691, l6: 0.001464
[epoch: 563/1000, batch:   936/ 1052, ite: 148040] train loss: 0.004138, tar: 0.000086 
l0: 0.000058, l1: 0.000062, l2: 0.000099, l3: 0.000150, l4: 0.000345, l5: 0.000992, l6: 0.001701
[epoch: 563/1000, batch:  1016/ 1052, ite: 148060] train loss: 0.004132, tar: 0.000085 
[Epoch 563/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000058, l1: 0.000060, l2: 0.000119, l3: 0.000234, l4: 0.000348, l5: 0.000909, l6: 0.002603
[epoch: 564/1000, batch:    44/ 1052, ite: 148080] train loss: 0.004127, tar: 0.000085 
l0: 0.000025, l1: 0.000025, l2: 0.000066, l3: 0.000174, l4: 0.000553, l5: 0.000815, l6: 0.001515
[epoch: 564/1000, batch:   124/ 1052, ite: 148100] train loss: 0.004132, tar: 0.000086 
l0: 0.000070, l1: 0.000070, l2: 0.000096, l3: 0.000169, l4: 0.000396, l5: 0.001821, l6: 0.002913
[epoch: 564/1000, batch:   204/ 1052, ite: 148120] train loss: 0.004136, tar: 0.000086 
l0: 0.000028, l1: 0.000025, l2: 0.000082, l3: 0.000158, l4: 0.000390, l5: 0.000924, l6: 0.001340
[epoch: 564/1000, batch:   284/ 1052, ite: 148140] train loss: 0.004132, tar: 0.000085 
l0: 0.000063, l1: 0.000062, l2: 0.000103, l3: 0.000159, l4: 0.000436, l5: 0.000923, l6: 0.001452
[epoch: 564/1000, batch:   364/ 1052, ite: 148160] train loss: 0.004127, tar: 0.000085 
l0: 0.000123, l1: 0.000126, l2: 0.000166, l3: 0.000277, l4: 0.000671, l5: 0.001249, l6: 0.002819
[epoch: 564/1000, batch:   444/ 1052, ite: 148180] train loss: 0.004120, tar: 0.000085 
l0: 0.000080, l1: 0.000083, l2: 0.000122, l3: 0.000214, l4: 0.000453, l5: 0.000673, l6: 0.002193
[epoch: 564/1000, batch:   524/ 1052, ite: 148200] train loss: 0.004125, tar: 0.000085 
l0: 0.000069, l1: 0.000072, l2: 0.000114, l3: 0.000174, l4: 0.000281, l5: 0.000570, l6: 0.001746
[epoch: 564/1000, batch:   604/ 1052, ite: 148220] train loss: 0.004115, tar: 0.000085 
l0: 0.000017, l1: 0.000018, l2: 0.000041, l3: 0.000077, l4: 0.000289, l5: 0.000789, l6: 0.000905
[epoch: 564/1000, batch:   684/ 1052, ite: 148240] train loss: 0.004115, tar: 0.000085 
l0: 0.000141, l1: 0.000143, l2: 0.000218, l3: 0.000389, l4: 0.000983, l5: 0.001930, l6: 0.003291
[epoch: 564/1000, batch:   764/ 1052, ite: 148260] train loss: 0.004123, tar: 0.000085 
l0: 0.000069, l1: 0.000070, l2: 0.000088, l3: 0.000123, l4: 0.000263, l5: 0.000684, l6: 0.001338
[epoch: 564/1000, batch:   844/ 1052, ite: 148280] train loss: 0.004111, tar: 0.000085 
l0: 0.000094, l1: 0.000097, l2: 0.000116, l3: 0.000233, l4: 0.000416, l5: 0.000970, l6: 0.001717
[epoch: 564/1000, batch:   924/ 1052, ite: 148300] train loss: 0.004116, tar: 0.000084 
l0: 0.000096, l1: 0.000095, l2: 0.000156, l3: 0.000292, l4: 0.000474, l5: 0.001766, l6: 0.003528
[epoch: 564/1000, batch:  1004/ 1052, ite: 148320] train loss: 0.004118, tar: 0.000084 
[Epoch 564/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000031, l1: 0.000031, l2: 0.000084, l3: 0.000209, l4: 0.000625, l5: 0.000962, l6: 0.001243
[epoch: 565/1000, batch:    32/ 1052, ite: 148340] train loss: 0.004110, tar: 0.000084 
l0: 0.000028, l1: 0.000028, l2: 0.000069, l3: 0.000169, l4: 0.000329, l5: 0.000631, l6: 0.001201
[epoch: 565/1000, batch:   112/ 1052, ite: 148360] train loss: 0.004106, tar: 0.000084 
l0: 0.000051, l1: 0.000050, l2: 0.000082, l3: 0.000176, l4: 0.000489, l5: 0.001310, l6: 0.001692
[epoch: 565/1000, batch:   192/ 1052, ite: 148380] train loss: 0.004100, tar: 0.000084 
l0: 0.000062, l1: 0.000062, l2: 0.000064, l3: 0.000204, l4: 0.000380, l5: 0.000505, l6: 0.001217
[epoch: 565/1000, batch:   272/ 1052, ite: 148400] train loss: 0.004100, tar: 0.000084 
l0: 0.000053, l1: 0.000055, l2: 0.000111, l3: 0.000254, l4: 0.000327, l5: 0.000938, l6: 0.001791
[epoch: 565/1000, batch:   352/ 1052, ite: 148420] train loss: 0.004098, tar: 0.000084 
l0: 0.000407, l1: 0.000400, l2: 0.000483, l3: 0.000516, l4: 0.000742, l5: 0.001710, l6: 0.002536
[epoch: 565/1000, batch:   432/ 1052, ite: 148440] train loss: 0.004113, tar: 0.000084 
l0: 0.000140, l1: 0.000142, l2: 0.000173, l3: 0.000271, l4: 0.000601, l5: 0.001066, l6: 0.002936
[epoch: 565/1000, batch:   512/ 1052, ite: 148460] train loss: 0.004113, tar: 0.000084 
l0: 0.000049, l1: 0.000048, l2: 0.000084, l3: 0.000128, l4: 0.000310, l5: 0.001120, l6: 0.001865
[epoch: 565/1000, batch:   592/ 1052, ite: 148480] train loss: 0.004115, tar: 0.000084 
l0: 0.000078, l1: 0.000078, l2: 0.000191, l3: 0.000447, l4: 0.001041, l5: 0.002650, l6: 0.003414
[epoch: 565/1000, batch:   672/ 1052, ite: 148500] train loss: 0.004115, tar: 0.000084 
l0: 0.000039, l1: 0.000040, l2: 0.000089, l3: 0.000145, l4: 0.000309, l5: 0.000596, l6: 0.002033
[epoch: 565/1000, batch:   752/ 1052, ite: 148520] train loss: 0.004117, tar: 0.000084 
l0: 0.000072, l1: 0.000073, l2: 0.000116, l3: 0.000190, l4: 0.000424, l5: 0.001066, l6: 0.001944
[epoch: 565/1000, batch:   832/ 1052, ite: 148540] train loss: 0.004125, tar: 0.000084 
l0: 0.000183, l1: 0.000190, l2: 0.000228, l3: 0.000353, l4: 0.000547, l5: 0.001080, l6: 0.002054
[epoch: 565/1000, batch:   912/ 1052, ite: 148560] train loss: 0.004120, tar: 0.000084 
l0: 0.000104, l1: 0.000104, l2: 0.000152, l3: 0.000209, l4: 0.000604, l5: 0.001461, l6: 0.003115
[epoch: 565/1000, batch:   992/ 1052, ite: 148580] train loss: 0.004116, tar: 0.000084 
[Epoch 565/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000044, l1: 0.000045, l2: 0.000084, l3: 0.000137, l4: 0.000303, l5: 0.001065, l6: 0.001670
[epoch: 566/1000, batch:    20/ 1052, ite: 148600] train loss: 0.004120, tar: 0.000083 
l0: 0.000044, l1: 0.000043, l2: 0.000100, l3: 0.000280, l4: 0.000535, l5: 0.000776, l6: 0.001655
[epoch: 566/1000, batch:   100/ 1052, ite: 148620] train loss: 0.004117, tar: 0.000083 
l0: 0.000028, l1: 0.000028, l2: 0.000038, l3: 0.000085, l4: 0.000233, l5: 0.000803, l6: 0.000885
[epoch: 566/1000, batch:   180/ 1052, ite: 148640] train loss: 0.004109, tar: 0.000083 
l0: 0.000043, l1: 0.000045, l2: 0.000082, l3: 0.000145, l4: 0.000394, l5: 0.000850, l6: 0.001149
[epoch: 566/1000, batch:   260/ 1052, ite: 148660] train loss: 0.004109, tar: 0.000083 
l0: 0.000089, l1: 0.000090, l2: 0.000168, l3: 0.000376, l4: 0.000600, l5: 0.001923, l6: 0.004053
[epoch: 566/1000, batch:   340/ 1052, ite: 148680] train loss: 0.004113, tar: 0.000083 
l0: 0.000119, l1: 0.000121, l2: 0.000217, l3: 0.000317, l4: 0.000714, l5: 0.001314, l6: 0.002648
[epoch: 566/1000, batch:   420/ 1052, ite: 148700] train loss: 0.004117, tar: 0.000083 
l0: 0.000163, l1: 0.000164, l2: 0.000247, l3: 0.000485, l4: 0.000940, l5: 0.001317, l6: 0.002721
[epoch: 566/1000, batch:   500/ 1052, ite: 148720] train loss: 0.004122, tar: 0.000083 
l0: 0.000080, l1: 0.000082, l2: 0.000164, l3: 0.000278, l4: 0.000667, l5: 0.000936, l6: 0.002266
[epoch: 566/1000, batch:   580/ 1052, ite: 148740] train loss: 0.004119, tar: 0.000083 
l0: 0.000036, l1: 0.000037, l2: 0.000051, l3: 0.000107, l4: 0.000302, l5: 0.000624, l6: 0.001254
[epoch: 566/1000, batch:   660/ 1052, ite: 148760] train loss: 0.004123, tar: 0.000083 
l0: 0.000014, l1: 0.000014, l2: 0.000047, l3: 0.000149, l4: 0.000323, l5: 0.000418, l6: 0.001703
[epoch: 566/1000, batch:   740/ 1052, ite: 148780] train loss: 0.004127, tar: 0.000083 
l0: 0.000036, l1: 0.000034, l2: 0.000091, l3: 0.000208, l4: 0.000408, l5: 0.001387, l6: 0.002246
[epoch: 566/1000, batch:   820/ 1052, ite: 148800] train loss: 0.004121, tar: 0.000083 
l0: 0.000122, l1: 0.000123, l2: 0.000153, l3: 0.000198, l4: 0.000313, l5: 0.000968, l6: 0.001650
[epoch: 566/1000, batch:   900/ 1052, ite: 148820] train loss: 0.004122, tar: 0.000083 
l0: 0.000107, l1: 0.000109, l2: 0.000143, l3: 0.000233, l4: 0.000582, l5: 0.001262, l6: 0.001832
[epoch: 566/1000, batch:   980/ 1052, ite: 148840] train loss: 0.004124, tar: 0.000083 
[Epoch 566/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000043, l1: 0.000044, l2: 0.000076, l3: 0.000151, l4: 0.000303, l5: 0.000725, l6: 0.001813
[epoch: 567/1000, batch:     8/ 1052, ite: 148860] train loss: 0.004116, tar: 0.000082 
l0: 0.000046, l1: 0.000048, l2: 0.000066, l3: 0.000174, l4: 0.000427, l5: 0.000532, l6: 0.001465
[epoch: 567/1000, batch:    88/ 1052, ite: 148880] train loss: 0.004110, tar: 0.000082 
l0: 0.000072, l1: 0.000073, l2: 0.000112, l3: 0.000227, l4: 0.000458, l5: 0.001228, l6: 0.002165
[epoch: 567/1000, batch:   168/ 1052, ite: 148900] train loss: 0.004114, tar: 0.000082 
l0: 0.000058, l1: 0.000060, l2: 0.000070, l3: 0.000110, l4: 0.000273, l5: 0.000645, l6: 0.001188
[epoch: 567/1000, batch:   248/ 1052, ite: 148920] train loss: 0.004113, tar: 0.000082 
l0: 0.000050, l1: 0.000047, l2: 0.000069, l3: 0.000129, l4: 0.000349, l5: 0.000983, l6: 0.001396
[epoch: 567/1000, batch:   328/ 1052, ite: 148940] train loss: 0.004111, tar: 0.000082 
l0: 0.000073, l1: 0.000069, l2: 0.000093, l3: 0.000149, l4: 0.000422, l5: 0.001035, l6: 0.001989
[epoch: 567/1000, batch:   408/ 1052, ite: 148960] train loss: 0.004110, tar: 0.000082 
l0: 0.000033, l1: 0.000033, l2: 0.000077, l3: 0.000166, l4: 0.000309, l5: 0.001137, l6: 0.001714
[epoch: 567/1000, batch:   488/ 1052, ite: 148980] train loss: 0.004115, tar: 0.000082 
l0: 0.000030, l1: 0.000031, l2: 0.000042, l3: 0.000103, l4: 0.000476, l5: 0.001080, l6: 0.000984
[epoch: 567/1000, batch:   568/ 1052, ite: 149000] train loss: 0.004113, tar: 0.000082 
l0: 0.000140, l1: 0.000140, l2: 0.000185, l3: 0.000253, l4: 0.000562, l5: 0.001180, l6: 0.002476
[epoch: 567/1000, batch:   648/ 1052, ite: 149020] train loss: 0.004121, tar: 0.000082 
l0: 0.000036, l1: 0.000035, l2: 0.000052, l3: 0.000128, l4: 0.000363, l5: 0.000837, l6: 0.001648
[epoch: 567/1000, batch:   728/ 1052, ite: 149040] train loss: 0.004116, tar: 0.000082 
l0: 0.000174, l1: 0.000174, l2: 0.000209, l3: 0.000317, l4: 0.000552, l5: 0.001607, l6: 0.002471
[epoch: 567/1000, batch:   808/ 1052, ite: 149060] train loss: 0.004115, tar: 0.000082 
l0: 0.000066, l1: 0.000067, l2: 0.000098, l3: 0.000199, l4: 0.000350, l5: 0.000406, l6: 0.001996
[epoch: 567/1000, batch:   888/ 1052, ite: 149080] train loss: 0.004111, tar: 0.000082 
l0: 0.000034, l1: 0.000035, l2: 0.000048, l3: 0.000113, l4: 0.000200, l5: 0.000567, l6: 0.001286
[epoch: 567/1000, batch:   968/ 1052, ite: 149100] train loss: 0.004114, tar: 0.000082 
l0: 0.000209, l1: 0.000210, l2: 0.000298, l3: 0.000499, l4: 0.001031, l5: 0.001424, l6: 0.002048
[epoch: 567/1000, batch:  1048/ 1052, ite: 149120] train loss: 0.004119, tar: 0.000082 
[Epoch 567/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000131, l1: 0.000133, l2: 0.000163, l3: 0.000325, l4: 0.000522, l5: 0.001412, l6: 0.003536
[epoch: 568/1000, batch:    76/ 1052, ite: 149140] train loss: 0.004116, tar: 0.000082 
l0: 0.000035, l1: 0.000035, l2: 0.000068, l3: 0.000159, l4: 0.000389, l5: 0.001020, l6: 0.001590
[epoch: 568/1000, batch:   156/ 1052, ite: 149160] train loss: 0.004121, tar: 0.000082 
l0: 0.000062, l1: 0.000062, l2: 0.000104, l3: 0.000243, l4: 0.000650, l5: 0.001506, l6: 0.002256
[epoch: 568/1000, batch:   236/ 1052, ite: 149180] train loss: 0.004122, tar: 0.000082 
l0: 0.000096, l1: 0.000100, l2: 0.000140, l3: 0.000256, l4: 0.000835, l5: 0.001690, l6: 0.002836
[epoch: 568/1000, batch:   316/ 1052, ite: 149200] train loss: 0.004120, tar: 0.000083 
l0: 0.000072, l1: 0.000073, l2: 0.000180, l3: 0.000215, l4: 0.000382, l5: 0.000724, l6: 0.002041
[epoch: 568/1000, batch:   396/ 1052, ite: 149220] train loss: 0.004126, tar: 0.000083 
l0: 0.000100, l1: 0.000099, l2: 0.000147, l3: 0.000210, l4: 0.000537, l5: 0.000635, l6: 0.002450
[epoch: 568/1000, batch:   476/ 1052, ite: 149240] train loss: 0.004124, tar: 0.000083 
l0: 0.000050, l1: 0.000050, l2: 0.000075, l3: 0.000214, l4: 0.000473, l5: 0.001418, l6: 0.001205
[epoch: 568/1000, batch:   556/ 1052, ite: 149260] train loss: 0.004126, tar: 0.000083 
l0: 0.000073, l1: 0.000074, l2: 0.000108, l3: 0.000183, l4: 0.000585, l5: 0.001423, l6: 0.001558
[epoch: 568/1000, batch:   636/ 1052, ite: 149280] train loss: 0.004124, tar: 0.000083 
l0: 0.000039, l1: 0.000039, l2: 0.000075, l3: 0.000139, l4: 0.000344, l5: 0.000581, l6: 0.002636
[epoch: 568/1000, batch:   716/ 1052, ite: 149300] train loss: 0.004127, tar: 0.000082 
l0: 0.000018, l1: 0.000018, l2: 0.000046, l3: 0.000114, l4: 0.000323, l5: 0.001146, l6: 0.001136
[epoch: 568/1000, batch:   796/ 1052, ite: 149320] train loss: 0.004130, tar: 0.000082 
l0: 0.000037, l1: 0.000038, l2: 0.000070, l3: 0.000148, l4: 0.000275, l5: 0.000993, l6: 0.002055
[epoch: 568/1000, batch:   876/ 1052, ite: 149340] train loss: 0.004130, tar: 0.000082 
l0: 0.000054, l1: 0.000058, l2: 0.000100, l3: 0.000134, l4: 0.000309, l5: 0.000730, l6: 0.001210
[epoch: 568/1000, batch:   956/ 1052, ite: 149360] train loss: 0.004134, tar: 0.000082 
l0: 0.000080, l1: 0.000079, l2: 0.000096, l3: 0.000208, l4: 0.000431, l5: 0.000887, l6: 0.001991
[epoch: 568/1000, batch:  1036/ 1052, ite: 149380] train loss: 0.004133, tar: 0.000082 
[Epoch 568/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000084, l1: 0.000088, l2: 0.000091, l3: 0.000171, l4: 0.000388, l5: 0.000574, l6: 0.000946
[epoch: 569/1000, batch:    64/ 1052, ite: 149400] train loss: 0.004135, tar: 0.000083 
l0: 0.000058, l1: 0.000058, l2: 0.000089, l3: 0.000223, l4: 0.000466, l5: 0.001122, l6: 0.001463
[epoch: 569/1000, batch:   144/ 1052, ite: 149420] train loss: 0.004131, tar: 0.000083 
l0: 0.000153, l1: 0.000148, l2: 0.000190, l3: 0.000272, l4: 0.000865, l5: 0.000931, l6: 0.002280
[epoch: 569/1000, batch:   224/ 1052, ite: 149440] train loss: 0.004132, tar: 0.000083 
l0: 0.000081, l1: 0.000081, l2: 0.000144, l3: 0.000265, l4: 0.000519, l5: 0.000878, l6: 0.001656
[epoch: 569/1000, batch:   304/ 1052, ite: 149460] train loss: 0.004133, tar: 0.000083 
l0: 0.000131, l1: 0.000132, l2: 0.000173, l3: 0.000297, l4: 0.000619, l5: 0.001385, l6: 0.002192
[epoch: 569/1000, batch:   384/ 1052, ite: 149480] train loss: 0.004133, tar: 0.000083 
l0: 0.000051, l1: 0.000051, l2: 0.000135, l3: 0.000201, l4: 0.000371, l5: 0.000802, l6: 0.001504
[epoch: 569/1000, batch:   464/ 1052, ite: 149500] train loss: 0.004134, tar: 0.000083 
l0: 0.000016, l1: 0.000014, l2: 0.000046, l3: 0.000125, l4: 0.000612, l5: 0.001227, l6: 0.001910
[epoch: 569/1000, batch:   544/ 1052, ite: 149520] train loss: 0.004134, tar: 0.000083 
l0: 0.000084, l1: 0.000085, l2: 0.000127, l3: 0.000189, l4: 0.000345, l5: 0.000845, l6: 0.000951
[epoch: 569/1000, batch:   624/ 1052, ite: 149540] train loss: 0.004137, tar: 0.000083 
l0: 0.000064, l1: 0.000065, l2: 0.000069, l3: 0.000192, l4: 0.000355, l5: 0.001019, l6: 0.001582
[epoch: 569/1000, batch:   704/ 1052, ite: 149560] train loss: 0.004136, tar: 0.000083 
l0: 0.000146, l1: 0.000147, l2: 0.000219, l3: 0.000325, l4: 0.000634, l5: 0.000970, l6: 0.002150
[epoch: 569/1000, batch:   784/ 1052, ite: 149580] train loss: 0.004134, tar: 0.000083 
l0: 0.000053, l1: 0.000053, l2: 0.000078, l3: 0.000213, l4: 0.000424, l5: 0.001350, l6: 0.001505
[epoch: 569/1000, batch:   864/ 1052, ite: 149600] train loss: 0.004135, tar: 0.000083 
l0: 0.000098, l1: 0.000100, l2: 0.000169, l3: 0.000257, l4: 0.000531, l5: 0.001199, l6: 0.002132
[epoch: 569/1000, batch:   944/ 1052, ite: 149620] train loss: 0.004133, tar: 0.000083 
l0: 0.000013, l1: 0.000012, l2: 0.000039, l3: 0.000125, l4: 0.000321, l5: 0.000897, l6: 0.001291
[epoch: 569/1000, batch:  1024/ 1052, ite: 149640] train loss: 0.004129, tar: 0.000083 
[Epoch 569/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000113, l1: 0.000115, l2: 0.000158, l3: 0.000329, l4: 0.000705, l5: 0.002010, l6: 0.003042
[epoch: 570/1000, batch:    52/ 1052, ite: 149660] train loss: 0.004128, tar: 0.000083 
l0: 0.000188, l1: 0.000191, l2: 0.000275, l3: 0.000366, l4: 0.000739, l5: 0.001221, l6: 0.003526
[epoch: 570/1000, batch:   132/ 1052, ite: 149680] train loss: 0.004129, tar: 0.000083 
l0: 0.000090, l1: 0.000088, l2: 0.000154, l3: 0.000212, l4: 0.000466, l5: 0.001313, l6: 0.002502
[epoch: 570/1000, batch:   212/ 1052, ite: 149700] train loss: 0.004128, tar: 0.000083 
l0: 0.000043, l1: 0.000044, l2: 0.000053, l3: 0.000115, l4: 0.000284, l5: 0.001064, l6: 0.001956
[epoch: 570/1000, batch:   292/ 1052, ite: 149720] train loss: 0.004129, tar: 0.000083 
l0: 0.000125, l1: 0.000125, l2: 0.000151, l3: 0.000197, l4: 0.000581, l5: 0.001053, l6: 0.001878
[epoch: 570/1000, batch:   372/ 1052, ite: 149740] train loss: 0.004133, tar: 0.000083 
l0: 0.000031, l1: 0.000032, l2: 0.000050, l3: 0.000121, l4: 0.000384, l5: 0.001067, l6: 0.001191
[epoch: 570/1000, batch:   452/ 1052, ite: 149760] train loss: 0.004133, tar: 0.000083 
l0: 0.000174, l1: 0.000176, l2: 0.000209, l3: 0.000394, l4: 0.000889, l5: 0.001441, l6: 0.002626
[epoch: 570/1000, batch:   532/ 1052, ite: 149780] train loss: 0.004138, tar: 0.000083 
l0: 0.000080, l1: 0.000080, l2: 0.000119, l3: 0.000165, l4: 0.000469, l5: 0.001225, l6: 0.001812
[epoch: 570/1000, batch:   612/ 1052, ite: 149800] train loss: 0.004133, tar: 0.000083 
l0: 0.000032, l1: 0.000034, l2: 0.000089, l3: 0.000162, l4: 0.000292, l5: 0.001085, l6: 0.001949
[epoch: 570/1000, batch:   692/ 1052, ite: 149820] train loss: 0.004131, tar: 0.000083 
l0: 0.000108, l1: 0.000104, l2: 0.000174, l3: 0.000474, l4: 0.000837, l5: 0.001683, l6: 0.003175
[epoch: 570/1000, batch:   772/ 1052, ite: 149840] train loss: 0.004131, tar: 0.000083 
l0: 0.000038, l1: 0.000038, l2: 0.000076, l3: 0.000135, l4: 0.000304, l5: 0.000796, l6: 0.001264
[epoch: 570/1000, batch:   852/ 1052, ite: 149860] train loss: 0.004131, tar: 0.000083 
l0: 0.000073, l1: 0.000075, l2: 0.000105, l3: 0.000217, l4: 0.000644, l5: 0.001289, l6: 0.002347
[epoch: 570/1000, batch:   932/ 1052, ite: 149880] train loss: 0.004131, tar: 0.000083 
l0: 0.000050, l1: 0.000048, l2: 0.000095, l3: 0.000214, l4: 0.000469, l5: 0.000969, l6: 0.001281
[epoch: 570/1000, batch:  1012/ 1052, ite: 149900] train loss: 0.004130, tar: 0.000083 
[Epoch 570/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000025, l1: 0.000027, l2: 0.000067, l3: 0.000140, l4: 0.000383, l5: 0.001325, l6: 0.001812
[epoch: 571/1000, batch:    40/ 1052, ite: 149920] train loss: 0.004126, tar: 0.000083 
l0: 0.000072, l1: 0.000071, l2: 0.000095, l3: 0.000217, l4: 0.000652, l5: 0.001252, l6: 0.002219
[epoch: 571/1000, batch:   120/ 1052, ite: 149940] train loss: 0.004127, tar: 0.000083 
l0: 0.000052, l1: 0.000054, l2: 0.000096, l3: 0.000178, l4: 0.000514, l5: 0.001942, l6: 0.002629
[epoch: 571/1000, batch:   200/ 1052, ite: 149960] train loss: 0.004132, tar: 0.000083 
l0: 0.000067, l1: 0.000066, l2: 0.000102, l3: 0.000245, l4: 0.000651, l5: 0.001867, l6: 0.002986
[epoch: 571/1000, batch:   280/ 1052, ite: 149980] train loss: 0.004133, tar: 0.000083 
l0: 0.000048, l1: 0.000049, l2: 0.000076, l3: 0.000169, l4: 0.000505, l5: 0.000658, l6: 0.001739
[epoch: 571/1000, batch:   360/ 1052, ite: 150000] train loss: 0.004136, tar: 0.000083 
l0: 0.000085, l1: 0.000087, l2: 0.000143, l3: 0.000216, l4: 0.000685, l5: 0.001205, l6: 0.002747
[epoch: 571/1000, batch:   440/ 1052, ite: 150020] train loss: 0.004135, tar: 0.000083 
l0: 0.000201, l1: 0.000202, l2: 0.000295, l3: 0.000443, l4: 0.000946, l5: 0.001554, l6: 0.002960
[epoch: 571/1000, batch:   520/ 1052, ite: 150040] train loss: 0.004139, tar: 0.000083 
l0: 0.000009, l1: 0.000009, l2: 0.000020, l3: 0.000083, l4: 0.000451, l5: 0.000676, l6: 0.001581
[epoch: 571/1000, batch:   600/ 1052, ite: 150060] train loss: 0.004136, tar: 0.000083 
l0: 0.000097, l1: 0.000099, l2: 0.000153, l3: 0.000229, l4: 0.000615, l5: 0.001577, l6: 0.002667
[epoch: 571/1000, batch:   680/ 1052, ite: 150080] train loss: 0.004140, tar: 0.000083 
l0: 0.000038, l1: 0.000040, l2: 0.000052, l3: 0.000135, l4: 0.000234, l5: 0.000758, l6: 0.001359
[epoch: 571/1000, batch:   760/ 1052, ite: 150100] train loss: 0.004140, tar: 0.000083 
l0: 0.000041, l1: 0.000042, l2: 0.000069, l3: 0.000126, l4: 0.000443, l5: 0.000952, l6: 0.001538
[epoch: 571/1000, batch:   840/ 1052, ite: 150120] train loss: 0.004133, tar: 0.000082 
l0: 0.000018, l1: 0.000018, l2: 0.000024, l3: 0.000049, l4: 0.000138, l5: 0.000694, l6: 0.000930
[epoch: 571/1000, batch:   920/ 1052, ite: 150140] train loss: 0.004131, tar: 0.000082 
l0: 0.000060, l1: 0.000061, l2: 0.000079, l3: 0.000134, l4: 0.000297, l5: 0.000970, l6: 0.001502
[epoch: 571/1000, batch:  1000/ 1052, ite: 150160] train loss: 0.004129, tar: 0.000082 
[Epoch 571/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000024, l1: 0.000025, l2: 0.000079, l3: 0.000263, l4: 0.000728, l5: 0.001837, l6: 0.002425
[epoch: 572/1000, batch:    28/ 1052, ite: 150180] train loss: 0.004132, tar: 0.000082 
l0: 0.000127, l1: 0.000128, l2: 0.000197, l3: 0.000297, l4: 0.000540, l5: 0.001667, l6: 0.003145
[epoch: 572/1000, batch:   108/ 1052, ite: 150200] train loss: 0.004132, tar: 0.000082 
l0: 0.000133, l1: 0.000130, l2: 0.000188, l3: 0.000307, l4: 0.000562, l5: 0.001351, l6: 0.002401
[epoch: 572/1000, batch:   188/ 1052, ite: 150220] train loss: 0.004133, tar: 0.000082 
l0: 0.000008, l1: 0.000008, l2: 0.000048, l3: 0.000128, l4: 0.000334, l5: 0.000587, l6: 0.001539
[epoch: 572/1000, batch:   268/ 1052, ite: 150240] train loss: 0.004131, tar: 0.000082 
l0: 0.000029, l1: 0.000029, l2: 0.000082, l3: 0.000150, l4: 0.000309, l5: 0.000633, l6: 0.001539
[epoch: 572/1000, batch:   348/ 1052, ite: 150260] train loss: 0.004132, tar: 0.000082 
l0: 0.000092, l1: 0.000092, l2: 0.000118, l3: 0.000169, l4: 0.000383, l5: 0.000872, l6: 0.000936
[epoch: 572/1000, batch:   428/ 1052, ite: 150280] train loss: 0.004131, tar: 0.000082 
l0: 0.000018, l1: 0.000018, l2: 0.000034, l3: 0.000081, l4: 0.000332, l5: 0.000655, l6: 0.000983
[epoch: 572/1000, batch:   508/ 1052, ite: 150300] train loss: 0.004129, tar: 0.000082 
l0: 0.000040, l1: 0.000039, l2: 0.000088, l3: 0.000133, l4: 0.000304, l5: 0.001042, l6: 0.001331
[epoch: 572/1000, batch:   588/ 1052, ite: 150320] train loss: 0.004127, tar: 0.000082 
l0: 0.000100, l1: 0.000101, l2: 0.000120, l3: 0.000175, l4: 0.000443, l5: 0.001014, l6: 0.001553
[epoch: 572/1000, batch:   668/ 1052, ite: 150340] train loss: 0.004125, tar: 0.000082 
l0: 0.000114, l1: 0.000110, l2: 0.000145, l3: 0.000213, l4: 0.000531, l5: 0.001153, l6: 0.001676
[epoch: 572/1000, batch:   748/ 1052, ite: 150360] train loss: 0.004127, tar: 0.000082 
l0: 0.000098, l1: 0.000098, l2: 0.000188, l3: 0.000489, l4: 0.000892, l5: 0.001467, l6: 0.002978
[epoch: 572/1000, batch:   828/ 1052, ite: 150380] train loss: 0.004128, tar: 0.000082 
l0: 0.000102, l1: 0.000099, l2: 0.000217, l3: 0.000453, l4: 0.000993, l5: 0.003090, l6: 0.004436
[epoch: 572/1000, batch:   908/ 1052, ite: 150400] train loss: 0.004130, tar: 0.000082 
l0: 0.000097, l1: 0.000094, l2: 0.000218, l3: 0.000534, l4: 0.001291, l5: 0.002009, l6: 0.004540
[epoch: 572/1000, batch:   988/ 1052, ite: 150420] train loss: 0.004129, tar: 0.000082 
[Epoch 572/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000052, l1: 0.000053, l2: 0.000097, l3: 0.000228, l4: 0.000612, l5: 0.001287, l6: 0.001731
[epoch: 573/1000, batch:    16/ 1052, ite: 150440] train loss: 0.004130, tar: 0.000082 
l0: 0.000062, l1: 0.000061, l2: 0.000136, l3: 0.000296, l4: 0.000461, l5: 0.001059, l6: 0.001521
[epoch: 573/1000, batch:    96/ 1052, ite: 150460] train loss: 0.004129, tar: 0.000082 
l0: 0.000103, l1: 0.000102, l2: 0.000182, l3: 0.000364, l4: 0.000835, l5: 0.001547, l6: 0.003253
[epoch: 573/1000, batch:   176/ 1052, ite: 150480] train loss: 0.004132, tar: 0.000082 
l0: 0.000066, l1: 0.000064, l2: 0.000113, l3: 0.000233, l4: 0.000575, l5: 0.000596, l6: 0.001486
[epoch: 573/1000, batch:   256/ 1052, ite: 150500] train loss: 0.004132, tar: 0.000082 
l0: 0.000060, l1: 0.000061, l2: 0.000097, l3: 0.000212, l4: 0.000404, l5: 0.001034, l6: 0.001588
[epoch: 573/1000, batch:   336/ 1052, ite: 150520] train loss: 0.004131, tar: 0.000082 
l0: 0.000060, l1: 0.000059, l2: 0.000085, l3: 0.000194, l4: 0.000306, l5: 0.000871, l6: 0.001559
[epoch: 573/1000, batch:   416/ 1052, ite: 150540] train loss: 0.004133, tar: 0.000082 
l0: 0.000078, l1: 0.000078, l2: 0.000106, l3: 0.000198, l4: 0.000629, l5: 0.001657, l6: 0.002393
[epoch: 573/1000, batch:   496/ 1052, ite: 150560] train loss: 0.004129, tar: 0.000082 
l0: 0.000080, l1: 0.000082, l2: 0.000119, l3: 0.000236, l4: 0.000487, l5: 0.001034, l6: 0.001720
[epoch: 573/1000, batch:   576/ 1052, ite: 150580] train loss: 0.004128, tar: 0.000082 
l0: 0.000096, l1: 0.000099, l2: 0.000149, l3: 0.000230, l4: 0.000488, l5: 0.001512, l6: 0.003131
[epoch: 573/1000, batch:   656/ 1052, ite: 150600] train loss: 0.004131, tar: 0.000082 
l0: 0.000031, l1: 0.000031, l2: 0.000061, l3: 0.000205, l4: 0.000265, l5: 0.000845, l6: 0.001028
[epoch: 573/1000, batch:   736/ 1052, ite: 150620] train loss: 0.004132, tar: 0.000082 
l0: 0.000056, l1: 0.000056, l2: 0.000117, l3: 0.000178, l4: 0.000393, l5: 0.000773, l6: 0.001797
[epoch: 573/1000, batch:   816/ 1052, ite: 150640] train loss: 0.004130, tar: 0.000082 
l0: 0.000047, l1: 0.000047, l2: 0.000073, l3: 0.000168, l4: 0.000384, l5: 0.000969, l6: 0.000724
[epoch: 573/1000, batch:   896/ 1052, ite: 150660] train loss: 0.004130, tar: 0.000082 
l0: 0.000020, l1: 0.000019, l2: 0.000050, l3: 0.000142, l4: 0.000493, l5: 0.001267, l6: 0.001158
[epoch: 573/1000, batch:   976/ 1052, ite: 150680] train loss: 0.004125, tar: 0.000082 
[Epoch 573/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000030, l1: 0.000029, l2: 0.000097, l3: 0.000277, l4: 0.000301, l5: 0.000828, l6: 0.001329
[epoch: 574/1000, batch:     4/ 1052, ite: 150700] train loss: 0.004125, tar: 0.000082 
l0: 0.000167, l1: 0.000172, l2: 0.000243, l3: 0.000329, l4: 0.000692, l5: 0.001268, l6: 0.001896
[epoch: 574/1000, batch:    84/ 1052, ite: 150720] train loss: 0.004129, tar: 0.000082 
l0: 0.000031, l1: 0.000031, l2: 0.000064, l3: 0.000112, l4: 0.000356, l5: 0.000625, l6: 0.001054
[epoch: 574/1000, batch:   164/ 1052, ite: 150740] train loss: 0.004130, tar: 0.000082 
l0: 0.000054, l1: 0.000058, l2: 0.000075, l3: 0.000210, l4: 0.000472, l5: 0.000964, l6: 0.002081
[epoch: 574/1000, batch:   244/ 1052, ite: 150760] train loss: 0.004129, tar: 0.000082 
l0: 0.000033, l1: 0.000034, l2: 0.000067, l3: 0.000137, l4: 0.000293, l5: 0.000687, l6: 0.001037
[epoch: 574/1000, batch:   324/ 1052, ite: 150780] train loss: 0.004127, tar: 0.000082 
l0: 0.000062, l1: 0.000060, l2: 0.000107, l3: 0.000168, l4: 0.000463, l5: 0.000881, l6: 0.001780
[epoch: 574/1000, batch:   404/ 1052, ite: 150800] train loss: 0.004127, tar: 0.000082 
l0: 0.000060, l1: 0.000061, l2: 0.000075, l3: 0.000143, l4: 0.000342, l5: 0.000553, l6: 0.001260
[epoch: 574/1000, batch:   484/ 1052, ite: 150820] train loss: 0.004127, tar: 0.000082 
l0: 0.000102, l1: 0.000105, l2: 0.000175, l3: 0.000381, l4: 0.000682, l5: 0.001406, l6: 0.002109
[epoch: 574/1000, batch:   564/ 1052, ite: 150840] train loss: 0.004126, tar: 0.000082 
l0: 0.000024, l1: 0.000024, l2: 0.000036, l3: 0.000034, l4: 0.000175, l5: 0.000504, l6: 0.000535
[epoch: 574/1000, batch:   644/ 1052, ite: 150860] train loss: 0.004125, tar: 0.000082 
l0: 0.000176, l1: 0.000177, l2: 0.000239, l3: 0.000514, l4: 0.000942, l5: 0.001888, l6: 0.003238
[epoch: 574/1000, batch:   724/ 1052, ite: 150880] train loss: 0.004125, tar: 0.000081 
l0: 0.000131, l1: 0.000130, l2: 0.000239, l3: 0.000262, l4: 0.000427, l5: 0.000999, l6: 0.002401
[epoch: 574/1000, batch:   804/ 1052, ite: 150900] train loss: 0.004123, tar: 0.000081 
l0: 0.000018, l1: 0.000020, l2: 0.000034, l3: 0.000095, l4: 0.000163, l5: 0.000845, l6: 0.001545
[epoch: 574/1000, batch:   884/ 1052, ite: 150920] train loss: 0.004120, tar: 0.000081 
l0: 0.000011, l1: 0.000010, l2: 0.000052, l3: 0.000177, l4: 0.000810, l5: 0.001243, l6: 0.002004
[epoch: 574/1000, batch:   964/ 1052, ite: 150940] train loss: 0.004124, tar: 0.000081 
l0: 0.000060, l1: 0.000059, l2: 0.000106, l3: 0.000181, l4: 0.000331, l5: 0.000864, l6: 0.001637
[epoch: 574/1000, batch:  1044/ 1052, ite: 150960] train loss: 0.004124, tar: 0.000081 
[Epoch 574/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000044, l1: 0.000045, l2: 0.000084, l3: 0.000145, l4: 0.000443, l5: 0.000717, l6: 0.001291
[epoch: 575/1000, batch:    72/ 1052, ite: 150980] train loss: 0.004121, tar: 0.000081 
l0: 0.000076, l1: 0.000076, l2: 0.000140, l3: 0.000207, l4: 0.000518, l5: 0.001348, l6: 0.002677
[epoch: 575/1000, batch:   152/ 1052, ite: 151000] train loss: 0.004121, tar: 0.000081 
l0: 0.000035, l1: 0.000036, l2: 0.000041, l3: 0.000101, l4: 0.000400, l5: 0.000547, l6: 0.001181
[epoch: 575/1000, batch:   232/ 1052, ite: 151020] train loss: 0.004119, tar: 0.000081 
l0: 0.000071, l1: 0.000071, l2: 0.000121, l3: 0.000280, l4: 0.000545, l5: 0.000778, l6: 0.001619
[epoch: 575/1000, batch:   312/ 1052, ite: 151040] train loss: 0.004120, tar: 0.000081 
l0: 0.000124, l1: 0.000121, l2: 0.000156, l3: 0.000218, l4: 0.000495, l5: 0.000748, l6: 0.001843
[epoch: 575/1000, batch:   392/ 1052, ite: 151060] train loss: 0.004119, tar: 0.000081 
l0: 0.000025, l1: 0.000026, l2: 0.000049, l3: 0.000214, l4: 0.000527, l5: 0.000881, l6: 0.001447
[epoch: 575/1000, batch:   472/ 1052, ite: 151080] train loss: 0.004118, tar: 0.000081 
l0: 0.000055, l1: 0.000054, l2: 0.000082, l3: 0.000125, l4: 0.000543, l5: 0.000708, l6: 0.001550
[epoch: 575/1000, batch:   552/ 1052, ite: 151100] train loss: 0.004118, tar: 0.000081 
l0: 0.000105, l1: 0.000107, l2: 0.000131, l3: 0.000217, l4: 0.000397, l5: 0.000735, l6: 0.001450
[epoch: 575/1000, batch:   632/ 1052, ite: 151120] train loss: 0.004119, tar: 0.000081 
l0: 0.000052, l1: 0.000054, l2: 0.000078, l3: 0.000126, l4: 0.000472, l5: 0.000741, l6: 0.002032
[epoch: 575/1000, batch:   712/ 1052, ite: 151140] train loss: 0.004122, tar: 0.000081 
l0: 0.000010, l1: 0.000010, l2: 0.000031, l3: 0.000058, l4: 0.000208, l5: 0.000588, l6: 0.000989
[epoch: 575/1000, batch:   792/ 1052, ite: 151160] train loss: 0.004120, tar: 0.000081 
l0: 0.000187, l1: 0.000192, l2: 0.000222, l3: 0.000252, l4: 0.000351, l5: 0.001293, l6: 0.002269
[epoch: 575/1000, batch:   872/ 1052, ite: 151180] train loss: 0.004119, tar: 0.000081 
l0: 0.000116, l1: 0.000117, l2: 0.000161, l3: 0.000322, l4: 0.000590, l5: 0.001059, l6: 0.002333
[epoch: 575/1000, batch:   952/ 1052, ite: 151200] train loss: 0.004121, tar: 0.000081 
l0: 0.000305, l1: 0.000293, l2: 0.000365, l3: 0.000403, l4: 0.000464, l5: 0.000983, l6: 0.001540
[epoch: 575/1000, batch:  1032/ 1052, ite: 151220] train loss: 0.004120, tar: 0.000081 
[Epoch 575/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000172, l1: 0.000174, l2: 0.000264, l3: 0.000506, l4: 0.000985, l5: 0.002123, l6: 0.003504
[epoch: 576/1000, batch:    60/ 1052, ite: 151240] train loss: 0.004124, tar: 0.000081 
l0: 0.000009, l1: 0.000009, l2: 0.000044, l3: 0.000115, l4: 0.000396, l5: 0.000513, l6: 0.000872
[epoch: 576/1000, batch:   140/ 1052, ite: 151260] train loss: 0.004122, tar: 0.000081 
l0: 0.000021, l1: 0.000022, l2: 0.000063, l3: 0.000215, l4: 0.000725, l5: 0.001094, l6: 0.002670
[epoch: 576/1000, batch:   220/ 1052, ite: 151280] train loss: 0.004122, tar: 0.000081 
l0: 0.000017, l1: 0.000017, l2: 0.000052, l3: 0.000127, l4: 0.000379, l5: 0.000786, l6: 0.001279
[epoch: 576/1000, batch:   300/ 1052, ite: 151300] train loss: 0.004122, tar: 0.000081 
l0: 0.000066, l1: 0.000067, l2: 0.000101, l3: 0.000225, l4: 0.000348, l5: 0.001312, l6: 0.001840
[epoch: 576/1000, batch:   380/ 1052, ite: 151320] train loss: 0.004123, tar: 0.000081 
l0: 0.000132, l1: 0.000130, l2: 0.000170, l3: 0.000288, l4: 0.000528, l5: 0.001692, l6: 0.002840
[epoch: 576/1000, batch:   460/ 1052, ite: 151340] train loss: 0.004123, tar: 0.000081 
l0: 0.000035, l1: 0.000035, l2: 0.000092, l3: 0.000193, l4: 0.000389, l5: 0.001154, l6: 0.001990
[epoch: 576/1000, batch:   540/ 1052, ite: 151360] train loss: 0.004121, tar: 0.000081 
l0: 0.000033, l1: 0.000032, l2: 0.000052, l3: 0.000093, l4: 0.000235, l5: 0.000698, l6: 0.000977
[epoch: 576/1000, batch:   620/ 1052, ite: 151380] train loss: 0.004121, tar: 0.000081 
l0: 0.000043, l1: 0.000042, l2: 0.000071, l3: 0.000102, l4: 0.000312, l5: 0.000904, l6: 0.001923
[epoch: 576/1000, batch:   700/ 1052, ite: 151400] train loss: 0.004121, tar: 0.000081 
l0: 0.000028, l1: 0.000028, l2: 0.000035, l3: 0.000131, l4: 0.000354, l5: 0.000787, l6: 0.001291
[epoch: 576/1000, batch:   780/ 1052, ite: 151420] train loss: 0.004122, tar: 0.000081 
l0: 0.000123, l1: 0.000125, l2: 0.000169, l3: 0.000314, l4: 0.000953, l5: 0.001640, l6: 0.003879
[epoch: 576/1000, batch:   860/ 1052, ite: 151440] train loss: 0.004122, tar: 0.000081 
l0: 0.000020, l1: 0.000020, l2: 0.000023, l3: 0.000060, l4: 0.000288, l5: 0.000526, l6: 0.000513
[epoch: 576/1000, batch:   940/ 1052, ite: 151460] train loss: 0.004121, tar: 0.000081 
l0: 0.000037, l1: 0.000038, l2: 0.000038, l3: 0.000149, l4: 0.000304, l5: 0.000831, l6: 0.001176
[epoch: 576/1000, batch:  1020/ 1052, ite: 151480] train loss: 0.004120, tar: 0.000081 
[Epoch 576/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000208, l1: 0.000205, l2: 0.000338, l3: 0.000448, l4: 0.000732, l5: 0.001223, l6: 0.002630
[epoch: 577/1000, batch:    48/ 1052, ite: 151500] train loss: 0.004120, tar: 0.000081 
l0: 0.000044, l1: 0.000045, l2: 0.000076, l3: 0.000166, l4: 0.000581, l5: 0.001522, l6: 0.002268
[epoch: 577/1000, batch:   128/ 1052, ite: 151520] train loss: 0.004118, tar: 0.000081 
l0: 0.000041, l1: 0.000041, l2: 0.000053, l3: 0.000115, l4: 0.000381, l5: 0.000927, l6: 0.001512
[epoch: 577/1000, batch:   208/ 1052, ite: 151540] train loss: 0.004116, tar: 0.000081 
l0: 0.000041, l1: 0.000040, l2: 0.000075, l3: 0.000139, l4: 0.000247, l5: 0.000984, l6: 0.001594
[epoch: 577/1000, batch:   288/ 1052, ite: 151560] train loss: 0.004116, tar: 0.000081 
l0: 0.000047, l1: 0.000046, l2: 0.000111, l3: 0.000207, l4: 0.000344, l5: 0.000632, l6: 0.001888
[epoch: 577/1000, batch:   368/ 1052, ite: 151580] train loss: 0.004115, tar: 0.000081 
l0: 0.000071, l1: 0.000073, l2: 0.000102, l3: 0.000148, l4: 0.000557, l5: 0.001170, l6: 0.001932
[epoch: 577/1000, batch:   448/ 1052, ite: 151600] train loss: 0.004115, tar: 0.000081 
l0: 0.000018, l1: 0.000019, l2: 0.000060, l3: 0.000140, l4: 0.000398, l5: 0.000832, l6: 0.001273
[epoch: 577/1000, batch:   528/ 1052, ite: 151620] train loss: 0.004114, tar: 0.000081 
l0: 0.000105, l1: 0.000108, l2: 0.000169, l3: 0.000298, l4: 0.000691, l5: 0.001220, l6: 0.002311
[epoch: 577/1000, batch:   608/ 1052, ite: 151640] train loss: 0.004113, tar: 0.000081 
l0: 0.000064, l1: 0.000062, l2: 0.000090, l3: 0.000142, l4: 0.000437, l5: 0.001115, l6: 0.001087
[epoch: 577/1000, batch:   688/ 1052, ite: 151660] train loss: 0.004112, tar: 0.000080 
l0: 0.000073, l1: 0.000076, l2: 0.000075, l3: 0.000148, l4: 0.000391, l5: 0.000852, l6: 0.002057
[epoch: 577/1000, batch:   768/ 1052, ite: 151680] train loss: 0.004113, tar: 0.000081 
l0: 0.000037, l1: 0.000036, l2: 0.000060, l3: 0.000156, l4: 0.000355, l5: 0.000914, l6: 0.001836
[epoch: 577/1000, batch:   848/ 1052, ite: 151700] train loss: 0.004112, tar: 0.000080 
l0: 0.000040, l1: 0.000040, l2: 0.000067, l3: 0.000147, l4: 0.000349, l5: 0.000888, l6: 0.001369
[epoch: 577/1000, batch:   928/ 1052, ite: 151720] train loss: 0.004115, tar: 0.000081 
l0: 0.000111, l1: 0.000112, l2: 0.000174, l3: 0.000314, l4: 0.000677, l5: 0.001699, l6: 0.001646
[epoch: 577/1000, batch:  1008/ 1052, ite: 151740] train loss: 0.004114, tar: 0.000081 
[Epoch 577/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000002, l1: 0.000002, l2: 0.000008, l3: 0.000031, l4: 0.000216, l5: 0.000240, l6: 0.000509
[epoch: 578/1000, batch:    36/ 1052, ite: 151760] train loss: 0.004116, tar: 0.000081 
l0: 0.000054, l1: 0.000051, l2: 0.000124, l3: 0.000304, l4: 0.000611, l5: 0.000921, l6: 0.001440
[epoch: 578/1000, batch:   116/ 1052, ite: 151780] train loss: 0.004116, tar: 0.000081 
l0: 0.000106, l1: 0.000105, l2: 0.000093, l3: 0.000108, l4: 0.000378, l5: 0.000830, l6: 0.001331
[epoch: 578/1000, batch:   196/ 1052, ite: 151800] train loss: 0.004114, tar: 0.000081 
l0: 0.000063, l1: 0.000063, l2: 0.000090, l3: 0.000225, l4: 0.000390, l5: 0.001278, l6: 0.002330
[epoch: 578/1000, batch:   276/ 1052, ite: 151820] train loss: 0.004115, tar: 0.000081 
l0: 0.000039, l1: 0.000039, l2: 0.000050, l3: 0.000103, l4: 0.000222, l5: 0.000955, l6: 0.002122
[epoch: 578/1000, batch:   356/ 1052, ite: 151840] train loss: 0.004114, tar: 0.000081 
l0: 0.000084, l1: 0.000086, l2: 0.000138, l3: 0.000207, l4: 0.000455, l5: 0.001056, l6: 0.001706
[epoch: 578/1000, batch:   436/ 1052, ite: 151860] train loss: 0.004117, tar: 0.000081 
l0: 0.000153, l1: 0.000153, l2: 0.000232, l3: 0.000353, l4: 0.000740, l5: 0.001603, l6: 0.003379
[epoch: 578/1000, batch:   516/ 1052, ite: 151880] train loss: 0.004117, tar: 0.000081 
l0: 0.000135, l1: 0.000135, l2: 0.000189, l3: 0.000320, l4: 0.000655, l5: 0.001287, l6: 0.002625
[epoch: 578/1000, batch:   596/ 1052, ite: 151900] train loss: 0.004116, tar: 0.000081 
l0: 0.000087, l1: 0.000088, l2: 0.000141, l3: 0.000195, l4: 0.000340, l5: 0.000896, l6: 0.001293
[epoch: 578/1000, batch:   676/ 1052, ite: 151920] train loss: 0.004118, tar: 0.000081 
l0: 0.000202, l1: 0.000203, l2: 0.000231, l3: 0.000432, l4: 0.000880, l5: 0.001755, l6: 0.003568
[epoch: 578/1000, batch:   756/ 1052, ite: 151940] train loss: 0.004117, tar: 0.000081 
l0: 0.000027, l1: 0.000028, l2: 0.000061, l3: 0.000152, l4: 0.000334, l5: 0.001172, l6: 0.001529
[epoch: 578/1000, batch:   836/ 1052, ite: 151960] train loss: 0.004116, tar: 0.000081 
l0: 0.000097, l1: 0.000099, l2: 0.000141, l3: 0.000292, l4: 0.000620, l5: 0.001535, l6: 0.003056
[epoch: 578/1000, batch:   916/ 1052, ite: 151980] train loss: 0.004117, tar: 0.000081 
l0: 0.000047, l1: 0.000050, l2: 0.000059, l3: 0.000115, l4: 0.000406, l5: 0.000650, l6: 0.001519
[epoch: 578/1000, batch:   996/ 1052, ite: 152000] train loss: 0.004118, tar: 0.000081 
[Epoch 578/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000028, l1: 0.000028, l2: 0.000077, l3: 0.000291, l4: 0.001008, l5: 0.002184, l6: 0.004143
[epoch: 579/1000, batch:    24/ 1052, ite: 152020] train loss: 0.004117, tar: 0.000081 
l0: 0.000058, l1: 0.000061, l2: 0.000084, l3: 0.000176, l4: 0.000401, l5: 0.001292, l6: 0.001804
[epoch: 579/1000, batch:   104/ 1052, ite: 152040] train loss: 0.004118, tar: 0.000081 
l0: 0.000195, l1: 0.000197, l2: 0.000298, l3: 0.000517, l4: 0.000719, l5: 0.002175, l6: 0.003897
[epoch: 579/1000, batch:   184/ 1052, ite: 152060] train loss: 0.004118, tar: 0.000081 
l0: 0.000091, l1: 0.000090, l2: 0.000115, l3: 0.000226, l4: 0.000237, l5: 0.000605, l6: 0.001061
[epoch: 579/1000, batch:   264/ 1052, ite: 152080] train loss: 0.004116, tar: 0.000081 
l0: 0.000029, l1: 0.000029, l2: 0.000091, l3: 0.000236, l4: 0.000444, l5: 0.001257, l6: 0.001862
[epoch: 579/1000, batch:   344/ 1052, ite: 152100] train loss: 0.004114, tar: 0.000081 
l0: 0.000073, l1: 0.000073, l2: 0.000156, l3: 0.000243, l4: 0.000395, l5: 0.000689, l6: 0.001545
[epoch: 579/1000, batch:   424/ 1052, ite: 152120] train loss: 0.004117, tar: 0.000081 
l0: 0.000085, l1: 0.000083, l2: 0.000149, l3: 0.000279, l4: 0.000494, l5: 0.000853, l6: 0.001460
[epoch: 579/1000, batch:   504/ 1052, ite: 152140] train loss: 0.004115, tar: 0.000081 
l0: 0.000103, l1: 0.000103, l2: 0.000133, l3: 0.000239, l4: 0.000424, l5: 0.001074, l6: 0.002162
[epoch: 579/1000, batch:   584/ 1052, ite: 152160] train loss: 0.004119, tar: 0.000081 
l0: 0.000020, l1: 0.000021, l2: 0.000042, l3: 0.000094, l4: 0.000361, l5: 0.000817, l6: 0.001046
[epoch: 579/1000, batch:   664/ 1052, ite: 152180] train loss: 0.004118, tar: 0.000081 
l0: 0.000052, l1: 0.000049, l2: 0.000150, l3: 0.000340, l4: 0.000829, l5: 0.001295, l6: 0.001874
[epoch: 579/1000, batch:   744/ 1052, ite: 152200] train loss: 0.004118, tar: 0.000081 
l0: 0.000056, l1: 0.000055, l2: 0.000150, l3: 0.000292, l4: 0.000695, l5: 0.001811, l6: 0.003882
[epoch: 579/1000, batch:   824/ 1052, ite: 152220] train loss: 0.004120, tar: 0.000081 
l0: 0.000126, l1: 0.000130, l2: 0.000190, l3: 0.000287, l4: 0.000485, l5: 0.000960, l6: 0.002770
[epoch: 579/1000, batch:   904/ 1052, ite: 152240] train loss: 0.004120, tar: 0.000081 
l0: 0.000027, l1: 0.000025, l2: 0.000073, l3: 0.000126, l4: 0.000315, l5: 0.001018, l6: 0.001852
[epoch: 579/1000, batch:   984/ 1052, ite: 152260] train loss: 0.004117, tar: 0.000081 
[Epoch 579/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000006, l1: 0.000006, l2: 0.000029, l3: 0.000083, l4: 0.000376, l5: 0.000935, l6: 0.001638
[epoch: 580/1000, batch:    12/ 1052, ite: 152280] train loss: 0.004116, tar: 0.000081 
l0: 0.000097, l1: 0.000096, l2: 0.000154, l3: 0.000219, l4: 0.000359, l5: 0.001004, l6: 0.001477
[epoch: 580/1000, batch:    92/ 1052, ite: 152300] train loss: 0.004115, tar: 0.000081 
l0: 0.000063, l1: 0.000062, l2: 0.000121, l3: 0.000176, l4: 0.000301, l5: 0.000652, l6: 0.001077
[epoch: 580/1000, batch:   172/ 1052, ite: 152320] train loss: 0.004114, tar: 0.000081 
l0: 0.000094, l1: 0.000095, l2: 0.000127, l3: 0.000237, l4: 0.000404, l5: 0.001479, l6: 0.002009
[epoch: 580/1000, batch:   252/ 1052, ite: 152340] train loss: 0.004114, tar: 0.000081 
l0: 0.000012, l1: 0.000012, l2: 0.000032, l3: 0.000099, l4: 0.000329, l5: 0.000551, l6: 0.000805
[epoch: 580/1000, batch:   332/ 1052, ite: 152360] train loss: 0.004114, tar: 0.000081 
l0: 0.000193, l1: 0.000195, l2: 0.000241, l3: 0.000387, l4: 0.000713, l5: 0.002158, l6: 0.003309
[epoch: 580/1000, batch:   412/ 1052, ite: 152380] train loss: 0.004116, tar: 0.000081 
l0: 0.000068, l1: 0.000070, l2: 0.000128, l3: 0.000262, l4: 0.000479, l5: 0.000892, l6: 0.001341
[epoch: 580/1000, batch:   492/ 1052, ite: 152400] train loss: 0.004116, tar: 0.000081 
l0: 0.000068, l1: 0.000069, l2: 0.000090, l3: 0.000140, l4: 0.000390, l5: 0.000724, l6: 0.001416
[epoch: 580/1000, batch:   572/ 1052, ite: 152420] train loss: 0.004116, tar: 0.000081 
l0: 0.000046, l1: 0.000045, l2: 0.000057, l3: 0.000110, l4: 0.000264, l5: 0.000793, l6: 0.001092
[epoch: 580/1000, batch:   652/ 1052, ite: 152440] train loss: 0.004115, tar: 0.000081 
l0: 0.000038, l1: 0.000037, l2: 0.000076, l3: 0.000170, l4: 0.000313, l5: 0.000792, l6: 0.001375
[epoch: 580/1000, batch:   732/ 1052, ite: 152460] train loss: 0.004115, tar: 0.000081 
l0: 0.000078, l1: 0.000082, l2: 0.000135, l3: 0.000150, l4: 0.000463, l5: 0.001319, l6: 0.002460
[epoch: 580/1000, batch:   812/ 1052, ite: 152480] train loss: 0.004117, tar: 0.000081 
l0: 0.000090, l1: 0.000089, l2: 0.000117, l3: 0.000261, l4: 0.000592, l5: 0.000659, l6: 0.001651
[epoch: 580/1000, batch:   892/ 1052, ite: 152500] train loss: 0.004117, tar: 0.000081 
l0: 0.000046, l1: 0.000045, l2: 0.000061, l3: 0.000137, l4: 0.000425, l5: 0.000827, l6: 0.001533
[epoch: 580/1000, batch:   972/ 1052, ite: 152520] train loss: 0.004116, tar: 0.000081 
l0: 0.000057, l1: 0.000055, l2: 0.000111, l3: 0.000204, l4: 0.000335, l5: 0.001335, l6: 0.002916
[epoch: 580/1000, batch:  1052/ 1052, ite: 152540] train loss: 0.004115, tar: 0.000081 
[Epoch 580/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000014, l1: 0.000013, l2: 0.000046, l3: 0.000137, l4: 0.000321, l5: 0.000844, l6: 0.002216
[epoch: 581/1000, batch:    80/ 1052, ite: 152560] train loss: 0.004115, tar: 0.000081 
l0: 0.000049, l1: 0.000050, l2: 0.000114, l3: 0.000243, l4: 0.000716, l5: 0.001794, l6: 0.003088
[epoch: 581/1000, batch:   160/ 1052, ite: 152580] train loss: 0.004116, tar: 0.000081 
l0: 0.000074, l1: 0.000075, l2: 0.000084, l3: 0.000153, l4: 0.000316, l5: 0.001105, l6: 0.001820
[epoch: 581/1000, batch:   240/ 1052, ite: 152600] train loss: 0.004117, tar: 0.000081 
l0: 0.000250, l1: 0.000250, l2: 0.000281, l3: 0.000440, l4: 0.000744, l5: 0.001314, l6: 0.003716
[epoch: 581/1000, batch:   320/ 1052, ite: 152620] train loss: 0.004120, tar: 0.000081 
l0: 0.000057, l1: 0.000058, l2: 0.000131, l3: 0.000198, l4: 0.000420, l5: 0.000719, l6: 0.001276
[epoch: 581/1000, batch:   400/ 1052, ite: 152640] train loss: 0.004120, tar: 0.000081 
l0: 0.000054, l1: 0.000054, l2: 0.000127, l3: 0.000237, l4: 0.000647, l5: 0.001014, l6: 0.001984
[epoch: 581/1000, batch:   480/ 1052, ite: 152660] train loss: 0.004120, tar: 0.000081 
l0: 0.000061, l1: 0.000062, l2: 0.000101, l3: 0.000135, l4: 0.000298, l5: 0.000678, l6: 0.001835
[epoch: 581/1000, batch:   560/ 1052, ite: 152680] train loss: 0.004119, tar: 0.000081 
l0: 0.000062, l1: 0.000062, l2: 0.000095, l3: 0.000139, l4: 0.000254, l5: 0.000932, l6: 0.001914
[epoch: 581/1000, batch:   640/ 1052, ite: 152700] train loss: 0.004121, tar: 0.000081 
l0: 0.000058, l1: 0.000057, l2: 0.000093, l3: 0.000164, l4: 0.000355, l5: 0.000777, l6: 0.001524
[epoch: 581/1000, batch:   720/ 1052, ite: 152720] train loss: 0.004120, tar: 0.000081 
l0: 0.000046, l1: 0.000046, l2: 0.000052, l3: 0.000080, l4: 0.000323, l5: 0.000673, l6: 0.000857
[epoch: 581/1000, batch:   800/ 1052, ite: 152740] train loss: 0.004119, tar: 0.000081 
l0: 0.000064, l1: 0.000063, l2: 0.000088, l3: 0.000155, l4: 0.000366, l5: 0.001083, l6: 0.002706
[epoch: 581/1000, batch:   880/ 1052, ite: 152760] train loss: 0.004120, tar: 0.000081 
l0: 0.000037, l1: 0.000038, l2: 0.000078, l3: 0.000169, l4: 0.000457, l5: 0.000931, l6: 0.000966
[epoch: 581/1000, batch:   960/ 1052, ite: 152780] train loss: 0.004118, tar: 0.000081 
l0: 0.000090, l1: 0.000088, l2: 0.000134, l3: 0.000256, l4: 0.000463, l5: 0.001159, l6: 0.003075
[epoch: 581/1000, batch:  1040/ 1052, ite: 152800] train loss: 0.004117, tar: 0.000081 
[Epoch 581/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000114, l1: 0.000102, l2: 0.000170, l3: 0.000256, l4: 0.000498, l5: 0.001925, l6: 0.002700
[epoch: 582/1000, batch:    68/ 1052, ite: 152820] train loss: 0.004116, tar: 0.000081 
l0: 0.000023, l1: 0.000023, l2: 0.000048, l3: 0.000068, l4: 0.000299, l5: 0.000638, l6: 0.001228
[epoch: 582/1000, batch:   148/ 1052, ite: 152840] train loss: 0.004115, tar: 0.000080 
l0: 0.000043, l1: 0.000042, l2: 0.000070, l3: 0.000127, l4: 0.000263, l5: 0.000605, l6: 0.000958
[epoch: 582/1000, batch:   228/ 1052, ite: 152860] train loss: 0.004114, tar: 0.000080 
l0: 0.000141, l1: 0.000140, l2: 0.000225, l3: 0.000319, l4: 0.000474, l5: 0.000958, l6: 0.001784
[epoch: 582/1000, batch:   308/ 1052, ite: 152880] train loss: 0.004114, tar: 0.000081 
l0: 0.000129, l1: 0.000135, l2: 0.000163, l3: 0.000163, l4: 0.000331, l5: 0.001273, l6: 0.001928
[epoch: 582/1000, batch:   388/ 1052, ite: 152900] train loss: 0.004114, tar: 0.000081 
l0: 0.000028, l1: 0.000029, l2: 0.000052, l3: 0.000274, l4: 0.000457, l5: 0.000964, l6: 0.001689
[epoch: 582/1000, batch:   468/ 1052, ite: 152920] train loss: 0.004114, tar: 0.000081 
l0: 0.000071, l1: 0.000070, l2: 0.000129, l3: 0.000281, l4: 0.000374, l5: 0.000980, l6: 0.001451
[epoch: 582/1000, batch:   548/ 1052, ite: 152940] train loss: 0.004115, tar: 0.000081 
l0: 0.000029, l1: 0.000030, l2: 0.000044, l3: 0.000090, l4: 0.000295, l5: 0.000684, l6: 0.000861
[epoch: 582/1000, batch:   628/ 1052, ite: 152960] train loss: 0.004115, tar: 0.000081 
l0: 0.000089, l1: 0.000096, l2: 0.000135, l3: 0.000216, l4: 0.000448, l5: 0.000968, l6: 0.001833
[epoch: 582/1000, batch:   708/ 1052, ite: 152980] train loss: 0.004116, tar: 0.000081 
l0: 0.000124, l1: 0.000127, l2: 0.000140, l3: 0.000232, l4: 0.000470, l5: 0.000732, l6: 0.001444
[epoch: 582/1000, batch:   788/ 1052, ite: 153000] train loss: 0.004116, tar: 0.000081 
l0: 0.000100, l1: 0.000101, l2: 0.000103, l3: 0.000188, l4: 0.000376, l5: 0.000916, l6: 0.001641
[epoch: 582/1000, batch:   868/ 1052, ite: 153020] train loss: 0.004115, tar: 0.000081 
l0: 0.000059, l1: 0.000060, l2: 0.000088, l3: 0.000139, l4: 0.000222, l5: 0.000654, l6: 0.001650
[epoch: 582/1000, batch:   948/ 1052, ite: 153040] train loss: 0.004113, tar: 0.000081 
l0: 0.000071, l1: 0.000062, l2: 0.000136, l3: 0.000295, l4: 0.000354, l5: 0.001255, l6: 0.002056
[epoch: 582/1000, batch:  1028/ 1052, ite: 153060] train loss: 0.004116, tar: 0.000081 
[Epoch 582/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000027, l1: 0.000028, l2: 0.000043, l3: 0.000155, l4: 0.000247, l5: 0.000833, l6: 0.000930
[epoch: 583/1000, batch:    56/ 1052, ite: 153080] train loss: 0.004115, tar: 0.000080 
l0: 0.000137, l1: 0.000137, l2: 0.000173, l3: 0.000271, l4: 0.000592, l5: 0.001479, l6: 0.002704
[epoch: 583/1000, batch:   136/ 1052, ite: 153100] train loss: 0.004113, tar: 0.000080 
l0: 0.000009, l1: 0.000009, l2: 0.000039, l3: 0.000102, l4: 0.000430, l5: 0.000913, l6: 0.001398
[epoch: 583/1000, batch:   216/ 1052, ite: 153120] train loss: 0.004112, tar: 0.000080 
l0: 0.000031, l1: 0.000032, l2: 0.000066, l3: 0.000166, l4: 0.000293, l5: 0.001045, l6: 0.001253
[epoch: 583/1000, batch:   296/ 1052, ite: 153140] train loss: 0.004112, tar: 0.000080 
l0: 0.000031, l1: 0.000030, l2: 0.000054, l3: 0.000140, l4: 0.000487, l5: 0.000787, l6: 0.001555
[epoch: 583/1000, batch:   376/ 1052, ite: 153160] train loss: 0.004111, tar: 0.000080 
l0: 0.000118, l1: 0.000119, l2: 0.000151, l3: 0.000273, l4: 0.000549, l5: 0.001245, l6: 0.001535
[epoch: 583/1000, batch:   456/ 1052, ite: 153180] train loss: 0.004112, tar: 0.000080 
l0: 0.000052, l1: 0.000054, l2: 0.000084, l3: 0.000217, l4: 0.000532, l5: 0.001276, l6: 0.001373
[epoch: 583/1000, batch:   536/ 1052, ite: 153200] train loss: 0.004112, tar: 0.000080 
l0: 0.000064, l1: 0.000064, l2: 0.000107, l3: 0.000273, l4: 0.000635, l5: 0.001584, l6: 0.002937
[epoch: 583/1000, batch:   616/ 1052, ite: 153220] train loss: 0.004111, tar: 0.000080 
l0: 0.000049, l1: 0.000048, l2: 0.000091, l3: 0.000186, l4: 0.000394, l5: 0.001129, l6: 0.001346
[epoch: 583/1000, batch:   696/ 1052, ite: 153240] train loss: 0.004113, tar: 0.000080 
l0: 0.000222, l1: 0.000230, l2: 0.000283, l3: 0.000415, l4: 0.000674, l5: 0.001553, l6: 0.004132
[epoch: 583/1000, batch:   776/ 1052, ite: 153260] train loss: 0.004113, tar: 0.000080 
l0: 0.000078, l1: 0.000077, l2: 0.000114, l3: 0.000199, l4: 0.000352, l5: 0.000741, l6: 0.001629
[epoch: 583/1000, batch:   856/ 1052, ite: 153280] train loss: 0.004113, tar: 0.000080 
l0: 0.000038, l1: 0.000038, l2: 0.000064, l3: 0.000183, l4: 0.000324, l5: 0.000919, l6: 0.001315
[epoch: 583/1000, batch:   936/ 1052, ite: 153300] train loss: 0.004112, tar: 0.000080 
l0: 0.000043, l1: 0.000045, l2: 0.000066, l3: 0.000120, l4: 0.000463, l5: 0.000885, l6: 0.001452
[epoch: 583/1000, batch:  1016/ 1052, ite: 153320] train loss: 0.004114, tar: 0.000080 
[Epoch 583/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000069, l1: 0.000070, l2: 0.000101, l3: 0.000172, l4: 0.000375, l5: 0.001158, l6: 0.001791
[epoch: 584/1000, batch:    44/ 1052, ite: 153340] train loss: 0.004115, tar: 0.000080 
l0: 0.000071, l1: 0.000072, l2: 0.000108, l3: 0.000208, l4: 0.000496, l5: 0.000807, l6: 0.001455
[epoch: 584/1000, batch:   124/ 1052, ite: 153360] train loss: 0.004115, tar: 0.000081 
l0: 0.000043, l1: 0.000044, l2: 0.000061, l3: 0.000124, l4: 0.000375, l5: 0.000848, l6: 0.001324
[epoch: 584/1000, batch:   204/ 1052, ite: 153380] train loss: 0.004115, tar: 0.000080 
l0: 0.000043, l1: 0.000044, l2: 0.000059, l3: 0.000127, l4: 0.000254, l5: 0.000530, l6: 0.001018
[epoch: 584/1000, batch:   284/ 1052, ite: 153400] train loss: 0.004116, tar: 0.000080 
l0: 0.000077, l1: 0.000076, l2: 0.000145, l3: 0.000290, l4: 0.000618, l5: 0.000987, l6: 0.003015
[epoch: 584/1000, batch:   364/ 1052, ite: 153420] train loss: 0.004117, tar: 0.000080 
l0: 0.000069, l1: 0.000066, l2: 0.000102, l3: 0.000247, l4: 0.000427, l5: 0.000725, l6: 0.001299
[epoch: 584/1000, batch:   444/ 1052, ite: 153440] train loss: 0.004116, tar: 0.000080 
l0: 0.000060, l1: 0.000062, l2: 0.000110, l3: 0.000254, l4: 0.000554, l5: 0.000954, l6: 0.002307
[epoch: 584/1000, batch:   524/ 1052, ite: 153460] train loss: 0.004115, tar: 0.000080 
l0: 0.000050, l1: 0.000050, l2: 0.000088, l3: 0.000200, l4: 0.000534, l5: 0.001415, l6: 0.002210
[epoch: 584/1000, batch:   604/ 1052, ite: 153480] train loss: 0.004116, tar: 0.000080 
l0: 0.000114, l1: 0.000120, l2: 0.000191, l3: 0.000316, l4: 0.000536, l5: 0.001321, l6: 0.002976
[epoch: 584/1000, batch:   684/ 1052, ite: 153500] train loss: 0.004115, tar: 0.000080 
l0: 0.000102, l1: 0.000107, l2: 0.000140, l3: 0.000197, l4: 0.000590, l5: 0.000861, l6: 0.002244
[epoch: 584/1000, batch:   764/ 1052, ite: 153520] train loss: 0.004115, tar: 0.000080 
l0: 0.000106, l1: 0.000104, l2: 0.000168, l3: 0.000281, l4: 0.000555, l5: 0.001101, l6: 0.001305
[epoch: 584/1000, batch:   844/ 1052, ite: 153540] train loss: 0.004117, tar: 0.000080 
l0: 0.000028, l1: 0.000029, l2: 0.000063, l3: 0.000140, l4: 0.000322, l5: 0.000780, l6: 0.001252
[epoch: 584/1000, batch:   924/ 1052, ite: 153560] train loss: 0.004117, tar: 0.000081 
l0: 0.000017, l1: 0.000017, l2: 0.000037, l3: 0.000103, l4: 0.000380, l5: 0.000699, l6: 0.001027
[epoch: 584/1000, batch:  1004/ 1052, ite: 153580] train loss: 0.004115, tar: 0.000080 
[Epoch 584/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000087, l1: 0.000084, l2: 0.000135, l3: 0.000218, l4: 0.000730, l5: 0.000929, l6: 0.002093
[epoch: 585/1000, batch:    32/ 1052, ite: 153600] train loss: 0.004116, tar: 0.000080 
l0: 0.000048, l1: 0.000047, l2: 0.000077, l3: 0.000166, l4: 0.000376, l5: 0.000791, l6: 0.001405
[epoch: 585/1000, batch:   112/ 1052, ite: 153620] train loss: 0.004117, tar: 0.000080 
l0: 0.000114, l1: 0.000115, l2: 0.000150, l3: 0.000260, l4: 0.000612, l5: 0.001425, l6: 0.003229
[epoch: 585/1000, batch:   192/ 1052, ite: 153640] train loss: 0.004116, tar: 0.000080 
l0: 0.000137, l1: 0.000138, l2: 0.000196, l3: 0.000374, l4: 0.000611, l5: 0.001324, l6: 0.002210
[epoch: 585/1000, batch:   272/ 1052, ite: 153660] train loss: 0.004116, tar: 0.000080 
l0: 0.000190, l1: 0.000191, l2: 0.000263, l3: 0.000373, l4: 0.000832, l5: 0.001193, l6: 0.003772
[epoch: 585/1000, batch:   352/ 1052, ite: 153680] train loss: 0.004115, tar: 0.000080 
l0: 0.000022, l1: 0.000022, l2: 0.000064, l3: 0.000225, l4: 0.000681, l5: 0.001166, l6: 0.001555
[epoch: 585/1000, batch:   432/ 1052, ite: 153700] train loss: 0.004114, tar: 0.000080 
l0: 0.000150, l1: 0.000149, l2: 0.000243, l3: 0.000386, l4: 0.000626, l5: 0.001206, l6: 0.002562
[epoch: 585/1000, batch:   512/ 1052, ite: 153720] train loss: 0.004115, tar: 0.000080 
l0: 0.000052, l1: 0.000056, l2: 0.000082, l3: 0.000188, l4: 0.000357, l5: 0.000974, l6: 0.001942
[epoch: 585/1000, batch:   592/ 1052, ite: 153740] train loss: 0.004115, tar: 0.000080 
l0: 0.000014, l1: 0.000014, l2: 0.000031, l3: 0.000116, l4: 0.000267, l5: 0.000593, l6: 0.000623
[epoch: 585/1000, batch:   672/ 1052, ite: 153760] train loss: 0.004116, tar: 0.000080 
l0: 0.000064, l1: 0.000063, l2: 0.000101, l3: 0.000194, l4: 0.000467, l5: 0.001396, l6: 0.001824
[epoch: 585/1000, batch:   752/ 1052, ite: 153780] train loss: 0.004116, tar: 0.000080 
l0: 0.000084, l1: 0.000086, l2: 0.000097, l3: 0.000119, l4: 0.000356, l5: 0.000666, l6: 0.000978
[epoch: 585/1000, batch:   832/ 1052, ite: 153800] train loss: 0.004115, tar: 0.000080 
l0: 0.000464, l1: 0.000459, l2: 0.000362, l3: 0.000311, l4: 0.000498, l5: 0.000916, l6: 0.001772
[epoch: 585/1000, batch:   912/ 1052, ite: 153820] train loss: 0.004115, tar: 0.000080 
l0: 0.000073, l1: 0.000073, l2: 0.000124, l3: 0.000177, l4: 0.000309, l5: 0.001065, l6: 0.001743
[epoch: 585/1000, batch:   992/ 1052, ite: 153840] train loss: 0.004116, tar: 0.000080 
[Epoch 585/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000039, l1: 0.000040, l2: 0.000083, l3: 0.000189, l4: 0.000243, l5: 0.001041, l6: 0.001449
[epoch: 586/1000, batch:    20/ 1052, ite: 153860] train loss: 0.004117, tar: 0.000080 
l0: 0.000176, l1: 0.000178, l2: 0.000202, l3: 0.000232, l4: 0.000546, l5: 0.001689, l6: 0.002634
[epoch: 586/1000, batch:   100/ 1052, ite: 153880] train loss: 0.004118, tar: 0.000080 
l0: 0.000061, l1: 0.000064, l2: 0.000098, l3: 0.000203, l4: 0.000468, l5: 0.000741, l6: 0.001441
[epoch: 586/1000, batch:   180/ 1052, ite: 153900] train loss: 0.004119, tar: 0.000081 
l0: 0.000019, l1: 0.000020, l2: 0.000038, l3: 0.000115, l4: 0.000340, l5: 0.000786, l6: 0.001351
[epoch: 586/1000, batch:   260/ 1052, ite: 153920] train loss: 0.004118, tar: 0.000080 
l0: 0.000319, l1: 0.000315, l2: 0.000362, l3: 0.000500, l4: 0.000894, l5: 0.001785, l6: 0.003526
[epoch: 586/1000, batch:   340/ 1052, ite: 153940] train loss: 0.004118, tar: 0.000080 
l0: 0.000124, l1: 0.000122, l2: 0.000204, l3: 0.000328, l4: 0.000585, l5: 0.001505, l6: 0.002431
[epoch: 586/1000, batch:   420/ 1052, ite: 153960] train loss: 0.004119, tar: 0.000081 
l0: 0.000014, l1: 0.000012, l2: 0.000052, l3: 0.000118, l4: 0.000268, l5: 0.000800, l6: 0.001842
[epoch: 586/1000, batch:   500/ 1052, ite: 153980] train loss: 0.004118, tar: 0.000080 
l0: 0.000027, l1: 0.000028, l2: 0.000042, l3: 0.000144, l4: 0.000328, l5: 0.000689, l6: 0.001131
[epoch: 586/1000, batch:   580/ 1052, ite: 154000] train loss: 0.004118, tar: 0.000080 
l0: 0.000051, l1: 0.000055, l2: 0.000083, l3: 0.000167, l4: 0.000340, l5: 0.000880, l6: 0.001696
[epoch: 586/1000, batch:   660/ 1052, ite: 154020] train loss: 0.004117, tar: 0.000080 
l0: 0.000067, l1: 0.000068, l2: 0.000115, l3: 0.000249, l4: 0.000402, l5: 0.001168, l6: 0.002785
[epoch: 586/1000, batch:   740/ 1052, ite: 154040] train loss: 0.004117, tar: 0.000080 
l0: 0.000016, l1: 0.000016, l2: 0.000038, l3: 0.000119, l4: 0.000240, l5: 0.000827, l6: 0.001346
[epoch: 586/1000, batch:   820/ 1052, ite: 154060] train loss: 0.004116, tar: 0.000080 
l0: 0.000083, l1: 0.000082, l2: 0.000097, l3: 0.000203, l4: 0.000410, l5: 0.001169, l6: 0.001949
[epoch: 586/1000, batch:   900/ 1052, ite: 154080] train loss: 0.004118, tar: 0.000081 
l0: 0.000130, l1: 0.000127, l2: 0.000166, l3: 0.000366, l4: 0.000568, l5: 0.000843, l6: 0.002110
[epoch: 586/1000, batch:   980/ 1052, ite: 154100] train loss: 0.004118, tar: 0.000081 
[Epoch 586/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000059, l1: 0.000060, l2: 0.000082, l3: 0.000192, l4: 0.000508, l5: 0.000991, l6: 0.001782
[epoch: 587/1000, batch:     8/ 1052, ite: 154120] train loss: 0.004119, tar: 0.000081 
l0: 0.000098, l1: 0.000100, l2: 0.000117, l3: 0.000163, l4: 0.000332, l5: 0.001137, l6: 0.002460
[epoch: 587/1000, batch:    88/ 1052, ite: 154140] train loss: 0.004117, tar: 0.000081 
l0: 0.000045, l1: 0.000044, l2: 0.000068, l3: 0.000170, l4: 0.000337, l5: 0.000906, l6: 0.001222
[epoch: 587/1000, batch:   168/ 1052, ite: 154160] train loss: 0.004118, tar: 0.000081 
l0: 0.000011, l1: 0.000010, l2: 0.000035, l3: 0.000056, l4: 0.000325, l5: 0.000949, l6: 0.000880
[epoch: 587/1000, batch:   248/ 1052, ite: 154180] train loss: 0.004119, tar: 0.000081 
l0: 0.000031, l1: 0.000030, l2: 0.000051, l3: 0.000094, l4: 0.000205, l5: 0.000543, l6: 0.001316
[epoch: 587/1000, batch:   328/ 1052, ite: 154200] train loss: 0.004119, tar: 0.000081 
l0: 0.000221, l1: 0.000223, l2: 0.000267, l3: 0.000416, l4: 0.001028, l5: 0.001794, l6: 0.003067
[epoch: 587/1000, batch:   408/ 1052, ite: 154220] train loss: 0.004120, tar: 0.000081 
l0: 0.000011, l1: 0.000010, l2: 0.000057, l3: 0.000092, l4: 0.000327, l5: 0.000814, l6: 0.001418
[epoch: 587/1000, batch:   488/ 1052, ite: 154240] train loss: 0.004120, tar: 0.000081 
l0: 0.000137, l1: 0.000145, l2: 0.000288, l3: 0.000335, l4: 0.000841, l5: 0.001365, l6: 0.002298
[epoch: 587/1000, batch:   568/ 1052, ite: 154260] train loss: 0.004120, tar: 0.000081 
l0: 0.000135, l1: 0.000127, l2: 0.000238, l3: 0.000361, l4: 0.000708, l5: 0.000989, l6: 0.002811
[epoch: 587/1000, batch:   648/ 1052, ite: 154280] train loss: 0.004122, tar: 0.000081 
l0: 0.000089, l1: 0.000093, l2: 0.000136, l3: 0.000191, l4: 0.000362, l5: 0.000721, l6: 0.001237
[epoch: 587/1000, batch:   728/ 1052, ite: 154300] train loss: 0.004122, tar: 0.000082 
l0: 0.000070, l1: 0.000070, l2: 0.000143, l3: 0.000249, l4: 0.000375, l5: 0.001257, l6: 0.002037
[epoch: 587/1000, batch:   808/ 1052, ite: 154320] train loss: 0.004124, tar: 0.000082 
l0: 0.000028, l1: 0.000024, l2: 0.000110, l3: 0.000222, l4: 0.000385, l5: 0.000566, l6: 0.001563
[epoch: 587/1000, batch:   888/ 1052, ite: 154340] train loss: 0.004127, tar: 0.000082 
l0: 0.000097, l1: 0.000102, l2: 0.000107, l3: 0.000169, l4: 0.000419, l5: 0.000511, l6: 0.001387
[epoch: 587/1000, batch:   968/ 1052, ite: 154360] train loss: 0.004126, tar: 0.000082 
l0: 0.000116, l1: 0.000127, l2: 0.000100, l3: 0.000075, l4: 0.000106, l5: 0.000416, l6: 0.000582
[epoch: 587/1000, batch:  1048/ 1052, ite: 154380] train loss: 0.004128, tar: 0.000082 
[Epoch 587/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000137, l1: 0.000139, l2: 0.000217, l3: 0.000515, l4: 0.000871, l5: 0.001946, l6: 0.003375
[epoch: 588/1000, batch:    76/ 1052, ite: 154400] train loss: 0.004130, tar: 0.000082 
l0: 0.000120, l1: 0.000122, l2: 0.000151, l3: 0.000243, l4: 0.000596, l5: 0.000899, l6: 0.002220
[epoch: 588/1000, batch:   156/ 1052, ite: 154420] train loss: 0.004130, tar: 0.000083 
l0: 0.000019, l1: 0.000017, l2: 0.000045, l3: 0.000155, l4: 0.000437, l5: 0.000508, l6: 0.000970
[epoch: 588/1000, batch:   236/ 1052, ite: 154440] train loss: 0.004129, tar: 0.000083 
l0: 0.000156, l1: 0.000159, l2: 0.000250, l3: 0.000357, l4: 0.000774, l5: 0.001236, l6: 0.001863
[epoch: 588/1000, batch:   316/ 1052, ite: 154460] train loss: 0.004131, tar: 0.000083 
l0: 0.000149, l1: 0.000157, l2: 0.000196, l3: 0.000242, l4: 0.000642, l5: 0.001174, l6: 0.002353
[epoch: 588/1000, batch:   396/ 1052, ite: 154480] train loss: 0.004133, tar: 0.000083 
l0: 0.000182, l1: 0.000193, l2: 0.000302, l3: 0.000384, l4: 0.001128, l5: 0.002416, l6: 0.003478
[epoch: 588/1000, batch:   476/ 1052, ite: 154500] train loss: 0.004133, tar: 0.000083 
l0: 0.000072, l1: 0.000068, l2: 0.000101, l3: 0.000145, l4: 0.000450, l5: 0.001277, l6: 0.001433
[epoch: 588/1000, batch:   556/ 1052, ite: 154520] train loss: 0.004135, tar: 0.000084 
l0: 0.000131, l1: 0.000122, l2: 0.000211, l3: 0.000329, l4: 0.000591, l5: 0.001681, l6: 0.002607
[epoch: 588/1000, batch:   636/ 1052, ite: 154540] train loss: 0.004137, tar: 0.000084 
l0: 0.000043, l1: 0.000042, l2: 0.000061, l3: 0.000127, l4: 0.000254, l5: 0.000943, l6: 0.001378
[epoch: 588/1000, batch:   716/ 1052, ite: 154560] train loss: 0.004138, tar: 0.000084 
l0: 0.000215, l1: 0.000221, l2: 0.000290, l3: 0.000389, l4: 0.000830, l5: 0.002376, l6: 0.003948
[epoch: 588/1000, batch:   796/ 1052, ite: 154580] train loss: 0.004138, tar: 0.000084 
l0: 0.000038, l1: 0.000041, l2: 0.000068, l3: 0.000141, l4: 0.000320, l5: 0.000832, l6: 0.000978
[epoch: 588/1000, batch:   876/ 1052, ite: 154600] train loss: 0.004138, tar: 0.000084 
l0: 0.000090, l1: 0.000086, l2: 0.000122, l3: 0.000180, l4: 0.000520, l5: 0.001004, l6: 0.001388
[epoch: 588/1000, batch:   956/ 1052, ite: 154620] train loss: 0.004138, tar: 0.000084 
l0: 0.000026, l1: 0.000028, l2: 0.000041, l3: 0.000065, l4: 0.000151, l5: 0.000485, l6: 0.001220
[epoch: 588/1000, batch:  1036/ 1052, ite: 154640] train loss: 0.004140, tar: 0.000084 
[Epoch 588/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000066, l1: 0.000063, l2: 0.000087, l3: 0.000135, l4: 0.000226, l5: 0.000796, l6: 0.000757
[epoch: 589/1000, batch:    64/ 1052, ite: 154660] train loss: 0.004141, tar: 0.000084 
l0: 0.000078, l1: 0.000075, l2: 0.000081, l3: 0.000173, l4: 0.000524, l5: 0.000973, l6: 0.000855
[epoch: 589/1000, batch:   144/ 1052, ite: 154680] train loss: 0.004140, tar: 0.000084 
l0: 0.000028, l1: 0.000024, l2: 0.000075, l3: 0.000134, l4: 0.000390, l5: 0.000835, l6: 0.001468
[epoch: 589/1000, batch:   224/ 1052, ite: 154700] train loss: 0.004139, tar: 0.000084 
l0: 0.000050, l1: 0.000047, l2: 0.000109, l3: 0.000204, l4: 0.000370, l5: 0.001042, l6: 0.002049
[epoch: 589/1000, batch:   304/ 1052, ite: 154720] train loss: 0.004140, tar: 0.000084 
l0: 0.000260, l1: 0.000275, l2: 0.000298, l3: 0.000420, l4: 0.000711, l5: 0.001809, l6: 0.004334
[epoch: 589/1000, batch:   384/ 1052, ite: 154740] train loss: 0.004142, tar: 0.000084 
l0: 0.000120, l1: 0.000126, l2: 0.000144, l3: 0.000237, l4: 0.000497, l5: 0.000793, l6: 0.001981
[epoch: 589/1000, batch:   464/ 1052, ite: 154760] train loss: 0.004141, tar: 0.000084 
l0: 0.000095, l1: 0.000109, l2: 0.000102, l3: 0.000174, l4: 0.000617, l5: 0.001354, l6: 0.001939
[epoch: 589/1000, batch:   544/ 1052, ite: 154780] train loss: 0.004141, tar: 0.000084 
l0: 0.000040, l1: 0.000038, l2: 0.000107, l3: 0.000223, l4: 0.000311, l5: 0.000824, l6: 0.001693
[epoch: 589/1000, batch:   624/ 1052, ite: 154800] train loss: 0.004140, tar: 0.000084 
l0: 0.000053, l1: 0.000057, l2: 0.000062, l3: 0.000122, l4: 0.000279, l5: 0.000700, l6: 0.001446
[epoch: 589/1000, batch:   704/ 1052, ite: 154820] train loss: 0.004141, tar: 0.000084 
l0: 0.000224, l1: 0.000229, l2: 0.000292, l3: 0.000426, l4: 0.000863, l5: 0.001702, l6: 0.003425
[epoch: 589/1000, batch:   784/ 1052, ite: 154840] train loss: 0.004143, tar: 0.000084 
l0: 0.000059, l1: 0.000058, l2: 0.000080, l3: 0.000117, l4: 0.000321, l5: 0.000887, l6: 0.001547
[epoch: 589/1000, batch:   864/ 1052, ite: 154860] train loss: 0.004144, tar: 0.000084 
l0: 0.000071, l1: 0.000072, l2: 0.000099, l3: 0.000147, l4: 0.000471, l5: 0.001117, l6: 0.001767
[epoch: 589/1000, batch:   944/ 1052, ite: 154880] train loss: 0.004143, tar: 0.000084 
l0: 0.000096, l1: 0.000110, l2: 0.000102, l3: 0.000180, l4: 0.000340, l5: 0.000810, l6: 0.001317
[epoch: 589/1000, batch:  1024/ 1052, ite: 154900] train loss: 0.004142, tar: 0.000084 
[Epoch 589/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000171, l1: 0.000178, l2: 0.000196, l3: 0.000320, l4: 0.000679, l5: 0.001391, l6: 0.003284
[epoch: 590/1000, batch:    52/ 1052, ite: 154920] train loss: 0.004142, tar: 0.000084 
l0: 0.000094, l1: 0.000091, l2: 0.000143, l3: 0.000292, l4: 0.000586, l5: 0.000979, l6: 0.001975
[epoch: 590/1000, batch:   132/ 1052, ite: 154940] train loss: 0.004141, tar: 0.000084 
l0: 0.000024, l1: 0.000022, l2: 0.000069, l3: 0.000180, l4: 0.000461, l5: 0.000882, l6: 0.001535
[epoch: 590/1000, batch:   212/ 1052, ite: 154960] train loss: 0.004140, tar: 0.000084 
l0: 0.000095, l1: 0.000098, l2: 0.000137, l3: 0.000223, l4: 0.000565, l5: 0.000972, l6: 0.002153
[epoch: 590/1000, batch:   292/ 1052, ite: 154980] train loss: 0.004141, tar: 0.000084 
l0: 0.000067, l1: 0.000070, l2: 0.000097, l3: 0.000142, l4: 0.000271, l5: 0.000798, l6: 0.001167
[epoch: 590/1000, batch:   372/ 1052, ite: 155000] train loss: 0.004141, tar: 0.000084 
l0: 0.000113, l1: 0.000112, l2: 0.000145, l3: 0.000299, l4: 0.000524, l5: 0.001080, l6: 0.001839
[epoch: 590/1000, batch:   452/ 1052, ite: 155020] train loss: 0.004141, tar: 0.000084 
l0: 0.000146, l1: 0.000151, l2: 0.000188, l3: 0.000360, l4: 0.000745, l5: 0.000848, l6: 0.001334
[epoch: 590/1000, batch:   532/ 1052, ite: 155040] train loss: 0.004141, tar: 0.000084 
l0: 0.000146, l1: 0.000146, l2: 0.000227, l3: 0.000426, l4: 0.000879, l5: 0.002103, l6: 0.003469
[epoch: 590/1000, batch:   612/ 1052, ite: 155060] train loss: 0.004141, tar: 0.000084 
l0: 0.000102, l1: 0.000105, l2: 0.000121, l3: 0.000228, l4: 0.000441, l5: 0.000598, l6: 0.001450
[epoch: 590/1000, batch:   692/ 1052, ite: 155080] train loss: 0.004141, tar: 0.000084 
l0: 0.000051, l1: 0.000052, l2: 0.000082, l3: 0.000184, l4: 0.000352, l5: 0.000761, l6: 0.001445
[epoch: 590/1000, batch:   772/ 1052, ite: 155100] train loss: 0.004142, tar: 0.000084 
l0: 0.000214, l1: 0.000210, l2: 0.000348, l3: 0.000406, l4: 0.000935, l5: 0.002220, l6: 0.005706
[epoch: 590/1000, batch:   852/ 1052, ite: 155120] train loss: 0.004143, tar: 0.000084 
l0: 0.000074, l1: 0.000075, l2: 0.000119, l3: 0.000167, l4: 0.000283, l5: 0.001317, l6: 0.001985
[epoch: 590/1000, batch:   932/ 1052, ite: 155140] train loss: 0.004142, tar: 0.000084 
l0: 0.000052, l1: 0.000055, l2: 0.000073, l3: 0.000154, l4: 0.000365, l5: 0.000580, l6: 0.001803
[epoch: 590/1000, batch:  1012/ 1052, ite: 155160] train loss: 0.004141, tar: 0.000084 
[Epoch 590/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000137, l1: 0.000141, l2: 0.000151, l3: 0.000220, l4: 0.000494, l5: 0.000891, l6: 0.002111
[epoch: 591/1000, batch:    40/ 1052, ite: 155180] train loss: 0.004141, tar: 0.000084 
l0: 0.000034, l1: 0.000034, l2: 0.000059, l3: 0.000082, l4: 0.000459, l5: 0.000938, l6: 0.001550
[epoch: 591/1000, batch:   120/ 1052, ite: 155200] train loss: 0.004141, tar: 0.000084 
l0: 0.000120, l1: 0.000125, l2: 0.000159, l3: 0.000223, l4: 0.000421, l5: 0.001632, l6: 0.002566
[epoch: 591/1000, batch:   200/ 1052, ite: 155220] train loss: 0.004141, tar: 0.000084 
l0: 0.000011, l1: 0.000011, l2: 0.000028, l3: 0.000042, l4: 0.000276, l5: 0.000518, l6: 0.001306
[epoch: 591/1000, batch:   280/ 1052, ite: 155240] train loss: 0.004140, tar: 0.000084 
l0: 0.000085, l1: 0.000086, l2: 0.000130, l3: 0.000221, l4: 0.000381, l5: 0.001230, l6: 0.002158
[epoch: 591/1000, batch:   360/ 1052, ite: 155260] train loss: 0.004139, tar: 0.000084 
l0: 0.000101, l1: 0.000103, l2: 0.000127, l3: 0.000205, l4: 0.000401, l5: 0.000970, l6: 0.002570
[epoch: 591/1000, batch:   440/ 1052, ite: 155280] train loss: 0.004138, tar: 0.000084 
l0: 0.000040, l1: 0.000040, l2: 0.000079, l3: 0.000199, l4: 0.000317, l5: 0.001125, l6: 0.002109
[epoch: 591/1000, batch:   520/ 1052, ite: 155300] train loss: 0.004139, tar: 0.000084 
l0: 0.000026, l1: 0.000028, l2: 0.000059, l3: 0.000176, l4: 0.000364, l5: 0.000760, l6: 0.001685
[epoch: 591/1000, batch:   600/ 1052, ite: 155320] train loss: 0.004140, tar: 0.000084 
l0: 0.000107, l1: 0.000108, l2: 0.000132, l3: 0.000244, l4: 0.000585, l5: 0.001264, l6: 0.001957
[epoch: 591/1000, batch:   680/ 1052, ite: 155340] train loss: 0.004139, tar: 0.000084 
l0: 0.000049, l1: 0.000048, l2: 0.000056, l3: 0.000119, l4: 0.000337, l5: 0.001256, l6: 0.001954
[epoch: 591/1000, batch:   760/ 1052, ite: 155360] train loss: 0.004139, tar: 0.000084 
l0: 0.000019, l1: 0.000018, l2: 0.000078, l3: 0.000238, l4: 0.000498, l5: 0.001224, l6: 0.001772
[epoch: 591/1000, batch:   840/ 1052, ite: 155380] train loss: 0.004139, tar: 0.000084 
l0: 0.000080, l1: 0.000076, l2: 0.000164, l3: 0.000318, l4: 0.000754, l5: 0.001170, l6: 0.003162
[epoch: 591/1000, batch:   920/ 1052, ite: 155400] train loss: 0.004140, tar: 0.000084 
l0: 0.000091, l1: 0.000092, l2: 0.000133, l3: 0.000219, l4: 0.000574, l5: 0.001345, l6: 0.002702
[epoch: 591/1000, batch:  1000/ 1052, ite: 155420] train loss: 0.004139, tar: 0.000084 
[Epoch 591/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000165, l1: 0.000166, l2: 0.000192, l3: 0.000306, l4: 0.000666, l5: 0.001049, l6: 0.002828
[epoch: 592/1000, batch:    28/ 1052, ite: 155440] train loss: 0.004138, tar: 0.000084 
l0: 0.000050, l1: 0.000051, l2: 0.000097, l3: 0.000280, l4: 0.000923, l5: 0.001694, l6: 0.003919
[epoch: 592/1000, batch:   108/ 1052, ite: 155460] train loss: 0.004138, tar: 0.000084 
l0: 0.000057, l1: 0.000057, l2: 0.000082, l3: 0.000168, l4: 0.000679, l5: 0.001467, l6: 0.002058
[epoch: 592/1000, batch:   188/ 1052, ite: 155480] train loss: 0.004139, tar: 0.000084 
l0: 0.000089, l1: 0.000092, l2: 0.000162, l3: 0.000314, l4: 0.000660, l5: 0.001142, l6: 0.001707
[epoch: 592/1000, batch:   268/ 1052, ite: 155500] train loss: 0.004138, tar: 0.000084 
l0: 0.000271, l1: 0.000285, l2: 0.000260, l3: 0.000308, l4: 0.000398, l5: 0.000893, l6: 0.002137
[epoch: 592/1000, batch:   348/ 1052, ite: 155520] train loss: 0.004139, tar: 0.000084 
l0: 0.000035, l1: 0.000037, l2: 0.000042, l3: 0.000145, l4: 0.000268, l5: 0.000846, l6: 0.001142
[epoch: 592/1000, batch:   428/ 1052, ite: 155540] train loss: 0.004139, tar: 0.000084 
l0: 0.000167, l1: 0.000179, l2: 0.000216, l3: 0.000318, l4: 0.000551, l5: 0.001272, l6: 0.001791
[epoch: 592/1000, batch:   508/ 1052, ite: 155560] train loss: 0.004138, tar: 0.000084 
l0: 0.000154, l1: 0.000154, l2: 0.000217, l3: 0.000312, l4: 0.000513, l5: 0.001273, l6: 0.002658
[epoch: 592/1000, batch:   588/ 1052, ite: 155580] train loss: 0.004138, tar: 0.000084 
l0: 0.000039, l1: 0.000038, l2: 0.000076, l3: 0.000173, l4: 0.000555, l5: 0.001187, l6: 0.001840
[epoch: 592/1000, batch:   668/ 1052, ite: 155600] train loss: 0.004137, tar: 0.000084 
l0: 0.000102, l1: 0.000104, l2: 0.000100, l3: 0.000161, l4: 0.000281, l5: 0.001031, l6: 0.002091
[epoch: 592/1000, batch:   748/ 1052, ite: 155620] train loss: 0.004136, tar: 0.000084 
l0: 0.000039, l1: 0.000040, l2: 0.000086, l3: 0.000193, l4: 0.000517, l5: 0.001344, l6: 0.002210
[epoch: 592/1000, batch:   828/ 1052, ite: 155640] train loss: 0.004136, tar: 0.000084 
l0: 0.000020, l1: 0.000020, l2: 0.000042, l3: 0.000128, l4: 0.000399, l5: 0.000967, l6: 0.001568
[epoch: 592/1000, batch:   908/ 1052, ite: 155660] train loss: 0.004135, tar: 0.000083 
l0: 0.000098, l1: 0.000103, l2: 0.000143, l3: 0.000251, l4: 0.000435, l5: 0.001267, l6: 0.001798
[epoch: 592/1000, batch:   988/ 1052, ite: 155680] train loss: 0.004133, tar: 0.000083 
[Epoch 592/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000032, l1: 0.000030, l2: 0.000091, l3: 0.000187, l4: 0.000614, l5: 0.001117, l6: 0.001644
[epoch: 593/1000, batch:    16/ 1052, ite: 155700] train loss: 0.004134, tar: 0.000083 
l0: 0.000072, l1: 0.000069, l2: 0.000136, l3: 0.000185, l4: 0.000339, l5: 0.000767, l6: 0.001048
[epoch: 593/1000, batch:    96/ 1052, ite: 155720] train loss: 0.004134, tar: 0.000083 
l0: 0.000015, l1: 0.000018, l2: 0.000029, l3: 0.000085, l4: 0.000277, l5: 0.000855, l6: 0.000543
[epoch: 593/1000, batch:   176/ 1052, ite: 155740] train loss: 0.004132, tar: 0.000083 
l0: 0.000057, l1: 0.000060, l2: 0.000071, l3: 0.000149, l4: 0.000386, l5: 0.000609, l6: 0.001447
[epoch: 593/1000, batch:   256/ 1052, ite: 155760] train loss: 0.004131, tar: 0.000083 
l0: 0.000069, l1: 0.000068, l2: 0.000152, l3: 0.000323, l4: 0.000764, l5: 0.002310, l6: 0.003831
[epoch: 593/1000, batch:   336/ 1052, ite: 155780] train loss: 0.004131, tar: 0.000083 
l0: 0.000065, l1: 0.000064, l2: 0.000119, l3: 0.000232, l4: 0.000530, l5: 0.001718, l6: 0.001891
[epoch: 593/1000, batch:   416/ 1052, ite: 155800] train loss: 0.004131, tar: 0.000083 
l0: 0.000025, l1: 0.000030, l2: 0.000072, l3: 0.000135, l4: 0.000231, l5: 0.000785, l6: 0.001677
[epoch: 593/1000, batch:   496/ 1052, ite: 155820] train loss: 0.004132, tar: 0.000083 
l0: 0.000100, l1: 0.000097, l2: 0.000212, l3: 0.000367, l4: 0.000728, l5: 0.001197, l6: 0.002072
[epoch: 593/1000, batch:   576/ 1052, ite: 155840] train loss: 0.004132, tar: 0.000083 
l0: 0.000037, l1: 0.000038, l2: 0.000050, l3: 0.000121, l4: 0.000239, l5: 0.001082, l6: 0.001522
[epoch: 593/1000, batch:   656/ 1052, ite: 155860] train loss: 0.004132, tar: 0.000083 
l0: 0.000065, l1: 0.000064, l2: 0.000092, l3: 0.000181, l4: 0.000357, l5: 0.001222, l6: 0.002776
[epoch: 593/1000, batch:   736/ 1052, ite: 155880] train loss: 0.004133, tar: 0.000083 
l0: 0.000104, l1: 0.000103, l2: 0.000140, l3: 0.000223, l4: 0.000351, l5: 0.001329, l6: 0.003216
[epoch: 593/1000, batch:   816/ 1052, ite: 155900] train loss: 0.004132, tar: 0.000083 
l0: 0.000102, l1: 0.000104, l2: 0.000153, l3: 0.000257, l4: 0.000633, l5: 0.001490, l6: 0.001460
[epoch: 593/1000, batch:   896/ 1052, ite: 155920] train loss: 0.004133, tar: 0.000083 
l0: 0.000049, l1: 0.000046, l2: 0.000084, l3: 0.000158, l4: 0.000320, l5: 0.000347, l6: 0.000957
[epoch: 593/1000, batch:   976/ 1052, ite: 155940] train loss: 0.004132, tar: 0.000083 
[Epoch 593/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000105, l1: 0.000108, l2: 0.000156, l3: 0.000332, l4: 0.000626, l5: 0.001682, l6: 0.002604
[epoch: 594/1000, batch:     4/ 1052, ite: 155960] train loss: 0.004132, tar: 0.000083 
l0: 0.000010, l1: 0.000009, l2: 0.000061, l3: 0.000194, l4: 0.000534, l5: 0.000999, l6: 0.002658
[epoch: 594/1000, batch:    84/ 1052, ite: 155980] train loss: 0.004131, tar: 0.000083 
l0: 0.000039, l1: 0.000041, l2: 0.000065, l3: 0.000119, l4: 0.000287, l5: 0.000517, l6: 0.002285
[epoch: 594/1000, batch:   164/ 1052, ite: 156000] train loss: 0.004131, tar: 0.000083 
l0: 0.000044, l1: 0.000046, l2: 0.000042, l3: 0.000091, l4: 0.000226, l5: 0.000790, l6: 0.001411
[epoch: 594/1000, batch:   244/ 1052, ite: 156020] train loss: 0.004130, tar: 0.000083 
l0: 0.000009, l1: 0.000009, l2: 0.000033, l3: 0.000089, l4: 0.000294, l5: 0.000689, l6: 0.001126
[epoch: 594/1000, batch:   324/ 1052, ite: 156040] train loss: 0.004130, tar: 0.000083 
l0: 0.000078, l1: 0.000080, l2: 0.000077, l3: 0.000174, l4: 0.000454, l5: 0.000964, l6: 0.001574
[epoch: 594/1000, batch:   404/ 1052, ite: 156060] train loss: 0.004131, tar: 0.000083 
l0: 0.000028, l1: 0.000029, l2: 0.000085, l3: 0.000268, l4: 0.000904, l5: 0.001577, l6: 0.002466
[epoch: 594/1000, batch:   484/ 1052, ite: 156080] train loss: 0.004131, tar: 0.000083 
l0: 0.000015, l1: 0.000017, l2: 0.000019, l3: 0.000069, l4: 0.000296, l5: 0.000782, l6: 0.001118
[epoch: 594/1000, batch:   564/ 1052, ite: 156100] train loss: 0.004131, tar: 0.000083 
l0: 0.000042, l1: 0.000041, l2: 0.000060, l3: 0.000109, l4: 0.000317, l5: 0.000678, l6: 0.001218
[epoch: 594/1000, batch:   644/ 1052, ite: 156120] train loss: 0.004131, tar: 0.000083 
l0: 0.000131, l1: 0.000134, l2: 0.000194, l3: 0.000326, l4: 0.000782, l5: 0.001997, l6: 0.003620
[epoch: 594/1000, batch:   724/ 1052, ite: 156140] train loss: 0.004131, tar: 0.000083 
l0: 0.000039, l1: 0.000040, l2: 0.000110, l3: 0.000234, l4: 0.000551, l5: 0.001100, l6: 0.003279
[epoch: 594/1000, batch:   804/ 1052, ite: 156160] train loss: 0.004130, tar: 0.000083 
l0: 0.000100, l1: 0.000099, l2: 0.000163, l3: 0.000235, l4: 0.000555, l5: 0.001316, l6: 0.002650
[epoch: 594/1000, batch:   884/ 1052, ite: 156180] train loss: 0.004130, tar: 0.000083 
l0: 0.000057, l1: 0.000057, l2: 0.000118, l3: 0.000251, l4: 0.000670, l5: 0.001285, l6: 0.002399
[epoch: 594/1000, batch:   964/ 1052, ite: 156200] train loss: 0.004130, tar: 0.000083 
l0: 0.000034, l1: 0.000035, l2: 0.000103, l3: 0.000160, l4: 0.000458, l5: 0.001516, l6: 0.001541
[epoch: 594/1000, batch:  1044/ 1052, ite: 156220] train loss: 0.004130, tar: 0.000083 
[Epoch 594/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000048, l1: 0.000048, l2: 0.000069, l3: 0.000160, l4: 0.000338, l5: 0.000547, l6: 0.001617
[epoch: 595/1000, batch:    72/ 1052, ite: 156240] train loss: 0.004130, tar: 0.000083 
l0: 0.000068, l1: 0.000068, l2: 0.000101, l3: 0.000219, l4: 0.000433, l5: 0.000868, l6: 0.000871
[epoch: 595/1000, batch:   152/ 1052, ite: 156260] train loss: 0.004129, tar: 0.000083 
l0: 0.000051, l1: 0.000051, l2: 0.000088, l3: 0.000172, l4: 0.000400, l5: 0.000688, l6: 0.002262
[epoch: 595/1000, batch:   232/ 1052, ite: 156280] train loss: 0.004128, tar: 0.000083 
l0: 0.000220, l1: 0.000222, l2: 0.000372, l3: 0.000534, l4: 0.001134, l5: 0.002570, l6: 0.004018
[epoch: 595/1000, batch:   312/ 1052, ite: 156300] train loss: 0.004128, tar: 0.000082 
l0: 0.000034, l1: 0.000032, l2: 0.000072, l3: 0.000217, l4: 0.000590, l5: 0.001337, l6: 0.001980
[epoch: 595/1000, batch:   392/ 1052, ite: 156320] train loss: 0.004128, tar: 0.000082 
l0: 0.000050, l1: 0.000052, l2: 0.000061, l3: 0.000133, l4: 0.000339, l5: 0.001425, l6: 0.001939
[epoch: 595/1000, batch:   472/ 1052, ite: 156340] train loss: 0.004127, tar: 0.000082 
l0: 0.000092, l1: 0.000093, l2: 0.000150, l3: 0.000356, l4: 0.000604, l5: 0.001657, l6: 0.003121
[epoch: 595/1000, batch:   552/ 1052, ite: 156360] train loss: 0.004127, tar: 0.000082 
l0: 0.000024, l1: 0.000024, l2: 0.000051, l3: 0.000102, l4: 0.000186, l5: 0.001163, l6: 0.001603
[epoch: 595/1000, batch:   632/ 1052, ite: 156380] train loss: 0.004127, tar: 0.000082 
l0: 0.000181, l1: 0.000179, l2: 0.000269, l3: 0.000644, l4: 0.000887, l5: 0.001284, l6: 0.002762
[epoch: 595/1000, batch:   712/ 1052, ite: 156400] train loss: 0.004127, tar: 0.000082 
l0: 0.000059, l1: 0.000060, l2: 0.000152, l3: 0.000558, l4: 0.001107, l5: 0.002492, l6: 0.003573
[epoch: 595/1000, batch:   792/ 1052, ite: 156420] train loss: 0.004129, tar: 0.000082 
l0: 0.000037, l1: 0.000036, l2: 0.000059, l3: 0.000115, l4: 0.000422, l5: 0.000886, l6: 0.001614
[epoch: 595/1000, batch:   872/ 1052, ite: 156440] train loss: 0.004129, tar: 0.000082 
l0: 0.000022, l1: 0.000022, l2: 0.000034, l3: 0.000083, l4: 0.000286, l5: 0.000711, l6: 0.000675
[epoch: 595/1000, batch:   952/ 1052, ite: 156460] train loss: 0.004128, tar: 0.000082 
l0: 0.000113, l1: 0.000114, l2: 0.000173, l3: 0.000283, l4: 0.000595, l5: 0.001095, l6: 0.002521
[epoch: 595/1000, batch:  1032/ 1052, ite: 156480] train loss: 0.004129, tar: 0.000082 
[Epoch 595/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000018, l1: 0.000018, l2: 0.000048, l3: 0.000150, l4: 0.000433, l5: 0.000884, l6: 0.001900
[epoch: 596/1000, batch:    60/ 1052, ite: 156500] train loss: 0.004129, tar: 0.000082 
l0: 0.000050, l1: 0.000052, l2: 0.000074, l3: 0.000146, l4: 0.000314, l5: 0.001030, l6: 0.002043
[epoch: 596/1000, batch:   140/ 1052, ite: 156520] train loss: 0.004129, tar: 0.000082 
l0: 0.000019, l1: 0.000019, l2: 0.000044, l3: 0.000143, l4: 0.000339, l5: 0.000676, l6: 0.001169
[epoch: 596/1000, batch:   220/ 1052, ite: 156540] train loss: 0.004128, tar: 0.000082 
l0: 0.000044, l1: 0.000048, l2: 0.000042, l3: 0.000130, l4: 0.000348, l5: 0.000478, l6: 0.001493
[epoch: 596/1000, batch:   300/ 1052, ite: 156560] train loss: 0.004127, tar: 0.000082 
l0: 0.000054, l1: 0.000055, l2: 0.000064, l3: 0.000145, l4: 0.000248, l5: 0.000746, l6: 0.001332
[epoch: 596/1000, batch:   380/ 1052, ite: 156580] train loss: 0.004127, tar: 0.000082 
l0: 0.000029, l1: 0.000031, l2: 0.000040, l3: 0.000082, l4: 0.000242, l5: 0.000716, l6: 0.001248
[epoch: 596/1000, batch:   460/ 1052, ite: 156600] train loss: 0.004128, tar: 0.000082 
l0: 0.000084, l1: 0.000085, l2: 0.000097, l3: 0.000221, l4: 0.000490, l5: 0.000943, l6: 0.001686
[epoch: 596/1000, batch:   540/ 1052, ite: 156620] train loss: 0.004128, tar: 0.000082 
l0: 0.000060, l1: 0.000059, l2: 0.000151, l3: 0.000352, l4: 0.000816, l5: 0.001428, l6: 0.002904
[epoch: 596/1000, batch:   620/ 1052, ite: 156640] train loss: 0.004127, tar: 0.000082 
l0: 0.000053, l1: 0.000051, l2: 0.000103, l3: 0.000170, l4: 0.000417, l5: 0.000912, l6: 0.001844
[epoch: 596/1000, batch:   700/ 1052, ite: 156660] train loss: 0.004127, tar: 0.000082 
l0: 0.000047, l1: 0.000044, l2: 0.000150, l3: 0.000402, l4: 0.000841, l5: 0.001775, l6: 0.002874
[epoch: 596/1000, batch:   780/ 1052, ite: 156680] train loss: 0.004126, tar: 0.000082 
l0: 0.000041, l1: 0.000044, l2: 0.000089, l3: 0.000172, l4: 0.000377, l5: 0.000860, l6: 0.001744
[epoch: 596/1000, batch:   860/ 1052, ite: 156700] train loss: 0.004126, tar: 0.000082 
l0: 0.000075, l1: 0.000076, l2: 0.000107, l3: 0.000221, l4: 0.000547, l5: 0.001213, l6: 0.001475
[epoch: 596/1000, batch:   940/ 1052, ite: 156720] train loss: 0.004126, tar: 0.000082 
l0: 0.000047, l1: 0.000046, l2: 0.000101, l3: 0.000168, l4: 0.000550, l5: 0.000984, l6: 0.002183
[epoch: 596/1000, batch:  1020/ 1052, ite: 156740] train loss: 0.004127, tar: 0.000082 
[Epoch 596/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000233, l1: 0.000235, l2: 0.000247, l3: 0.000381, l4: 0.000881, l5: 0.001422, l6: 0.003020
[epoch: 597/1000, batch:    48/ 1052, ite: 156760] train loss: 0.004127, tar: 0.000082 
l0: 0.000100, l1: 0.000099, l2: 0.000121, l3: 0.000218, l4: 0.000558, l5: 0.000904, l6: 0.002876
[epoch: 597/1000, batch:   128/ 1052, ite: 156780] train loss: 0.004127, tar: 0.000082 
l0: 0.000034, l1: 0.000035, l2: 0.000061, l3: 0.000105, l4: 0.000575, l5: 0.000984, l6: 0.001815
[epoch: 597/1000, batch:   208/ 1052, ite: 156800] train loss: 0.004127, tar: 0.000082 
l0: 0.000018, l1: 0.000018, l2: 0.000030, l3: 0.000091, l4: 0.000221, l5: 0.000351, l6: 0.000915
[epoch: 597/1000, batch:   288/ 1052, ite: 156820] train loss: 0.004128, tar: 0.000082 
l0: 0.000098, l1: 0.000100, l2: 0.000099, l3: 0.000206, l4: 0.000809, l5: 0.000989, l6: 0.001374
[epoch: 597/1000, batch:   368/ 1052, ite: 156840] train loss: 0.004128, tar: 0.000082 
l0: 0.000229, l1: 0.000228, l2: 0.000240, l3: 0.000344, l4: 0.000544, l5: 0.001341, l6: 0.003520
[epoch: 597/1000, batch:   448/ 1052, ite: 156860] train loss: 0.004127, tar: 0.000082 
l0: 0.000039, l1: 0.000038, l2: 0.000063, l3: 0.000115, l4: 0.000318, l5: 0.000378, l6: 0.001673
[epoch: 597/1000, batch:   528/ 1052, ite: 156880] train loss: 0.004127, tar: 0.000082 
l0: 0.000016, l1: 0.000017, l2: 0.000031, l3: 0.000115, l4: 0.000381, l5: 0.001056, l6: 0.001445
[epoch: 597/1000, batch:   608/ 1052, ite: 156900] train loss: 0.004128, tar: 0.000082 
l0: 0.000052, l1: 0.000053, l2: 0.000105, l3: 0.000244, l4: 0.000582, l5: 0.001322, l6: 0.002180
[epoch: 597/1000, batch:   688/ 1052, ite: 156920] train loss: 0.004127, tar: 0.000082 
l0: 0.000032, l1: 0.000032, l2: 0.000059, l3: 0.000107, l4: 0.000477, l5: 0.000942, l6: 0.001845
[epoch: 597/1000, batch:   768/ 1052, ite: 156940] train loss: 0.004127, tar: 0.000082 
l0: 0.000108, l1: 0.000109, l2: 0.000132, l3: 0.000162, l4: 0.000325, l5: 0.000779, l6: 0.001213
[epoch: 597/1000, batch:   848/ 1052, ite: 156960] train loss: 0.004125, tar: 0.000082 
l0: 0.000037, l1: 0.000038, l2: 0.000080, l3: 0.000164, l4: 0.000393, l5: 0.000855, l6: 0.001372
[epoch: 597/1000, batch:   928/ 1052, ite: 156980] train loss: 0.004126, tar: 0.000082 
l0: 0.000052, l1: 0.000051, l2: 0.000098, l3: 0.000155, l4: 0.000383, l5: 0.001014, l6: 0.001228
[epoch: 597/1000, batch:  1008/ 1052, ite: 157000] train loss: 0.004125, tar: 0.000082 
[Epoch 597/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000091, l1: 0.000091, l2: 0.000204, l3: 0.000503, l4: 0.000940, l5: 0.001892, l6: 0.002819
[epoch: 598/1000, batch:    36/ 1052, ite: 157020] train loss: 0.004126, tar: 0.000082 
l0: 0.000022, l1: 0.000023, l2: 0.000055, l3: 0.000094, l4: 0.000354, l5: 0.000547, l6: 0.000684
[epoch: 598/1000, batch:   116/ 1052, ite: 157040] train loss: 0.004126, tar: 0.000082 
l0: 0.000036, l1: 0.000033, l2: 0.000089, l3: 0.000219, l4: 0.000608, l5: 0.001054, l6: 0.002201
[epoch: 598/1000, batch:   196/ 1052, ite: 157060] train loss: 0.004126, tar: 0.000082 
l0: 0.000138, l1: 0.000140, l2: 0.000175, l3: 0.000258, l4: 0.000593, l5: 0.001370, l6: 0.002596
[epoch: 598/1000, batch:   276/ 1052, ite: 157080] train loss: 0.004127, tar: 0.000082 
l0: 0.000043, l1: 0.000044, l2: 0.000053, l3: 0.000096, l4: 0.000339, l5: 0.001289, l6: 0.000894
[epoch: 598/1000, batch:   356/ 1052, ite: 157100] train loss: 0.004126, tar: 0.000082 
l0: 0.000103, l1: 0.000106, l2: 0.000105, l3: 0.000189, l4: 0.000377, l5: 0.001171, l6: 0.002106
[epoch: 598/1000, batch:   436/ 1052, ite: 157120] train loss: 0.004127, tar: 0.000082 
l0: 0.000042, l1: 0.000042, l2: 0.000060, l3: 0.000189, l4: 0.000833, l5: 0.001226, l6: 0.002175
[epoch: 598/1000, batch:   516/ 1052, ite: 157140] train loss: 0.004126, tar: 0.000082 
l0: 0.000069, l1: 0.000070, l2: 0.000105, l3: 0.000206, l4: 0.000445, l5: 0.000825, l6: 0.002127
[epoch: 598/1000, batch:   596/ 1052, ite: 157160] train loss: 0.004126, tar: 0.000082 
l0: 0.000029, l1: 0.000029, l2: 0.000046, l3: 0.000076, l4: 0.000361, l5: 0.000564, l6: 0.001042
[epoch: 598/1000, batch:   676/ 1052, ite: 157180] train loss: 0.004126, tar: 0.000082 
l0: 0.000009, l1: 0.000010, l2: 0.000052, l3: 0.000233, l4: 0.000609, l5: 0.001536, l6: 0.001991
[epoch: 598/1000, batch:   756/ 1052, ite: 157200] train loss: 0.004124, tar: 0.000082 
l0: 0.000084, l1: 0.000084, l2: 0.000116, l3: 0.000239, l4: 0.000455, l5: 0.001224, l6: 0.002436
[epoch: 598/1000, batch:   836/ 1052, ite: 157220] train loss: 0.004124, tar: 0.000082 
l0: 0.000014, l1: 0.000013, l2: 0.000056, l3: 0.000174, l4: 0.000381, l5: 0.000330, l6: 0.001265
[epoch: 598/1000, batch:   916/ 1052, ite: 157240] train loss: 0.004124, tar: 0.000082 
l0: 0.000043, l1: 0.000043, l2: 0.000062, l3: 0.000089, l4: 0.000245, l5: 0.000515, l6: 0.000976
[epoch: 598/1000, batch:   996/ 1052, ite: 157260] train loss: 0.004124, tar: 0.000082 
[Epoch 598/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000060, l1: 0.000061, l2: 0.000092, l3: 0.000187, l4: 0.000361, l5: 0.000741, l6: 0.001585
[epoch: 599/1000, batch:    24/ 1052, ite: 157280] train loss: 0.004123, tar: 0.000082 
l0: 0.000031, l1: 0.000031, l2: 0.000078, l3: 0.000187, l4: 0.000550, l5: 0.000753, l6: 0.002090
[epoch: 599/1000, batch:   104/ 1052, ite: 157300] train loss: 0.004123, tar: 0.000081 
l0: 0.000061, l1: 0.000061, l2: 0.000139, l3: 0.000278, l4: 0.000396, l5: 0.001136, l6: 0.001758
[epoch: 599/1000, batch:   184/ 1052, ite: 157320] train loss: 0.004122, tar: 0.000081 
l0: 0.000080, l1: 0.000079, l2: 0.000105, l3: 0.000225, l4: 0.000546, l5: 0.001040, l6: 0.002147
[epoch: 599/1000, batch:   264/ 1052, ite: 157340] train loss: 0.004122, tar: 0.000081 
l0: 0.000010, l1: 0.000009, l2: 0.000046, l3: 0.000189, l4: 0.000339, l5: 0.000854, l6: 0.002244
[epoch: 599/1000, batch:   344/ 1052, ite: 157360] train loss: 0.004122, tar: 0.000081 
l0: 0.000156, l1: 0.000154, l2: 0.000238, l3: 0.000418, l4: 0.000968, l5: 0.001556, l6: 0.002823
[epoch: 599/1000, batch:   424/ 1052, ite: 157380] train loss: 0.004122, tar: 0.000081 
l0: 0.000082, l1: 0.000082, l2: 0.000159, l3: 0.000389, l4: 0.000625, l5: 0.001516, l6: 0.001706
[epoch: 599/1000, batch:   504/ 1052, ite: 157400] train loss: 0.004122, tar: 0.000081 
l0: 0.000080, l1: 0.000079, l2: 0.000119, l3: 0.000259, l4: 0.000550, l5: 0.000685, l6: 0.002134
[epoch: 599/1000, batch:   584/ 1052, ite: 157420] train loss: 0.004122, tar: 0.000081 
l0: 0.000040, l1: 0.000039, l2: 0.000070, l3: 0.000107, l4: 0.000311, l5: 0.000722, l6: 0.001378
[epoch: 599/1000, batch:   664/ 1052, ite: 157440] train loss: 0.004122, tar: 0.000081 
l0: 0.000177, l1: 0.000175, l2: 0.000252, l3: 0.000565, l4: 0.001293, l5: 0.001691, l6: 0.002743
[epoch: 599/1000, batch:   744/ 1052, ite: 157460] train loss: 0.004122, tar: 0.000081 
l0: 0.000083, l1: 0.000084, l2: 0.000097, l3: 0.000122, l4: 0.000253, l5: 0.000856, l6: 0.001432
[epoch: 599/1000, batch:   824/ 1052, ite: 157480] train loss: 0.004122, tar: 0.000081 
l0: 0.000069, l1: 0.000070, l2: 0.000100, l3: 0.000168, l4: 0.000429, l5: 0.000961, l6: 0.000845
[epoch: 599/1000, batch:   904/ 1052, ite: 157500] train loss: 0.004122, tar: 0.000081 
l0: 0.000063, l1: 0.000064, l2: 0.000081, l3: 0.000174, l4: 0.000567, l5: 0.001411, l6: 0.002076
[epoch: 599/1000, batch:   984/ 1052, ite: 157520] train loss: 0.004122, tar: 0.000081 
[Epoch 599/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000074, l1: 0.000079, l2: 0.000103, l3: 0.000146, l4: 0.000492, l5: 0.000973, l6: 0.001524
[epoch: 600/1000, batch:    12/ 1052, ite: 157540] train loss: 0.004122, tar: 0.000081 
l0: 0.000169, l1: 0.000172, l2: 0.000193, l3: 0.000339, l4: 0.000659, l5: 0.001284, l6: 0.002293
[epoch: 600/1000, batch:    92/ 1052, ite: 157560] train loss: 0.004122, tar: 0.000081 
l0: 0.000010, l1: 0.000009, l2: 0.000059, l3: 0.000106, l4: 0.000443, l5: 0.000934, l6: 0.001844
[epoch: 600/1000, batch:   172/ 1052, ite: 157580] train loss: 0.004122, tar: 0.000081 
l0: 0.000128, l1: 0.000127, l2: 0.000130, l3: 0.000166, l4: 0.000451, l5: 0.000583, l6: 0.001475
[epoch: 600/1000, batch:   252/ 1052, ite: 157600] train loss: 0.004121, tar: 0.000081 
l0: 0.000015, l1: 0.000015, l2: 0.000034, l3: 0.000130, l4: 0.000297, l5: 0.000976, l6: 0.001246
[epoch: 600/1000, batch:   332/ 1052, ite: 157620] train loss: 0.004120, tar: 0.000081 
l0: 0.000082, l1: 0.000081, l2: 0.000111, l3: 0.000240, l4: 0.000389, l5: 0.001403, l6: 0.002397
[epoch: 600/1000, batch:   412/ 1052, ite: 157640] train loss: 0.004120, tar: 0.000081 
l0: 0.000046, l1: 0.000046, l2: 0.000105, l3: 0.000132, l4: 0.000290, l5: 0.001082, l6: 0.001915
[epoch: 600/1000, batch:   492/ 1052, ite: 157660] train loss: 0.004119, tar: 0.000081 
l0: 0.000039, l1: 0.000039, l2: 0.000124, l3: 0.000260, l4: 0.000773, l5: 0.001897, l6: 0.002860
[epoch: 600/1000, batch:   572/ 1052, ite: 157680] train loss: 0.004120, tar: 0.000081 
l0: 0.000065, l1: 0.000067, l2: 0.000105, l3: 0.000252, l4: 0.000453, l5: 0.000959, l6: 0.002317
[epoch: 600/1000, batch:   652/ 1052, ite: 157700] train loss: 0.004119, tar: 0.000081 
l0: 0.000051, l1: 0.000053, l2: 0.000064, l3: 0.000119, l4: 0.000357, l5: 0.000509, l6: 0.001084
[epoch: 600/1000, batch:   732/ 1052, ite: 157720] train loss: 0.004119, tar: 0.000081 
l0: 0.000078, l1: 0.000079, l2: 0.000131, l3: 0.000291, l4: 0.000711, l5: 0.001821, l6: 0.003199
[epoch: 600/1000, batch:   812/ 1052, ite: 157740] train loss: 0.004119, tar: 0.000081 
l0: 0.000046, l1: 0.000045, l2: 0.000079, l3: 0.000156, l4: 0.000607, l5: 0.000929, l6: 0.002046
[epoch: 600/1000, batch:   892/ 1052, ite: 157760] train loss: 0.004120, tar: 0.000081 
l0: 0.000024, l1: 0.000023, l2: 0.000072, l3: 0.000230, l4: 0.000501, l5: 0.001087, l6: 0.002573
[epoch: 600/1000, batch:   972/ 1052, ite: 157780] train loss: 0.004120, tar: 0.000081 
l0: 0.000046, l1: 0.000049, l2: 0.000046, l3: 0.000094, l4: 0.000432, l5: 0.000971, l6: 0.001252
[epoch: 600/1000, batch:  1052/ 1052, ite: 157800] train loss: 0.004120, tar: 0.000081 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_157800.pkl
[Epoch 600/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000066, l1: 0.000066, l2: 0.000089, l3: 0.000166, l4: 0.000479, l5: 0.001220, l6: 0.001536
[epoch: 601/1000, batch:    80/ 1052, ite: 157820] train loss: 0.004182, tar: 0.000072 
l0: 0.000028, l1: 0.000027, l2: 0.000054, l3: 0.000121, l4: 0.000225, l5: 0.000469, l6: 0.001095
[epoch: 601/1000, batch:   160/ 1052, ite: 157840] train loss: 0.003827, tar: 0.000068 
l0: 0.000116, l1: 0.000118, l2: 0.000134, l3: 0.000246, l4: 0.000680, l5: 0.001771, l6: 0.003333
[epoch: 601/1000, batch:   240/ 1052, ite: 157860] train loss: 0.004033, tar: 0.000069 
l0: 0.000010, l1: 0.000011, l2: 0.000021, l3: 0.000063, l4: 0.000172, l5: 0.000367, l6: 0.001184
[epoch: 601/1000, batch:   320/ 1052, ite: 157880] train loss: 0.003999, tar: 0.000069 
l0: 0.000082, l1: 0.000087, l2: 0.000124, l3: 0.000197, l4: 0.000791, l5: 0.001863, l6: 0.003036
[epoch: 601/1000, batch:   400/ 1052, ite: 157900] train loss: 0.003953, tar: 0.000070 
l0: 0.000052, l1: 0.000052, l2: 0.000082, l3: 0.000167, l4: 0.000425, l5: 0.000900, l6: 0.001568
[epoch: 601/1000, batch:   480/ 1052, ite: 157920] train loss: 0.003956, tar: 0.000071 
l0: 0.000051, l1: 0.000053, l2: 0.000071, l3: 0.000124, l4: 0.000332, l5: 0.000985, l6: 0.001050
[epoch: 601/1000, batch:   560/ 1052, ite: 157940] train loss: 0.003990, tar: 0.000072 
l0: 0.000339, l1: 0.000345, l2: 0.000431, l3: 0.000715, l4: 0.001416, l5: 0.002486, l6: 0.004870
[epoch: 601/1000, batch:   640/ 1052, ite: 157960] train loss: 0.003963, tar: 0.000071 
l0: 0.000122, l1: 0.000120, l2: 0.000141, l3: 0.000255, l4: 0.000474, l5: 0.000853, l6: 0.001334
[epoch: 601/1000, batch:   720/ 1052, ite: 157980] train loss: 0.004002, tar: 0.000073 
l0: 0.000028, l1: 0.000028, l2: 0.000057, l3: 0.000104, l4: 0.000339, l5: 0.000548, l6: 0.000786
[epoch: 601/1000, batch:   800/ 1052, ite: 158000] train loss: 0.004022, tar: 0.000074 
l0: 0.000025, l1: 0.000025, l2: 0.000058, l3: 0.000115, l4: 0.000378, l5: 0.000539, l6: 0.001425
[epoch: 601/1000, batch:   880/ 1052, ite: 158020] train loss: 0.004020, tar: 0.000072 
l0: 0.000021, l1: 0.000022, l2: 0.000049, l3: 0.000109, l4: 0.000211, l5: 0.000674, l6: 0.001353
[epoch: 601/1000, batch:   960/ 1052, ite: 158040] train loss: 0.004040, tar: 0.000073 
l0: 0.000058, l1: 0.000061, l2: 0.000096, l3: 0.000334, l4: 0.001099, l5: 0.001134, l6: 0.002829
[epoch: 601/1000, batch:  1040/ 1052, ite: 158060] train loss: 0.004028, tar: 0.000073 
[Epoch 601/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000069, l1: 0.000069, l2: 0.000102, l3: 0.000235, l4: 0.000388, l5: 0.000858, l6: 0.001637
[epoch: 602/1000, batch:    68/ 1052, ite: 158080] train loss: 0.004004, tar: 0.000073 
l0: 0.000025, l1: 0.000022, l2: 0.000035, l3: 0.000103, l4: 0.000228, l5: 0.000598, l6: 0.001409
[epoch: 602/1000, batch:   148/ 1052, ite: 158100] train loss: 0.003974, tar: 0.000072 
l0: 0.000062, l1: 0.000063, l2: 0.000092, l3: 0.000160, l4: 0.000425, l5: 0.000771, l6: 0.001832
[epoch: 602/1000, batch:   228/ 1052, ite: 158120] train loss: 0.004002, tar: 0.000073 
l0: 0.000109, l1: 0.000111, l2: 0.000161, l3: 0.000292, l4: 0.000518, l5: 0.001181, l6: 0.002392
[epoch: 602/1000, batch:   308/ 1052, ite: 158140] train loss: 0.004014, tar: 0.000072 
l0: 0.000171, l1: 0.000170, l2: 0.000193, l3: 0.000277, l4: 0.000543, l5: 0.001104, l6: 0.002208
[epoch: 602/1000, batch:   388/ 1052, ite: 158160] train loss: 0.004007, tar: 0.000072 
l0: 0.000061, l1: 0.000059, l2: 0.000117, l3: 0.000254, l4: 0.000852, l5: 0.001232, l6: 0.002693
[epoch: 602/1000, batch:   468/ 1052, ite: 158180] train loss: 0.004008, tar: 0.000071 
l0: 0.000010, l1: 0.000011, l2: 0.000024, l3: 0.000055, l4: 0.000155, l5: 0.000858, l6: 0.000786
[epoch: 602/1000, batch:   548/ 1052, ite: 158200] train loss: 0.004052, tar: 0.000072 
l0: 0.000051, l1: 0.000051, l2: 0.000080, l3: 0.000162, l4: 0.000379, l5: 0.001086, l6: 0.002400
[epoch: 602/1000, batch:   628/ 1052, ite: 158220] train loss: 0.004046, tar: 0.000071 
l0: 0.000124, l1: 0.000125, l2: 0.000160, l3: 0.000229, l4: 0.000442, l5: 0.000999, l6: 0.002036
[epoch: 602/1000, batch:   708/ 1052, ite: 158240] train loss: 0.004067, tar: 0.000071 
l0: 0.000014, l1: 0.000013, l2: 0.000026, l3: 0.000055, l4: 0.000240, l5: 0.000279, l6: 0.000726
[epoch: 602/1000, batch:   788/ 1052, ite: 158260] train loss: 0.004056, tar: 0.000071 
l0: 0.000050, l1: 0.000051, l2: 0.000063, l3: 0.000148, l4: 0.000573, l5: 0.001118, l6: 0.001890
[epoch: 602/1000, batch:   868/ 1052, ite: 158280] train loss: 0.004061, tar: 0.000071 
l0: 0.000113, l1: 0.000114, l2: 0.000128, l3: 0.000159, l4: 0.000332, l5: 0.000676, l6: 0.001568
[epoch: 602/1000, batch:   948/ 1052, ite: 158300] train loss: 0.004076, tar: 0.000072 
l0: 0.000087, l1: 0.000087, l2: 0.000158, l3: 0.000208, l4: 0.000270, l5: 0.000795, l6: 0.002074
[epoch: 602/1000, batch:  1028/ 1052, ite: 158320] train loss: 0.004072, tar: 0.000072 
[Epoch 602/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000051, l1: 0.000052, l2: 0.000078, l3: 0.000134, l4: 0.000403, l5: 0.000696, l6: 0.002015
[epoch: 603/1000, batch:    56/ 1052, ite: 158340] train loss: 0.004071, tar: 0.000072 
l0: 0.000030, l1: 0.000030, l2: 0.000082, l3: 0.000128, l4: 0.000379, l5: 0.001400, l6: 0.001655
[epoch: 603/1000, batch:   136/ 1052, ite: 158360] train loss: 0.004066, tar: 0.000071 
l0: 0.000158, l1: 0.000159, l2: 0.000241, l3: 0.000405, l4: 0.000688, l5: 0.001395, l6: 0.001997
[epoch: 603/1000, batch:   216/ 1052, ite: 158380] train loss: 0.004080, tar: 0.000072 
l0: 0.000076, l1: 0.000078, l2: 0.000127, l3: 0.000222, l4: 0.000583, l5: 0.001097, l6: 0.002063
[epoch: 603/1000, batch:   296/ 1052, ite: 158400] train loss: 0.004080, tar: 0.000072 
l0: 0.000048, l1: 0.000048, l2: 0.000094, l3: 0.000248, l4: 0.000511, l5: 0.001093, l6: 0.001754
[epoch: 603/1000, batch:   376/ 1052, ite: 158420] train loss: 0.004087, tar: 0.000072 
l0: 0.000093, l1: 0.000093, l2: 0.000161, l3: 0.000309, l4: 0.000627, l5: 0.002422, l6: 0.003929
[epoch: 603/1000, batch:   456/ 1052, ite: 158440] train loss: 0.004093, tar: 0.000072 
l0: 0.000123, l1: 0.000124, l2: 0.000174, l3: 0.000320, l4: 0.000608, l5: 0.000930, l6: 0.001453
[epoch: 603/1000, batch:   536/ 1052, ite: 158460] train loss: 0.004097, tar: 0.000072 
l0: 0.000041, l1: 0.000041, l2: 0.000056, l3: 0.000099, l4: 0.000352, l5: 0.000736, l6: 0.000985
[epoch: 603/1000, batch:   616/ 1052, ite: 158480] train loss: 0.004085, tar: 0.000072 
l0: 0.000012, l1: 0.000013, l2: 0.000029, l3: 0.000091, l4: 0.000281, l5: 0.000614, l6: 0.001516
[epoch: 603/1000, batch:   696/ 1052, ite: 158500] train loss: 0.004087, tar: 0.000072 
l0: 0.000029, l1: 0.000031, l2: 0.000041, l3: 0.000115, l4: 0.000425, l5: 0.000939, l6: 0.002034
[epoch: 603/1000, batch:   776/ 1052, ite: 158520] train loss: 0.004083, tar: 0.000072 
l0: 0.000049, l1: 0.000044, l2: 0.000068, l3: 0.000196, l4: 0.000274, l5: 0.000802, l6: 0.001392
[epoch: 603/1000, batch:   856/ 1052, ite: 158540] train loss: 0.004086, tar: 0.000072 
l0: 0.000047, l1: 0.000046, l2: 0.000070, l3: 0.000134, l4: 0.000404, l5: 0.000958, l6: 0.002187
[epoch: 603/1000, batch:   936/ 1052, ite: 158560] train loss: 0.004083, tar: 0.000072 
l0: 0.000067, l1: 0.000065, l2: 0.000106, l3: 0.000244, l4: 0.000465, l5: 0.000856, l6: 0.001652
[epoch: 603/1000, batch:  1016/ 1052, ite: 158580] train loss: 0.004079, tar: 0.000072 
[Epoch 603/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000032, l1: 0.000035, l2: 0.000051, l3: 0.000145, l4: 0.000684, l5: 0.001165, l6: 0.002082
[epoch: 604/1000, batch:    44/ 1052, ite: 158600] train loss: 0.004074, tar: 0.000072 
l0: 0.000009, l1: 0.000008, l2: 0.000043, l3: 0.000113, l4: 0.000280, l5: 0.000898, l6: 0.001336
[epoch: 604/1000, batch:   124/ 1052, ite: 158620] train loss: 0.004069, tar: 0.000071 
l0: 0.000038, l1: 0.000041, l2: 0.000073, l3: 0.000123, l4: 0.000342, l5: 0.000737, l6: 0.001484
[epoch: 604/1000, batch:   204/ 1052, ite: 158640] train loss: 0.004077, tar: 0.000072 
l0: 0.000019, l1: 0.000020, l2: 0.000024, l3: 0.000059, l4: 0.000329, l5: 0.000513, l6: 0.000936
[epoch: 604/1000, batch:   284/ 1052, ite: 158660] train loss: 0.004071, tar: 0.000072 
l0: 0.000138, l1: 0.000143, l2: 0.000173, l3: 0.000302, l4: 0.000871, l5: 0.001967, l6: 0.005000
[epoch: 604/1000, batch:   364/ 1052, ite: 158680] train loss: 0.004077, tar: 0.000072 
l0: 0.000028, l1: 0.000029, l2: 0.000064, l3: 0.000167, l4: 0.000586, l5: 0.001254, l6: 0.001916
[epoch: 604/1000, batch:   444/ 1052, ite: 158700] train loss: 0.004082, tar: 0.000072 
l0: 0.000032, l1: 0.000032, l2: 0.000073, l3: 0.000219, l4: 0.000440, l5: 0.001264, l6: 0.001962
[epoch: 604/1000, batch:   524/ 1052, ite: 158720] train loss: 0.004082, tar: 0.000073 
l0: 0.000116, l1: 0.000121, l2: 0.000236, l3: 0.000481, l4: 0.000927, l5: 0.001330, l6: 0.003115
[epoch: 604/1000, batch:   604/ 1052, ite: 158740] train loss: 0.004087, tar: 0.000073 
l0: 0.000076, l1: 0.000075, l2: 0.000109, l3: 0.000189, l4: 0.000441, l5: 0.000741, l6: 0.001996
[epoch: 604/1000, batch:   684/ 1052, ite: 158760] train loss: 0.004084, tar: 0.000074 
l0: 0.000034, l1: 0.000035, l2: 0.000048, l3: 0.000087, l4: 0.000419, l5: 0.000776, l6: 0.001145
[epoch: 604/1000, batch:   764/ 1052, ite: 158780] train loss: 0.004077, tar: 0.000074 
l0: 0.000050, l1: 0.000051, l2: 0.000067, l3: 0.000182, l4: 0.000506, l5: 0.000821, l6: 0.001455
[epoch: 604/1000, batch:   844/ 1052, ite: 158800] train loss: 0.004073, tar: 0.000074 
l0: 0.000025, l1: 0.000024, l2: 0.000102, l3: 0.000402, l4: 0.000921, l5: 0.001832, l6: 0.003212
[epoch: 604/1000, batch:   924/ 1052, ite: 158820] train loss: 0.004075, tar: 0.000074 
l0: 0.000130, l1: 0.000132, l2: 0.000153, l3: 0.000254, l4: 0.000552, l5: 0.001141, l6: 0.002586
[epoch: 604/1000, batch:  1004/ 1052, ite: 158840] train loss: 0.004081, tar: 0.000074 
[Epoch 604/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000100, l1: 0.000102, l2: 0.000142, l3: 0.000210, l4: 0.000595, l5: 0.001167, l6: 0.001997
[epoch: 605/1000, batch:    32/ 1052, ite: 158860] train loss: 0.004081, tar: 0.000075 
l0: 0.000066, l1: 0.000084, l2: 0.000107, l3: 0.000173, l4: 0.000369, l5: 0.001367, l6: 0.002085
[epoch: 605/1000, batch:   112/ 1052, ite: 158880] train loss: 0.004094, tar: 0.000075 
l0: 0.000036, l1: 0.000047, l2: 0.000050, l3: 0.000111, l4: 0.000402, l5: 0.000924, l6: 0.001743
[epoch: 605/1000, batch:   192/ 1052, ite: 158900] train loss: 0.004093, tar: 0.000076 
l0: 0.000116, l1: 0.000112, l2: 0.000180, l3: 0.000233, l4: 0.000479, l5: 0.000946, l6: 0.001951
[epoch: 605/1000, batch:   272/ 1052, ite: 158920] train loss: 0.004081, tar: 0.000075 
l0: 0.000023, l1: 0.000022, l2: 0.000060, l3: 0.000137, l4: 0.000418, l5: 0.001101, l6: 0.001587
[epoch: 605/1000, batch:   352/ 1052, ite: 158940] train loss: 0.004070, tar: 0.000075 
l0: 0.000045, l1: 0.000049, l2: 0.000067, l3: 0.000122, l4: 0.000396, l5: 0.000815, l6: 0.001334
[epoch: 605/1000, batch:   432/ 1052, ite: 158960] train loss: 0.004092, tar: 0.000075 
l0: 0.000032, l1: 0.000035, l2: 0.000065, l3: 0.000156, l4: 0.000289, l5: 0.000931, l6: 0.001257
[epoch: 605/1000, batch:   512/ 1052, ite: 158980] train loss: 0.004088, tar: 0.000075 
l0: 0.000014, l1: 0.000014, l2: 0.000042, l3: 0.000113, l4: 0.000752, l5: 0.001285, l6: 0.001853
[epoch: 605/1000, batch:   592/ 1052, ite: 159000] train loss: 0.004081, tar: 0.000075 
l0: 0.000051, l1: 0.000048, l2: 0.000105, l3: 0.000182, l4: 0.000355, l5: 0.000732, l6: 0.001764
[epoch: 605/1000, batch:   672/ 1052, ite: 159020] train loss: 0.004086, tar: 0.000075 
l0: 0.000034, l1: 0.000035, l2: 0.000062, l3: 0.000184, l4: 0.000926, l5: 0.001796, l6: 0.003006
[epoch: 605/1000, batch:   752/ 1052, ite: 159040] train loss: 0.004084, tar: 0.000075 
l0: 0.000061, l1: 0.000060, l2: 0.000133, l3: 0.000221, l4: 0.000479, l5: 0.001104, l6: 0.002279
[epoch: 605/1000, batch:   832/ 1052, ite: 159060] train loss: 0.004081, tar: 0.000075 
l0: 0.000099, l1: 0.000099, l2: 0.000162, l3: 0.000279, l4: 0.000581, l5: 0.001283, l6: 0.002149
[epoch: 605/1000, batch:   912/ 1052, ite: 159080] train loss: 0.004078, tar: 0.000075 
l0: 0.000061, l1: 0.000065, l2: 0.000111, l3: 0.000336, l4: 0.000594, l5: 0.001174, l6: 0.002357
[epoch: 605/1000, batch:   992/ 1052, ite: 159100] train loss: 0.004083, tar: 0.000075 
[Epoch 605/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000038, l1: 0.000039, l2: 0.000057, l3: 0.000152, l4: 0.000536, l5: 0.000948, l6: 0.002121
[epoch: 606/1000, batch:    20/ 1052, ite: 159120] train loss: 0.004088, tar: 0.000075 
l0: 0.000070, l1: 0.000064, l2: 0.000129, l3: 0.000311, l4: 0.000573, l5: 0.001264, l6: 0.003248
[epoch: 606/1000, batch:   100/ 1052, ite: 159140] train loss: 0.004093, tar: 0.000075 
l0: 0.000035, l1: 0.000036, l2: 0.000096, l3: 0.000225, l4: 0.000500, l5: 0.001580, l6: 0.002315
[epoch: 606/1000, batch:   180/ 1052, ite: 159160] train loss: 0.004087, tar: 0.000075 
l0: 0.000019, l1: 0.000021, l2: 0.000053, l3: 0.000143, l4: 0.000666, l5: 0.001314, l6: 0.002962
[epoch: 606/1000, batch:   260/ 1052, ite: 159180] train loss: 0.004093, tar: 0.000074 
l0: 0.000063, l1: 0.000064, l2: 0.000084, l3: 0.000189, l4: 0.000411, l5: 0.000839, l6: 0.002125
[epoch: 606/1000, batch:   340/ 1052, ite: 159200] train loss: 0.004101, tar: 0.000074 
l0: 0.000790, l1: 0.001136, l2: 0.000959, l3: 0.000322, l4: 0.000754, l5: 0.001656, l6: 0.002794
[epoch: 606/1000, batch:   420/ 1052, ite: 159220] train loss: 0.004097, tar: 0.000075 
l0: 0.000025, l1: 0.000021, l2: 0.000045, l3: 0.000111, l4: 0.000233, l5: 0.000566, l6: 0.001423
[epoch: 606/1000, batch:   500/ 1052, ite: 159240] train loss: 0.004107, tar: 0.000075 
l0: 0.000055, l1: 0.000057, l2: 0.000077, l3: 0.000232, l4: 0.000367, l5: 0.000612, l6: 0.001485
[epoch: 606/1000, batch:   580/ 1052, ite: 159260] train loss: 0.004106, tar: 0.000075 
l0: 0.000008, l1: 0.000007, l2: 0.000025, l3: 0.000132, l4: 0.000433, l5: 0.000923, l6: 0.002040
[epoch: 606/1000, batch:   660/ 1052, ite: 159280] train loss: 0.004113, tar: 0.000076 
l0: 0.000097, l1: 0.000107, l2: 0.000130, l3: 0.000260, l4: 0.000509, l5: 0.000953, l6: 0.001330
[epoch: 606/1000, batch:   740/ 1052, ite: 159300] train loss: 0.004111, tar: 0.000076 
l0: 0.000022, l1: 0.000023, l2: 0.000069, l3: 0.000145, l4: 0.000330, l5: 0.001013, l6: 0.001643
[epoch: 606/1000, batch:   820/ 1052, ite: 159320] train loss: 0.004106, tar: 0.000076 
l0: 0.000087, l1: 0.000079, l2: 0.000104, l3: 0.000144, l4: 0.000330, l5: 0.000891, l6: 0.000942
[epoch: 606/1000, batch:   900/ 1052, ite: 159340] train loss: 0.004109, tar: 0.000076 
l0: 0.000077, l1: 0.000080, l2: 0.000091, l3: 0.000157, l4: 0.000480, l5: 0.000910, l6: 0.001677
[epoch: 606/1000, batch:   980/ 1052, ite: 159360] train loss: 0.004110, tar: 0.000076 
[Epoch 606/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000030, l1: 0.000030, l2: 0.000040, l3: 0.000088, l4: 0.000376, l5: 0.000764, l6: 0.001456
[epoch: 607/1000, batch:     8/ 1052, ite: 159380] train loss: 0.004112, tar: 0.000076 
l0: 0.000163, l1: 0.000157, l2: 0.000165, l3: 0.000339, l4: 0.000734, l5: 0.001466, l6: 0.002432
[epoch: 607/1000, batch:    88/ 1052, ite: 159400] train loss: 0.004121, tar: 0.000077 
l0: 0.000173, l1: 0.000175, l2: 0.000229, l3: 0.000305, l4: 0.000792, l5: 0.002165, l6: 0.003271
[epoch: 607/1000, batch:   168/ 1052, ite: 159420] train loss: 0.004116, tar: 0.000076 
l0: 0.000139, l1: 0.000140, l2: 0.000156, l3: 0.000237, l4: 0.000503, l5: 0.001396, l6: 0.001520
[epoch: 607/1000, batch:   248/ 1052, ite: 159440] train loss: 0.004120, tar: 0.000077 
l0: 0.000149, l1: 0.000152, l2: 0.000215, l3: 0.000359, l4: 0.000776, l5: 0.001495, l6: 0.002831
[epoch: 607/1000, batch:   328/ 1052, ite: 159460] train loss: 0.004115, tar: 0.000077 
l0: 0.000032, l1: 0.000032, l2: 0.000066, l3: 0.000199, l4: 0.000542, l5: 0.001213, l6: 0.002312
[epoch: 607/1000, batch:   408/ 1052, ite: 159480] train loss: 0.004109, tar: 0.000076 
l0: 0.000014, l1: 0.000016, l2: 0.000042, l3: 0.000083, l4: 0.000231, l5: 0.000427, l6: 0.001326
[epoch: 607/1000, batch:   488/ 1052, ite: 159500] train loss: 0.004115, tar: 0.000076 
l0: 0.000098, l1: 0.000100, l2: 0.000143, l3: 0.000237, l4: 0.000441, l5: 0.000993, l6: 0.001802
[epoch: 607/1000, batch:   568/ 1052, ite: 159520] train loss: 0.004108, tar: 0.000076 
l0: 0.000110, l1: 0.000114, l2: 0.000173, l3: 0.000459, l4: 0.001008, l5: 0.002399, l6: 0.004274
[epoch: 607/1000, batch:   648/ 1052, ite: 159540] train loss: 0.004114, tar: 0.000076 
l0: 0.000028, l1: 0.000029, l2: 0.000049, l3: 0.000094, l4: 0.000226, l5: 0.000528, l6: 0.000871
[epoch: 607/1000, batch:   728/ 1052, ite: 159560] train loss: 0.004115, tar: 0.000076 
l0: 0.000129, l1: 0.000133, l2: 0.000212, l3: 0.000339, l4: 0.000799, l5: 0.002028, l6: 0.003481
[epoch: 607/1000, batch:   808/ 1052, ite: 159580] train loss: 0.004117, tar: 0.000077 
l0: 0.000039, l1: 0.000041, l2: 0.000058, l3: 0.000175, l4: 0.000429, l5: 0.001005, l6: 0.001385
[epoch: 607/1000, batch:   888/ 1052, ite: 159600] train loss: 0.004120, tar: 0.000076 
l0: 0.000070, l1: 0.000069, l2: 0.000101, l3: 0.000203, l4: 0.000398, l5: 0.000928, l6: 0.001508
[epoch: 607/1000, batch:   968/ 1052, ite: 159620] train loss: 0.004114, tar: 0.000076 
l0: 0.000025, l1: 0.000025, l2: 0.000057, l3: 0.000202, l4: 0.000318, l5: 0.000702, l6: 0.001656
[epoch: 607/1000, batch:  1048/ 1052, ite: 159640] train loss: 0.004112, tar: 0.000076 
[Epoch 607/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000039, l1: 0.000042, l2: 0.000049, l3: 0.000140, l4: 0.000287, l5: 0.000904, l6: 0.001662
[epoch: 608/1000, batch:    76/ 1052, ite: 159660] train loss: 0.004121, tar: 0.000076 
l0: 0.000129, l1: 0.000132, l2: 0.000163, l3: 0.000295, l4: 0.000620, l5: 0.001415, l6: 0.001747
[epoch: 608/1000, batch:   156/ 1052, ite: 159680] train loss: 0.004118, tar: 0.000076 
l0: 0.000019, l1: 0.000021, l2: 0.000074, l3: 0.000186, l4: 0.000381, l5: 0.001037, l6: 0.001618
[epoch: 608/1000, batch:   236/ 1052, ite: 159700] train loss: 0.004114, tar: 0.000076 
l0: 0.000090, l1: 0.000088, l2: 0.000105, l3: 0.000178, l4: 0.000298, l5: 0.001189, l6: 0.001210
[epoch: 608/1000, batch:   316/ 1052, ite: 159720] train loss: 0.004112, tar: 0.000076 
l0: 0.000067, l1: 0.000067, l2: 0.000071, l3: 0.000111, l4: 0.000323, l5: 0.000916, l6: 0.001369
[epoch: 608/1000, batch:   396/ 1052, ite: 159740] train loss: 0.004109, tar: 0.000076 
l0: 0.000056, l1: 0.000054, l2: 0.000103, l3: 0.000185, l4: 0.000313, l5: 0.001668, l6: 0.002548
[epoch: 608/1000, batch:   476/ 1052, ite: 159760] train loss: 0.004113, tar: 0.000076 
l0: 0.000042, l1: 0.000040, l2: 0.000068, l3: 0.000117, l4: 0.000223, l5: 0.000787, l6: 0.001684
[epoch: 608/1000, batch:   556/ 1052, ite: 159780] train loss: 0.004107, tar: 0.000075 
l0: 0.000010, l1: 0.000010, l2: 0.000044, l3: 0.000111, l4: 0.000330, l5: 0.000628, l6: 0.001173
[epoch: 608/1000, batch:   636/ 1052, ite: 159800] train loss: 0.004104, tar: 0.000075 
l0: 0.000154, l1: 0.000159, l2: 0.000155, l3: 0.000265, l4: 0.000519, l5: 0.001017, l6: 0.002330
[epoch: 608/1000, batch:   716/ 1052, ite: 159820] train loss: 0.004099, tar: 0.000075 
l0: 0.000132, l1: 0.000135, l2: 0.000159, l3: 0.000222, l4: 0.000501, l5: 0.000772, l6: 0.001773
[epoch: 608/1000, batch:   796/ 1052, ite: 159840] train loss: 0.004099, tar: 0.000075 
l0: 0.000199, l1: 0.000216, l2: 0.000295, l3: 0.000608, l4: 0.001007, l5: 0.001894, l6: 0.002864
[epoch: 608/1000, batch:   876/ 1052, ite: 159860] train loss: 0.004098, tar: 0.000075 
l0: 0.000052, l1: 0.000054, l2: 0.000079, l3: 0.000340, l4: 0.000731, l5: 0.001436, l6: 0.001759
[epoch: 608/1000, batch:   956/ 1052, ite: 159880] train loss: 0.004097, tar: 0.000075 
l0: 0.000077, l1: 0.000076, l2: 0.000109, l3: 0.000158, l4: 0.000434, l5: 0.001038, l6: 0.001528
[epoch: 608/1000, batch:  1036/ 1052, ite: 159900] train loss: 0.004102, tar: 0.000075 
[Epoch 608/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000033, l1: 0.000033, l2: 0.000037, l3: 0.000085, l4: 0.000367, l5: 0.000433, l6: 0.000731
[epoch: 609/1000, batch:    64/ 1052, ite: 159920] train loss: 0.004102, tar: 0.000075 
l0: 0.000099, l1: 0.000100, l2: 0.000168, l3: 0.000259, l4: 0.000632, l5: 0.000966, l6: 0.002740
[epoch: 609/1000, batch:   144/ 1052, ite: 159940] train loss: 0.004104, tar: 0.000075 
l0: 0.000057, l1: 0.000060, l2: 0.000087, l3: 0.000206, l4: 0.000474, l5: 0.001320, l6: 0.002217
[epoch: 609/1000, batch:   224/ 1052, ite: 159960] train loss: 0.004103, tar: 0.000075 
l0: 0.000038, l1: 0.000039, l2: 0.000043, l3: 0.000085, l4: 0.000175, l5: 0.000669, l6: 0.001314
[epoch: 609/1000, batch:   304/ 1052, ite: 159980] train loss: 0.004098, tar: 0.000075 
l0: 0.000079, l1: 0.000079, l2: 0.000106, l3: 0.000206, l4: 0.000366, l5: 0.000590, l6: 0.001280
[epoch: 609/1000, batch:   384/ 1052, ite: 160000] train loss: 0.004098, tar: 0.000075 
l0: 0.000097, l1: 0.000095, l2: 0.000141, l3: 0.000205, l4: 0.000510, l5: 0.001214, l6: 0.001395
[epoch: 609/1000, batch:   464/ 1052, ite: 160020] train loss: 0.004092, tar: 0.000075 
l0: 0.000042, l1: 0.000040, l2: 0.000075, l3: 0.000153, l4: 0.000387, l5: 0.000873, l6: 0.001781
[epoch: 609/1000, batch:   544/ 1052, ite: 160040] train loss: 0.004090, tar: 0.000075 
l0: 0.000086, l1: 0.000088, l2: 0.000107, l3: 0.000160, l4: 0.000465, l5: 0.001027, l6: 0.001325
[epoch: 609/1000, batch:   624/ 1052, ite: 160060] train loss: 0.004097, tar: 0.000075 
l0: 0.000137, l1: 0.000142, l2: 0.000191, l3: 0.000350, l4: 0.000669, l5: 0.000945, l6: 0.001698
[epoch: 609/1000, batch:   704/ 1052, ite: 160080] train loss: 0.004095, tar: 0.000075 
l0: 0.000067, l1: 0.000062, l2: 0.000133, l3: 0.000245, l4: 0.000466, l5: 0.001040, l6: 0.002553
[epoch: 609/1000, batch:   784/ 1052, ite: 160100] train loss: 0.004096, tar: 0.000075 
l0: 0.000011, l1: 0.000011, l2: 0.000066, l3: 0.000176, l4: 0.000570, l5: 0.001023, l6: 0.001684
[epoch: 609/1000, batch:   864/ 1052, ite: 160120] train loss: 0.004095, tar: 0.000075 
l0: 0.000089, l1: 0.000092, l2: 0.000172, l3: 0.000268, l4: 0.000385, l5: 0.000980, l6: 0.001832
[epoch: 609/1000, batch:   944/ 1052, ite: 160140] train loss: 0.004095, tar: 0.000075 
l0: 0.000058, l1: 0.000061, l2: 0.000071, l3: 0.000226, l4: 0.000465, l5: 0.000810, l6: 0.001519
[epoch: 609/1000, batch:  1024/ 1052, ite: 160160] train loss: 0.004094, tar: 0.000075 
[Epoch 609/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000148, l1: 0.000147, l2: 0.000214, l3: 0.000364, l4: 0.000943, l5: 0.002344, l6: 0.003308
[epoch: 610/1000, batch:    52/ 1052, ite: 160180] train loss: 0.004100, tar: 0.000075 
l0: 0.000038, l1: 0.000040, l2: 0.000071, l3: 0.000164, l4: 0.000632, l5: 0.001288, l6: 0.002556
[epoch: 610/1000, batch:   132/ 1052, ite: 160200] train loss: 0.004103, tar: 0.000075 
l0: 0.000011, l1: 0.000013, l2: 0.000022, l3: 0.000131, l4: 0.000403, l5: 0.000821, l6: 0.001052
[epoch: 610/1000, batch:   212/ 1052, ite: 160220] train loss: 0.004099, tar: 0.000075 
l0: 0.000081, l1: 0.000081, l2: 0.000144, l3: 0.000283, l4: 0.000417, l5: 0.001648, l6: 0.001820
[epoch: 610/1000, batch:   292/ 1052, ite: 160240] train loss: 0.004095, tar: 0.000074 
l0: 0.000011, l1: 0.000011, l2: 0.000044, l3: 0.000099, l4: 0.000314, l5: 0.000777, l6: 0.000988
[epoch: 610/1000, batch:   372/ 1052, ite: 160260] train loss: 0.004097, tar: 0.000074 
l0: 0.000112, l1: 0.000113, l2: 0.000172, l3: 0.000280, l4: 0.000621, l5: 0.000922, l6: 0.001509
[epoch: 610/1000, batch:   452/ 1052, ite: 160280] train loss: 0.004097, tar: 0.000074 
l0: 0.000043, l1: 0.000041, l2: 0.000093, l3: 0.000173, l4: 0.000510, l5: 0.001108, l6: 0.002304
[epoch: 610/1000, batch:   532/ 1052, ite: 160300] train loss: 0.004095, tar: 0.000074 
l0: 0.000041, l1: 0.000038, l2: 0.000065, l3: 0.000109, l4: 0.000332, l5: 0.000828, l6: 0.001010
[epoch: 610/1000, batch:   612/ 1052, ite: 160320] train loss: 0.004093, tar: 0.000074 
l0: 0.000087, l1: 0.000089, l2: 0.000124, l3: 0.000268, l4: 0.000518, l5: 0.000858, l6: 0.001919
[epoch: 610/1000, batch:   692/ 1052, ite: 160340] train loss: 0.004095, tar: 0.000074 
l0: 0.000052, l1: 0.000049, l2: 0.000094, l3: 0.000106, l4: 0.000294, l5: 0.000407, l6: 0.000579
[epoch: 610/1000, batch:   772/ 1052, ite: 160360] train loss: 0.004096, tar: 0.000074 
l0: 0.000094, l1: 0.000093, l2: 0.000140, l3: 0.000332, l4: 0.000607, l5: 0.001415, l6: 0.002319
[epoch: 610/1000, batch:   852/ 1052, ite: 160380] train loss: 0.004097, tar: 0.000074 
l0: 0.000031, l1: 0.000031, l2: 0.000054, l3: 0.000159, l4: 0.000316, l5: 0.000673, l6: 0.001706
[epoch: 610/1000, batch:   932/ 1052, ite: 160400] train loss: 0.004095, tar: 0.000074 
l0: 0.000213, l1: 0.000220, l2: 0.000278, l3: 0.000413, l4: 0.000681, l5: 0.001307, l6: 0.005204
[epoch: 610/1000, batch:  1012/ 1052, ite: 160420] train loss: 0.004094, tar: 0.000074 
[Epoch 610/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000081, l1: 0.000082, l2: 0.000119, l3: 0.000222, l4: 0.000653, l5: 0.000959, l6: 0.002265
[epoch: 611/1000, batch:    40/ 1052, ite: 160440] train loss: 0.004095, tar: 0.000074 
l0: 0.000041, l1: 0.000041, l2: 0.000107, l3: 0.000256, l4: 0.000431, l5: 0.001020, l6: 0.001455
[epoch: 611/1000, batch:   120/ 1052, ite: 160460] train loss: 0.004091, tar: 0.000074 
l0: 0.000044, l1: 0.000044, l2: 0.000064, l3: 0.000135, l4: 0.000341, l5: 0.000862, l6: 0.001413
[epoch: 611/1000, batch:   200/ 1052, ite: 160480] train loss: 0.004093, tar: 0.000074 
l0: 0.000024, l1: 0.000024, l2: 0.000102, l3: 0.000214, l4: 0.000580, l5: 0.001182, l6: 0.002191
[epoch: 611/1000, batch:   280/ 1052, ite: 160500] train loss: 0.004097, tar: 0.000074 
l0: 0.000100, l1: 0.000099, l2: 0.000151, l3: 0.000270, l4: 0.000538, l5: 0.001258, l6: 0.002338
[epoch: 611/1000, batch:   360/ 1052, ite: 160520] train loss: 0.004098, tar: 0.000074 
l0: 0.000018, l1: 0.000017, l2: 0.000065, l3: 0.000150, l4: 0.000366, l5: 0.000840, l6: 0.001431
[epoch: 611/1000, batch:   440/ 1052, ite: 160540] train loss: 0.004099, tar: 0.000074 
l0: 0.000048, l1: 0.000045, l2: 0.000107, l3: 0.000166, l4: 0.000329, l5: 0.000521, l6: 0.001186
[epoch: 611/1000, batch:   520/ 1052, ite: 160560] train loss: 0.004094, tar: 0.000074 
l0: 0.000103, l1: 0.000103, l2: 0.000106, l3: 0.000126, l4: 0.000333, l5: 0.000730, l6: 0.001013
[epoch: 611/1000, batch:   600/ 1052, ite: 160580] train loss: 0.004094, tar: 0.000074 
l0: 0.000038, l1: 0.000038, l2: 0.000052, l3: 0.000113, l4: 0.000307, l5: 0.000688, l6: 0.001419
[epoch: 611/1000, batch:   680/ 1052, ite: 160600] train loss: 0.004090, tar: 0.000074 
l0: 0.000051, l1: 0.000048, l2: 0.000078, l3: 0.000169, l4: 0.000306, l5: 0.000651, l6: 0.001285
[epoch: 611/1000, batch:   760/ 1052, ite: 160620] train loss: 0.004091, tar: 0.000074 
l0: 0.000009, l1: 0.000009, l2: 0.000036, l3: 0.000087, l4: 0.000237, l5: 0.000572, l6: 0.001655
[epoch: 611/1000, batch:   840/ 1052, ite: 160640] train loss: 0.004089, tar: 0.000074 
l0: 0.000036, l1: 0.000037, l2: 0.000066, l3: 0.000219, l4: 0.000396, l5: 0.001409, l6: 0.002561
[epoch: 611/1000, batch:   920/ 1052, ite: 160660] train loss: 0.004089, tar: 0.000074 
l0: 0.000029, l1: 0.000028, l2: 0.000081, l3: 0.000139, l4: 0.000389, l5: 0.000894, l6: 0.001408
[epoch: 611/1000, batch:  1000/ 1052, ite: 160680] train loss: 0.004092, tar: 0.000074 
[Epoch 611/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000101, l1: 0.000103, l2: 0.000111, l3: 0.000165, l4: 0.000435, l5: 0.001155, l6: 0.002377
[epoch: 612/1000, batch:    28/ 1052, ite: 160700] train loss: 0.004091, tar: 0.000074 
l0: 0.000016, l1: 0.000015, l2: 0.000060, l3: 0.000189, l4: 0.000864, l5: 0.001489, l6: 0.002710
[epoch: 612/1000, batch:   108/ 1052, ite: 160720] train loss: 0.004092, tar: 0.000074 
l0: 0.000130, l1: 0.000131, l2: 0.000201, l3: 0.000371, l4: 0.000488, l5: 0.001615, l6: 0.002195
[epoch: 612/1000, batch:   188/ 1052, ite: 160740] train loss: 0.004102, tar: 0.000074 
l0: 0.000113, l1: 0.000109, l2: 0.000140, l3: 0.000322, l4: 0.000475, l5: 0.001399, l6: 0.004088
[epoch: 612/1000, batch:   268/ 1052, ite: 160760] train loss: 0.004102, tar: 0.000074 
l0: 0.000053, l1: 0.000053, l2: 0.000052, l3: 0.000112, l4: 0.000419, l5: 0.000716, l6: 0.000874
[epoch: 612/1000, batch:   348/ 1052, ite: 160780] train loss: 0.004097, tar: 0.000074 
l0: 0.000079, l1: 0.000079, l2: 0.000104, l3: 0.000182, l4: 0.000361, l5: 0.000666, l6: 0.001749
[epoch: 612/1000, batch:   428/ 1052, ite: 160800] train loss: 0.004097, tar: 0.000074 
l0: 0.000069, l1: 0.000075, l2: 0.000152, l3: 0.000312, l4: 0.000715, l5: 0.001920, l6: 0.002426
[epoch: 612/1000, batch:   508/ 1052, ite: 160820] train loss: 0.004097, tar: 0.000074 
l0: 0.000020, l1: 0.000022, l2: 0.000045, l3: 0.000118, l4: 0.000248, l5: 0.000756, l6: 0.001233
[epoch: 612/1000, batch:   588/ 1052, ite: 160840] train loss: 0.004094, tar: 0.000074 
l0: 0.000042, l1: 0.000042, l2: 0.000073, l3: 0.000098, l4: 0.000349, l5: 0.000883, l6: 0.002092
[epoch: 612/1000, batch:   668/ 1052, ite: 160860] train loss: 0.004095, tar: 0.000074 
l0: 0.000100, l1: 0.000095, l2: 0.000187, l3: 0.000336, l4: 0.000727, l5: 0.001555, l6: 0.002559
[epoch: 612/1000, batch:   748/ 1052, ite: 160880] train loss: 0.004096, tar: 0.000074 
l0: 0.000050, l1: 0.000050, l2: 0.000061, l3: 0.000102, l4: 0.000283, l5: 0.000799, l6: 0.001274
[epoch: 612/1000, batch:   828/ 1052, ite: 160900] train loss: 0.004094, tar: 0.000074 
l0: 0.000064, l1: 0.000060, l2: 0.000187, l3: 0.000367, l4: 0.000760, l5: 0.001508, l6: 0.003985
[epoch: 612/1000, batch:   908/ 1052, ite: 160920] train loss: 0.004095, tar: 0.000074 
l0: 0.000154, l1: 0.000170, l2: 0.000149, l3: 0.000180, l4: 0.000552, l5: 0.000681, l6: 0.001352
[epoch: 612/1000, batch:   988/ 1052, ite: 160940] train loss: 0.004092, tar: 0.000074 
[Epoch 612/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000021, l1: 0.000020, l2: 0.000049, l3: 0.000125, l4: 0.000475, l5: 0.001186, l6: 0.001478
[epoch: 613/1000, batch:    16/ 1052, ite: 160960] train loss: 0.004093, tar: 0.000074 
l0: 0.000048, l1: 0.000049, l2: 0.000089, l3: 0.000172, l4: 0.000424, l5: 0.000808, l6: 0.002016
[epoch: 613/1000, batch:    96/ 1052, ite: 160980] train loss: 0.004092, tar: 0.000074 
l0: 0.000108, l1: 0.000109, l2: 0.000137, l3: 0.000221, l4: 0.000411, l5: 0.000670, l6: 0.002079
[epoch: 613/1000, batch:   176/ 1052, ite: 161000] train loss: 0.004090, tar: 0.000074 
l0: 0.000106, l1: 0.000103, l2: 0.000187, l3: 0.000281, l4: 0.000475, l5: 0.001135, l6: 0.001763
[epoch: 613/1000, batch:   256/ 1052, ite: 161020] train loss: 0.004090, tar: 0.000074 
l0: 0.000074, l1: 0.000074, l2: 0.000149, l3: 0.000444, l4: 0.001250, l5: 0.002233, l6: 0.003936
[epoch: 613/1000, batch:   336/ 1052, ite: 161040] train loss: 0.004093, tar: 0.000074 
l0: 0.000075, l1: 0.000076, l2: 0.000091, l3: 0.000192, l4: 0.000414, l5: 0.000740, l6: 0.001685
[epoch: 613/1000, batch:   416/ 1052, ite: 161060] train loss: 0.004094, tar: 0.000074 
l0: 0.000035, l1: 0.000034, l2: 0.000067, l3: 0.000140, l4: 0.000387, l5: 0.001030, l6: 0.001235
[epoch: 613/1000, batch:   496/ 1052, ite: 161080] train loss: 0.004093, tar: 0.000074 
l0: 0.000029, l1: 0.000030, l2: 0.000037, l3: 0.000095, l4: 0.000190, l5: 0.000425, l6: 0.000897
[epoch: 613/1000, batch:   576/ 1052, ite: 161100] train loss: 0.004094, tar: 0.000074 
l0: 0.000040, l1: 0.000039, l2: 0.000069, l3: 0.000108, l4: 0.000274, l5: 0.000666, l6: 0.001009
[epoch: 613/1000, batch:   656/ 1052, ite: 161120] train loss: 0.004090, tar: 0.000073 
l0: 0.000040, l1: 0.000040, l2: 0.000068, l3: 0.000121, l4: 0.000182, l5: 0.000551, l6: 0.000382
[epoch: 613/1000, batch:   736/ 1052, ite: 161140] train loss: 0.004088, tar: 0.000073 
l0: 0.000209, l1: 0.000205, l2: 0.000278, l3: 0.000483, l4: 0.000965, l5: 0.001981, l6: 0.003201
[epoch: 613/1000, batch:   816/ 1052, ite: 161160] train loss: 0.004086, tar: 0.000073 
l0: 0.000047, l1: 0.000048, l2: 0.000065, l3: 0.000108, l4: 0.000353, l5: 0.001107, l6: 0.001792
[epoch: 613/1000, batch:   896/ 1052, ite: 161180] train loss: 0.004085, tar: 0.000073 
l0: 0.000045, l1: 0.000046, l2: 0.000060, l3: 0.000082, l4: 0.000192, l5: 0.000839, l6: 0.000403
[epoch: 613/1000, batch:   976/ 1052, ite: 161200] train loss: 0.004085, tar: 0.000073 
[Epoch 613/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000028, l1: 0.000029, l2: 0.000074, l3: 0.000214, l4: 0.000658, l5: 0.001345, l6: 0.001927
[epoch: 614/1000, batch:     4/ 1052, ite: 161220] train loss: 0.004088, tar: 0.000073 
l0: 0.000024, l1: 0.000024, l2: 0.000044, l3: 0.000084, l4: 0.000159, l5: 0.000728, l6: 0.000925
[epoch: 614/1000, batch:    84/ 1052, ite: 161240] train loss: 0.004088, tar: 0.000073 
l0: 0.000216, l1: 0.000214, l2: 0.000245, l3: 0.000394, l4: 0.000965, l5: 0.002040, l6: 0.002559
[epoch: 614/1000, batch:   164/ 1052, ite: 161260] train loss: 0.004087, tar: 0.000073 
l0: 0.000128, l1: 0.000129, l2: 0.000200, l3: 0.000285, l4: 0.000997, l5: 0.001800, l6: 0.003191
[epoch: 614/1000, batch:   244/ 1052, ite: 161280] train loss: 0.004088, tar: 0.000073 
l0: 0.000041, l1: 0.000041, l2: 0.000066, l3: 0.000087, l4: 0.000354, l5: 0.000724, l6: 0.001259
[epoch: 614/1000, batch:   324/ 1052, ite: 161300] train loss: 0.004086, tar: 0.000073 
l0: 0.000039, l1: 0.000038, l2: 0.000103, l3: 0.000275, l4: 0.000701, l5: 0.001500, l6: 0.002266
[epoch: 614/1000, batch:   404/ 1052, ite: 161320] train loss: 0.004088, tar: 0.000073 
l0: 0.000035, l1: 0.000033, l2: 0.000138, l3: 0.000479, l4: 0.001295, l5: 0.002096, l6: 0.003852
[epoch: 614/1000, batch:   484/ 1052, ite: 161340] train loss: 0.004091, tar: 0.000073 
l0: 0.000049, l1: 0.000050, l2: 0.000075, l3: 0.000150, l4: 0.000274, l5: 0.001134, l6: 0.001417
[epoch: 614/1000, batch:   564/ 1052, ite: 161360] train loss: 0.004088, tar: 0.000073 
l0: 0.000107, l1: 0.000109, l2: 0.000170, l3: 0.000271, l4: 0.000449, l5: 0.001574, l6: 0.003349
[epoch: 614/1000, batch:   644/ 1052, ite: 161380] train loss: 0.004087, tar: 0.000073 
l0: 0.000055, l1: 0.000055, l2: 0.000088, l3: 0.000241, l4: 0.000290, l5: 0.000765, l6: 0.001471
[epoch: 614/1000, batch:   724/ 1052, ite: 161400] train loss: 0.004087, tar: 0.000073 
l0: 0.000031, l1: 0.000030, l2: 0.000130, l3: 0.000223, l4: 0.000733, l5: 0.001859, l6: 0.002257
[epoch: 614/1000, batch:   804/ 1052, ite: 161420] train loss: 0.004087, tar: 0.000073 
l0: 0.000041, l1: 0.000040, l2: 0.000122, l3: 0.000309, l4: 0.000590, l5: 0.001459, l6: 0.002871
[epoch: 614/1000, batch:   884/ 1052, ite: 161440] train loss: 0.004085, tar: 0.000073 
l0: 0.000079, l1: 0.000079, l2: 0.000121, l3: 0.000310, l4: 0.000568, l5: 0.001614, l6: 0.002690
[epoch: 614/1000, batch:   964/ 1052, ite: 161460] train loss: 0.004086, tar: 0.000073 
l0: 0.000008, l1: 0.000008, l2: 0.000029, l3: 0.000108, l4: 0.000246, l5: 0.001458, l6: 0.001721
[epoch: 614/1000, batch:  1044/ 1052, ite: 161480] train loss: 0.004085, tar: 0.000073 
[Epoch 614/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000024, l1: 0.000024, l2: 0.000071, l3: 0.000166, l4: 0.000446, l5: 0.001381, l6: 0.001525
[epoch: 615/1000, batch:    72/ 1052, ite: 161500] train loss: 0.004085, tar: 0.000073 
l0: 0.000014, l1: 0.000013, l2: 0.000037, l3: 0.000131, l4: 0.000270, l5: 0.000647, l6: 0.001782
[epoch: 615/1000, batch:   152/ 1052, ite: 161520] train loss: 0.004086, tar: 0.000073 
l0: 0.000236, l1: 0.000239, l2: 0.000281, l3: 0.000373, l4: 0.000779, l5: 0.001555, l6: 0.002779
[epoch: 615/1000, batch:   232/ 1052, ite: 161540] train loss: 0.004085, tar: 0.000073 
l0: 0.000031, l1: 0.000030, l2: 0.000049, l3: 0.000114, l4: 0.000254, l5: 0.000489, l6: 0.000827
[epoch: 615/1000, batch:   312/ 1052, ite: 161560] train loss: 0.004089, tar: 0.000073 
l0: 0.000109, l1: 0.000109, l2: 0.000179, l3: 0.000266, l4: 0.000702, l5: 0.001492, l6: 0.003389
[epoch: 615/1000, batch:   392/ 1052, ite: 161580] train loss: 0.004087, tar: 0.000073 
l0: 0.000031, l1: 0.000032, l2: 0.000071, l3: 0.000157, l4: 0.000399, l5: 0.000861, l6: 0.001715
[epoch: 615/1000, batch:   472/ 1052, ite: 161600] train loss: 0.004086, tar: 0.000073 
l0: 0.000044, l1: 0.000043, l2: 0.000082, l3: 0.000194, l4: 0.000487, l5: 0.001039, l6: 0.001679
[epoch: 615/1000, batch:   552/ 1052, ite: 161620] train loss: 0.004085, tar: 0.000073 
l0: 0.000027, l1: 0.000026, l2: 0.000059, l3: 0.000152, l4: 0.000297, l5: 0.001162, l6: 0.001270
[epoch: 615/1000, batch:   632/ 1052, ite: 161640] train loss: 0.004084, tar: 0.000073 
l0: 0.000012, l1: 0.000012, l2: 0.000050, l3: 0.000166, l4: 0.000393, l5: 0.000804, l6: 0.002512
[epoch: 615/1000, batch:   712/ 1052, ite: 161660] train loss: 0.004085, tar: 0.000073 
l0: 0.000044, l1: 0.000044, l2: 0.000065, l3: 0.000165, l4: 0.000329, l5: 0.000681, l6: 0.001527
[epoch: 615/1000, batch:   792/ 1052, ite: 161680] train loss: 0.004085, tar: 0.000073 
l0: 0.000065, l1: 0.000066, l2: 0.000109, l3: 0.000344, l4: 0.000535, l5: 0.001756, l6: 0.003586
[epoch: 615/1000, batch:   872/ 1052, ite: 161700] train loss: 0.004085, tar: 0.000073 
l0: 0.000087, l1: 0.000090, l2: 0.000144, l3: 0.000285, l4: 0.000460, l5: 0.001085, l6: 0.002232
[epoch: 615/1000, batch:   952/ 1052, ite: 161720] train loss: 0.004086, tar: 0.000073 
l0: 0.000015, l1: 0.000016, l2: 0.000048, l3: 0.000178, l4: 0.000297, l5: 0.000705, l6: 0.001480
[epoch: 615/1000, batch:  1032/ 1052, ite: 161740] train loss: 0.004084, tar: 0.000073 
[Epoch 615/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000105, l1: 0.000104, l2: 0.000135, l3: 0.000192, l4: 0.000486, l5: 0.001060, l6: 0.001806
[epoch: 616/1000, batch:    60/ 1052, ite: 161760] train loss: 0.004083, tar: 0.000073 
l0: 0.000035, l1: 0.000035, l2: 0.000092, l3: 0.000292, l4: 0.000707, l5: 0.001683, l6: 0.003037
[epoch: 616/1000, batch:   140/ 1052, ite: 161780] train loss: 0.004087, tar: 0.000073 
l0: 0.000078, l1: 0.000083, l2: 0.000139, l3: 0.000244, l4: 0.000246, l5: 0.000686, l6: 0.001160
[epoch: 616/1000, batch:   220/ 1052, ite: 161800] train loss: 0.004088, tar: 0.000073 
l0: 0.000021, l1: 0.000021, l2: 0.000046, l3: 0.000099, l4: 0.000321, l5: 0.000432, l6: 0.000964
[epoch: 616/1000, batch:   300/ 1052, ite: 161820] train loss: 0.004085, tar: 0.000073 
l0: 0.000060, l1: 0.000059, l2: 0.000118, l3: 0.000177, l4: 0.000374, l5: 0.000587, l6: 0.002125
[epoch: 616/1000, batch:   380/ 1052, ite: 161840] train loss: 0.004086, tar: 0.000073 
l0: 0.000023, l1: 0.000024, l2: 0.000047, l3: 0.000107, l4: 0.000241, l5: 0.000639, l6: 0.001471
[epoch: 616/1000, batch:   460/ 1052, ite: 161860] train loss: 0.004084, tar: 0.000073 
l0: 0.000018, l1: 0.000019, l2: 0.000047, l3: 0.000119, l4: 0.000432, l5: 0.001407, l6: 0.002090
[epoch: 616/1000, batch:   540/ 1052, ite: 161880] train loss: 0.004085, tar: 0.000073 
l0: 0.000040, l1: 0.000040, l2: 0.000088, l3: 0.000214, l4: 0.000410, l5: 0.001622, l6: 0.002750
[epoch: 616/1000, batch:   620/ 1052, ite: 161900] train loss: 0.004084, tar: 0.000073 
l0: 0.000099, l1: 0.000101, l2: 0.000159, l3: 0.000314, l4: 0.000632, l5: 0.001160, l6: 0.001993
[epoch: 616/1000, batch:   700/ 1052, ite: 161920] train loss: 0.004082, tar: 0.000073 
l0: 0.000032, l1: 0.000035, l2: 0.000071, l3: 0.000167, l4: 0.000318, l5: 0.000611, l6: 0.001129
[epoch: 616/1000, batch:   780/ 1052, ite: 161940] train loss: 0.004081, tar: 0.000073 
l0: 0.000112, l1: 0.000114, l2: 0.000148, l3: 0.000213, l4: 0.000455, l5: 0.001486, l6: 0.003206
[epoch: 616/1000, batch:   860/ 1052, ite: 161960] train loss: 0.004082, tar: 0.000073 
l0: 0.000014, l1: 0.000014, l2: 0.000055, l3: 0.000123, l4: 0.000434, l5: 0.001196, l6: 0.001899
[epoch: 616/1000, batch:   940/ 1052, ite: 161980] train loss: 0.004083, tar: 0.000073 
l0: 0.000067, l1: 0.000069, l2: 0.000095, l3: 0.000164, l4: 0.000358, l5: 0.000948, l6: 0.002057
[epoch: 616/1000, batch:  1020/ 1052, ite: 162000] train loss: 0.004083, tar: 0.000073 
[Epoch 616/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000036, l1: 0.000035, l2: 0.000088, l3: 0.000104, l4: 0.000423, l5: 0.000818, l6: 0.001588
[epoch: 617/1000, batch:    48/ 1052, ite: 162020] train loss: 0.004081, tar: 0.000073 
l0: 0.000063, l1: 0.000060, l2: 0.000100, l3: 0.000104, l4: 0.000255, l5: 0.000495, l6: 0.000673
[epoch: 617/1000, batch:   128/ 1052, ite: 162040] train loss: 0.004081, tar: 0.000073 
l0: 0.000280, l1: 0.000280, l2: 0.000326, l3: 0.000324, l4: 0.000585, l5: 0.001237, l6: 0.003333
[epoch: 617/1000, batch:   208/ 1052, ite: 162060] train loss: 0.004082, tar: 0.000073 
l0: 0.000041, l1: 0.000040, l2: 0.000086, l3: 0.000196, l4: 0.000338, l5: 0.000888, l6: 0.001321
[epoch: 617/1000, batch:   288/ 1052, ite: 162080] train loss: 0.004085, tar: 0.000073 
l0: 0.000057, l1: 0.000059, l2: 0.000085, l3: 0.000140, l4: 0.000455, l5: 0.000921, l6: 0.001020
[epoch: 617/1000, batch:   368/ 1052, ite: 162100] train loss: 0.004084, tar: 0.000073 
l0: 0.000070, l1: 0.000070, l2: 0.000098, l3: 0.000144, l4: 0.000336, l5: 0.000684, l6: 0.002345
[epoch: 617/1000, batch:   448/ 1052, ite: 162120] train loss: 0.004083, tar: 0.000073 
l0: 0.000050, l1: 0.000051, l2: 0.000083, l3: 0.000180, l4: 0.000569, l5: 0.001140, l6: 0.001841
[epoch: 617/1000, batch:   528/ 1052, ite: 162140] train loss: 0.004083, tar: 0.000073 
l0: 0.000071, l1: 0.000074, l2: 0.000092, l3: 0.000158, l4: 0.000410, l5: 0.001019, l6: 0.002803
[epoch: 617/1000, batch:   608/ 1052, ite: 162160] train loss: 0.004082, tar: 0.000073 
l0: 0.000003, l1: 0.000003, l2: 0.000021, l3: 0.000055, l4: 0.000216, l5: 0.000496, l6: 0.001279
[epoch: 617/1000, batch:   688/ 1052, ite: 162180] train loss: 0.004078, tar: 0.000072 
l0: 0.000009, l1: 0.000012, l2: 0.000011, l3: 0.000049, l4: 0.000102, l5: 0.000463, l6: 0.000749
[epoch: 617/1000, batch:   768/ 1052, ite: 162200] train loss: 0.004076, tar: 0.000072 
l0: 0.000040, l1: 0.000043, l2: 0.000080, l3: 0.000158, l4: 0.000424, l5: 0.000984, l6: 0.002141
[epoch: 617/1000, batch:   848/ 1052, ite: 162220] train loss: 0.004078, tar: 0.000072 
l0: 0.000022, l1: 0.000024, l2: 0.000076, l3: 0.000138, l4: 0.000258, l5: 0.000835, l6: 0.001405
[epoch: 617/1000, batch:   928/ 1052, ite: 162240] train loss: 0.004077, tar: 0.000072 
l0: 0.000090, l1: 0.000090, l2: 0.000146, l3: 0.000265, l4: 0.000484, l5: 0.001138, l6: 0.002705
[epoch: 617/1000, batch:  1008/ 1052, ite: 162260] train loss: 0.004077, tar: 0.000072 
[Epoch 617/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000037, l1: 0.000038, l2: 0.000096, l3: 0.000263, l4: 0.000346, l5: 0.001263, l6: 0.002029
[epoch: 618/1000, batch:    36/ 1052, ite: 162280] train loss: 0.004080, tar: 0.000072 
l0: 0.000034, l1: 0.000037, l2: 0.000057, l3: 0.000198, l4: 0.000458, l5: 0.000925, l6: 0.001404
[epoch: 618/1000, batch:   116/ 1052, ite: 162300] train loss: 0.004080, tar: 0.000073 
l0: 0.000092, l1: 0.000093, l2: 0.000101, l3: 0.000169, l4: 0.000330, l5: 0.000837, l6: 0.001866
[epoch: 618/1000, batch:   196/ 1052, ite: 162320] train loss: 0.004081, tar: 0.000073 
l0: 0.000019, l1: 0.000020, l2: 0.000071, l3: 0.000169, l4: 0.000314, l5: 0.000653, l6: 0.001149
[epoch: 618/1000, batch:   276/ 1052, ite: 162340] train loss: 0.004081, tar: 0.000072 
l0: 0.000022, l1: 0.000023, l2: 0.000069, l3: 0.000155, l4: 0.000306, l5: 0.001218, l6: 0.001917
[epoch: 618/1000, batch:   356/ 1052, ite: 162360] train loss: 0.004081, tar: 0.000072 
l0: 0.000108, l1: 0.000103, l2: 0.000315, l3: 0.000600, l4: 0.000863, l5: 0.002776, l6: 0.006086
[epoch: 618/1000, batch:   436/ 1052, ite: 162380] train loss: 0.004082, tar: 0.000072 
l0: 0.000011, l1: 0.000012, l2: 0.000019, l3: 0.000090, l4: 0.000309, l5: 0.000766, l6: 0.000915
[epoch: 618/1000, batch:   516/ 1052, ite: 162400] train loss: 0.004080, tar: 0.000072 
l0: 0.000023, l1: 0.000023, l2: 0.000043, l3: 0.000089, l4: 0.000319, l5: 0.000772, l6: 0.001127
[epoch: 618/1000, batch:   596/ 1052, ite: 162420] train loss: 0.004080, tar: 0.000072 
l0: 0.000010, l1: 0.000010, l2: 0.000026, l3: 0.000092, l4: 0.000200, l5: 0.000521, l6: 0.001734
[epoch: 618/1000, batch:   676/ 1052, ite: 162440] train loss: 0.004082, tar: 0.000072 
l0: 0.000064, l1: 0.000062, l2: 0.000086, l3: 0.000132, l4: 0.000341, l5: 0.000686, l6: 0.001494
[epoch: 618/1000, batch:   756/ 1052, ite: 162460] train loss: 0.004081, tar: 0.000072 
l0: 0.000115, l1: 0.000118, l2: 0.000154, l3: 0.000342, l4: 0.000725, l5: 0.001421, l6: 0.001977
[epoch: 618/1000, batch:   836/ 1052, ite: 162480] train loss: 0.004082, tar: 0.000073 
l0: 0.000052, l1: 0.000054, l2: 0.000051, l3: 0.000113, l4: 0.000476, l5: 0.000706, l6: 0.001181
[epoch: 618/1000, batch:   916/ 1052, ite: 162500] train loss: 0.004081, tar: 0.000072 
l0: 0.000117, l1: 0.000119, l2: 0.000181, l3: 0.000273, l4: 0.000522, l5: 0.001513, l6: 0.003608
[epoch: 618/1000, batch:   996/ 1052, ite: 162520] train loss: 0.004080, tar: 0.000072 
[Epoch 618/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000076, l1: 0.000075, l2: 0.000114, l3: 0.000195, l4: 0.000492, l5: 0.002021, l6: 0.002772
[epoch: 619/1000, batch:    24/ 1052, ite: 162540] train loss: 0.004077, tar: 0.000072 
l0: 0.000063, l1: 0.000061, l2: 0.000086, l3: 0.000206, l4: 0.000467, l5: 0.001067, l6: 0.001611
[epoch: 619/1000, batch:   104/ 1052, ite: 162560] train loss: 0.004076, tar: 0.000072 
l0: 0.000033, l1: 0.000031, l2: 0.000072, l3: 0.000179, l4: 0.000357, l5: 0.000812, l6: 0.001530
[epoch: 619/1000, batch:   184/ 1052, ite: 162580] train loss: 0.004074, tar: 0.000072 
l0: 0.000074, l1: 0.000077, l2: 0.000097, l3: 0.000185, l4: 0.000403, l5: 0.000856, l6: 0.001841
[epoch: 619/1000, batch:   264/ 1052, ite: 162600] train loss: 0.004076, tar: 0.000072 
l0: 0.000036, l1: 0.000038, l2: 0.000082, l3: 0.000241, l4: 0.000598, l5: 0.001613, l6: 0.001814
[epoch: 619/1000, batch:   344/ 1052, ite: 162620] train loss: 0.004074, tar: 0.000072 
l0: 0.000099, l1: 0.000100, l2: 0.000115, l3: 0.000184, l4: 0.000484, l5: 0.000720, l6: 0.002468
[epoch: 619/1000, batch:   424/ 1052, ite: 162640] train loss: 0.004076, tar: 0.000072 
l0: 0.000018, l1: 0.000019, l2: 0.000050, l3: 0.000134, l4: 0.000368, l5: 0.000749, l6: 0.001709
[epoch: 619/1000, batch:   504/ 1052, ite: 162660] train loss: 0.004074, tar: 0.000072 
l0: 0.000053, l1: 0.000052, l2: 0.000093, l3: 0.000176, l4: 0.000364, l5: 0.000803, l6: 0.001041
[epoch: 619/1000, batch:   584/ 1052, ite: 162680] train loss: 0.004074, tar: 0.000072 
l0: 0.000052, l1: 0.000052, l2: 0.000090, l3: 0.000196, l4: 0.000324, l5: 0.001430, l6: 0.001468
[epoch: 619/1000, batch:   664/ 1052, ite: 162700] train loss: 0.004074, tar: 0.000072 
l0: 0.000010, l1: 0.000011, l2: 0.000027, l3: 0.000094, l4: 0.000252, l5: 0.000569, l6: 0.000807
[epoch: 619/1000, batch:   744/ 1052, ite: 162720] train loss: 0.004073, tar: 0.000072 
l0: 0.000122, l1: 0.000121, l2: 0.000147, l3: 0.000263, l4: 0.000553, l5: 0.001294, l6: 0.003339
[epoch: 619/1000, batch:   824/ 1052, ite: 162740] train loss: 0.004074, tar: 0.000072 
l0: 0.000038, l1: 0.000038, l2: 0.000051, l3: 0.000085, l4: 0.000436, l5: 0.000906, l6: 0.001224
[epoch: 619/1000, batch:   904/ 1052, ite: 162760] train loss: 0.004073, tar: 0.000072 
l0: 0.000021, l1: 0.000020, l2: 0.000121, l3: 0.000274, l4: 0.000650, l5: 0.001504, l6: 0.002356
[epoch: 619/1000, batch:   984/ 1052, ite: 162780] train loss: 0.004076, tar: 0.000072 
[Epoch 619/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000082, l1: 0.000081, l2: 0.000095, l3: 0.000205, l4: 0.000600, l5: 0.000906, l6: 0.002019
[epoch: 620/1000, batch:    12/ 1052, ite: 162800] train loss: 0.004078, tar: 0.000072 
l0: 0.000041, l1: 0.000042, l2: 0.000060, l3: 0.000114, l4: 0.000256, l5: 0.000743, l6: 0.001583
[epoch: 620/1000, batch:    92/ 1052, ite: 162820] train loss: 0.004077, tar: 0.000072 
l0: 0.000059, l1: 0.000064, l2: 0.000077, l3: 0.000193, l4: 0.000595, l5: 0.001241, l6: 0.002977
[epoch: 620/1000, batch:   172/ 1052, ite: 162840] train loss: 0.004076, tar: 0.000072 
l0: 0.000056, l1: 0.000056, l2: 0.000096, l3: 0.000118, l4: 0.000320, l5: 0.000857, l6: 0.001722
[epoch: 620/1000, batch:   252/ 1052, ite: 162860] train loss: 0.004076, tar: 0.000072 
l0: 0.000024, l1: 0.000024, l2: 0.000027, l3: 0.000063, l4: 0.000224, l5: 0.000379, l6: 0.000907
[epoch: 620/1000, batch:   332/ 1052, ite: 162880] train loss: 0.004074, tar: 0.000072 
l0: 0.000090, l1: 0.000092, l2: 0.000131, l3: 0.000188, l4: 0.000421, l5: 0.001457, l6: 0.002715
[epoch: 620/1000, batch:   412/ 1052, ite: 162900] train loss: 0.004076, tar: 0.000072 
l0: 0.000032, l1: 0.000031, l2: 0.000059, l3: 0.000170, l4: 0.000390, l5: 0.000700, l6: 0.001864
[epoch: 620/1000, batch:   492/ 1052, ite: 162920] train loss: 0.004076, tar: 0.000072 
l0: 0.000092, l1: 0.000093, l2: 0.000129, l3: 0.000304, l4: 0.000729, l5: 0.001281, l6: 0.003179
[epoch: 620/1000, batch:   572/ 1052, ite: 162940] train loss: 0.004075, tar: 0.000072 
l0: 0.000032, l1: 0.000035, l2: 0.000061, l3: 0.000134, l4: 0.000419, l5: 0.001007, l6: 0.002055
[epoch: 620/1000, batch:   652/ 1052, ite: 162960] train loss: 0.004075, tar: 0.000072 
l0: 0.000020, l1: 0.000020, l2: 0.000070, l3: 0.000202, l4: 0.000506, l5: 0.001145, l6: 0.002096
[epoch: 620/1000, batch:   732/ 1052, ite: 162980] train loss: 0.004075, tar: 0.000072 
l0: 0.000046, l1: 0.000047, l2: 0.000095, l3: 0.000171, l4: 0.000401, l5: 0.000673, l6: 0.000960
[epoch: 620/1000, batch:   812/ 1052, ite: 163000] train loss: 0.004073, tar: 0.000072 
l0: 0.000023, l1: 0.000023, l2: 0.000063, l3: 0.000208, l4: 0.000511, l5: 0.001286, l6: 0.002424
[epoch: 620/1000, batch:   892/ 1052, ite: 163020] train loss: 0.004073, tar: 0.000072 
l0: 0.000053, l1: 0.000054, l2: 0.000087, l3: 0.000252, l4: 0.000620, l5: 0.001119, l6: 0.002149
[epoch: 620/1000, batch:   972/ 1052, ite: 163040] train loss: 0.004074, tar: 0.000072 
l0: 0.000067, l1: 0.000070, l2: 0.000134, l3: 0.000218, l4: 0.000489, l5: 0.001134, l6: 0.001835
[epoch: 620/1000, batch:  1052/ 1052, ite: 163060] train loss: 0.004074, tar: 0.000072 
[Epoch 620/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000045, l1: 0.000044, l2: 0.000063, l3: 0.000141, l4: 0.000462, l5: 0.000662, l6: 0.001147
[epoch: 621/1000, batch:    80/ 1052, ite: 163080] train loss: 0.004075, tar: 0.000072 
l0: 0.000077, l1: 0.000083, l2: 0.000107, l3: 0.000142, l4: 0.000302, l5: 0.000971, l6: 0.001213
[epoch: 621/1000, batch:   160/ 1052, ite: 163100] train loss: 0.004077, tar: 0.000072 
l0: 0.000032, l1: 0.000032, l2: 0.000067, l3: 0.000165, l4: 0.000436, l5: 0.001138, l6: 0.001496
[epoch: 621/1000, batch:   240/ 1052, ite: 163120] train loss: 0.004078, tar: 0.000072 
l0: 0.000091, l1: 0.000092, l2: 0.000108, l3: 0.000206, l4: 0.000452, l5: 0.000785, l6: 0.001952
[epoch: 621/1000, batch:   320/ 1052, ite: 163140] train loss: 0.004081, tar: 0.000072 
l0: 0.000026, l1: 0.000025, l2: 0.000064, l3: 0.000193, l4: 0.000517, l5: 0.001018, l6: 0.001482
[epoch: 621/1000, batch:   400/ 1052, ite: 163160] train loss: 0.004080, tar: 0.000072 
l0: 0.000017, l1: 0.000016, l2: 0.000042, l3: 0.000146, l4: 0.000485, l5: 0.001185, l6: 0.002324
[epoch: 621/1000, batch:   480/ 1052, ite: 163180] train loss: 0.004081, tar: 0.000072 
l0: 0.000090, l1: 0.000087, l2: 0.000159, l3: 0.000204, l4: 0.000470, l5: 0.000546, l6: 0.001673
[epoch: 621/1000, batch:   560/ 1052, ite: 163200] train loss: 0.004079, tar: 0.000072 
l0: 0.000138, l1: 0.000140, l2: 0.000169, l3: 0.000246, l4: 0.000604, l5: 0.001273, l6: 0.001920
[epoch: 621/1000, batch:   640/ 1052, ite: 163220] train loss: 0.004079, tar: 0.000072 
l0: 0.000138, l1: 0.000136, l2: 0.000255, l3: 0.000606, l4: 0.001216, l5: 0.002469, l6: 0.003296
[epoch: 621/1000, batch:   720/ 1052, ite: 163240] train loss: 0.004079, tar: 0.000072 
l0: 0.000041, l1: 0.000040, l2: 0.000083, l3: 0.000115, l4: 0.000253, l5: 0.000452, l6: 0.001537
[epoch: 621/1000, batch:   800/ 1052, ite: 163260] train loss: 0.004078, tar: 0.000072 
l0: 0.000087, l1: 0.000087, l2: 0.000186, l3: 0.000251, l4: 0.000456, l5: 0.001226, l6: 0.002561
[epoch: 621/1000, batch:   880/ 1052, ite: 163280] train loss: 0.004079, tar: 0.000072 
l0: 0.000073, l1: 0.000069, l2: 0.000111, l3: 0.000348, l4: 0.000824, l5: 0.001247, l6: 0.001412
[epoch: 621/1000, batch:   960/ 1052, ite: 163300] train loss: 0.004077, tar: 0.000072 
l0: 0.000026, l1: 0.000027, l2: 0.000036, l3: 0.000121, l4: 0.000340, l5: 0.000693, l6: 0.001117
[epoch: 621/1000, batch:  1040/ 1052, ite: 163320] train loss: 0.004076, tar: 0.000072 
[Epoch 621/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000106, l1: 0.000105, l2: 0.000166, l3: 0.000261, l4: 0.000464, l5: 0.001409, l6: 0.002297
[epoch: 622/1000, batch:    68/ 1052, ite: 163340] train loss: 0.004078, tar: 0.000072 
l0: 0.000014, l1: 0.000015, l2: 0.000040, l3: 0.000065, l4: 0.000276, l5: 0.000804, l6: 0.001541
[epoch: 622/1000, batch:   148/ 1052, ite: 163360] train loss: 0.004078, tar: 0.000072 
l0: 0.000025, l1: 0.000027, l2: 0.000042, l3: 0.000162, l4: 0.000501, l5: 0.000802, l6: 0.001542
[epoch: 622/1000, batch:   228/ 1052, ite: 163380] train loss: 0.004077, tar: 0.000072 
l0: 0.000028, l1: 0.000026, l2: 0.000127, l3: 0.000384, l4: 0.000723, l5: 0.001374, l6: 0.003090
[epoch: 622/1000, batch:   308/ 1052, ite: 163400] train loss: 0.004076, tar: 0.000072 
l0: 0.000175, l1: 0.000176, l2: 0.000227, l3: 0.000321, l4: 0.000620, l5: 0.001974, l6: 0.002545
[epoch: 622/1000, batch:   388/ 1052, ite: 163420] train loss: 0.004077, tar: 0.000072 
l0: 0.000127, l1: 0.000129, l2: 0.000151, l3: 0.000262, l4: 0.000353, l5: 0.000991, l6: 0.001306
[epoch: 622/1000, batch:   468/ 1052, ite: 163440] train loss: 0.004077, tar: 0.000072 
l0: 0.000011, l1: 0.000010, l2: 0.000054, l3: 0.000105, l4: 0.000328, l5: 0.000860, l6: 0.000970
[epoch: 622/1000, batch:   548/ 1052, ite: 163460] train loss: 0.004077, tar: 0.000072 
l0: 0.000140, l1: 0.000144, l2: 0.000184, l3: 0.000297, l4: 0.000581, l5: 0.001222, l6: 0.001640
[epoch: 622/1000, batch:   628/ 1052, ite: 163480] train loss: 0.004077, tar: 0.000072 
l0: 0.000027, l1: 0.000028, l2: 0.000056, l3: 0.000142, l4: 0.000292, l5: 0.000906, l6: 0.001436
[epoch: 622/1000, batch:   708/ 1052, ite: 163500] train loss: 0.004077, tar: 0.000072 
l0: 0.000071, l1: 0.000069, l2: 0.000125, l3: 0.000223, l4: 0.000693, l5: 0.001225, l6: 0.001716
[epoch: 622/1000, batch:   788/ 1052, ite: 163520] train loss: 0.004077, tar: 0.000072 
l0: 0.000234, l1: 0.000239, l2: 0.000320, l3: 0.000330, l4: 0.000747, l5: 0.001251, l6: 0.003697
[epoch: 622/1000, batch:   868/ 1052, ite: 163540] train loss: 0.004077, tar: 0.000072 
l0: 0.000093, l1: 0.000095, l2: 0.000129, l3: 0.000213, l4: 0.000434, l5: 0.000965, l6: 0.001102
[epoch: 622/1000, batch:   948/ 1052, ite: 163560] train loss: 0.004078, tar: 0.000072 
l0: 0.000158, l1: 0.000161, l2: 0.000215, l3: 0.000296, l4: 0.000493, l5: 0.001570, l6: 0.003054
[epoch: 622/1000, batch:  1028/ 1052, ite: 163580] train loss: 0.004079, tar: 0.000072 
[Epoch 622/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000067, l1: 0.000066, l2: 0.000098, l3: 0.000182, l4: 0.000487, l5: 0.001381, l6: 0.002672
[epoch: 623/1000, batch:    56/ 1052, ite: 163600] train loss: 0.004078, tar: 0.000072 
l0: 0.000008, l1: 0.000007, l2: 0.000020, l3: 0.000100, l4: 0.000352, l5: 0.000777, l6: 0.001146
[epoch: 623/1000, batch:   136/ 1052, ite: 163620] train loss: 0.004079, tar: 0.000072 
l0: 0.000084, l1: 0.000084, l2: 0.000133, l3: 0.000155, l4: 0.000409, l5: 0.000823, l6: 0.001479
[epoch: 623/1000, batch:   216/ 1052, ite: 163640] train loss: 0.004077, tar: 0.000072 
l0: 0.000091, l1: 0.000086, l2: 0.000197, l3: 0.000370, l4: 0.000451, l5: 0.001218, l6: 0.002080
[epoch: 623/1000, batch:   296/ 1052, ite: 163660] train loss: 0.004076, tar: 0.000072 
l0: 0.000021, l1: 0.000021, l2: 0.000085, l3: 0.000252, l4: 0.000396, l5: 0.001630, l6: 0.002832
[epoch: 623/1000, batch:   376/ 1052, ite: 163680] train loss: 0.004079, tar: 0.000072 
l0: 0.000170, l1: 0.000173, l2: 0.000261, l3: 0.000406, l4: 0.000607, l5: 0.001387, l6: 0.001466
[epoch: 623/1000, batch:   456/ 1052, ite: 163700] train loss: 0.004078, tar: 0.000072 
l0: 0.000026, l1: 0.000025, l2: 0.000079, l3: 0.000194, l4: 0.000406, l5: 0.001151, l6: 0.001762
[epoch: 623/1000, batch:   536/ 1052, ite: 163720] train loss: 0.004080, tar: 0.000072 
l0: 0.000076, l1: 0.000075, l2: 0.000147, l3: 0.000507, l4: 0.001202, l5: 0.002047, l6: 0.002608
[epoch: 623/1000, batch:   616/ 1052, ite: 163740] train loss: 0.004080, tar: 0.000072 
l0: 0.000019, l1: 0.000021, l2: 0.000070, l3: 0.000221, l4: 0.000435, l5: 0.001645, l6: 0.002687
[epoch: 623/1000, batch:   696/ 1052, ite: 163760] train loss: 0.004078, tar: 0.000072 
l0: 0.000093, l1: 0.000091, l2: 0.000127, l3: 0.000205, l4: 0.000477, l5: 0.000724, l6: 0.001818
[epoch: 623/1000, batch:   776/ 1052, ite: 163780] train loss: 0.004079, tar: 0.000072 
l0: 0.000096, l1: 0.000099, l2: 0.000099, l3: 0.000212, l4: 0.000379, l5: 0.001162, l6: 0.001641
[epoch: 623/1000, batch:   856/ 1052, ite: 163800] train loss: 0.004079, tar: 0.000072 
l0: 0.000039, l1: 0.000038, l2: 0.000062, l3: 0.000117, l4: 0.000314, l5: 0.000704, l6: 0.001255
[epoch: 623/1000, batch:   936/ 1052, ite: 163820] train loss: 0.004079, tar: 0.000072 
l0: 0.000059, l1: 0.000058, l2: 0.000131, l3: 0.000218, l4: 0.000488, l5: 0.001144, l6: 0.002353
[epoch: 623/1000, batch:  1016/ 1052, ite: 163840] train loss: 0.004081, tar: 0.000072 
[Epoch 623/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000086, l1: 0.000086, l2: 0.000117, l3: 0.000133, l4: 0.000349, l5: 0.000801, l6: 0.001223
[epoch: 624/1000, batch:    44/ 1052, ite: 163860] train loss: 0.004078, tar: 0.000072 
l0: 0.000043, l1: 0.000041, l2: 0.000151, l3: 0.000320, l4: 0.000795, l5: 0.001817, l6: 0.002924
[epoch: 624/1000, batch:   124/ 1052, ite: 163880] train loss: 0.004079, tar: 0.000072 
l0: 0.000037, l1: 0.000036, l2: 0.000071, l3: 0.000180, l4: 0.000448, l5: 0.001139, l6: 0.001743
[epoch: 624/1000, batch:   204/ 1052, ite: 163900] train loss: 0.004078, tar: 0.000072 
l0: 0.000026, l1: 0.000026, l2: 0.000050, l3: 0.000118, l4: 0.000366, l5: 0.000636, l6: 0.001125
[epoch: 624/1000, batch:   284/ 1052, ite: 163920] train loss: 0.004078, tar: 0.000072 
l0: 0.000026, l1: 0.000026, l2: 0.000039, l3: 0.000104, l4: 0.000264, l5: 0.000704, l6: 0.000789
[epoch: 624/1000, batch:   364/ 1052, ite: 163940] train loss: 0.004076, tar: 0.000072 
l0: 0.000237, l1: 0.000236, l2: 0.000303, l3: 0.000391, l4: 0.000948, l5: 0.002368, l6: 0.002552
[epoch: 624/1000, batch:   444/ 1052, ite: 163960] train loss: 0.004078, tar: 0.000072 
l0: 0.000049, l1: 0.000047, l2: 0.000129, l3: 0.000204, l4: 0.000533, l5: 0.001200, l6: 0.002259
[epoch: 624/1000, batch:   524/ 1052, ite: 163980] train loss: 0.004077, tar: 0.000072 
l0: 0.000047, l1: 0.000050, l2: 0.000166, l3: 0.000326, l4: 0.000608, l5: 0.001869, l6: 0.003586
[epoch: 624/1000, batch:   604/ 1052, ite: 164000] train loss: 0.004077, tar: 0.000072 
l0: 0.000033, l1: 0.000033, l2: 0.000073, l3: 0.000151, l4: 0.000385, l5: 0.000667, l6: 0.001145
[epoch: 624/1000, batch:   684/ 1052, ite: 164020] train loss: 0.004075, tar: 0.000072 
l0: 0.000129, l1: 0.000129, l2: 0.000131, l3: 0.000203, l4: 0.000582, l5: 0.001098, l6: 0.001755
[epoch: 624/1000, batch:   764/ 1052, ite: 164040] train loss: 0.004076, tar: 0.000072 
l0: 0.000059, l1: 0.000059, l2: 0.000100, l3: 0.000199, l4: 0.000455, l5: 0.001106, l6: 0.002631
[epoch: 624/1000, batch:   844/ 1052, ite: 164060] train loss: 0.004077, tar: 0.000072 
l0: 0.000060, l1: 0.000058, l2: 0.000111, l3: 0.000272, l4: 0.000337, l5: 0.000958, l6: 0.001668
[epoch: 624/1000, batch:   924/ 1052, ite: 164080] train loss: 0.004077, tar: 0.000072 
l0: 0.000086, l1: 0.000085, l2: 0.000048, l3: 0.000126, l4: 0.000368, l5: 0.000383, l6: 0.000929
[epoch: 624/1000, batch:  1004/ 1052, ite: 164100] train loss: 0.004077, tar: 0.000072 
[Epoch 624/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000023, l1: 0.000022, l2: 0.000055, l3: 0.000171, l4: 0.000494, l5: 0.001361, l6: 0.003123
[epoch: 625/1000, batch:    32/ 1052, ite: 164120] train loss: 0.004077, tar: 0.000072 
l0: 0.000028, l1: 0.000028, l2: 0.000079, l3: 0.000164, l4: 0.000329, l5: 0.001024, l6: 0.002103
[epoch: 625/1000, batch:   112/ 1052, ite: 164140] train loss: 0.004078, tar: 0.000072 
l0: 0.000096, l1: 0.000100, l2: 0.000143, l3: 0.000233, l4: 0.000597, l5: 0.001450, l6: 0.002294
[epoch: 625/1000, batch:   192/ 1052, ite: 164160] train loss: 0.004079, tar: 0.000072 
l0: 0.000049, l1: 0.000048, l2: 0.000111, l3: 0.000251, l4: 0.000493, l5: 0.001215, l6: 0.001669
[epoch: 625/1000, batch:   272/ 1052, ite: 164180] train loss: 0.004078, tar: 0.000072 
l0: 0.000177, l1: 0.000178, l2: 0.000265, l3: 0.000476, l4: 0.000850, l5: 0.001412, l6: 0.001823
[epoch: 625/1000, batch:   352/ 1052, ite: 164200] train loss: 0.004076, tar: 0.000072 
l0: 0.000093, l1: 0.000094, l2: 0.000125, l3: 0.000229, l4: 0.000489, l5: 0.001342, l6: 0.002065
[epoch: 625/1000, batch:   432/ 1052, ite: 164220] train loss: 0.004076, tar: 0.000072 
l0: 0.000019, l1: 0.000019, l2: 0.000056, l3: 0.000136, l4: 0.000753, l5: 0.001844, l6: 0.002159
[epoch: 625/1000, batch:   512/ 1052, ite: 164240] train loss: 0.004077, tar: 0.000072 
l0: 0.000100, l1: 0.000098, l2: 0.000160, l3: 0.000252, l4: 0.000547, l5: 0.001533, l6: 0.002797
[epoch: 625/1000, batch:   592/ 1052, ite: 164260] train loss: 0.004078, tar: 0.000072 
l0: 0.000012, l1: 0.000012, l2: 0.000059, l3: 0.000159, l4: 0.000589, l5: 0.001206, l6: 0.002160
[epoch: 625/1000, batch:   672/ 1052, ite: 164280] train loss: 0.004077, tar: 0.000072 
l0: 0.000023, l1: 0.000022, l2: 0.000055, l3: 0.000155, l4: 0.000319, l5: 0.000653, l6: 0.000969
[epoch: 625/1000, batch:   752/ 1052, ite: 164300] train loss: 0.004078, tar: 0.000072 
l0: 0.000052, l1: 0.000052, l2: 0.000074, l3: 0.000165, l4: 0.000261, l5: 0.000933, l6: 0.001571
[epoch: 625/1000, batch:   832/ 1052, ite: 164320] train loss: 0.004076, tar: 0.000072 
l0: 0.000140, l1: 0.000143, l2: 0.000167, l3: 0.000258, l4: 0.000467, l5: 0.000880, l6: 0.001923
[epoch: 625/1000, batch:   912/ 1052, ite: 164340] train loss: 0.004075, tar: 0.000072 
l0: 0.000045, l1: 0.000045, l2: 0.000100, l3: 0.000251, l4: 0.000946, l5: 0.001883, l6: 0.002755
[epoch: 625/1000, batch:   992/ 1052, ite: 164360] train loss: 0.004077, tar: 0.000072 
[Epoch 625/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000036, l1: 0.000033, l2: 0.000059, l3: 0.000123, l4: 0.000308, l5: 0.000881, l6: 0.002184
[epoch: 626/1000, batch:    20/ 1052, ite: 164380] train loss: 0.004074, tar: 0.000072 
l0: 0.000023, l1: 0.000025, l2: 0.000051, l3: 0.000128, l4: 0.000616, l5: 0.000932, l6: 0.001484
[epoch: 626/1000, batch:   100/ 1052, ite: 164400] train loss: 0.004074, tar: 0.000071 
l0: 0.000148, l1: 0.000147, l2: 0.000196, l3: 0.000317, l4: 0.000677, l5: 0.001515, l6: 0.002456
[epoch: 626/1000, batch:   180/ 1052, ite: 164420] train loss: 0.004074, tar: 0.000071 
l0: 0.000056, l1: 0.000059, l2: 0.000073, l3: 0.000174, l4: 0.000501, l5: 0.000959, l6: 0.001594
[epoch: 626/1000, batch:   260/ 1052, ite: 164440] train loss: 0.004073, tar: 0.000071 
l0: 0.000061, l1: 0.000063, l2: 0.000109, l3: 0.000201, l4: 0.000530, l5: 0.000917, l6: 0.001392
[epoch: 626/1000, batch:   340/ 1052, ite: 164460] train loss: 0.004074, tar: 0.000071 
l0: 0.000083, l1: 0.000087, l2: 0.000128, l3: 0.000192, l4: 0.000497, l5: 0.000878, l6: 0.001605
[epoch: 626/1000, batch:   420/ 1052, ite: 164480] train loss: 0.004074, tar: 0.000071 
l0: 0.000058, l1: 0.000061, l2: 0.000080, l3: 0.000141, l4: 0.000412, l5: 0.001566, l6: 0.001997
[epoch: 626/1000, batch:   500/ 1052, ite: 164500] train loss: 0.004074, tar: 0.000071 
l0: 0.000021, l1: 0.000020, l2: 0.000038, l3: 0.000164, l4: 0.000461, l5: 0.001004, l6: 0.001298
[epoch: 626/1000, batch:   580/ 1052, ite: 164520] train loss: 0.004073, tar: 0.000071 
l0: 0.000016, l1: 0.000016, l2: 0.000026, l3: 0.000072, l4: 0.000232, l5: 0.000716, l6: 0.001070
[epoch: 626/1000, batch:   660/ 1052, ite: 164540] train loss: 0.004075, tar: 0.000072 
l0: 0.000055, l1: 0.000052, l2: 0.000102, l3: 0.000136, l4: 0.000312, l5: 0.000882, l6: 0.001585
[epoch: 626/1000, batch:   740/ 1052, ite: 164560] train loss: 0.004073, tar: 0.000072 
l0: 0.000046, l1: 0.000047, l2: 0.000074, l3: 0.000142, l4: 0.000307, l5: 0.001095, l6: 0.002171
[epoch: 626/1000, batch:   820/ 1052, ite: 164580] train loss: 0.004073, tar: 0.000071 
l0: 0.000079, l1: 0.000081, l2: 0.000173, l3: 0.000461, l4: 0.000964, l5: 0.002592, l6: 0.004085
[epoch: 626/1000, batch:   900/ 1052, ite: 164600] train loss: 0.004075, tar: 0.000071 
l0: 0.000029, l1: 0.000029, l2: 0.000052, l3: 0.000091, l4: 0.000343, l5: 0.000893, l6: 0.000884
[epoch: 626/1000, batch:   980/ 1052, ite: 164620] train loss: 0.004075, tar: 0.000071 
[Epoch 626/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000035, l1: 0.000036, l2: 0.000076, l3: 0.000169, l4: 0.000307, l5: 0.001328, l6: 0.001893
[epoch: 627/1000, batch:     8/ 1052, ite: 164640] train loss: 0.004075, tar: 0.000071 
l0: 0.000026, l1: 0.000030, l2: 0.000041, l3: 0.000093, l4: 0.000374, l5: 0.001217, l6: 0.001489
[epoch: 627/1000, batch:    88/ 1052, ite: 164660] train loss: 0.004075, tar: 0.000071 
l0: 0.000050, l1: 0.000050, l2: 0.000085, l3: 0.000135, l4: 0.000519, l5: 0.000946, l6: 0.002016
[epoch: 627/1000, batch:   168/ 1052, ite: 164680] train loss: 0.004075, tar: 0.000071 
l0: 0.000122, l1: 0.000123, l2: 0.000151, l3: 0.000224, l4: 0.000424, l5: 0.000714, l6: 0.001391
[epoch: 627/1000, batch:   248/ 1052, ite: 164700] train loss: 0.004075, tar: 0.000071 
l0: 0.000129, l1: 0.000133, l2: 0.000185, l3: 0.000312, l4: 0.000653, l5: 0.001211, l6: 0.001630
[epoch: 627/1000, batch:   328/ 1052, ite: 164720] train loss: 0.004075, tar: 0.000071 
l0: 0.000116, l1: 0.000116, l2: 0.000137, l3: 0.000245, l4: 0.000674, l5: 0.001213, l6: 0.001883
[epoch: 627/1000, batch:   408/ 1052, ite: 164740] train loss: 0.004074, tar: 0.000071 
l0: 0.000024, l1: 0.000025, l2: 0.000053, l3: 0.000130, l4: 0.000495, l5: 0.001152, l6: 0.001752
[epoch: 627/1000, batch:   488/ 1052, ite: 164760] train loss: 0.004074, tar: 0.000071 
l0: 0.000148, l1: 0.000152, l2: 0.000147, l3: 0.000198, l4: 0.000510, l5: 0.000680, l6: 0.002411
[epoch: 627/1000, batch:   568/ 1052, ite: 164780] train loss: 0.004074, tar: 0.000071 
l0: 0.000049, l1: 0.000048, l2: 0.000141, l3: 0.000373, l4: 0.000968, l5: 0.001794, l6: 0.003634
[epoch: 627/1000, batch:   648/ 1052, ite: 164800] train loss: 0.004074, tar: 0.000071 
l0: 0.000149, l1: 0.000153, l2: 0.000164, l3: 0.000240, l4: 0.000557, l5: 0.001151, l6: 0.002355
[epoch: 627/1000, batch:   728/ 1052, ite: 164820] train loss: 0.004073, tar: 0.000071 
l0: 0.000036, l1: 0.000035, l2: 0.000078, l3: 0.000148, l4: 0.000496, l5: 0.000509, l6: 0.000932
[epoch: 627/1000, batch:   808/ 1052, ite: 164840] train loss: 0.004075, tar: 0.000071 
l0: 0.000043, l1: 0.000044, l2: 0.000077, l3: 0.000147, l4: 0.000299, l5: 0.001373, l6: 0.001785
[epoch: 627/1000, batch:   888/ 1052, ite: 164860] train loss: 0.004073, tar: 0.000071 
l0: 0.000032, l1: 0.000032, l2: 0.000090, l3: 0.000195, l4: 0.000573, l5: 0.001234, l6: 0.001889
[epoch: 627/1000, batch:   968/ 1052, ite: 164880] train loss: 0.004073, tar: 0.000071 
l0: 0.000017, l1: 0.000018, l2: 0.000020, l3: 0.000060, l4: 0.000160, l5: 0.000408, l6: 0.001021
[epoch: 627/1000, batch:  1048/ 1052, ite: 164900] train loss: 0.004073, tar: 0.000071 
[Epoch 627/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000061, l1: 0.000058, l2: 0.000092, l3: 0.000225, l4: 0.000252, l5: 0.001146, l6: 0.001891
[epoch: 628/1000, batch:    76/ 1052, ite: 164920] train loss: 0.004073, tar: 0.000071 
l0: 0.000041, l1: 0.000042, l2: 0.000158, l3: 0.000387, l4: 0.000865, l5: 0.001742, l6: 0.003439
[epoch: 628/1000, batch:   156/ 1052, ite: 164940] train loss: 0.004074, tar: 0.000071 
l0: 0.000007, l1: 0.000007, l2: 0.000064, l3: 0.000146, l4: 0.000352, l5: 0.000903, l6: 0.001887
[epoch: 628/1000, batch:   236/ 1052, ite: 164960] train loss: 0.004075, tar: 0.000071 
l0: 0.000132, l1: 0.000130, l2: 0.000184, l3: 0.000347, l4: 0.000683, l5: 0.002070, l6: 0.004657
[epoch: 628/1000, batch:   316/ 1052, ite: 164980] train loss: 0.004074, tar: 0.000071 
l0: 0.000026, l1: 0.000026, l2: 0.000063, l3: 0.000146, l4: 0.000217, l5: 0.000665, l6: 0.001286
[epoch: 628/1000, batch:   396/ 1052, ite: 165000] train loss: 0.004074, tar: 0.000071 
l0: 0.000048, l1: 0.000047, l2: 0.000091, l3: 0.000195, l4: 0.000494, l5: 0.001377, l6: 0.002097
[epoch: 628/1000, batch:   476/ 1052, ite: 165020] train loss: 0.004074, tar: 0.000071 
l0: 0.000066, l1: 0.000067, l2: 0.000081, l3: 0.000201, l4: 0.000259, l5: 0.000794, l6: 0.001587
[epoch: 628/1000, batch:   556/ 1052, ite: 165040] train loss: 0.004073, tar: 0.000071 
l0: 0.000010, l1: 0.000010, l2: 0.000026, l3: 0.000050, l4: 0.000189, l5: 0.000645, l6: 0.001182
[epoch: 628/1000, batch:   636/ 1052, ite: 165060] train loss: 0.004073, tar: 0.000071 
l0: 0.000057, l1: 0.000060, l2: 0.000075, l3: 0.000111, l4: 0.000272, l5: 0.000694, l6: 0.001120
[epoch: 628/1000, batch:   716/ 1052, ite: 165080] train loss: 0.004074, tar: 0.000071 
l0: 0.000072, l1: 0.000071, l2: 0.000171, l3: 0.000368, l4: 0.000738, l5: 0.001422, l6: 0.004195
[epoch: 628/1000, batch:   796/ 1052, ite: 165100] train loss: 0.004075, tar: 0.000071 
l0: 0.000011, l1: 0.000011, l2: 0.000030, l3: 0.000074, l4: 0.000159, l5: 0.000484, l6: 0.000913
[epoch: 628/1000, batch:   876/ 1052, ite: 165120] train loss: 0.004075, tar: 0.000071 
l0: 0.000168, l1: 0.000170, l2: 0.000238, l3: 0.000369, l4: 0.000717, l5: 0.001344, l6: 0.002828
[epoch: 628/1000, batch:   956/ 1052, ite: 165140] train loss: 0.004074, tar: 0.000071 
l0: 0.000091, l1: 0.000091, l2: 0.000132, l3: 0.000304, l4: 0.000920, l5: 0.001450, l6: 0.002658
[epoch: 628/1000, batch:  1036/ 1052, ite: 165160] train loss: 0.004073, tar: 0.000071 
[Epoch 628/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000139, l1: 0.000136, l2: 0.000306, l3: 0.000617, l4: 0.001326, l5: 0.003188, l6: 0.006001
[epoch: 629/1000, batch:    64/ 1052, ite: 165180] train loss: 0.004074, tar: 0.000071 
l0: 0.000064, l1: 0.000065, l2: 0.000074, l3: 0.000176, l4: 0.000402, l5: 0.000695, l6: 0.001273
[epoch: 629/1000, batch:   144/ 1052, ite: 165200] train loss: 0.004074, tar: 0.000071 
l0: 0.000050, l1: 0.000052, l2: 0.000085, l3: 0.000142, l4: 0.000353, l5: 0.000907, l6: 0.001779
[epoch: 629/1000, batch:   224/ 1052, ite: 165220] train loss: 0.004074, tar: 0.000071 
l0: 0.000021, l1: 0.000020, l2: 0.000037, l3: 0.000192, l4: 0.000416, l5: 0.000854, l6: 0.001534
[epoch: 629/1000, batch:   304/ 1052, ite: 165240] train loss: 0.004072, tar: 0.000071 
l0: 0.000015, l1: 0.000016, l2: 0.000047, l3: 0.000145, l4: 0.000350, l5: 0.000680, l6: 0.001752
[epoch: 629/1000, batch:   384/ 1052, ite: 165260] train loss: 0.004072, tar: 0.000071 
l0: 0.000039, l1: 0.000036, l2: 0.000140, l3: 0.000328, l4: 0.000759, l5: 0.001451, l6: 0.002573
[epoch: 629/1000, batch:   464/ 1052, ite: 165280] train loss: 0.004073, tar: 0.000071 
l0: 0.000024, l1: 0.000025, l2: 0.000058, l3: 0.000047, l4: 0.000163, l5: 0.000613, l6: 0.001844
[epoch: 629/1000, batch:   544/ 1052, ite: 165300] train loss: 0.004071, tar: 0.000071 
l0: 0.000032, l1: 0.000032, l2: 0.000052, l3: 0.000118, l4: 0.000289, l5: 0.001115, l6: 0.001494
[epoch: 629/1000, batch:   624/ 1052, ite: 165320] train loss: 0.004071, tar: 0.000071 
l0: 0.000090, l1: 0.000088, l2: 0.000122, l3: 0.000169, l4: 0.000354, l5: 0.000804, l6: 0.001318
[epoch: 629/1000, batch:   704/ 1052, ite: 165340] train loss: 0.004069, tar: 0.000071 
l0: 0.000047, l1: 0.000048, l2: 0.000081, l3: 0.000150, l4: 0.000329, l5: 0.000900, l6: 0.000801
[epoch: 629/1000, batch:   784/ 1052, ite: 165360] train loss: 0.004069, tar: 0.000071 
l0: 0.000100, l1: 0.000099, l2: 0.000165, l3: 0.000313, l4: 0.000477, l5: 0.001238, l6: 0.001692
[epoch: 629/1000, batch:   864/ 1052, ite: 165380] train loss: 0.004070, tar: 0.000071 
l0: 0.000139, l1: 0.000143, l2: 0.000209, l3: 0.000368, l4: 0.000934, l5: 0.001739, l6: 0.001845
[epoch: 629/1000, batch:   944/ 1052, ite: 165400] train loss: 0.004069, tar: 0.000071 
l0: 0.000217, l1: 0.000221, l2: 0.000260, l3: 0.000451, l4: 0.000915, l5: 0.001699, l6: 0.002908
[epoch: 629/1000, batch:  1024/ 1052, ite: 165420] train loss: 0.004070, tar: 0.000071 
[Epoch 629/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000061, l1: 0.000062, l2: 0.000097, l3: 0.000145, l4: 0.000442, l5: 0.001164, l6: 0.001572
[epoch: 630/1000, batch:    52/ 1052, ite: 165440] train loss: 0.004071, tar: 0.000071 
l0: 0.000067, l1: 0.000070, l2: 0.000074, l3: 0.000159, l4: 0.000687, l5: 0.000986, l6: 0.002416
[epoch: 630/1000, batch:   132/ 1052, ite: 165460] train loss: 0.004072, tar: 0.000071 
l0: 0.000139, l1: 0.000138, l2: 0.000251, l3: 0.000482, l4: 0.000952, l5: 0.002300, l6: 0.003529
[epoch: 630/1000, batch:   212/ 1052, ite: 165480] train loss: 0.004073, tar: 0.000071 
l0: 0.000010, l1: 0.000009, l2: 0.000054, l3: 0.000210, l4: 0.000521, l5: 0.001038, l6: 0.001663
[epoch: 630/1000, batch:   292/ 1052, ite: 165500] train loss: 0.004074, tar: 0.000071 
l0: 0.000006, l1: 0.000006, l2: 0.000015, l3: 0.000062, l4: 0.000218, l5: 0.000630, l6: 0.000850
[epoch: 630/1000, batch:   372/ 1052, ite: 165520] train loss: 0.004074, tar: 0.000071 
l0: 0.000035, l1: 0.000034, l2: 0.000061, l3: 0.000086, l4: 0.000195, l5: 0.000758, l6: 0.000688
[epoch: 630/1000, batch:   452/ 1052, ite: 165540] train loss: 0.004073, tar: 0.000071 
l0: 0.000050, l1: 0.000052, l2: 0.000098, l3: 0.000223, l4: 0.000561, l5: 0.001337, l6: 0.003590
[epoch: 630/1000, batch:   532/ 1052, ite: 165560] train loss: 0.004074, tar: 0.000071 
l0: 0.000079, l1: 0.000084, l2: 0.000094, l3: 0.000170, l4: 0.000474, l5: 0.001170, l6: 0.001341
[epoch: 630/1000, batch:   612/ 1052, ite: 165580] train loss: 0.004073, tar: 0.000071 
l0: 0.000059, l1: 0.000061, l2: 0.000080, l3: 0.000115, l4: 0.000376, l5: 0.000757, l6: 0.001495
[epoch: 630/1000, batch:   692/ 1052, ite: 165600] train loss: 0.004071, tar: 0.000071 
l0: 0.000051, l1: 0.000050, l2: 0.000073, l3: 0.000110, l4: 0.000378, l5: 0.000965, l6: 0.001466
[epoch: 630/1000, batch:   772/ 1052, ite: 165620] train loss: 0.004070, tar: 0.000071 
l0: 0.000106, l1: 0.000105, l2: 0.000147, l3: 0.000272, l4: 0.000560, l5: 0.001625, l6: 0.002403
[epoch: 630/1000, batch:   852/ 1052, ite: 165640] train loss: 0.004073, tar: 0.000071 
l0: 0.000015, l1: 0.000016, l2: 0.000056, l3: 0.000106, l4: 0.000330, l5: 0.000548, l6: 0.001209
[epoch: 630/1000, batch:   932/ 1052, ite: 165660] train loss: 0.004072, tar: 0.000071 
l0: 0.000014, l1: 0.000014, l2: 0.000018, l3: 0.000080, l4: 0.000259, l5: 0.000663, l6: 0.000905
[epoch: 630/1000, batch:  1012/ 1052, ite: 165680] train loss: 0.004071, tar: 0.000071 
[Epoch 630/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000010, l1: 0.000011, l2: 0.000020, l3: 0.000071, l4: 0.000239, l5: 0.000792, l6: 0.001366
[epoch: 631/1000, batch:    40/ 1052, ite: 165700] train loss: 0.004071, tar: 0.000071 
l0: 0.000218, l1: 0.000219, l2: 0.000273, l3: 0.000438, l4: 0.000962, l5: 0.001571, l6: 0.003283
[epoch: 631/1000, batch:   120/ 1052, ite: 165720] train loss: 0.004071, tar: 0.000071 
l0: 0.000020, l1: 0.000020, l2: 0.000056, l3: 0.000120, l4: 0.000319, l5: 0.000509, l6: 0.001599
[epoch: 631/1000, batch:   200/ 1052, ite: 165740] train loss: 0.004071, tar: 0.000071 
l0: 0.000024, l1: 0.000025, l2: 0.000046, l3: 0.000137, l4: 0.000259, l5: 0.000888, l6: 0.001469
[epoch: 631/1000, batch:   280/ 1052, ite: 165760] train loss: 0.004070, tar: 0.000071 
l0: 0.000069, l1: 0.000071, l2: 0.000102, l3: 0.000151, l4: 0.000547, l5: 0.001234, l6: 0.002430
[epoch: 631/1000, batch:   360/ 1052, ite: 165780] train loss: 0.004068, tar: 0.000071 
l0: 0.000044, l1: 0.000046, l2: 0.000067, l3: 0.000272, l4: 0.000815, l5: 0.001412, l6: 0.003512
[epoch: 631/1000, batch:   440/ 1052, ite: 165800] train loss: 0.004067, tar: 0.000071 
l0: 0.000119, l1: 0.000122, l2: 0.000134, l3: 0.000183, l4: 0.000322, l5: 0.001357, l6: 0.002013
[epoch: 631/1000, batch:   520/ 1052, ite: 165820] train loss: 0.004069, tar: 0.000071 
l0: 0.000021, l1: 0.000023, l2: 0.000025, l3: 0.000201, l4: 0.000494, l5: 0.000866, l6: 0.001402
[epoch: 631/1000, batch:   600/ 1052, ite: 165840] train loss: 0.004068, tar: 0.000071 
l0: 0.000036, l1: 0.000035, l2: 0.000090, l3: 0.000273, l4: 0.000538, l5: 0.001126, l6: 0.002987
[epoch: 631/1000, batch:   680/ 1052, ite: 165860] train loss: 0.004068, tar: 0.000071 
l0: 0.000179, l1: 0.000181, l2: 0.000232, l3: 0.000504, l4: 0.000948, l5: 0.001503, l6: 0.002185
[epoch: 631/1000, batch:   760/ 1052, ite: 165880] train loss: 0.004069, tar: 0.000071 
l0: 0.000052, l1: 0.000052, l2: 0.000069, l3: 0.000108, l4: 0.000340, l5: 0.001039, l6: 0.001815
[epoch: 631/1000, batch:   840/ 1052, ite: 165900] train loss: 0.004069, tar: 0.000071 
l0: 0.000082, l1: 0.000084, l2: 0.000108, l3: 0.000125, l4: 0.000424, l5: 0.001214, l6: 0.000897
[epoch: 631/1000, batch:   920/ 1052, ite: 165920] train loss: 0.004069, tar: 0.000071 
l0: 0.000056, l1: 0.000057, l2: 0.000070, l3: 0.000154, l4: 0.000304, l5: 0.001023, l6: 0.001681
[epoch: 631/1000, batch:  1000/ 1052, ite: 165940] train loss: 0.004069, tar: 0.000071 
[Epoch 631/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000049, l1: 0.000049, l2: 0.000079, l3: 0.000181, l4: 0.000386, l5: 0.000932, l6: 0.001615
[epoch: 632/1000, batch:    28/ 1052, ite: 165960] train loss: 0.004068, tar: 0.000071 
l0: 0.000170, l1: 0.000180, l2: 0.000200, l3: 0.000287, l4: 0.000418, l5: 0.001300, l6: 0.002214
[epoch: 632/1000, batch:   108/ 1052, ite: 165980] train loss: 0.004070, tar: 0.000071 
l0: 0.000119, l1: 0.000118, l2: 0.000160, l3: 0.000265, l4: 0.000405, l5: 0.000909, l6: 0.002300
[epoch: 632/1000, batch:   188/ 1052, ite: 166000] train loss: 0.004070, tar: 0.000071 
l0: 0.000041, l1: 0.000039, l2: 0.000100, l3: 0.000169, l4: 0.000440, l5: 0.000751, l6: 0.001002
[epoch: 632/1000, batch:   268/ 1052, ite: 166020] train loss: 0.004070, tar: 0.000071 
l0: 0.000022, l1: 0.000023, l2: 0.000038, l3: 0.000115, l4: 0.000523, l5: 0.000949, l6: 0.001723
[epoch: 632/1000, batch:   348/ 1052, ite: 166040] train loss: 0.004070, tar: 0.000071 
l0: 0.000024, l1: 0.000025, l2: 0.000049, l3: 0.000203, l4: 0.000453, l5: 0.001046, l6: 0.001453
[epoch: 632/1000, batch:   428/ 1052, ite: 166060] train loss: 0.004070, tar: 0.000071 
l0: 0.000053, l1: 0.000053, l2: 0.000131, l3: 0.000351, l4: 0.000751, l5: 0.001945, l6: 0.003152
[epoch: 632/1000, batch:   508/ 1052, ite: 166080] train loss: 0.004069, tar: 0.000071 
l0: 0.000037, l1: 0.000039, l2: 0.000048, l3: 0.000135, l4: 0.000337, l5: 0.000661, l6: 0.001055
[epoch: 632/1000, batch:   588/ 1052, ite: 166100] train loss: 0.004070, tar: 0.000071 
l0: 0.000060, l1: 0.000061, l2: 0.000149, l3: 0.000231, l4: 0.000423, l5: 0.000885, l6: 0.001653
[epoch: 632/1000, batch:   668/ 1052, ite: 166120] train loss: 0.004069, tar: 0.000071 
l0: 0.000082, l1: 0.000083, l2: 0.000096, l3: 0.000219, l4: 0.000440, l5: 0.001462, l6: 0.002020
[epoch: 632/1000, batch:   748/ 1052, ite: 166140] train loss: 0.004068, tar: 0.000071 
l0: 0.000063, l1: 0.000067, l2: 0.000108, l3: 0.000186, l4: 0.000433, l5: 0.000971, l6: 0.002422
[epoch: 632/1000, batch:   828/ 1052, ite: 166160] train loss: 0.004070, tar: 0.000071 
l0: 0.000084, l1: 0.000087, l2: 0.000105, l3: 0.000187, l4: 0.000464, l5: 0.001327, l6: 0.002506
[epoch: 632/1000, batch:   908/ 1052, ite: 166180] train loss: 0.004070, tar: 0.000071 
l0: 0.000035, l1: 0.000036, l2: 0.000077, l3: 0.000182, l4: 0.000488, l5: 0.000983, l6: 0.001993
[epoch: 632/1000, batch:   988/ 1052, ite: 166200] train loss: 0.004070, tar: 0.000071 
[Epoch 632/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000107, l1: 0.000108, l2: 0.000151, l3: 0.000267, l4: 0.000613, l5: 0.001188, l6: 0.001876
[epoch: 633/1000, batch:    16/ 1052, ite: 166220] train loss: 0.004069, tar: 0.000071 
l0: 0.000085, l1: 0.000093, l2: 0.000105, l3: 0.000173, l4: 0.000307, l5: 0.001100, l6: 0.002918
[epoch: 633/1000, batch:    96/ 1052, ite: 166240] train loss: 0.004070, tar: 0.000071 
l0: 0.000065, l1: 0.000066, l2: 0.000155, l3: 0.000284, l4: 0.000606, l5: 0.001104, l6: 0.002341
[epoch: 633/1000, batch:   176/ 1052, ite: 166260] train loss: 0.004071, tar: 0.000071 
l0: 0.000036, l1: 0.000035, l2: 0.000083, l3: 0.000186, l4: 0.000432, l5: 0.001012, l6: 0.001692
[epoch: 633/1000, batch:   256/ 1052, ite: 166280] train loss: 0.004070, tar: 0.000071 
l0: 0.000067, l1: 0.000070, l2: 0.000110, l3: 0.000244, l4: 0.000552, l5: 0.002149, l6: 0.002128
[epoch: 633/1000, batch:   336/ 1052, ite: 166300] train loss: 0.004070, tar: 0.000071 
l0: 0.000025, l1: 0.000026, l2: 0.000038, l3: 0.000174, l4: 0.000502, l5: 0.000859, l6: 0.002312
[epoch: 633/1000, batch:   416/ 1052, ite: 166320] train loss: 0.004069, tar: 0.000071 
l0: 0.000075, l1: 0.000071, l2: 0.000108, l3: 0.000258, l4: 0.000897, l5: 0.001782, l6: 0.002855
[epoch: 633/1000, batch:   496/ 1052, ite: 166340] train loss: 0.004070, tar: 0.000071 
l0: 0.000135, l1: 0.000137, l2: 0.000167, l3: 0.000267, l4: 0.000410, l5: 0.001246, l6: 0.001623
[epoch: 633/1000, batch:   576/ 1052, ite: 166360] train loss: 0.004071, tar: 0.000071 
l0: 0.000055, l1: 0.000056, l2: 0.000061, l3: 0.000119, l4: 0.000350, l5: 0.000708, l6: 0.001104
[epoch: 633/1000, batch:   656/ 1052, ite: 166380] train loss: 0.004069, tar: 0.000071 
l0: 0.000043, l1: 0.000043, l2: 0.000083, l3: 0.000220, l4: 0.000447, l5: 0.000729, l6: 0.001119
[epoch: 633/1000, batch:   736/ 1052, ite: 166400] train loss: 0.004070, tar: 0.000071 
l0: 0.000167, l1: 0.000170, l2: 0.000188, l3: 0.000312, l4: 0.000677, l5: 0.001741, l6: 0.003091
[epoch: 633/1000, batch:   816/ 1052, ite: 166420] train loss: 0.004070, tar: 0.000071 
l0: 0.000025, l1: 0.000025, l2: 0.000052, l3: 0.000128, l4: 0.000315, l5: 0.000812, l6: 0.001846
[epoch: 633/1000, batch:   896/ 1052, ite: 166440] train loss: 0.004070, tar: 0.000071 
l0: 0.000025, l1: 0.000024, l2: 0.000054, l3: 0.000144, l4: 0.000328, l5: 0.001027, l6: 0.001618
[epoch: 633/1000, batch:   976/ 1052, ite: 166460] train loss: 0.004070, tar: 0.000071 
[Epoch 633/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000049, l1: 0.000050, l2: 0.000098, l3: 0.000203, l4: 0.000557, l5: 0.000907, l6: 0.001705
[epoch: 634/1000, batch:     4/ 1052, ite: 166480] train loss: 0.004069, tar: 0.000071 
l0: 0.000141, l1: 0.000141, l2: 0.000183, l3: 0.000366, l4: 0.000710, l5: 0.001193, l6: 0.002050
[epoch: 634/1000, batch:    84/ 1052, ite: 166500] train loss: 0.004068, tar: 0.000070 
l0: 0.000063, l1: 0.000063, l2: 0.000134, l3: 0.000261, l4: 0.000504, l5: 0.001337, l6: 0.001798
[epoch: 634/1000, batch:   164/ 1052, ite: 166520] train loss: 0.004068, tar: 0.000070 
l0: 0.000063, l1: 0.000062, l2: 0.000111, l3: 0.000215, l4: 0.000563, l5: 0.001816, l6: 0.003525
[epoch: 634/1000, batch:   244/ 1052, ite: 166540] train loss: 0.004069, tar: 0.000070 
l0: 0.000184, l1: 0.000193, l2: 0.000230, l3: 0.000248, l4: 0.000628, l5: 0.000940, l6: 0.001432
[epoch: 634/1000, batch:   324/ 1052, ite: 166560] train loss: 0.004070, tar: 0.000071 
l0: 0.000033, l1: 0.000033, l2: 0.000044, l3: 0.000089, l4: 0.000255, l5: 0.000533, l6: 0.001323
[epoch: 634/1000, batch:   404/ 1052, ite: 166580] train loss: 0.004070, tar: 0.000071 
l0: 0.000012, l1: 0.000013, l2: 0.000028, l3: 0.000095, l4: 0.000308, l5: 0.000724, l6: 0.001134
[epoch: 634/1000, batch:   484/ 1052, ite: 166600] train loss: 0.004069, tar: 0.000070 
l0: 0.000129, l1: 0.000127, l2: 0.000172, l3: 0.000272, l4: 0.000515, l5: 0.001296, l6: 0.002054
[epoch: 634/1000, batch:   564/ 1052, ite: 166620] train loss: 0.004068, tar: 0.000070 
l0: 0.000046, l1: 0.000047, l2: 0.000078, l3: 0.000165, l4: 0.000377, l5: 0.000912, l6: 0.001758
[epoch: 634/1000, batch:   644/ 1052, ite: 166640] train loss: 0.004068, tar: 0.000070 
l0: 0.000053, l1: 0.000052, l2: 0.000106, l3: 0.000183, l4: 0.000426, l5: 0.001231, l6: 0.002014
[epoch: 634/1000, batch:   724/ 1052, ite: 166660] train loss: 0.004070, tar: 0.000071 
l0: 0.000082, l1: 0.000083, l2: 0.000157, l3: 0.000199, l4: 0.000409, l5: 0.001031, l6: 0.001170
[epoch: 634/1000, batch:   804/ 1052, ite: 166680] train loss: 0.004070, tar: 0.000071 
l0: 0.000048, l1: 0.000048, l2: 0.000078, l3: 0.000158, l4: 0.000456, l5: 0.000928, l6: 0.001399
[epoch: 634/1000, batch:   884/ 1052, ite: 166700] train loss: 0.004070, tar: 0.000071 
l0: 0.000043, l1: 0.000042, l2: 0.000106, l3: 0.000287, l4: 0.000605, l5: 0.001285, l6: 0.002693
[epoch: 634/1000, batch:   964/ 1052, ite: 166720] train loss: 0.004071, tar: 0.000071 
l0: 0.000215, l1: 0.000214, l2: 0.000326, l3: 0.000560, l4: 0.001125, l5: 0.002204, l6: 0.003917
[epoch: 634/1000, batch:  1044/ 1052, ite: 166740] train loss: 0.004070, tar: 0.000071 
[Epoch 634/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000018, l1: 0.000019, l2: 0.000075, l3: 0.000173, l4: 0.000362, l5: 0.000945, l6: 0.001865
[epoch: 635/1000, batch:    72/ 1052, ite: 166760] train loss: 0.004070, tar: 0.000071 
l0: 0.000026, l1: 0.000027, l2: 0.000046, l3: 0.000091, l4: 0.000255, l5: 0.000779, l6: 0.001677
[epoch: 635/1000, batch:   152/ 1052, ite: 166780] train loss: 0.004070, tar: 0.000070 
l0: 0.000117, l1: 0.000126, l2: 0.000156, l3: 0.000300, l4: 0.000747, l5: 0.001908, l6: 0.004801
[epoch: 635/1000, batch:   232/ 1052, ite: 166800] train loss: 0.004070, tar: 0.000071 
l0: 0.000077, l1: 0.000075, l2: 0.000102, l3: 0.000236, l4: 0.000546, l5: 0.001012, l6: 0.001577
[epoch: 635/1000, batch:   312/ 1052, ite: 166820] train loss: 0.004070, tar: 0.000071 
l0: 0.000305, l1: 0.000322, l2: 0.000363, l3: 0.000498, l4: 0.000826, l5: 0.002017, l6: 0.004452
[epoch: 635/1000, batch:   392/ 1052, ite: 166840] train loss: 0.004070, tar: 0.000071 
l0: 0.000083, l1: 0.000080, l2: 0.000129, l3: 0.000258, l4: 0.000582, l5: 0.000950, l6: 0.001666
[epoch: 635/1000, batch:   472/ 1052, ite: 166860] train loss: 0.004071, tar: 0.000071 
l0: 0.000069, l1: 0.000070, l2: 0.000119, l3: 0.000281, l4: 0.000594, l5: 0.001513, l6: 0.002245
[epoch: 635/1000, batch:   552/ 1052, ite: 166880] train loss: 0.004071, tar: 0.000071 
l0: 0.000125, l1: 0.000130, l2: 0.000180, l3: 0.000365, l4: 0.000620, l5: 0.001115, l6: 0.002368
[epoch: 635/1000, batch:   632/ 1052, ite: 166900] train loss: 0.004072, tar: 0.000071 
l0: 0.000066, l1: 0.000071, l2: 0.000085, l3: 0.000090, l4: 0.000345, l5: 0.001205, l6: 0.001756
[epoch: 635/1000, batch:   712/ 1052, ite: 166920] train loss: 0.004073, tar: 0.000071 
l0: 0.000028, l1: 0.000027, l2: 0.000058, l3: 0.000169, l4: 0.000335, l5: 0.000974, l6: 0.002077
[epoch: 635/1000, batch:   792/ 1052, ite: 166940] train loss: 0.004073, tar: 0.000071 
l0: 0.000012, l1: 0.000012, l2: 0.000039, l3: 0.000121, l4: 0.000315, l5: 0.000811, l6: 0.001606
[epoch: 635/1000, batch:   872/ 1052, ite: 166960] train loss: 0.004071, tar: 0.000071 
l0: 0.000100, l1: 0.000104, l2: 0.000126, l3: 0.000164, l4: 0.000361, l5: 0.000916, l6: 0.001506
[epoch: 635/1000, batch:   952/ 1052, ite: 166980] train loss: 0.004071, tar: 0.000071 
l0: 0.000051, l1: 0.000051, l2: 0.000070, l3: 0.000145, l4: 0.000342, l5: 0.000946, l6: 0.000804
[epoch: 635/1000, batch:  1032/ 1052, ite: 167000] train loss: 0.004070, tar: 0.000071 
[Epoch 635/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000049, l1: 0.000049, l2: 0.000071, l3: 0.000138, l4: 0.000496, l5: 0.000934, l6: 0.002421
[epoch: 636/1000, batch:    60/ 1052, ite: 167020] train loss: 0.004070, tar: 0.000071 
l0: 0.000031, l1: 0.000031, l2: 0.000070, l3: 0.000178, l4: 0.000441, l5: 0.001137, l6: 0.001314
[epoch: 636/1000, batch:   140/ 1052, ite: 167040] train loss: 0.004070, tar: 0.000071 
l0: 0.000080, l1: 0.000082, l2: 0.000140, l3: 0.000254, l4: 0.000852, l5: 0.001735, l6: 0.003022
[epoch: 636/1000, batch:   220/ 1052, ite: 167060] train loss: 0.004070, tar: 0.000070 
l0: 0.000074, l1: 0.000073, l2: 0.000120, l3: 0.000318, l4: 0.000502, l5: 0.000888, l6: 0.001497
[epoch: 636/1000, batch:   300/ 1052, ite: 167080] train loss: 0.004071, tar: 0.000071 
l0: 0.000064, l1: 0.000066, l2: 0.000085, l3: 0.000114, l4: 0.000317, l5: 0.000628, l6: 0.001968
[epoch: 636/1000, batch:   380/ 1052, ite: 167100] train loss: 0.004072, tar: 0.000071 
l0: 0.000050, l1: 0.000051, l2: 0.000053, l3: 0.000169, l4: 0.000316, l5: 0.000925, l6: 0.001509
[epoch: 636/1000, batch:   460/ 1052, ite: 167120] train loss: 0.004072, tar: 0.000071 
l0: 0.000078, l1: 0.000080, l2: 0.000109, l3: 0.000211, l4: 0.000374, l5: 0.000752, l6: 0.001538
[epoch: 636/1000, batch:   540/ 1052, ite: 167140] train loss: 0.004072, tar: 0.000071 
l0: 0.000021, l1: 0.000021, l2: 0.000076, l3: 0.000164, l4: 0.000398, l5: 0.001090, l6: 0.001154
[epoch: 636/1000, batch:   620/ 1052, ite: 167160] train loss: 0.004072, tar: 0.000070 
l0: 0.000024, l1: 0.000023, l2: 0.000039, l3: 0.000119, l4: 0.000274, l5: 0.000714, l6: 0.001370
[epoch: 636/1000, batch:   700/ 1052, ite: 167180] train loss: 0.004071, tar: 0.000070 
l0: 0.000026, l1: 0.000025, l2: 0.000071, l3: 0.000132, l4: 0.000263, l5: 0.000607, l6: 0.001363
[epoch: 636/1000, batch:   780/ 1052, ite: 167200] train loss: 0.004072, tar: 0.000070 
l0: 0.000150, l1: 0.000150, l2: 0.000229, l3: 0.000452, l4: 0.000818, l5: 0.001832, l6: 0.003382
[epoch: 636/1000, batch:   860/ 1052, ite: 167220] train loss: 0.004073, tar: 0.000071 
l0: 0.000117, l1: 0.000117, l2: 0.000165, l3: 0.000359, l4: 0.000715, l5: 0.000870, l6: 0.002390
[epoch: 636/1000, batch:   940/ 1052, ite: 167240] train loss: 0.004073, tar: 0.000071 
l0: 0.000201, l1: 0.000219, l2: 0.000269, l3: 0.000304, l4: 0.000465, l5: 0.000987, l6: 0.002088
[epoch: 636/1000, batch:  1020/ 1052, ite: 167260] train loss: 0.004073, tar: 0.000071 
[Epoch 636/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000061, l1: 0.000060, l2: 0.000071, l3: 0.000113, l4: 0.000318, l5: 0.000481, l6: 0.001126
[epoch: 637/1000, batch:    48/ 1052, ite: 167280] train loss: 0.004073, tar: 0.000071 
l0: 0.000079, l1: 0.000077, l2: 0.000107, l3: 0.000144, l4: 0.000395, l5: 0.000836, l6: 0.001426
[epoch: 637/1000, batch:   128/ 1052, ite: 167300] train loss: 0.004073, tar: 0.000071 
l0: 0.000049, l1: 0.000056, l2: 0.000076, l3: 0.000186, l4: 0.000358, l5: 0.000619, l6: 0.001547
[epoch: 637/1000, batch:   208/ 1052, ite: 167320] train loss: 0.004072, tar: 0.000071 
l0: 0.000071, l1: 0.000072, l2: 0.000107, l3: 0.000196, l4: 0.000337, l5: 0.000883, l6: 0.002325
[epoch: 637/1000, batch:   288/ 1052, ite: 167340] train loss: 0.004072, tar: 0.000071 
l0: 0.000048, l1: 0.000047, l2: 0.000121, l3: 0.000403, l4: 0.000670, l5: 0.001090, l6: 0.002342
[epoch: 637/1000, batch:   368/ 1052, ite: 167360] train loss: 0.004074, tar: 0.000071 
l0: 0.000058, l1: 0.000056, l2: 0.000124, l3: 0.000271, l4: 0.000739, l5: 0.001645, l6: 0.002187
[epoch: 637/1000, batch:   448/ 1052, ite: 167380] train loss: 0.004074, tar: 0.000071 
l0: 0.000029, l1: 0.000028, l2: 0.000052, l3: 0.000156, l4: 0.000380, l5: 0.001212, l6: 0.001570
[epoch: 637/1000, batch:   528/ 1052, ite: 167400] train loss: 0.004075, tar: 0.000071 
l0: 0.000101, l1: 0.000104, l2: 0.000123, l3: 0.000204, l4: 0.000396, l5: 0.001412, l6: 0.000903
[epoch: 637/1000, batch:   608/ 1052, ite: 167420] train loss: 0.004075, tar: 0.000071 
l0: 0.000064, l1: 0.000067, l2: 0.000206, l3: 0.000405, l4: 0.000968, l5: 0.001924, l6: 0.002323
[epoch: 637/1000, batch:   688/ 1052, ite: 167440] train loss: 0.004075, tar: 0.000071 
l0: 0.000018, l1: 0.000017, l2: 0.000055, l3: 0.000147, l4: 0.000371, l5: 0.001554, l6: 0.001859
[epoch: 637/1000, batch:   768/ 1052, ite: 167460] train loss: 0.004076, tar: 0.000071 
l0: 0.000030, l1: 0.000027, l2: 0.000043, l3: 0.000120, l4: 0.000728, l5: 0.001066, l6: 0.001941
[epoch: 637/1000, batch:   848/ 1052, ite: 167480] train loss: 0.004076, tar: 0.000071 
l0: 0.000190, l1: 0.000205, l2: 0.000203, l3: 0.000314, l4: 0.000942, l5: 0.001369, l6: 0.003482
[epoch: 637/1000, batch:   928/ 1052, ite: 167500] train loss: 0.004075, tar: 0.000071 
l0: 0.000056, l1: 0.000053, l2: 0.000092, l3: 0.000143, l4: 0.000276, l5: 0.001106, l6: 0.001654
[epoch: 637/1000, batch:  1008/ 1052, ite: 167520] train loss: 0.004074, tar: 0.000071 
[Epoch 637/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000024, l1: 0.000023, l2: 0.000072, l3: 0.000199, l4: 0.000499, l5: 0.001230, l6: 0.002675
[epoch: 638/1000, batch:    36/ 1052, ite: 167540] train loss: 0.004073, tar: 0.000071 
l0: 0.000097, l1: 0.000099, l2: 0.000124, l3: 0.000299, l4: 0.000388, l5: 0.001157, l6: 0.001819
[epoch: 638/1000, batch:   116/ 1052, ite: 167560] train loss: 0.004073, tar: 0.000071 
l0: 0.000143, l1: 0.000148, l2: 0.000179, l3: 0.000351, l4: 0.000580, l5: 0.001168, l6: 0.002116
[epoch: 638/1000, batch:   196/ 1052, ite: 167580] train loss: 0.004073, tar: 0.000071 
l0: 0.000086, l1: 0.000086, l2: 0.000116, l3: 0.000253, l4: 0.000718, l5: 0.001347, l6: 0.002640
[epoch: 638/1000, batch:   276/ 1052, ite: 167600] train loss: 0.004072, tar: 0.000071 
l0: 0.000069, l1: 0.000062, l2: 0.000122, l3: 0.000292, l4: 0.000633, l5: 0.001364, l6: 0.002622
[epoch: 638/1000, batch:   356/ 1052, ite: 167620] train loss: 0.004072, tar: 0.000071 
l0: 0.000045, l1: 0.000043, l2: 0.000088, l3: 0.000151, l4: 0.000225, l5: 0.000628, l6: 0.001003
[epoch: 638/1000, batch:   436/ 1052, ite: 167640] train loss: 0.004071, tar: 0.000071 
l0: 0.000027, l1: 0.000025, l2: 0.000064, l3: 0.000117, l4: 0.000197, l5: 0.001068, l6: 0.002187
[epoch: 638/1000, batch:   516/ 1052, ite: 167660] train loss: 0.004072, tar: 0.000071 
l0: 0.000036, l1: 0.000038, l2: 0.000067, l3: 0.000180, l4: 0.000182, l5: 0.000652, l6: 0.001001
[epoch: 638/1000, batch:   596/ 1052, ite: 167680] train loss: 0.004072, tar: 0.000071 
l0: 0.000020, l1: 0.000020, l2: 0.000060, l3: 0.000136, l4: 0.000316, l5: 0.000690, l6: 0.003006
[epoch: 638/1000, batch:   676/ 1052, ite: 167700] train loss: 0.004072, tar: 0.000071 
l0: 0.000031, l1: 0.000029, l2: 0.000057, l3: 0.000164, l4: 0.000490, l5: 0.001155, l6: 0.002134
[epoch: 638/1000, batch:   756/ 1052, ite: 167720] train loss: 0.004072, tar: 0.000071 
l0: 0.000042, l1: 0.000044, l2: 0.000064, l3: 0.000082, l4: 0.000289, l5: 0.000759, l6: 0.001166
[epoch: 638/1000, batch:   836/ 1052, ite: 167740] train loss: 0.004071, tar: 0.000071 
l0: 0.000054, l1: 0.000054, l2: 0.000127, l3: 0.000301, l4: 0.000563, l5: 0.001584, l6: 0.002432
[epoch: 638/1000, batch:   916/ 1052, ite: 167760] train loss: 0.004072, tar: 0.000071 
l0: 0.000021, l1: 0.000020, l2: 0.000071, l3: 0.000160, l4: 0.000431, l5: 0.000869, l6: 0.002811
[epoch: 638/1000, batch:   996/ 1052, ite: 167780] train loss: 0.004074, tar: 0.000071 
[Epoch 638/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000051, l1: 0.000052, l2: 0.000076, l3: 0.000121, l4: 0.000235, l5: 0.000664, l6: 0.001274
[epoch: 639/1000, batch:    24/ 1052, ite: 167800] train loss: 0.004073, tar: 0.000071 
l0: 0.000125, l1: 0.000125, l2: 0.000158, l3: 0.000252, l4: 0.000465, l5: 0.001404, l6: 0.002758
[epoch: 639/1000, batch:   104/ 1052, ite: 167820] train loss: 0.004073, tar: 0.000071 
l0: 0.000037, l1: 0.000043, l2: 0.000064, l3: 0.000095, l4: 0.000291, l5: 0.000897, l6: 0.001294
[epoch: 639/1000, batch:   184/ 1052, ite: 167840] train loss: 0.004073, tar: 0.000071 
l0: 0.000050, l1: 0.000051, l2: 0.000086, l3: 0.000274, l4: 0.000866, l5: 0.002128, l6: 0.002832
[epoch: 639/1000, batch:   264/ 1052, ite: 167860] train loss: 0.004073, tar: 0.000071 
l0: 0.000128, l1: 0.000130, l2: 0.000130, l3: 0.000172, l4: 0.000441, l5: 0.001122, l6: 0.003064
[epoch: 639/1000, batch:   344/ 1052, ite: 167880] train loss: 0.004072, tar: 0.000071 
l0: 0.000042, l1: 0.000039, l2: 0.000108, l3: 0.000199, l4: 0.000484, l5: 0.000604, l6: 0.002056
[epoch: 639/1000, batch:   424/ 1052, ite: 167900] train loss: 0.004072, tar: 0.000071 
l0: 0.000204, l1: 0.000204, l2: 0.000265, l3: 0.000407, l4: 0.000923, l5: 0.001612, l6: 0.002308
[epoch: 639/1000, batch:   504/ 1052, ite: 167920] train loss: 0.004072, tar: 0.000071 
l0: 0.000064, l1: 0.000066, l2: 0.000108, l3: 0.000183, l4: 0.000367, l5: 0.000864, l6: 0.001918
[epoch: 639/1000, batch:   584/ 1052, ite: 167940] train loss: 0.004073, tar: 0.000071 
l0: 0.000136, l1: 0.000140, l2: 0.000143, l3: 0.000257, l4: 0.000645, l5: 0.001573, l6: 0.002626
[epoch: 639/1000, batch:   664/ 1052, ite: 167960] train loss: 0.004074, tar: 0.000071 
l0: 0.000087, l1: 0.000086, l2: 0.000186, l3: 0.000381, l4: 0.000723, l5: 0.001797, l6: 0.002571
[epoch: 639/1000, batch:   744/ 1052, ite: 167980] train loss: 0.004074, tar: 0.000071 
l0: 0.000098, l1: 0.000095, l2: 0.000136, l3: 0.000264, l4: 0.000681, l5: 0.001263, l6: 0.002187
[epoch: 639/1000, batch:   824/ 1052, ite: 168000] train loss: 0.004073, tar: 0.000071 
l0: 0.000061, l1: 0.000059, l2: 0.000077, l3: 0.000139, l4: 0.000361, l5: 0.000682, l6: 0.001596
[epoch: 639/1000, batch:   904/ 1052, ite: 168020] train loss: 0.004073, tar: 0.000071 
l0: 0.000103, l1: 0.000104, l2: 0.000108, l3: 0.000228, l4: 0.000682, l5: 0.001491, l6: 0.002302
[epoch: 639/1000, batch:   984/ 1052, ite: 168040] train loss: 0.004074, tar: 0.000071 
[Epoch 639/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000008, l1: 0.000009, l2: 0.000012, l3: 0.000072, l4: 0.000224, l5: 0.000428, l6: 0.000925
[epoch: 640/1000, batch:    12/ 1052, ite: 168060] train loss: 0.004073, tar: 0.000071 
l0: 0.000016, l1: 0.000015, l2: 0.000034, l3: 0.000087, l4: 0.000346, l5: 0.001009, l6: 0.001498
[epoch: 640/1000, batch:    92/ 1052, ite: 168080] train loss: 0.004074, tar: 0.000071 
l0: 0.000058, l1: 0.000059, l2: 0.000123, l3: 0.000339, l4: 0.000555, l5: 0.001541, l6: 0.003857
[epoch: 640/1000, batch:   172/ 1052, ite: 168100] train loss: 0.004074, tar: 0.000071 
l0: 0.000012, l1: 0.000012, l2: 0.000030, l3: 0.000083, l4: 0.000303, l5: 0.000547, l6: 0.001067
[epoch: 640/1000, batch:   252/ 1052, ite: 168120] train loss: 0.004074, tar: 0.000071 
l0: 0.000042, l1: 0.000044, l2: 0.000054, l3: 0.000118, l4: 0.000483, l5: 0.001063, l6: 0.001037
[epoch: 640/1000, batch:   332/ 1052, ite: 168140] train loss: 0.004074, tar: 0.000071 
l0: 0.000031, l1: 0.000030, l2: 0.000044, l3: 0.000084, l4: 0.000393, l5: 0.000992, l6: 0.001146
[epoch: 640/1000, batch:   412/ 1052, ite: 168160] train loss: 0.004074, tar: 0.000071 
l0: 0.000021, l1: 0.000020, l2: 0.000037, l3: 0.000114, l4: 0.000209, l5: 0.000583, l6: 0.000826
[epoch: 640/1000, batch:   492/ 1052, ite: 168180] train loss: 0.004073, tar: 0.000071 
l0: 0.000035, l1: 0.000044, l2: 0.000047, l3: 0.000115, l4: 0.000415, l5: 0.000751, l6: 0.001451
[epoch: 640/1000, batch:   572/ 1052, ite: 168200] train loss: 0.004073, tar: 0.000071 
l0: 0.000147, l1: 0.000148, l2: 0.000226, l3: 0.000268, l4: 0.000848, l5: 0.001369, l6: 0.003223
[epoch: 640/1000, batch:   652/ 1052, ite: 168220] train loss: 0.004073, tar: 0.000071 
l0: 0.000178, l1: 0.000182, l2: 0.000171, l3: 0.000209, l4: 0.000294, l5: 0.000888, l6: 0.001560
[epoch: 640/1000, batch:   732/ 1052, ite: 168240] train loss: 0.004072, tar: 0.000071 
l0: 0.000056, l1: 0.000059, l2: 0.000074, l3: 0.000177, l4: 0.000497, l5: 0.001562, l6: 0.001759
[epoch: 640/1000, batch:   812/ 1052, ite: 168260] train loss: 0.004073, tar: 0.000071 
l0: 0.000039, l1: 0.000037, l2: 0.000092, l3: 0.000209, l4: 0.000486, l5: 0.000640, l6: 0.001548
[epoch: 640/1000, batch:   892/ 1052, ite: 168280] train loss: 0.004073, tar: 0.000071 
l0: 0.000030, l1: 0.000028, l2: 0.000054, l3: 0.000118, l4: 0.000319, l5: 0.001111, l6: 0.001758
[epoch: 640/1000, batch:   972/ 1052, ite: 168300] train loss: 0.004072, tar: 0.000071 
l0: 0.000027, l1: 0.000028, l2: 0.000053, l3: 0.000143, l4: 0.000574, l5: 0.001245, l6: 0.002322
[epoch: 640/1000, batch:  1052/ 1052, ite: 168320] train loss: 0.004073, tar: 0.000071 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_168320.pkl
[Epoch 640/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000294, l1: 0.000301, l2: 0.000301, l3: 0.000432, l4: 0.000702, l5: 0.001466, l6: 0.002780
[epoch: 641/1000, batch:    80/ 1052, ite: 168340] train loss: 0.004078, tar: 0.000073 
l0: 0.000051, l1: 0.000051, l2: 0.000065, l3: 0.000133, l4: 0.000440, l5: 0.001011, l6: 0.001378
[epoch: 641/1000, batch:   160/ 1052, ite: 168360] train loss: 0.004191, tar: 0.000073 
l0: 0.000036, l1: 0.000034, l2: 0.000065, l3: 0.000136, l4: 0.000411, l5: 0.000535, l6: 0.001637
[epoch: 641/1000, batch:   240/ 1052, ite: 168380] train loss: 0.004138, tar: 0.000067 
l0: 0.000047, l1: 0.000049, l2: 0.000059, l3: 0.000130, l4: 0.000384, l5: 0.001476, l6: 0.001003
[epoch: 641/1000, batch:   320/ 1052, ite: 168400] train loss: 0.004177, tar: 0.000069 
l0: 0.000071, l1: 0.000070, l2: 0.000147, l3: 0.000274, l4: 0.000630, l5: 0.001362, l6: 0.003781
[epoch: 641/1000, batch:   400/ 1052, ite: 168420] train loss: 0.004283, tar: 0.000069 
l0: 0.000014, l1: 0.000014, l2: 0.000046, l3: 0.000135, l4: 0.000305, l5: 0.000731, l6: 0.001956
[epoch: 641/1000, batch:   480/ 1052, ite: 168440] train loss: 0.004210, tar: 0.000068 
l0: 0.000088, l1: 0.000087, l2: 0.000111, l3: 0.000191, l4: 0.000499, l5: 0.001072, l6: 0.001742
[epoch: 641/1000, batch:   560/ 1052, ite: 168460] train loss: 0.004192, tar: 0.000068 
l0: 0.000094, l1: 0.000096, l2: 0.000129, l3: 0.000253, l4: 0.000480, l5: 0.001017, l6: 0.002047
[epoch: 641/1000, batch:   640/ 1052, ite: 168480] train loss: 0.004189, tar: 0.000069 
l0: 0.000041, l1: 0.000040, l2: 0.000065, l3: 0.000141, l4: 0.000224, l5: 0.000741, l6: 0.001566
[epoch: 641/1000, batch:   720/ 1052, ite: 168500] train loss: 0.004102, tar: 0.000068 
l0: 0.000154, l1: 0.000154, l2: 0.000168, l3: 0.000199, l4: 0.000276, l5: 0.000995, l6: 0.000899
[epoch: 641/1000, batch:   800/ 1052, ite: 168520] train loss: 0.004068, tar: 0.000067 
l0: 0.000040, l1: 0.000039, l2: 0.000081, l3: 0.000166, l4: 0.000545, l5: 0.001027, l6: 0.001828
[epoch: 641/1000, batch:   880/ 1052, ite: 168540] train loss: 0.004070, tar: 0.000067 
l0: 0.000003, l1: 0.000003, l2: 0.000003, l3: 0.000046, l4: 0.000330, l5: 0.000395, l6: 0.000849
[epoch: 641/1000, batch:   960/ 1052, ite: 168560] train loss: 0.004041, tar: 0.000065 
l0: 0.000044, l1: 0.000046, l2: 0.000067, l3: 0.000082, l4: 0.000183, l5: 0.000573, l6: 0.000782
[epoch: 641/1000, batch:  1040/ 1052, ite: 168580] train loss: 0.004024, tar: 0.000065 
[Epoch 641/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000031, l1: 0.000030, l2: 0.000057, l3: 0.000122, l4: 0.000341, l5: 0.000744, l6: 0.000828
[epoch: 642/1000, batch:    68/ 1052, ite: 168600] train loss: 0.004006, tar: 0.000064 
l0: 0.000122, l1: 0.000124, l2: 0.000204, l3: 0.000237, l4: 0.000511, l5: 0.001455, l6: 0.002550
[epoch: 642/1000, batch:   148/ 1052, ite: 168620] train loss: 0.004062, tar: 0.000065 
l0: 0.000015, l1: 0.000016, l2: 0.000063, l3: 0.000160, l4: 0.000359, l5: 0.000959, l6: 0.001064
[epoch: 642/1000, batch:   228/ 1052, ite: 168640] train loss: 0.004032, tar: 0.000063 
l0: 0.000069, l1: 0.000071, l2: 0.000111, l3: 0.000195, l4: 0.000509, l5: 0.000923, l6: 0.001304
[epoch: 642/1000, batch:   308/ 1052, ite: 168660] train loss: 0.004036, tar: 0.000063 
l0: 0.000115, l1: 0.000118, l2: 0.000182, l3: 0.000347, l4: 0.000613, l5: 0.001165, l6: 0.002503
[epoch: 642/1000, batch:   388/ 1052, ite: 168680] train loss: 0.004004, tar: 0.000063 
l0: 0.000122, l1: 0.000119, l2: 0.000173, l3: 0.000252, l4: 0.000666, l5: 0.001203, l6: 0.001899
[epoch: 642/1000, batch:   468/ 1052, ite: 168700] train loss: 0.004028, tar: 0.000063 
l0: 0.000002, l1: 0.000002, l2: 0.000030, l3: 0.000115, l4: 0.000325, l5: 0.000749, l6: 0.001264
[epoch: 642/1000, batch:   548/ 1052, ite: 168720] train loss: 0.004029, tar: 0.000063 
l0: 0.000118, l1: 0.000120, l2: 0.000152, l3: 0.000253, l4: 0.000633, l5: 0.001381, l6: 0.002385
[epoch: 642/1000, batch:   628/ 1052, ite: 168740] train loss: 0.004044, tar: 0.000063 
l0: 0.000003, l1: 0.000002, l2: 0.000021, l3: 0.000109, l4: 0.000458, l5: 0.000956, l6: 0.002147
[epoch: 642/1000, batch:   708/ 1052, ite: 168760] train loss: 0.004040, tar: 0.000063 
l0: 0.000019, l1: 0.000019, l2: 0.000087, l3: 0.000205, l4: 0.000526, l5: 0.001625, l6: 0.003086
[epoch: 642/1000, batch:   788/ 1052, ite: 168780] train loss: 0.004046, tar: 0.000063 
l0: 0.000021, l1: 0.000023, l2: 0.000035, l3: 0.000095, l4: 0.000277, l5: 0.000599, l6: 0.001288
[epoch: 642/1000, batch:   868/ 1052, ite: 168800] train loss: 0.004030, tar: 0.000062 
l0: 0.000049, l1: 0.000050, l2: 0.000085, l3: 0.000159, l4: 0.000350, l5: 0.000792, l6: 0.000896
[epoch: 642/1000, batch:   948/ 1052, ite: 168820] train loss: 0.004017, tar: 0.000062 
l0: 0.000046, l1: 0.000046, l2: 0.000089, l3: 0.000233, l4: 0.000528, l5: 0.000832, l6: 0.001664
[epoch: 642/1000, batch:  1028/ 1052, ite: 168840] train loss: 0.004012, tar: 0.000062 
[Epoch 642/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000031, l1: 0.000032, l2: 0.000048, l3: 0.000123, l4: 0.000403, l5: 0.001323, l6: 0.002130
[epoch: 643/1000, batch:    56/ 1052, ite: 168860] train loss: 0.004030, tar: 0.000063 
l0: 0.000088, l1: 0.000086, l2: 0.000150, l3: 0.000331, l4: 0.000585, l5: 0.001322, l6: 0.002861
[epoch: 643/1000, batch:   136/ 1052, ite: 168880] train loss: 0.004029, tar: 0.000063 
l0: 0.000011, l1: 0.000009, l2: 0.000020, l3: 0.000067, l4: 0.000200, l5: 0.000765, l6: 0.001134
[epoch: 643/1000, batch:   216/ 1052, ite: 168900] train loss: 0.004015, tar: 0.000063 
l0: 0.000055, l1: 0.000056, l2: 0.000083, l3: 0.000200, l4: 0.000980, l5: 0.001045, l6: 0.001542
[epoch: 643/1000, batch:   296/ 1052, ite: 168920] train loss: 0.004001, tar: 0.000062 
l0: 0.000012, l1: 0.000013, l2: 0.000023, l3: 0.000068, l4: 0.000414, l5: 0.000703, l6: 0.001208
[epoch: 643/1000, batch:   376/ 1052, ite: 168940] train loss: 0.004006, tar: 0.000063 
l0: 0.000021, l1: 0.000020, l2: 0.000074, l3: 0.000207, l4: 0.000444, l5: 0.001224, l6: 0.002700
[epoch: 643/1000, batch:   456/ 1052, ite: 168960] train loss: 0.004002, tar: 0.000062 
l0: 0.000108, l1: 0.000108, l2: 0.000196, l3: 0.000339, l4: 0.000739, l5: 0.001063, l6: 0.001819
[epoch: 643/1000, batch:   536/ 1052, ite: 168980] train loss: 0.004005, tar: 0.000062 
l0: 0.000077, l1: 0.000076, l2: 0.000096, l3: 0.000131, l4: 0.000478, l5: 0.000796, l6: 0.001791
[epoch: 643/1000, batch:   616/ 1052, ite: 169000] train loss: 0.004017, tar: 0.000062 
l0: 0.000045, l1: 0.000044, l2: 0.000105, l3: 0.000183, l4: 0.000410, l5: 0.000935, l6: 0.001308
[epoch: 643/1000, batch:   696/ 1052, ite: 169020] train loss: 0.004008, tar: 0.000062 
l0: 0.000058, l1: 0.000056, l2: 0.000120, l3: 0.000169, l4: 0.000319, l5: 0.000983, l6: 0.001718
[epoch: 643/1000, batch:   776/ 1052, ite: 169040] train loss: 0.004001, tar: 0.000062 
l0: 0.000165, l1: 0.000165, l2: 0.000251, l3: 0.000283, l4: 0.000584, l5: 0.001109, l6: 0.002392
[epoch: 643/1000, batch:   856/ 1052, ite: 169060] train loss: 0.004012, tar: 0.000062 
l0: 0.000017, l1: 0.000016, l2: 0.000037, l3: 0.000133, l4: 0.000286, l5: 0.000539, l6: 0.001081
[epoch: 643/1000, batch:   936/ 1052, ite: 169080] train loss: 0.004004, tar: 0.000062 
l0: 0.000014, l1: 0.000014, l2: 0.000040, l3: 0.000119, l4: 0.000259, l5: 0.000508, l6: 0.001291
[epoch: 643/1000, batch:  1016/ 1052, ite: 169100] train loss: 0.004010, tar: 0.000062 
[Epoch 643/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000015, l1: 0.000015, l2: 0.000034, l3: 0.000118, l4: 0.000346, l5: 0.000807, l6: 0.001852
[epoch: 644/1000, batch:    44/ 1052, ite: 169120] train loss: 0.004004, tar: 0.000062 
l0: 0.000233, l1: 0.000236, l2: 0.000269, l3: 0.000345, l4: 0.000632, l5: 0.001309, l6: 0.002958
[epoch: 644/1000, batch:   124/ 1052, ite: 169140] train loss: 0.004013, tar: 0.000063 
l0: 0.000024, l1: 0.000025, l2: 0.000060, l3: 0.000147, l4: 0.000316, l5: 0.000794, l6: 0.002442
[epoch: 644/1000, batch:   204/ 1052, ite: 169160] train loss: 0.004017, tar: 0.000063 
l0: 0.000008, l1: 0.000007, l2: 0.000028, l3: 0.000092, l4: 0.000131, l5: 0.000442, l6: 0.001366
[epoch: 644/1000, batch:   284/ 1052, ite: 169180] train loss: 0.004024, tar: 0.000063 
l0: 0.000029, l1: 0.000029, l2: 0.000036, l3: 0.000123, l4: 0.000279, l5: 0.001029, l6: 0.001579
[epoch: 644/1000, batch:   364/ 1052, ite: 169200] train loss: 0.004014, tar: 0.000063 
l0: 0.000095, l1: 0.000098, l2: 0.000111, l3: 0.000173, l4: 0.000452, l5: 0.001027, l6: 0.001391
[epoch: 644/1000, batch:   444/ 1052, ite: 169220] train loss: 0.004006, tar: 0.000062 
l0: 0.000015, l1: 0.000015, l2: 0.000032, l3: 0.000082, l4: 0.000367, l5: 0.000955, l6: 0.001647
[epoch: 644/1000, batch:   524/ 1052, ite: 169240] train loss: 0.004027, tar: 0.000063 
l0: 0.000014, l1: 0.000013, l2: 0.000054, l3: 0.000205, l4: 0.000287, l5: 0.000975, l6: 0.001352
[epoch: 644/1000, batch:   604/ 1052, ite: 169260] train loss: 0.004026, tar: 0.000063 
l0: 0.000033, l1: 0.000033, l2: 0.000046, l3: 0.000122, l4: 0.000456, l5: 0.000782, l6: 0.001497
[epoch: 644/1000, batch:   684/ 1052, ite: 169280] train loss: 0.004021, tar: 0.000063 
l0: 0.000038, l1: 0.000037, l2: 0.000128, l3: 0.000289, l4: 0.000576, l5: 0.000921, l6: 0.001590
[epoch: 644/1000, batch:   764/ 1052, ite: 169300] train loss: 0.004010, tar: 0.000063 
l0: 0.000026, l1: 0.000024, l2: 0.000069, l3: 0.000200, l4: 0.000448, l5: 0.001008, l6: 0.002367
[epoch: 644/1000, batch:   844/ 1052, ite: 169320] train loss: 0.004011, tar: 0.000063 
l0: 0.000030, l1: 0.000029, l2: 0.000069, l3: 0.000109, l4: 0.000275, l5: 0.000592, l6: 0.001408
[epoch: 644/1000, batch:   924/ 1052, ite: 169340] train loss: 0.004015, tar: 0.000063 
l0: 0.000086, l1: 0.000090, l2: 0.000138, l3: 0.000260, l4: 0.000518, l5: 0.001380, l6: 0.002173
[epoch: 644/1000, batch:  1004/ 1052, ite: 169360] train loss: 0.004018, tar: 0.000063 
[Epoch 644/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000053, l1: 0.000052, l2: 0.000098, l3: 0.000120, l4: 0.000306, l5: 0.000643, l6: 0.001399
[epoch: 645/1000, batch:    32/ 1052, ite: 169380] train loss: 0.004015, tar: 0.000063 
l0: 0.000046, l1: 0.000046, l2: 0.000071, l3: 0.000210, l4: 0.000621, l5: 0.001102, l6: 0.001537
[epoch: 645/1000, batch:   112/ 1052, ite: 169400] train loss: 0.004017, tar: 0.000063 
l0: 0.000009, l1: 0.000009, l2: 0.000048, l3: 0.000295, l4: 0.000643, l5: 0.001529, l6: 0.002621
[epoch: 645/1000, batch:   192/ 1052, ite: 169420] train loss: 0.004020, tar: 0.000063 
l0: 0.000063, l1: 0.000063, l2: 0.000080, l3: 0.000104, l4: 0.000185, l5: 0.000673, l6: 0.001464
[epoch: 645/1000, batch:   272/ 1052, ite: 169440] train loss: 0.004018, tar: 0.000063 
l0: 0.000001, l1: 0.000001, l2: 0.000015, l3: 0.000050, l4: 0.000138, l5: 0.000788, l6: 0.001194
[epoch: 645/1000, batch:   352/ 1052, ite: 169460] train loss: 0.004015, tar: 0.000062 
l0: 0.000064, l1: 0.000065, l2: 0.000139, l3: 0.000247, l4: 0.000406, l5: 0.001173, l6: 0.002054
[epoch: 645/1000, batch:   432/ 1052, ite: 169480] train loss: 0.004005, tar: 0.000062 
l0: 0.000056, l1: 0.000058, l2: 0.000108, l3: 0.000369, l4: 0.000914, l5: 0.001854, l6: 0.002354
[epoch: 645/1000, batch:   512/ 1052, ite: 169500] train loss: 0.004010, tar: 0.000063 
l0: 0.000059, l1: 0.000058, l2: 0.000085, l3: 0.000182, l4: 0.000397, l5: 0.000848, l6: 0.003047
[epoch: 645/1000, batch:   592/ 1052, ite: 169520] train loss: 0.004007, tar: 0.000062 
l0: 0.000033, l1: 0.000032, l2: 0.000073, l3: 0.000163, l4: 0.000508, l5: 0.001447, l6: 0.001621
[epoch: 645/1000, batch:   672/ 1052, ite: 169540] train loss: 0.004013, tar: 0.000063 
l0: 0.000065, l1: 0.000065, l2: 0.000115, l3: 0.000329, l4: 0.000611, l5: 0.001527, l6: 0.002697
[epoch: 645/1000, batch:   752/ 1052, ite: 169560] train loss: 0.004016, tar: 0.000063 
l0: 0.000039, l1: 0.000038, l2: 0.000075, l3: 0.000217, l4: 0.000645, l5: 0.000942, l6: 0.001830
[epoch: 645/1000, batch:   832/ 1052, ite: 169580] train loss: 0.004008, tar: 0.000063 
l0: 0.000009, l1: 0.000008, l2: 0.000040, l3: 0.000108, l4: 0.000281, l5: 0.000660, l6: 0.001412
[epoch: 645/1000, batch:   912/ 1052, ite: 169600] train loss: 0.004013, tar: 0.000063 
l0: 0.000057, l1: 0.000055, l2: 0.000110, l3: 0.000195, l4: 0.000407, l5: 0.000784, l6: 0.001361
[epoch: 645/1000, batch:   992/ 1052, ite: 169620] train loss: 0.004019, tar: 0.000063 
[Epoch 645/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000119, l1: 0.000118, l2: 0.000249, l3: 0.000321, l4: 0.000578, l5: 0.001289, l6: 0.003325
[epoch: 646/1000, batch:    20/ 1052, ite: 169640] train loss: 0.004022, tar: 0.000064 
l0: 0.000029, l1: 0.000028, l2: 0.000048, l3: 0.000109, l4: 0.000183, l5: 0.000732, l6: 0.001063
[epoch: 646/1000, batch:   100/ 1052, ite: 169660] train loss: 0.004024, tar: 0.000064 
l0: 0.000065, l1: 0.000064, l2: 0.000122, l3: 0.000203, l4: 0.000451, l5: 0.001182, l6: 0.001539
[epoch: 646/1000, batch:   180/ 1052, ite: 169680] train loss: 0.004022, tar: 0.000064 
l0: 0.000153, l1: 0.000159, l2: 0.000165, l3: 0.000281, l4: 0.000465, l5: 0.001593, l6: 0.002791
[epoch: 646/1000, batch:   260/ 1052, ite: 169700] train loss: 0.004018, tar: 0.000064 
l0: 0.000030, l1: 0.000030, l2: 0.000070, l3: 0.000129, l4: 0.000387, l5: 0.001171, l6: 0.002498
[epoch: 646/1000, batch:   340/ 1052, ite: 169720] train loss: 0.004027, tar: 0.000064 
l0: 0.000061, l1: 0.000064, l2: 0.000102, l3: 0.000220, l4: 0.000371, l5: 0.001581, l6: 0.002909
[epoch: 646/1000, batch:   420/ 1052, ite: 169740] train loss: 0.004027, tar: 0.000064 
l0: 0.000089, l1: 0.000089, l2: 0.000135, l3: 0.000241, l4: 0.000455, l5: 0.000799, l6: 0.001620
[epoch: 646/1000, batch:   500/ 1052, ite: 169760] train loss: 0.004020, tar: 0.000064 
l0: 0.000123, l1: 0.000122, l2: 0.000176, l3: 0.000344, l4: 0.000913, l5: 0.002386, l6: 0.003677
[epoch: 646/1000, batch:   580/ 1052, ite: 169780] train loss: 0.004025, tar: 0.000064 
l0: 0.000217, l1: 0.000223, l2: 0.000289, l3: 0.000469, l4: 0.000891, l5: 0.002137, l6: 0.003531
[epoch: 646/1000, batch:   660/ 1052, ite: 169800] train loss: 0.004028, tar: 0.000064 
l0: 0.000011, l1: 0.000011, l2: 0.000031, l3: 0.000090, l4: 0.000247, l5: 0.000695, l6: 0.001341
[epoch: 646/1000, batch:   740/ 1052, ite: 169820] train loss: 0.004023, tar: 0.000064 
l0: 0.000030, l1: 0.000031, l2: 0.000047, l3: 0.000179, l4: 0.000651, l5: 0.001068, l6: 0.002210
[epoch: 646/1000, batch:   820/ 1052, ite: 169840] train loss: 0.004015, tar: 0.000064 
l0: 0.000017, l1: 0.000019, l2: 0.000025, l3: 0.000095, l4: 0.000236, l5: 0.000605, l6: 0.000885
[epoch: 646/1000, batch:   900/ 1052, ite: 169860] train loss: 0.004019, tar: 0.000064 
l0: 0.000018, l1: 0.000018, l2: 0.000053, l3: 0.000144, l4: 0.000603, l5: 0.001706, l6: 0.002326
[epoch: 646/1000, batch:   980/ 1052, ite: 169880] train loss: 0.004028, tar: 0.000064 
[Epoch 646/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000009, l1: 0.000010, l2: 0.000020, l3: 0.000100, l4: 0.000267, l5: 0.000627, l6: 0.001052
[epoch: 647/1000, batch:     8/ 1052, ite: 169900] train loss: 0.004023, tar: 0.000064 
l0: 0.000054, l1: 0.000055, l2: 0.000136, l3: 0.000241, l4: 0.000337, l5: 0.000642, l6: 0.001447
[epoch: 647/1000, batch:    88/ 1052, ite: 169920] train loss: 0.004022, tar: 0.000064 
l0: 0.000036, l1: 0.000035, l2: 0.000062, l3: 0.000165, l4: 0.000359, l5: 0.000562, l6: 0.001332
[epoch: 647/1000, batch:   168/ 1052, ite: 169940] train loss: 0.004018, tar: 0.000064 
l0: 0.000100, l1: 0.000103, l2: 0.000139, l3: 0.000285, l4: 0.000474, l5: 0.001026, l6: 0.002001
[epoch: 647/1000, batch:   248/ 1052, ite: 169960] train loss: 0.004020, tar: 0.000064 
l0: 0.000019, l1: 0.000019, l2: 0.000043, l3: 0.000094, l4: 0.000205, l5: 0.000446, l6: 0.000640
[epoch: 647/1000, batch:   328/ 1052, ite: 169980] train loss: 0.004019, tar: 0.000064 
l0: 0.000002, l1: 0.000002, l2: 0.000014, l3: 0.000099, l4: 0.000351, l5: 0.000668, l6: 0.001243
[epoch: 647/1000, batch:   408/ 1052, ite: 170000] train loss: 0.004018, tar: 0.000064 
l0: 0.000100, l1: 0.000102, l2: 0.000120, l3: 0.000186, l4: 0.000448, l5: 0.001024, l6: 0.002046
[epoch: 647/1000, batch:   488/ 1052, ite: 170020] train loss: 0.004027, tar: 0.000064 
l0: 0.000008, l1: 0.000008, l2: 0.000011, l3: 0.000076, l4: 0.000158, l5: 0.000700, l6: 0.001128
[epoch: 647/1000, batch:   568/ 1052, ite: 170040] train loss: 0.004030, tar: 0.000064 
l0: 0.000094, l1: 0.000093, l2: 0.000118, l3: 0.000218, l4: 0.000673, l5: 0.001089, l6: 0.002297
[epoch: 647/1000, batch:   648/ 1052, ite: 170060] train loss: 0.004028, tar: 0.000064 
l0: 0.000029, l1: 0.000029, l2: 0.000070, l3: 0.000163, l4: 0.000377, l5: 0.000613, l6: 0.001339
[epoch: 647/1000, batch:   728/ 1052, ite: 170080] train loss: 0.004023, tar: 0.000064 
l0: 0.000075, l1: 0.000080, l2: 0.000108, l3: 0.000245, l4: 0.000753, l5: 0.001763, l6: 0.002546
[epoch: 647/1000, batch:   808/ 1052, ite: 170100] train loss: 0.004024, tar: 0.000064 
l0: 0.000036, l1: 0.000036, l2: 0.000081, l3: 0.000243, l4: 0.000264, l5: 0.000918, l6: 0.001954
[epoch: 647/1000, batch:   888/ 1052, ite: 170120] train loss: 0.004022, tar: 0.000064 
l0: 0.000013, l1: 0.000013, l2: 0.000049, l3: 0.000146, l4: 0.000300, l5: 0.000903, l6: 0.001472
[epoch: 647/1000, batch:   968/ 1052, ite: 170140] train loss: 0.004018, tar: 0.000064 
l0: 0.000043, l1: 0.000043, l2: 0.000113, l3: 0.000358, l4: 0.000733, l5: 0.001342, l6: 0.002096
[epoch: 647/1000, batch:  1048/ 1052, ite: 170160] train loss: 0.004019, tar: 0.000064 
[Epoch 647/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000018, l1: 0.000017, l2: 0.000058, l3: 0.000224, l4: 0.000546, l5: 0.001482, l6: 0.001786
[epoch: 648/1000, batch:    76/ 1052, ite: 170180] train loss: 0.004018, tar: 0.000064 
l0: 0.000031, l1: 0.000033, l2: 0.000075, l3: 0.000201, l4: 0.000628, l5: 0.001259, l6: 0.002743
[epoch: 648/1000, batch:   156/ 1052, ite: 170200] train loss: 0.004013, tar: 0.000064 
l0: 0.000015, l1: 0.000016, l2: 0.000034, l3: 0.000111, l4: 0.000317, l5: 0.000906, l6: 0.001520
[epoch: 648/1000, batch:   236/ 1052, ite: 170220] train loss: 0.004013, tar: 0.000064 
l0: 0.000136, l1: 0.000138, l2: 0.000186, l3: 0.000271, l4: 0.000554, l5: 0.001345, l6: 0.002032
[epoch: 648/1000, batch:   316/ 1052, ite: 170240] train loss: 0.004014, tar: 0.000064 
l0: 0.000031, l1: 0.000029, l2: 0.000053, l3: 0.000112, l4: 0.000236, l5: 0.000442, l6: 0.001225
[epoch: 648/1000, batch:   396/ 1052, ite: 170260] train loss: 0.004012, tar: 0.000064 
l0: 0.000008, l1: 0.000008, l2: 0.000019, l3: 0.000087, l4: 0.000263, l5: 0.000898, l6: 0.001756
[epoch: 648/1000, batch:   476/ 1052, ite: 170280] train loss: 0.004010, tar: 0.000064 
l0: 0.000096, l1: 0.000096, l2: 0.000154, l3: 0.000318, l4: 0.000581, l5: 0.001388, l6: 0.002100
[epoch: 648/1000, batch:   556/ 1052, ite: 170300] train loss: 0.004013, tar: 0.000063 
l0: 0.000091, l1: 0.000091, l2: 0.000117, l3: 0.000206, l4: 0.000515, l5: 0.000702, l6: 0.001873
[epoch: 648/1000, batch:   636/ 1052, ite: 170320] train loss: 0.004011, tar: 0.000063 
l0: 0.000142, l1: 0.000145, l2: 0.000189, l3: 0.000307, l4: 0.000828, l5: 0.001549, l6: 0.002946
[epoch: 648/1000, batch:   716/ 1052, ite: 170340] train loss: 0.004011, tar: 0.000064 
l0: 0.000054, l1: 0.000054, l2: 0.000068, l3: 0.000140, l4: 0.000374, l5: 0.000729, l6: 0.001057
[epoch: 648/1000, batch:   796/ 1052, ite: 170360] train loss: 0.004017, tar: 0.000064 
l0: 0.000094, l1: 0.000094, l2: 0.000151, l3: 0.000208, l4: 0.000494, l5: 0.001114, l6: 0.001597
[epoch: 648/1000, batch:   876/ 1052, ite: 170380] train loss: 0.004019, tar: 0.000064 
l0: 0.000052, l1: 0.000053, l2: 0.000061, l3: 0.000134, l4: 0.000434, l5: 0.000698, l6: 0.001108
[epoch: 648/1000, batch:   956/ 1052, ite: 170400] train loss: 0.004024, tar: 0.000064 
l0: 0.000037, l1: 0.000037, l2: 0.000094, l3: 0.000227, l4: 0.000824, l5: 0.001563, l6: 0.002235
[epoch: 648/1000, batch:  1036/ 1052, ite: 170420] train loss: 0.004019, tar: 0.000064 
[Epoch 648/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000227, l1: 0.000233, l2: 0.000336, l3: 0.000569, l4: 0.001125, l5: 0.002021, l6: 0.004340
[epoch: 649/1000, batch:    64/ 1052, ite: 170440] train loss: 0.004019, tar: 0.000064 
l0: 0.000057, l1: 0.000058, l2: 0.000074, l3: 0.000173, l4: 0.000308, l5: 0.000801, l6: 0.001138
[epoch: 649/1000, batch:   144/ 1052, ite: 170460] train loss: 0.004019, tar: 0.000064 
l0: 0.000020, l1: 0.000021, l2: 0.000046, l3: 0.000128, l4: 0.000335, l5: 0.001025, l6: 0.001861
[epoch: 649/1000, batch:   224/ 1052, ite: 170480] train loss: 0.004017, tar: 0.000064 
l0: 0.000023, l1: 0.000022, l2: 0.000062, l3: 0.000155, l4: 0.000573, l5: 0.000583, l6: 0.001593
[epoch: 649/1000, batch:   304/ 1052, ite: 170500] train loss: 0.004017, tar: 0.000064 
l0: 0.000030, l1: 0.000030, l2: 0.000062, l3: 0.000129, l4: 0.000339, l5: 0.000897, l6: 0.001516
[epoch: 649/1000, batch:   384/ 1052, ite: 170520] train loss: 0.004013, tar: 0.000064 
l0: 0.000062, l1: 0.000064, l2: 0.000130, l3: 0.000267, l4: 0.000924, l5: 0.001626, l6: 0.003185
[epoch: 649/1000, batch:   464/ 1052, ite: 170540] train loss: 0.004013, tar: 0.000064 
l0: 0.000051, l1: 0.000054, l2: 0.000095, l3: 0.000166, l4: 0.000437, l5: 0.000831, l6: 0.001303
[epoch: 649/1000, batch:   544/ 1052, ite: 170560] train loss: 0.004014, tar: 0.000064 
l0: 0.000029, l1: 0.000031, l2: 0.000044, l3: 0.000119, l4: 0.000239, l5: 0.000773, l6: 0.000904
[epoch: 649/1000, batch:   624/ 1052, ite: 170580] train loss: 0.004015, tar: 0.000064 
l0: 0.000167, l1: 0.000166, l2: 0.000234, l3: 0.000334, l4: 0.000604, l5: 0.001408, l6: 0.001624
[epoch: 649/1000, batch:   704/ 1052, ite: 170600] train loss: 0.004020, tar: 0.000064 
l0: 0.000101, l1: 0.000103, l2: 0.000133, l3: 0.000221, l4: 0.000604, l5: 0.000952, l6: 0.001904
[epoch: 649/1000, batch:   784/ 1052, ite: 170620] train loss: 0.004021, tar: 0.000064 
l0: 0.000040, l1: 0.000039, l2: 0.000100, l3: 0.000233, l4: 0.000490, l5: 0.001497, l6: 0.003584
[epoch: 649/1000, batch:   864/ 1052, ite: 170640] train loss: 0.004023, tar: 0.000064 
l0: 0.000049, l1: 0.000049, l2: 0.000067, l3: 0.000184, l4: 0.000386, l5: 0.000781, l6: 0.001140
[epoch: 649/1000, batch:   944/ 1052, ite: 170660] train loss: 0.004023, tar: 0.000064 
l0: 0.000023, l1: 0.000023, l2: 0.000039, l3: 0.000076, l4: 0.000147, l5: 0.000712, l6: 0.001075
[epoch: 649/1000, batch:  1024/ 1052, ite: 170680] train loss: 0.004022, tar: 0.000064 
[Epoch 649/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000036, l1: 0.000035, l2: 0.000059, l3: 0.000136, l4: 0.000321, l5: 0.001067, l6: 0.001712
[epoch: 650/1000, batch:    52/ 1052, ite: 170700] train loss: 0.004020, tar: 0.000064 
l0: 0.000104, l1: 0.000102, l2: 0.000175, l3: 0.000342, l4: 0.000724, l5: 0.001272, l6: 0.002114
[epoch: 650/1000, batch:   132/ 1052, ite: 170720] train loss: 0.004021, tar: 0.000064 
l0: 0.000125, l1: 0.000125, l2: 0.000164, l3: 0.000253, l4: 0.000504, l5: 0.001413, l6: 0.001893
[epoch: 650/1000, batch:   212/ 1052, ite: 170740] train loss: 0.004024, tar: 0.000064 
l0: 0.000046, l1: 0.000048, l2: 0.000058, l3: 0.000110, l4: 0.000468, l5: 0.000770, l6: 0.000995
[epoch: 650/1000, batch:   292/ 1052, ite: 170760] train loss: 0.004022, tar: 0.000064 
l0: 0.000001, l1: 0.000001, l2: 0.000007, l3: 0.000033, l4: 0.000194, l5: 0.000501, l6: 0.001046
[epoch: 650/1000, batch:   372/ 1052, ite: 170780] train loss: 0.004024, tar: 0.000064 
l0: 0.000159, l1: 0.000160, l2: 0.000170, l3: 0.000273, l4: 0.000852, l5: 0.001183, l6: 0.002912
[epoch: 650/1000, batch:   452/ 1052, ite: 170800] train loss: 0.004024, tar: 0.000064 
l0: 0.000025, l1: 0.000025, l2: 0.000086, l3: 0.000301, l4: 0.000927, l5: 0.001591, l6: 0.002398
[epoch: 650/1000, batch:   532/ 1052, ite: 170820] train loss: 0.004025, tar: 0.000063 
l0: 0.000007, l1: 0.000009, l2: 0.000037, l3: 0.000116, l4: 0.000273, l5: 0.000778, l6: 0.000946
[epoch: 650/1000, batch:   612/ 1052, ite: 170840] train loss: 0.004022, tar: 0.000063 
l0: 0.000078, l1: 0.000074, l2: 0.000105, l3: 0.000163, l4: 0.000367, l5: 0.000543, l6: 0.001046
[epoch: 650/1000, batch:   692/ 1052, ite: 170860] train loss: 0.004020, tar: 0.000063 
l0: 0.000038, l1: 0.000037, l2: 0.000074, l3: 0.000121, l4: 0.000259, l5: 0.000481, l6: 0.001082
[epoch: 650/1000, batch:   772/ 1052, ite: 170880] train loss: 0.004021, tar: 0.000063 
l0: 0.000041, l1: 0.000040, l2: 0.000107, l3: 0.000253, l4: 0.000545, l5: 0.000894, l6: 0.002164
[epoch: 650/1000, batch:   852/ 1052, ite: 170900] train loss: 0.004019, tar: 0.000063 
l0: 0.000014, l1: 0.000014, l2: 0.000021, l3: 0.000091, l4: 0.000197, l5: 0.000881, l6: 0.000777
[epoch: 650/1000, batch:   932/ 1052, ite: 170920] train loss: 0.004017, tar: 0.000063 
l0: 0.000009, l1: 0.000009, l2: 0.000027, l3: 0.000080, l4: 0.000365, l5: 0.001024, l6: 0.001713
[epoch: 650/1000, batch:  1012/ 1052, ite: 170940] train loss: 0.004019, tar: 0.000063 
[Epoch 650/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000013, l1: 0.000013, l2: 0.000036, l3: 0.000113, l4: 0.000367, l5: 0.001449, l6: 0.002147
[epoch: 651/1000, batch:    40/ 1052, ite: 170960] train loss: 0.004021, tar: 0.000064 
l0: 0.000026, l1: 0.000027, l2: 0.000053, l3: 0.000130, l4: 0.000622, l5: 0.001194, l6: 0.002115
[epoch: 651/1000, batch:   120/ 1052, ite: 170980] train loss: 0.004022, tar: 0.000063 
l0: 0.000122, l1: 0.000122, l2: 0.000193, l3: 0.000372, l4: 0.000670, l5: 0.001295, l6: 0.002043
[epoch: 651/1000, batch:   200/ 1052, ite: 171000] train loss: 0.004024, tar: 0.000063 
l0: 0.000038, l1: 0.000037, l2: 0.000053, l3: 0.000090, l4: 0.000204, l5: 0.000481, l6: 0.001488
[epoch: 651/1000, batch:   280/ 1052, ite: 171020] train loss: 0.004026, tar: 0.000063 
l0: 0.000025, l1: 0.000019, l2: 0.000027, l3: 0.000107, l4: 0.000155, l5: 0.000491, l6: 0.000566
[epoch: 651/1000, batch:   360/ 1052, ite: 171040] train loss: 0.004031, tar: 0.000064 
l0: 0.000142, l1: 0.000141, l2: 0.000205, l3: 0.000335, l4: 0.000593, l5: 0.001734, l6: 0.002166
[epoch: 651/1000, batch:   440/ 1052, ite: 171060] train loss: 0.004032, tar: 0.000064 
l0: 0.000216, l1: 0.000214, l2: 0.000244, l3: 0.000331, l4: 0.000540, l5: 0.001622, l6: 0.003330
[epoch: 651/1000, batch:   520/ 1052, ite: 171080] train loss: 0.004036, tar: 0.000064 
l0: 0.000107, l1: 0.000105, l2: 0.000156, l3: 0.000267, l4: 0.000510, l5: 0.001150, l6: 0.002624
[epoch: 651/1000, batch:   600/ 1052, ite: 171100] train loss: 0.004033, tar: 0.000064 
l0: 0.000054, l1: 0.000053, l2: 0.000061, l3: 0.000153, l4: 0.000212, l5: 0.000798, l6: 0.001100
[epoch: 651/1000, batch:   680/ 1052, ite: 171120] train loss: 0.004032, tar: 0.000064 
l0: 0.000066, l1: 0.000066, l2: 0.000080, l3: 0.000146, l4: 0.000303, l5: 0.000827, l6: 0.001391
[epoch: 651/1000, batch:   760/ 1052, ite: 171140] train loss: 0.004027, tar: 0.000064 
l0: 0.000035, l1: 0.000036, l2: 0.000062, l3: 0.000094, l4: 0.000474, l5: 0.000913, l6: 0.001612
[epoch: 651/1000, batch:   840/ 1052, ite: 171160] train loss: 0.004026, tar: 0.000064 
l0: 0.000089, l1: 0.000085, l2: 0.000111, l3: 0.000245, l4: 0.000417, l5: 0.000777, l6: 0.000957
[epoch: 651/1000, batch:   920/ 1052, ite: 171180] train loss: 0.004028, tar: 0.000064 
l0: 0.000032, l1: 0.000032, l2: 0.000065, l3: 0.000145, l4: 0.000392, l5: 0.000900, l6: 0.001870
[epoch: 651/1000, batch:  1000/ 1052, ite: 171200] train loss: 0.004026, tar: 0.000064 
[Epoch 651/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000048, l1: 0.000050, l2: 0.000062, l3: 0.000118, l4: 0.000284, l5: 0.000762, l6: 0.001126
[epoch: 652/1000, batch:    28/ 1052, ite: 171220] train loss: 0.004026, tar: 0.000064 
l0: 0.000031, l1: 0.000031, l2: 0.000071, l3: 0.000154, l4: 0.000343, l5: 0.001166, l6: 0.001806
[epoch: 652/1000, batch:   108/ 1052, ite: 171240] train loss: 0.004026, tar: 0.000064 
l0: 0.000094, l1: 0.000094, l2: 0.000139, l3: 0.000282, l4: 0.000442, l5: 0.001331, l6: 0.002558
[epoch: 652/1000, batch:   188/ 1052, ite: 171260] train loss: 0.004025, tar: 0.000064 
l0: 0.000099, l1: 0.000100, l2: 0.000167, l3: 0.000305, l4: 0.000677, l5: 0.001332, l6: 0.001561
[epoch: 652/1000, batch:   268/ 1052, ite: 171280] train loss: 0.004026, tar: 0.000064 
l0: 0.000046, l1: 0.000047, l2: 0.000064, l3: 0.000109, l4: 0.000335, l5: 0.001049, l6: 0.002190
[epoch: 652/1000, batch:   348/ 1052, ite: 171300] train loss: 0.004025, tar: 0.000064 
l0: 0.000097, l1: 0.000097, l2: 0.000147, l3: 0.000228, l4: 0.000594, l5: 0.001916, l6: 0.002872
[epoch: 652/1000, batch:   428/ 1052, ite: 171320] train loss: 0.004030, tar: 0.000064 
l0: 0.000218, l1: 0.000220, l2: 0.000275, l3: 0.000404, l4: 0.000920, l5: 0.001543, l6: 0.001822
[epoch: 652/1000, batch:   508/ 1052, ite: 171340] train loss: 0.004036, tar: 0.000064 
l0: 0.000087, l1: 0.000085, l2: 0.000149, l3: 0.000289, l4: 0.000878, l5: 0.001152, l6: 0.001854
[epoch: 652/1000, batch:   588/ 1052, ite: 171360] train loss: 0.004038, tar: 0.000065 
l0: 0.000065, l1: 0.000069, l2: 0.000098, l3: 0.000098, l4: 0.000148, l5: 0.000724, l6: 0.001214
[epoch: 652/1000, batch:   668/ 1052, ite: 171380] train loss: 0.004035, tar: 0.000065 
l0: 0.000046, l1: 0.000045, l2: 0.000059, l3: 0.000122, l4: 0.000439, l5: 0.000682, l6: 0.001308
[epoch: 652/1000, batch:   748/ 1052, ite: 171400] train loss: 0.004035, tar: 0.000064 
l0: 0.000006, l1: 0.000006, l2: 0.000027, l3: 0.000048, l4: 0.000110, l5: 0.000387, l6: 0.000752
[epoch: 652/1000, batch:   828/ 1052, ite: 171420] train loss: 0.004033, tar: 0.000064 
l0: 0.000061, l1: 0.000060, l2: 0.000073, l3: 0.000147, l4: 0.000338, l5: 0.000612, l6: 0.002085
[epoch: 652/1000, batch:   908/ 1052, ite: 171440] train loss: 0.004035, tar: 0.000065 
l0: 0.000035, l1: 0.000035, l2: 0.000099, l3: 0.000231, l4: 0.000728, l5: 0.001100, l6: 0.002319
[epoch: 652/1000, batch:   988/ 1052, ite: 171460] train loss: 0.004033, tar: 0.000065 
[Epoch 652/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000118, l1: 0.000121, l2: 0.000158, l3: 0.000252, l4: 0.000533, l5: 0.001228, l6: 0.001780
[epoch: 653/1000, batch:    16/ 1052, ite: 171480] train loss: 0.004032, tar: 0.000065 
l0: 0.000083, l1: 0.000077, l2: 0.000210, l3: 0.000404, l4: 0.000966, l5: 0.002616, l6: 0.003514
[epoch: 653/1000, batch:    96/ 1052, ite: 171500] train loss: 0.004030, tar: 0.000065 
l0: 0.000120, l1: 0.000119, l2: 0.000177, l3: 0.000264, l4: 0.000463, l5: 0.001633, l6: 0.003336
[epoch: 653/1000, batch:   176/ 1052, ite: 171520] train loss: 0.004029, tar: 0.000065 
l0: 0.000078, l1: 0.000080, l2: 0.000098, l3: 0.000122, l4: 0.000246, l5: 0.000849, l6: 0.001288
[epoch: 653/1000, batch:   256/ 1052, ite: 171540] train loss: 0.004029, tar: 0.000065 
l0: 0.000010, l1: 0.000011, l2: 0.000047, l3: 0.000111, l4: 0.000213, l5: 0.000616, l6: 0.001666
[epoch: 653/1000, batch:   336/ 1052, ite: 171560] train loss: 0.004029, tar: 0.000064 
l0: 0.000050, l1: 0.000052, l2: 0.000097, l3: 0.000241, l4: 0.000454, l5: 0.000962, l6: 0.001943
[epoch: 653/1000, batch:   416/ 1052, ite: 171580] train loss: 0.004029, tar: 0.000064 
l0: 0.000018, l1: 0.000019, l2: 0.000038, l3: 0.000181, l4: 0.000392, l5: 0.000807, l6: 0.002053
[epoch: 653/1000, batch:   496/ 1052, ite: 171600] train loss: 0.004029, tar: 0.000064 
l0: 0.000064, l1: 0.000066, l2: 0.000083, l3: 0.000186, l4: 0.000345, l5: 0.001002, l6: 0.002110
[epoch: 653/1000, batch:   576/ 1052, ite: 171620] train loss: 0.004029, tar: 0.000065 
l0: 0.000083, l1: 0.000078, l2: 0.000221, l3: 0.000546, l4: 0.000845, l5: 0.002289, l6: 0.004139
[epoch: 653/1000, batch:   656/ 1052, ite: 171640] train loss: 0.004032, tar: 0.000065 
l0: 0.000085, l1: 0.000087, l2: 0.000109, l3: 0.000197, l4: 0.000711, l5: 0.002112, l6: 0.002398
[epoch: 653/1000, batch:   736/ 1052, ite: 171660] train loss: 0.004030, tar: 0.000065 
l0: 0.000020, l1: 0.000021, l2: 0.000039, l3: 0.000086, l4: 0.000433, l5: 0.000749, l6: 0.001677
[epoch: 653/1000, batch:   816/ 1052, ite: 171680] train loss: 0.004027, tar: 0.000064 
l0: 0.000008, l1: 0.000007, l2: 0.000093, l3: 0.000240, l4: 0.000759, l5: 0.001304, l6: 0.001986
[epoch: 653/1000, batch:   896/ 1052, ite: 171700] train loss: 0.004029, tar: 0.000064 
l0: 0.000013, l1: 0.000013, l2: 0.000033, l3: 0.000174, l4: 0.000462, l5: 0.001414, l6: 0.001778
[epoch: 653/1000, batch:   976/ 1052, ite: 171720] train loss: 0.004030, tar: 0.000064 
[Epoch 653/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000075, l1: 0.000074, l2: 0.000116, l3: 0.000244, l4: 0.000600, l5: 0.000953, l6: 0.002026
[epoch: 654/1000, batch:     4/ 1052, ite: 171740] train loss: 0.004031, tar: 0.000064 
l0: 0.000057, l1: 0.000057, l2: 0.000089, l3: 0.000194, l4: 0.000315, l5: 0.000878, l6: 0.002500
[epoch: 654/1000, batch:    84/ 1052, ite: 171760] train loss: 0.004033, tar: 0.000065 
l0: 0.000141, l1: 0.000146, l2: 0.000147, l3: 0.000236, l4: 0.000567, l5: 0.001698, l6: 0.002711
[epoch: 654/1000, batch:   164/ 1052, ite: 171780] train loss: 0.004032, tar: 0.000065 
l0: 0.000016, l1: 0.000016, l2: 0.000078, l3: 0.000206, l4: 0.000384, l5: 0.000883, l6: 0.001915
[epoch: 654/1000, batch:   244/ 1052, ite: 171800] train loss: 0.004033, tar: 0.000065 
l0: 0.000039, l1: 0.000037, l2: 0.000081, l3: 0.000205, l4: 0.000305, l5: 0.001017, l6: 0.001290
[epoch: 654/1000, batch:   324/ 1052, ite: 171820] train loss: 0.004033, tar: 0.000065 
l0: 0.000009, l1: 0.000011, l2: 0.000021, l3: 0.000134, l4: 0.000337, l5: 0.000677, l6: 0.001239
[epoch: 654/1000, batch:   404/ 1052, ite: 171840] train loss: 0.004030, tar: 0.000065 
l0: 0.000058, l1: 0.000059, l2: 0.000118, l3: 0.000196, l4: 0.000423, l5: 0.000982, l6: 0.003706
[epoch: 654/1000, batch:   484/ 1052, ite: 171860] train loss: 0.004029, tar: 0.000064 
l0: 0.000043, l1: 0.000042, l2: 0.000135, l3: 0.000394, l4: 0.001123, l5: 0.002503, l6: 0.003796
[epoch: 654/1000, batch:   564/ 1052, ite: 171880] train loss: 0.004032, tar: 0.000064 
l0: 0.000134, l1: 0.000132, l2: 0.000178, l3: 0.000316, l4: 0.000539, l5: 0.001892, l6: 0.003352
[epoch: 654/1000, batch:   644/ 1052, ite: 171900] train loss: 0.004031, tar: 0.000064 
l0: 0.000068, l1: 0.000070, l2: 0.000088, l3: 0.000137, l4: 0.000432, l5: 0.000566, l6: 0.001430
[epoch: 654/1000, batch:   724/ 1052, ite: 171920] train loss: 0.004031, tar: 0.000064 
l0: 0.000041, l1: 0.000043, l2: 0.000084, l3: 0.000365, l4: 0.000726, l5: 0.001609, l6: 0.003534
[epoch: 654/1000, batch:   804/ 1052, ite: 171940] train loss: 0.004032, tar: 0.000064 
l0: 0.000069, l1: 0.000071, l2: 0.000113, l3: 0.000182, l4: 0.000386, l5: 0.001256, l6: 0.002579
[epoch: 654/1000, batch:   884/ 1052, ite: 171960] train loss: 0.004029, tar: 0.000064 
l0: 0.000009, l1: 0.000008, l2: 0.000035, l3: 0.000100, l4: 0.000544, l5: 0.000853, l6: 0.000899
[epoch: 654/1000, batch:   964/ 1052, ite: 171980] train loss: 0.004032, tar: 0.000064 
l0: 0.000156, l1: 0.000160, l2: 0.000206, l3: 0.000386, l4: 0.000793, l5: 0.001598, l6: 0.002219
[epoch: 654/1000, batch:  1044/ 1052, ite: 172000] train loss: 0.004031, tar: 0.000064 
[Epoch 654/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000151, l1: 0.000152, l2: 0.000205, l3: 0.000332, l4: 0.000508, l5: 0.001181, l6: 0.002284
[epoch: 655/1000, batch:    72/ 1052, ite: 172020] train loss: 0.004030, tar: 0.000064 
l0: 0.000048, l1: 0.000047, l2: 0.000085, l3: 0.000152, l4: 0.000542, l5: 0.000786, l6: 0.001304
[epoch: 655/1000, batch:   152/ 1052, ite: 172040] train loss: 0.004031, tar: 0.000064 
l0: 0.000062, l1: 0.000061, l2: 0.000078, l3: 0.000136, l4: 0.000476, l5: 0.001168, l6: 0.001382
[epoch: 655/1000, batch:   232/ 1052, ite: 172060] train loss: 0.004030, tar: 0.000064 
l0: 0.000126, l1: 0.000126, l2: 0.000147, l3: 0.000255, l4: 0.000379, l5: 0.000653, l6: 0.001214
[epoch: 655/1000, batch:   312/ 1052, ite: 172080] train loss: 0.004028, tar: 0.000064 
l0: 0.000014, l1: 0.000013, l2: 0.000040, l3: 0.000107, l4: 0.000534, l5: 0.000822, l6: 0.001703
[epoch: 655/1000, batch:   392/ 1052, ite: 172100] train loss: 0.004031, tar: 0.000064 
l0: 0.000035, l1: 0.000035, l2: 0.000063, l3: 0.000132, l4: 0.000373, l5: 0.001001, l6: 0.001425
[epoch: 655/1000, batch:   472/ 1052, ite: 172120] train loss: 0.004032, tar: 0.000064 
l0: 0.000019, l1: 0.000020, l2: 0.000034, l3: 0.000117, l4: 0.000502, l5: 0.000981, l6: 0.001510
[epoch: 655/1000, batch:   552/ 1052, ite: 172140] train loss: 0.004032, tar: 0.000064 
l0: 0.000045, l1: 0.000046, l2: 0.000080, l3: 0.000265, l4: 0.000789, l5: 0.000819, l6: 0.002376
[epoch: 655/1000, batch:   632/ 1052, ite: 172160] train loss: 0.004034, tar: 0.000064 
l0: 0.000072, l1: 0.000075, l2: 0.000089, l3: 0.000234, l4: 0.000522, l5: 0.001501, l6: 0.002303
[epoch: 655/1000, batch:   712/ 1052, ite: 172180] train loss: 0.004032, tar: 0.000064 
l0: 0.000062, l1: 0.000064, l2: 0.000094, l3: 0.000187, l4: 0.000554, l5: 0.000968, l6: 0.002195
[epoch: 655/1000, batch:   792/ 1052, ite: 172200] train loss: 0.004031, tar: 0.000064 
l0: 0.000029, l1: 0.000029, l2: 0.000112, l3: 0.000265, l4: 0.000564, l5: 0.001568, l6: 0.003000
[epoch: 655/1000, batch:   872/ 1052, ite: 172220] train loss: 0.004031, tar: 0.000064 
l0: 0.000023, l1: 0.000024, l2: 0.000041, l3: 0.000096, l4: 0.000155, l5: 0.000521, l6: 0.000811
[epoch: 655/1000, batch:   952/ 1052, ite: 172240] train loss: 0.004027, tar: 0.000064 
l0: 0.000085, l1: 0.000087, l2: 0.000089, l3: 0.000196, l4: 0.000434, l5: 0.001059, l6: 0.001647
[epoch: 655/1000, batch:  1032/ 1052, ite: 172260] train loss: 0.004028, tar: 0.000064 
[Epoch 655/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000025, l1: 0.000026, l2: 0.000041, l3: 0.000086, l4: 0.000314, l5: 0.000834, l6: 0.000636
[epoch: 656/1000, batch:    60/ 1052, ite: 172280] train loss: 0.004027, tar: 0.000064 
l0: 0.000033, l1: 0.000035, l2: 0.000107, l3: 0.000311, l4: 0.000766, l5: 0.001224, l6: 0.002402
[epoch: 656/1000, batch:   140/ 1052, ite: 172300] train loss: 0.004027, tar: 0.000064 
l0: 0.000013, l1: 0.000012, l2: 0.000053, l3: 0.000186, l4: 0.000448, l5: 0.001056, l6: 0.001356
[epoch: 656/1000, batch:   220/ 1052, ite: 172320] train loss: 0.004027, tar: 0.000064 
l0: 0.000012, l1: 0.000013, l2: 0.000049, l3: 0.000128, l4: 0.000310, l5: 0.000733, l6: 0.002104
[epoch: 656/1000, batch:   300/ 1052, ite: 172340] train loss: 0.004026, tar: 0.000064 
l0: 0.000043, l1: 0.000043, l2: 0.000045, l3: 0.000098, l4: 0.000202, l5: 0.000667, l6: 0.001345
[epoch: 656/1000, batch:   380/ 1052, ite: 172360] train loss: 0.004026, tar: 0.000064 
l0: 0.000088, l1: 0.000091, l2: 0.000105, l3: 0.000211, l4: 0.000481, l5: 0.001415, l6: 0.002396
[epoch: 656/1000, batch:   460/ 1052, ite: 172380] train loss: 0.004026, tar: 0.000064 
l0: 0.000179, l1: 0.000171, l2: 0.000118, l3: 0.000230, l4: 0.000395, l5: 0.000894, l6: 0.001280
[epoch: 656/1000, batch:   540/ 1052, ite: 172400] train loss: 0.004028, tar: 0.000064 
l0: 0.000057, l1: 0.000060, l2: 0.000098, l3: 0.000196, l4: 0.000436, l5: 0.001053, l6: 0.002176
[epoch: 656/1000, batch:   620/ 1052, ite: 172420] train loss: 0.004028, tar: 0.000064 
l0: 0.000038, l1: 0.000037, l2: 0.000083, l3: 0.000199, l4: 0.000432, l5: 0.000750, l6: 0.002439
[epoch: 656/1000, batch:   700/ 1052, ite: 172440] train loss: 0.004027, tar: 0.000064 
l0: 0.000031, l1: 0.000031, l2: 0.000074, l3: 0.000105, l4: 0.000260, l5: 0.001000, l6: 0.001511
[epoch: 656/1000, batch:   780/ 1052, ite: 172460] train loss: 0.004026, tar: 0.000064 
l0: 0.000043, l1: 0.000044, l2: 0.000134, l3: 0.000225, l4: 0.000421, l5: 0.001189, l6: 0.002355
[epoch: 656/1000, batch:   860/ 1052, ite: 172480] train loss: 0.004027, tar: 0.000064 
l0: 0.000017, l1: 0.000017, l2: 0.000033, l3: 0.000152, l4: 0.000517, l5: 0.001456, l6: 0.001844
[epoch: 656/1000, batch:   940/ 1052, ite: 172500] train loss: 0.004027, tar: 0.000064 
l0: 0.000099, l1: 0.000100, l2: 0.000144, l3: 0.000309, l4: 0.000734, l5: 0.001316, l6: 0.002697
[epoch: 656/1000, batch:  1020/ 1052, ite: 172520] train loss: 0.004029, tar: 0.000064 
[Epoch 656/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000074, l1: 0.000068, l2: 0.000109, l3: 0.000174, l4: 0.000347, l5: 0.001043, l6: 0.001895
[epoch: 657/1000, batch:    48/ 1052, ite: 172540] train loss: 0.004028, tar: 0.000064 
l0: 0.000064, l1: 0.000065, l2: 0.000106, l3: 0.000206, l4: 0.000451, l5: 0.001199, l6: 0.002844
[epoch: 657/1000, batch:   128/ 1052, ite: 172560] train loss: 0.004026, tar: 0.000064 
l0: 0.000061, l1: 0.000062, l2: 0.000137, l3: 0.000273, l4: 0.000429, l5: 0.000957, l6: 0.001431
[epoch: 657/1000, batch:   208/ 1052, ite: 172580] train loss: 0.004026, tar: 0.000064 
l0: 0.000041, l1: 0.000043, l2: 0.000071, l3: 0.000100, l4: 0.000385, l5: 0.001156, l6: 0.001593
[epoch: 657/1000, batch:   288/ 1052, ite: 172600] train loss: 0.004026, tar: 0.000064 
l0: 0.000291, l1: 0.000293, l2: 0.000351, l3: 0.000623, l4: 0.001185, l5: 0.003231, l6: 0.007350
[epoch: 657/1000, batch:   368/ 1052, ite: 172620] train loss: 0.004027, tar: 0.000064 
l0: 0.000004, l1: 0.000004, l2: 0.000011, l3: 0.000038, l4: 0.000249, l5: 0.000769, l6: 0.001380
[epoch: 657/1000, batch:   448/ 1052, ite: 172640] train loss: 0.004026, tar: 0.000064 
l0: 0.000056, l1: 0.000055, l2: 0.000088, l3: 0.000168, l4: 0.000373, l5: 0.000885, l6: 0.001322
[epoch: 657/1000, batch:   528/ 1052, ite: 172660] train loss: 0.004025, tar: 0.000064 
l0: 0.000054, l1: 0.000053, l2: 0.000113, l3: 0.000356, l4: 0.000767, l5: 0.001776, l6: 0.004073
[epoch: 657/1000, batch:   608/ 1052, ite: 172680] train loss: 0.004027, tar: 0.000064 
l0: 0.000021, l1: 0.000021, l2: 0.000032, l3: 0.000110, l4: 0.000265, l5: 0.000730, l6: 0.001121
[epoch: 657/1000, batch:   688/ 1052, ite: 172700] train loss: 0.004028, tar: 0.000064 
l0: 0.000007, l1: 0.000008, l2: 0.000027, l3: 0.000080, l4: 0.000281, l5: 0.000828, l6: 0.000880
[epoch: 657/1000, batch:   768/ 1052, ite: 172720] train loss: 0.004028, tar: 0.000064 
l0: 0.000007, l1: 0.000007, l2: 0.000026, l3: 0.000090, l4: 0.000431, l5: 0.000997, l6: 0.001699
[epoch: 657/1000, batch:   848/ 1052, ite: 172740] train loss: 0.004028, tar: 0.000064 
l0: 0.000114, l1: 0.000114, l2: 0.000141, l3: 0.000238, l4: 0.000809, l5: 0.001607, l6: 0.002373
[epoch: 657/1000, batch:   928/ 1052, ite: 172760] train loss: 0.004026, tar: 0.000064 
l0: 0.000059, l1: 0.000062, l2: 0.000073, l3: 0.000186, l4: 0.000467, l5: 0.001104, l6: 0.001544
[epoch: 657/1000, batch:  1008/ 1052, ite: 172780] train loss: 0.004030, tar: 0.000064 
[Epoch 657/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000010, l1: 0.000010, l2: 0.000018, l3: 0.000074, l4: 0.000349, l5: 0.000732, l6: 0.001272
[epoch: 658/1000, batch:    36/ 1052, ite: 172800] train loss: 0.004029, tar: 0.000064 
l0: 0.000044, l1: 0.000046, l2: 0.000094, l3: 0.000243, l4: 0.000768, l5: 0.001377, l6: 0.002720
[epoch: 658/1000, batch:   116/ 1052, ite: 172820] train loss: 0.004029, tar: 0.000064 
l0: 0.000033, l1: 0.000035, l2: 0.000091, l3: 0.000239, l4: 0.000428, l5: 0.001069, l6: 0.001573
[epoch: 658/1000, batch:   196/ 1052, ite: 172840] train loss: 0.004030, tar: 0.000064 
l0: 0.000049, l1: 0.000051, l2: 0.000070, l3: 0.000165, l4: 0.000329, l5: 0.000340, l6: 0.001480
[epoch: 658/1000, batch:   276/ 1052, ite: 172860] train loss: 0.004031, tar: 0.000064 
l0: 0.000045, l1: 0.000043, l2: 0.000106, l3: 0.000196, l4: 0.000434, l5: 0.001406, l6: 0.001899
[epoch: 658/1000, batch:   356/ 1052, ite: 172880] train loss: 0.004031, tar: 0.000064 
l0: 0.000099, l1: 0.000101, l2: 0.000123, l3: 0.000286, l4: 0.000766, l5: 0.001904, l6: 0.003638
[epoch: 658/1000, batch:   436/ 1052, ite: 172900] train loss: 0.004033, tar: 0.000064 
l0: 0.000100, l1: 0.000102, l2: 0.000177, l3: 0.000369, l4: 0.000753, l5: 0.001590, l6: 0.002406
[epoch: 658/1000, batch:   516/ 1052, ite: 172920] train loss: 0.004033, tar: 0.000064 
l0: 0.000085, l1: 0.000087, l2: 0.000152, l3: 0.000199, l4: 0.000448, l5: 0.001169, l6: 0.001487
[epoch: 658/1000, batch:   596/ 1052, ite: 172940] train loss: 0.004031, tar: 0.000064 
l0: 0.000056, l1: 0.000056, l2: 0.000080, l3: 0.000150, l4: 0.000317, l5: 0.000568, l6: 0.000567
[epoch: 658/1000, batch:   676/ 1052, ite: 172960] train loss: 0.004028, tar: 0.000064 
l0: 0.000012, l1: 0.000012, l2: 0.000028, l3: 0.000139, l4: 0.000735, l5: 0.000982, l6: 0.001683
[epoch: 658/1000, batch:   756/ 1052, ite: 172980] train loss: 0.004027, tar: 0.000064 
l0: 0.000043, l1: 0.000042, l2: 0.000075, l3: 0.000134, l4: 0.000283, l5: 0.000570, l6: 0.001017
[epoch: 658/1000, batch:   836/ 1052, ite: 173000] train loss: 0.004028, tar: 0.000064 
l0: 0.000028, l1: 0.000028, l2: 0.000059, l3: 0.000145, l4: 0.000336, l5: 0.000605, l6: 0.001098
[epoch: 658/1000, batch:   916/ 1052, ite: 173020] train loss: 0.004026, tar: 0.000064 
l0: 0.000072, l1: 0.000069, l2: 0.000175, l3: 0.000396, l4: 0.000925, l5: 0.001475, l6: 0.002265
[epoch: 658/1000, batch:   996/ 1052, ite: 173040] train loss: 0.004029, tar: 0.000064 
[Epoch 658/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000014, l1: 0.000015, l2: 0.000040, l3: 0.000063, l4: 0.000318, l5: 0.000552, l6: 0.001281
[epoch: 659/1000, batch:    24/ 1052, ite: 173060] train loss: 0.004029, tar: 0.000064 
l0: 0.000071, l1: 0.000068, l2: 0.000161, l3: 0.000429, l4: 0.000884, l5: 0.001629, l6: 0.002484
[epoch: 659/1000, batch:   104/ 1052, ite: 173080] train loss: 0.004029, tar: 0.000064 
l0: 0.000096, l1: 0.000097, l2: 0.000164, l3: 0.000325, l4: 0.000567, l5: 0.000976, l6: 0.002029
[epoch: 659/1000, batch:   184/ 1052, ite: 173100] train loss: 0.004031, tar: 0.000064 
l0: 0.000145, l1: 0.000146, l2: 0.000189, l3: 0.000302, l4: 0.000611, l5: 0.001525, l6: 0.003122
[epoch: 659/1000, batch:   264/ 1052, ite: 173120] train loss: 0.004030, tar: 0.000064 
l0: 0.000052, l1: 0.000058, l2: 0.000094, l3: 0.000263, l4: 0.000751, l5: 0.001710, l6: 0.002402
[epoch: 659/1000, batch:   344/ 1052, ite: 173140] train loss: 0.004032, tar: 0.000064 
l0: 0.000093, l1: 0.000090, l2: 0.000131, l3: 0.000272, l4: 0.000453, l5: 0.001249, l6: 0.002996
[epoch: 659/1000, batch:   424/ 1052, ite: 173160] train loss: 0.004030, tar: 0.000064 
l0: 0.000025, l1: 0.000023, l2: 0.000051, l3: 0.000127, l4: 0.000377, l5: 0.000886, l6: 0.001153
[epoch: 659/1000, batch:   504/ 1052, ite: 173180] train loss: 0.004028, tar: 0.000064 
l0: 0.000014, l1: 0.000013, l2: 0.000049, l3: 0.000139, l4: 0.000451, l5: 0.001314, l6: 0.002865
[epoch: 659/1000, batch:   584/ 1052, ite: 173200] train loss: 0.004027, tar: 0.000064 
l0: 0.000057, l1: 0.000059, l2: 0.000116, l3: 0.000195, l4: 0.000559, l5: 0.001493, l6: 0.002308
[epoch: 659/1000, batch:   664/ 1052, ite: 173220] train loss: 0.004026, tar: 0.000064 
l0: 0.000056, l1: 0.000060, l2: 0.000094, l3: 0.000214, l4: 0.000477, l5: 0.000751, l6: 0.001809
[epoch: 659/1000, batch:   744/ 1052, ite: 173240] train loss: 0.004025, tar: 0.000064 
l0: 0.000134, l1: 0.000137, l2: 0.000210, l3: 0.000378, l4: 0.000762, l5: 0.001217, l6: 0.002162
[epoch: 659/1000, batch:   824/ 1052, ite: 173260] train loss: 0.004024, tar: 0.000064 
l0: 0.000048, l1: 0.000049, l2: 0.000073, l3: 0.000126, l4: 0.000311, l5: 0.000995, l6: 0.002448
[epoch: 659/1000, batch:   904/ 1052, ite: 173280] train loss: 0.004027, tar: 0.000064 
l0: 0.000101, l1: 0.000100, l2: 0.000151, l3: 0.000183, l4: 0.000284, l5: 0.000637, l6: 0.002383
[epoch: 659/1000, batch:   984/ 1052, ite: 173300] train loss: 0.004027, tar: 0.000064 
[Epoch 659/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000015, l1: 0.000014, l2: 0.000060, l3: 0.000135, l4: 0.000224, l5: 0.001157, l6: 0.002077
[epoch: 660/1000, batch:    12/ 1052, ite: 173320] train loss: 0.004026, tar: 0.000064 
l0: 0.000074, l1: 0.000075, l2: 0.000100, l3: 0.000199, l4: 0.000584, l5: 0.001543, l6: 0.002369
[epoch: 660/1000, batch:    92/ 1052, ite: 173340] train loss: 0.004027, tar: 0.000064 
l0: 0.000009, l1: 0.000008, l2: 0.000031, l3: 0.000167, l4: 0.000406, l5: 0.001010, l6: 0.001773
[epoch: 660/1000, batch:   172/ 1052, ite: 173360] train loss: 0.004026, tar: 0.000064 
l0: 0.000038, l1: 0.000036, l2: 0.000096, l3: 0.000225, l4: 0.000501, l5: 0.001168, l6: 0.002037
[epoch: 660/1000, batch:   252/ 1052, ite: 173380] train loss: 0.004026, tar: 0.000064 
l0: 0.000061, l1: 0.000063, l2: 0.000095, l3: 0.000200, l4: 0.000375, l5: 0.001035, l6: 0.001183
[epoch: 660/1000, batch:   332/ 1052, ite: 173400] train loss: 0.004024, tar: 0.000064 
l0: 0.000063, l1: 0.000064, l2: 0.000090, l3: 0.000156, l4: 0.000383, l5: 0.000913, l6: 0.002033
[epoch: 660/1000, batch:   412/ 1052, ite: 173420] train loss: 0.004025, tar: 0.000064 
l0: 0.000032, l1: 0.000031, l2: 0.000060, l3: 0.000198, l4: 0.000480, l5: 0.000783, l6: 0.002032
[epoch: 660/1000, batch:   492/ 1052, ite: 173440] train loss: 0.004024, tar: 0.000064 
l0: 0.000047, l1: 0.000046, l2: 0.000107, l3: 0.000194, l4: 0.000332, l5: 0.000900, l6: 0.002296
[epoch: 660/1000, batch:   572/ 1052, ite: 173460] train loss: 0.004024, tar: 0.000064 
l0: 0.000070, l1: 0.000070, l2: 0.000095, l3: 0.000229, l4: 0.000455, l5: 0.000797, l6: 0.001319
[epoch: 660/1000, batch:   652/ 1052, ite: 173480] train loss: 0.004026, tar: 0.000064 
l0: 0.000065, l1: 0.000065, l2: 0.000118, l3: 0.000231, l4: 0.000399, l5: 0.000983, l6: 0.001578
[epoch: 660/1000, batch:   732/ 1052, ite: 173500] train loss: 0.004027, tar: 0.000064 
l0: 0.000120, l1: 0.000121, l2: 0.000122, l3: 0.000323, l4: 0.000671, l5: 0.001461, l6: 0.003034
[epoch: 660/1000, batch:   812/ 1052, ite: 173520] train loss: 0.004027, tar: 0.000064 
l0: 0.000018, l1: 0.000018, l2: 0.000045, l3: 0.000108, l4: 0.000172, l5: 0.000834, l6: 0.000895
[epoch: 660/1000, batch:   892/ 1052, ite: 173540] train loss: 0.004026, tar: 0.000064 
l0: 0.000019, l1: 0.000021, l2: 0.000043, l3: 0.000175, l4: 0.000524, l5: 0.001241, l6: 0.001821
[epoch: 660/1000, batch:   972/ 1052, ite: 173560] train loss: 0.004025, tar: 0.000064 
l0: 0.000017, l1: 0.000016, l2: 0.000043, l3: 0.000126, l4: 0.000266, l5: 0.000440, l6: 0.000612
[epoch: 660/1000, batch:  1052/ 1052, ite: 173580] train loss: 0.004023, tar: 0.000064 
[Epoch 660/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000044, l1: 0.000046, l2: 0.000069, l3: 0.000161, l4: 0.000310, l5: 0.000914, l6: 0.001408
[epoch: 661/1000, batch:    80/ 1052, ite: 173600] train loss: 0.004021, tar: 0.000064 
l0: 0.000004, l1: 0.000004, l2: 0.000009, l3: 0.000052, l4: 0.000192, l5: 0.000679, l6: 0.001524
[epoch: 661/1000, batch:   160/ 1052, ite: 173620] train loss: 0.004021, tar: 0.000064 
l0: 0.000097, l1: 0.000094, l2: 0.000148, l3: 0.000432, l4: 0.000735, l5: 0.001244, l6: 0.002178
[epoch: 661/1000, batch:   240/ 1052, ite: 173640] train loss: 0.004020, tar: 0.000064 
l0: 0.000056, l1: 0.000057, l2: 0.000070, l3: 0.000097, l4: 0.000328, l5: 0.000818, l6: 0.001278
[epoch: 661/1000, batch:   320/ 1052, ite: 173660] train loss: 0.004020, tar: 0.000064 
l0: 0.000107, l1: 0.000107, l2: 0.000132, l3: 0.000247, l4: 0.000497, l5: 0.001271, l6: 0.002958
[epoch: 661/1000, batch:   400/ 1052, ite: 173680] train loss: 0.004020, tar: 0.000064 
l0: 0.000014, l1: 0.000013, l2: 0.000050, l3: 0.000152, l4: 0.000406, l5: 0.000742, l6: 0.001774
[epoch: 661/1000, batch:   480/ 1052, ite: 173700] train loss: 0.004020, tar: 0.000064 
l0: 0.000085, l1: 0.000087, l2: 0.000104, l3: 0.000224, l4: 0.000407, l5: 0.000996, l6: 0.002190
[epoch: 661/1000, batch:   560/ 1052, ite: 173720] train loss: 0.004021, tar: 0.000064 
l0: 0.000091, l1: 0.000094, l2: 0.000113, l3: 0.000215, l4: 0.000499, l5: 0.001457, l6: 0.002137
[epoch: 661/1000, batch:   640/ 1052, ite: 173740] train loss: 0.004021, tar: 0.000064 
l0: 0.000016, l1: 0.000015, l2: 0.000028, l3: 0.000103, l4: 0.000277, l5: 0.000883, l6: 0.001678
[epoch: 661/1000, batch:   720/ 1052, ite: 173760] train loss: 0.004020, tar: 0.000064 
l0: 0.000036, l1: 0.000034, l2: 0.000074, l3: 0.000140, l4: 0.000448, l5: 0.001058, l6: 0.002132
[epoch: 661/1000, batch:   800/ 1052, ite: 173780] train loss: 0.004020, tar: 0.000064 
l0: 0.000038, l1: 0.000037, l2: 0.000063, l3: 0.000155, l4: 0.000441, l5: 0.001019, l6: 0.000861
[epoch: 661/1000, batch:   880/ 1052, ite: 173800] train loss: 0.004020, tar: 0.000064 
l0: 0.000101, l1: 0.000102, l2: 0.000117, l3: 0.000143, l4: 0.000479, l5: 0.000965, l6: 0.001678
[epoch: 661/1000, batch:   960/ 1052, ite: 173820] train loss: 0.004021, tar: 0.000064 
l0: 0.000042, l1: 0.000041, l2: 0.000076, l3: 0.000132, l4: 0.000345, l5: 0.001071, l6: 0.001579
[epoch: 661/1000, batch:  1040/ 1052, ite: 173840] train loss: 0.004022, tar: 0.000064 
[Epoch 661/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000012, l1: 0.000012, l2: 0.000042, l3: 0.000224, l4: 0.000319, l5: 0.000791, l6: 0.001427
[epoch: 662/1000, batch:    68/ 1052, ite: 173860] train loss: 0.004021, tar: 0.000064 
l0: 0.000023, l1: 0.000023, l2: 0.000074, l3: 0.000257, l4: 0.000551, l5: 0.001889, l6: 0.002140
[epoch: 662/1000, batch:   148/ 1052, ite: 173880] train loss: 0.004023, tar: 0.000064 
l0: 0.000063, l1: 0.000065, l2: 0.000087, l3: 0.000164, l4: 0.000431, l5: 0.001080, l6: 0.002062
[epoch: 662/1000, batch:   228/ 1052, ite: 173900] train loss: 0.004023, tar: 0.000064 
l0: 0.000070, l1: 0.000069, l2: 0.000160, l3: 0.000384, l4: 0.000780, l5: 0.001851, l6: 0.002257
[epoch: 662/1000, batch:   308/ 1052, ite: 173920] train loss: 0.004022, tar: 0.000064 
l0: 0.000096, l1: 0.000095, l2: 0.000126, l3: 0.000208, l4: 0.000388, l5: 0.000556, l6: 0.001074
[epoch: 662/1000, batch:   388/ 1052, ite: 173940] train loss: 0.004023, tar: 0.000064 
l0: 0.000048, l1: 0.000046, l2: 0.000066, l3: 0.000137, l4: 0.000244, l5: 0.000647, l6: 0.001010
[epoch: 662/1000, batch:   468/ 1052, ite: 173960] train loss: 0.004022, tar: 0.000064 
l0: 0.000045, l1: 0.000041, l2: 0.000068, l3: 0.000158, l4: 0.000468, l5: 0.000769, l6: 0.001414
[epoch: 662/1000, batch:   548/ 1052, ite: 173980] train loss: 0.004024, tar: 0.000064 
l0: 0.000035, l1: 0.000035, l2: 0.000079, l3: 0.000169, l4: 0.000523, l5: 0.001550, l6: 0.001839
[epoch: 662/1000, batch:   628/ 1052, ite: 174000] train loss: 0.004021, tar: 0.000063 
l0: 0.000049, l1: 0.000049, l2: 0.000073, l3: 0.000109, l4: 0.000435, l5: 0.000754, l6: 0.001535
[epoch: 662/1000, batch:   708/ 1052, ite: 174020] train loss: 0.004021, tar: 0.000063 
l0: 0.000040, l1: 0.000039, l2: 0.000076, l3: 0.000220, l4: 0.000616, l5: 0.001214, l6: 0.001959
[epoch: 662/1000, batch:   788/ 1052, ite: 174040] train loss: 0.004021, tar: 0.000063 
l0: 0.000028, l1: 0.000028, l2: 0.000069, l3: 0.000164, l4: 0.000422, l5: 0.000892, l6: 0.001789
[epoch: 662/1000, batch:   868/ 1052, ite: 174060] train loss: 0.004021, tar: 0.000063 
l0: 0.000091, l1: 0.000094, l2: 0.000124, l3: 0.000197, l4: 0.000375, l5: 0.000955, l6: 0.001635
[epoch: 662/1000, batch:   948/ 1052, ite: 174080] train loss: 0.004019, tar: 0.000063 
l0: 0.000029, l1: 0.000028, l2: 0.000066, l3: 0.000267, l4: 0.000365, l5: 0.001078, l6: 0.001501
[epoch: 662/1000, batch:  1028/ 1052, ite: 174100] train loss: 0.004019, tar: 0.000063 
[Epoch 662/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000052, l1: 0.000056, l2: 0.000097, l3: 0.000186, l4: 0.000460, l5: 0.000877, l6: 0.001496
[epoch: 663/1000, batch:    56/ 1052, ite: 174120] train loss: 0.004020, tar: 0.000063 
l0: 0.000030, l1: 0.000029, l2: 0.000064, l3: 0.000254, l4: 0.000430, l5: 0.000752, l6: 0.001957
[epoch: 663/1000, batch:   136/ 1052, ite: 174140] train loss: 0.004017, tar: 0.000063 
l0: 0.000129, l1: 0.000131, l2: 0.000152, l3: 0.000227, l4: 0.000468, l5: 0.001067, l6: 0.001639
[epoch: 663/1000, batch:   216/ 1052, ite: 174160] train loss: 0.004018, tar: 0.000063 
l0: 0.000012, l1: 0.000015, l2: 0.000035, l3: 0.000102, l4: 0.000274, l5: 0.000521, l6: 0.000894
[epoch: 663/1000, batch:   296/ 1052, ite: 174180] train loss: 0.004018, tar: 0.000063 
l0: 0.000008, l1: 0.000008, l2: 0.000032, l3: 0.000125, l4: 0.000474, l5: 0.001193, l6: 0.001443
[epoch: 663/1000, batch:   376/ 1052, ite: 174200] train loss: 0.004019, tar: 0.000063 
l0: 0.000245, l1: 0.000244, l2: 0.000297, l3: 0.000518, l4: 0.000854, l5: 0.001782, l6: 0.003209
[epoch: 663/1000, batch:   456/ 1052, ite: 174220] train loss: 0.004021, tar: 0.000063 
l0: 0.000005, l1: 0.000005, l2: 0.000019, l3: 0.000117, l4: 0.000250, l5: 0.000843, l6: 0.000747
[epoch: 663/1000, batch:   536/ 1052, ite: 174240] train loss: 0.004021, tar: 0.000063 
l0: 0.000085, l1: 0.000086, l2: 0.000127, l3: 0.000275, l4: 0.000660, l5: 0.001411, l6: 0.002654
[epoch: 663/1000, batch:   616/ 1052, ite: 174260] train loss: 0.004021, tar: 0.000063 
l0: 0.000047, l1: 0.000049, l2: 0.000061, l3: 0.000135, l4: 0.000609, l5: 0.000677, l6: 0.001406
[epoch: 663/1000, batch:   696/ 1052, ite: 174280] train loss: 0.004021, tar: 0.000063 
l0: 0.000029, l1: 0.000029, l2: 0.000068, l3: 0.000124, l4: 0.000490, l5: 0.000835, l6: 0.001728
[epoch: 663/1000, batch:   776/ 1052, ite: 174300] train loss: 0.004020, tar: 0.000063 
l0: 0.000098, l1: 0.000101, l2: 0.000191, l3: 0.000262, l4: 0.000475, l5: 0.001272, l6: 0.002453
[epoch: 663/1000, batch:   856/ 1052, ite: 174320] train loss: 0.004021, tar: 0.000063 
l0: 0.000034, l1: 0.000037, l2: 0.000033, l3: 0.000043, l4: 0.000132, l5: 0.000747, l6: 0.001081
[epoch: 663/1000, batch:   936/ 1052, ite: 174340] train loss: 0.004020, tar: 0.000063 
l0: 0.000041, l1: 0.000043, l2: 0.000068, l3: 0.000090, l4: 0.000353, l5: 0.000805, l6: 0.001385
[epoch: 663/1000, batch:  1016/ 1052, ite: 174360] train loss: 0.004017, tar: 0.000063 
[Epoch 663/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000031, l1: 0.000031, l2: 0.000064, l3: 0.000111, l4: 0.000380, l5: 0.000737, l6: 0.000808
[epoch: 664/1000, batch:    44/ 1052, ite: 174380] train loss: 0.004017, tar: 0.000063 
l0: 0.000037, l1: 0.000035, l2: 0.000116, l3: 0.000249, l4: 0.000575, l5: 0.001291, l6: 0.001641
[epoch: 664/1000, batch:   124/ 1052, ite: 174400] train loss: 0.004016, tar: 0.000063 
l0: 0.000023, l1: 0.000023, l2: 0.000048, l3: 0.000109, l4: 0.000330, l5: 0.000542, l6: 0.000985
[epoch: 664/1000, batch:   204/ 1052, ite: 174420] train loss: 0.004016, tar: 0.000063 
l0: 0.000063, l1: 0.000058, l2: 0.000132, l3: 0.000270, l4: 0.000666, l5: 0.001372, l6: 0.002893
[epoch: 664/1000, batch:   284/ 1052, ite: 174440] train loss: 0.004017, tar: 0.000063 
l0: 0.000105, l1: 0.000102, l2: 0.000185, l3: 0.000323, l4: 0.000619, l5: 0.001627, l6: 0.002711
[epoch: 664/1000, batch:   364/ 1052, ite: 174460] train loss: 0.004017, tar: 0.000063 
l0: 0.000050, l1: 0.000050, l2: 0.000061, l3: 0.000199, l4: 0.000440, l5: 0.000634, l6: 0.001093
[epoch: 664/1000, batch:   444/ 1052, ite: 174480] train loss: 0.004017, tar: 0.000063 
l0: 0.000049, l1: 0.000049, l2: 0.000055, l3: 0.000172, l4: 0.000440, l5: 0.000798, l6: 0.001709
[epoch: 664/1000, batch:   524/ 1052, ite: 174500] train loss: 0.004018, tar: 0.000063 
l0: 0.000153, l1: 0.000156, l2: 0.000193, l3: 0.000314, l4: 0.000682, l5: 0.001884, l6: 0.002698
[epoch: 664/1000, batch:   604/ 1052, ite: 174520] train loss: 0.004019, tar: 0.000063 
l0: 0.000155, l1: 0.000160, l2: 0.000180, l3: 0.000305, l4: 0.000659, l5: 0.001620, l6: 0.002466
[epoch: 664/1000, batch:   684/ 1052, ite: 174540] train loss: 0.004019, tar: 0.000063 
l0: 0.000034, l1: 0.000033, l2: 0.000086, l3: 0.000190, l4: 0.000588, l5: 0.001365, l6: 0.002099
[epoch: 664/1000, batch:   764/ 1052, ite: 174560] train loss: 0.004019, tar: 0.000063 
l0: 0.000097, l1: 0.000097, l2: 0.000232, l3: 0.000357, l4: 0.000722, l5: 0.001662, l6: 0.003589
[epoch: 664/1000, batch:   844/ 1052, ite: 174580] train loss: 0.004019, tar: 0.000063 
l0: 0.000115, l1: 0.000114, l2: 0.000156, l3: 0.000249, l4: 0.000479, l5: 0.001153, l6: 0.002863
[epoch: 664/1000, batch:   924/ 1052, ite: 174600] train loss: 0.004018, tar: 0.000063 
l0: 0.000057, l1: 0.000058, l2: 0.000073, l3: 0.000131, l4: 0.000305, l5: 0.000801, l6: 0.002423
[epoch: 664/1000, batch:  1004/ 1052, ite: 174620] train loss: 0.004018, tar: 0.000063 
[Epoch 664/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000007, l1: 0.000008, l2: 0.000043, l3: 0.000270, l4: 0.000739, l5: 0.001479, l6: 0.002665
[epoch: 665/1000, batch:    32/ 1052, ite: 174640] train loss: 0.004017, tar: 0.000063 
l0: 0.000023, l1: 0.000023, l2: 0.000065, l3: 0.000141, l4: 0.000458, l5: 0.000821, l6: 0.000921
[epoch: 665/1000, batch:   112/ 1052, ite: 174660] train loss: 0.004016, tar: 0.000063 
l0: 0.000107, l1: 0.000110, l2: 0.000144, l3: 0.000215, l4: 0.000537, l5: 0.000951, l6: 0.002047
[epoch: 665/1000, batch:   192/ 1052, ite: 174680] train loss: 0.004014, tar: 0.000063 
l0: 0.000008, l1: 0.000009, l2: 0.000023, l3: 0.000070, l4: 0.000447, l5: 0.001119, l6: 0.002407
[epoch: 665/1000, batch:   272/ 1052, ite: 174700] train loss: 0.004015, tar: 0.000063 
l0: 0.000066, l1: 0.000069, l2: 0.000109, l3: 0.000175, l4: 0.000467, l5: 0.001066, l6: 0.002160
[epoch: 665/1000, batch:   352/ 1052, ite: 174720] train loss: 0.004014, tar: 0.000063 
l0: 0.000129, l1: 0.000131, l2: 0.000184, l3: 0.000250, l4: 0.000406, l5: 0.000890, l6: 0.002031
[epoch: 665/1000, batch:   432/ 1052, ite: 174740] train loss: 0.004013, tar: 0.000063 
l0: 0.000004, l1: 0.000004, l2: 0.000026, l3: 0.000076, l4: 0.000367, l5: 0.001100, l6: 0.001630
[epoch: 665/1000, batch:   512/ 1052, ite: 174760] train loss: 0.004014, tar: 0.000063 
l0: 0.000146, l1: 0.000144, l2: 0.000266, l3: 0.000413, l4: 0.000880, l5: 0.002353, l6: 0.002797
[epoch: 665/1000, batch:   592/ 1052, ite: 174780] train loss: 0.004016, tar: 0.000063 
l0: 0.000032, l1: 0.000030, l2: 0.000083, l3: 0.000188, l4: 0.000340, l5: 0.001165, l6: 0.001138
[epoch: 665/1000, batch:   672/ 1052, ite: 174800] train loss: 0.004017, tar: 0.000063 
l0: 0.000085, l1: 0.000083, l2: 0.000117, l3: 0.000174, l4: 0.000385, l5: 0.000694, l6: 0.002073
[epoch: 665/1000, batch:   752/ 1052, ite: 174820] train loss: 0.004017, tar: 0.000063 
l0: 0.000025, l1: 0.000023, l2: 0.000108, l3: 0.000333, l4: 0.000665, l5: 0.001682, l6: 0.002810
[epoch: 665/1000, batch:   832/ 1052, ite: 174840] train loss: 0.004015, tar: 0.000063 
l0: 0.000097, l1: 0.000096, l2: 0.000110, l3: 0.000177, l4: 0.000510, l5: 0.001054, l6: 0.002102
[epoch: 665/1000, batch:   912/ 1052, ite: 174860] train loss: 0.004016, tar: 0.000063 
l0: 0.000034, l1: 0.000034, l2: 0.000052, l3: 0.000109, l4: 0.000286, l5: 0.000578, l6: 0.000841
[epoch: 665/1000, batch:   992/ 1052, ite: 174880] train loss: 0.004015, tar: 0.000063 
[Epoch 665/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000043, l1: 0.000042, l2: 0.000135, l3: 0.000272, l4: 0.000486, l5: 0.001765, l6: 0.003826
[epoch: 666/1000, batch:    20/ 1052, ite: 174900] train loss: 0.004014, tar: 0.000063 
l0: 0.000048, l1: 0.000049, l2: 0.000082, l3: 0.000155, l4: 0.000459, l5: 0.000867, l6: 0.001898
[epoch: 666/1000, batch:   100/ 1052, ite: 174920] train loss: 0.004013, tar: 0.000063 
l0: 0.000090, l1: 0.000091, l2: 0.000138, l3: 0.000228, l4: 0.000631, l5: 0.000924, l6: 0.001382
[epoch: 666/1000, batch:   180/ 1052, ite: 174940] train loss: 0.004014, tar: 0.000063 
l0: 0.000115, l1: 0.000118, l2: 0.000136, l3: 0.000199, l4: 0.000707, l5: 0.001263, l6: 0.002906
[epoch: 666/1000, batch:   260/ 1052, ite: 174960] train loss: 0.004013, tar: 0.000063 
l0: 0.000039, l1: 0.000040, l2: 0.000052, l3: 0.000099, l4: 0.000218, l5: 0.000540, l6: 0.001577
[epoch: 666/1000, batch:   340/ 1052, ite: 174980] train loss: 0.004014, tar: 0.000063 
l0: 0.000024, l1: 0.000024, l2: 0.000038, l3: 0.000126, l4: 0.000566, l5: 0.000656, l6: 0.001308
[epoch: 666/1000, batch:   420/ 1052, ite: 175000] train loss: 0.004014, tar: 0.000063 
l0: 0.000070, l1: 0.000070, l2: 0.000087, l3: 0.000232, l4: 0.000638, l5: 0.001078, l6: 0.002136
[epoch: 666/1000, batch:   500/ 1052, ite: 175020] train loss: 0.004015, tar: 0.000063 
l0: 0.000032, l1: 0.000031, l2: 0.000061, l3: 0.000176, l4: 0.000885, l5: 0.001874, l6: 0.002762
[epoch: 666/1000, batch:   580/ 1052, ite: 175040] train loss: 0.004016, tar: 0.000063 
l0: 0.000049, l1: 0.000048, l2: 0.000056, l3: 0.000108, l4: 0.000309, l5: 0.000608, l6: 0.001133
[epoch: 666/1000, batch:   660/ 1052, ite: 175060] train loss: 0.004016, tar: 0.000063 
l0: 0.000170, l1: 0.000169, l2: 0.000230, l3: 0.000401, l4: 0.000802, l5: 0.001874, l6: 0.003988
[epoch: 666/1000, batch:   740/ 1052, ite: 175080] train loss: 0.004017, tar: 0.000063 
l0: 0.000130, l1: 0.000136, l2: 0.000146, l3: 0.000222, l4: 0.000559, l5: 0.001073, l6: 0.001600
[epoch: 666/1000, batch:   820/ 1052, ite: 175100] train loss: 0.004016, tar: 0.000063 
l0: 0.000030, l1: 0.000028, l2: 0.000095, l3: 0.000324, l4: 0.000670, l5: 0.001612, l6: 0.001969
[epoch: 666/1000, batch:   900/ 1052, ite: 175120] train loss: 0.004015, tar: 0.000063 
l0: 0.000065, l1: 0.000067, l2: 0.000086, l3: 0.000177, l4: 0.000262, l5: 0.001150, l6: 0.002131
[epoch: 666/1000, batch:   980/ 1052, ite: 175140] train loss: 0.004014, tar: 0.000063 
[Epoch 666/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000013, l1: 0.000013, l2: 0.000026, l3: 0.000170, l4: 0.000740, l5: 0.001558, l6: 0.002889
[epoch: 667/1000, batch:     8/ 1052, ite: 175160] train loss: 0.004014, tar: 0.000063 
l0: 0.000076, l1: 0.000076, l2: 0.000123, l3: 0.000249, l4: 0.000551, l5: 0.001145, l6: 0.002315
[epoch: 667/1000, batch:    88/ 1052, ite: 175180] train loss: 0.004015, tar: 0.000063 
l0: 0.000023, l1: 0.000021, l2: 0.000079, l3: 0.000239, l4: 0.000517, l5: 0.000907, l6: 0.001970
[epoch: 667/1000, batch:   168/ 1052, ite: 175200] train loss: 0.004014, tar: 0.000063 
l0: 0.000020, l1: 0.000023, l2: 0.000020, l3: 0.000113, l4: 0.000480, l5: 0.001017, l6: 0.001887
[epoch: 667/1000, batch:   248/ 1052, ite: 175220] train loss: 0.004013, tar: 0.000063 
l0: 0.000036, l1: 0.000037, l2: 0.000057, l3: 0.000129, l4: 0.000270, l5: 0.000765, l6: 0.001231
[epoch: 667/1000, batch:   328/ 1052, ite: 175240] train loss: 0.004012, tar: 0.000063 
l0: 0.000083, l1: 0.000087, l2: 0.000116, l3: 0.000151, l4: 0.000378, l5: 0.000881, l6: 0.001122
[epoch: 667/1000, batch:   408/ 1052, ite: 175260] train loss: 0.004011, tar: 0.000063 
l0: 0.000119, l1: 0.000116, l2: 0.000196, l3: 0.000305, l4: 0.000643, l5: 0.001224, l6: 0.002199
[epoch: 667/1000, batch:   488/ 1052, ite: 175280] train loss: 0.004011, tar: 0.000063 
l0: 0.000022, l1: 0.000022, l2: 0.000038, l3: 0.000107, l4: 0.000358, l5: 0.000856, l6: 0.001435
[epoch: 667/1000, batch:   568/ 1052, ite: 175300] train loss: 0.004013, tar: 0.000063 
l0: 0.000050, l1: 0.000050, l2: 0.000065, l3: 0.000172, l4: 0.000565, l5: 0.000592, l6: 0.001843
[epoch: 667/1000, batch:   648/ 1052, ite: 175320] train loss: 0.004013, tar: 0.000063 
l0: 0.000116, l1: 0.000128, l2: 0.000240, l3: 0.000256, l4: 0.000627, l5: 0.003286, l6: 0.004424
[epoch: 667/1000, batch:   728/ 1052, ite: 175340] train loss: 0.004014, tar: 0.000063 
l0: 0.000042, l1: 0.000043, l2: 0.000051, l3: 0.000134, l4: 0.000380, l5: 0.000445, l6: 0.001293
[epoch: 667/1000, batch:   808/ 1052, ite: 175360] train loss: 0.004015, tar: 0.000063 
l0: 0.000049, l1: 0.000052, l2: 0.000083, l3: 0.000158, l4: 0.000526, l5: 0.000618, l6: 0.001613
[epoch: 667/1000, batch:   888/ 1052, ite: 175380] train loss: 0.004015, tar: 0.000063 
l0: 0.000038, l1: 0.000040, l2: 0.000044, l3: 0.000079, l4: 0.000267, l5: 0.000757, l6: 0.001157
[epoch: 667/1000, batch:   968/ 1052, ite: 175400] train loss: 0.004014, tar: 0.000063 
l0: 0.000061, l1: 0.000064, l2: 0.000110, l3: 0.000243, l4: 0.000460, l5: 0.000940, l6: 0.002555
[epoch: 667/1000, batch:  1048/ 1052, ite: 175420] train loss: 0.004014, tar: 0.000063 
[Epoch 667/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000015, l1: 0.000015, l2: 0.000031, l3: 0.000115, l4: 0.000614, l5: 0.001751, l6: 0.002034
[epoch: 668/1000, batch:    76/ 1052, ite: 175440] train loss: 0.004015, tar: 0.000063 
l0: 0.000018, l1: 0.000022, l2: 0.000056, l3: 0.000253, l4: 0.000529, l5: 0.001359, l6: 0.002783
[epoch: 668/1000, batch:   156/ 1052, ite: 175460] train loss: 0.004016, tar: 0.000063 
l0: 0.000033, l1: 0.000035, l2: 0.000040, l3: 0.000093, l4: 0.000234, l5: 0.000629, l6: 0.001497
[epoch: 668/1000, batch:   236/ 1052, ite: 175480] train loss: 0.004016, tar: 0.000063 
l0: 0.000023, l1: 0.000024, l2: 0.000078, l3: 0.000260, l4: 0.000997, l5: 0.002181, l6: 0.002745
[epoch: 668/1000, batch:   316/ 1052, ite: 175500] train loss: 0.004016, tar: 0.000063 
l0: 0.000016, l1: 0.000017, l2: 0.000068, l3: 0.000105, l4: 0.000278, l5: 0.001191, l6: 0.002100
[epoch: 668/1000, batch:   396/ 1052, ite: 175520] train loss: 0.004016, tar: 0.000063 
l0: 0.000031, l1: 0.000031, l2: 0.000052, l3: 0.000123, l4: 0.000251, l5: 0.001034, l6: 0.001795
[epoch: 668/1000, batch:   476/ 1052, ite: 175540] train loss: 0.004016, tar: 0.000063 
l0: 0.000028, l1: 0.000028, l2: 0.000062, l3: 0.000184, l4: 0.000369, l5: 0.000954, l6: 0.001899
[epoch: 668/1000, batch:   556/ 1052, ite: 175560] train loss: 0.004015, tar: 0.000063 
l0: 0.000050, l1: 0.000051, l2: 0.000064, l3: 0.000171, l4: 0.000403, l5: 0.000781, l6: 0.001727
[epoch: 668/1000, batch:   636/ 1052, ite: 175580] train loss: 0.004015, tar: 0.000063 
l0: 0.000062, l1: 0.000061, l2: 0.000138, l3: 0.000294, l4: 0.000495, l5: 0.001671, l6: 0.002629
[epoch: 668/1000, batch:   716/ 1052, ite: 175600] train loss: 0.004017, tar: 0.000063 
l0: 0.000074, l1: 0.000073, l2: 0.000122, l3: 0.000252, l4: 0.000493, l5: 0.000851, l6: 0.001785
[epoch: 668/1000, batch:   796/ 1052, ite: 175620] train loss: 0.004015, tar: 0.000063 
l0: 0.000081, l1: 0.000079, l2: 0.000123, l3: 0.000195, l4: 0.000479, l5: 0.001235, l6: 0.002604
[epoch: 668/1000, batch:   876/ 1052, ite: 175640] train loss: 0.004015, tar: 0.000063 
l0: 0.000088, l1: 0.000088, l2: 0.000130, l3: 0.000180, l4: 0.000505, l5: 0.001205, l6: 0.001462
[epoch: 668/1000, batch:   956/ 1052, ite: 175660] train loss: 0.004014, tar: 0.000063 
l0: 0.000118, l1: 0.000117, l2: 0.000144, l3: 0.000192, l4: 0.000533, l5: 0.000977, l6: 0.002195
[epoch: 668/1000, batch:  1036/ 1052, ite: 175680] train loss: 0.004016, tar: 0.000063 
[Epoch 668/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000087, l1: 0.000088, l2: 0.000162, l3: 0.000330, l4: 0.000805, l5: 0.001417, l6: 0.003355
[epoch: 669/1000, batch:    64/ 1052, ite: 175700] train loss: 0.004015, tar: 0.000063 
l0: 0.000081, l1: 0.000086, l2: 0.000194, l3: 0.000411, l4: 0.000795, l5: 0.001629, l6: 0.005107
[epoch: 669/1000, batch:   144/ 1052, ite: 175720] train loss: 0.004014, tar: 0.000063 
l0: 0.000083, l1: 0.000084, l2: 0.000124, l3: 0.000210, l4: 0.000391, l5: 0.000953, l6: 0.001618
[epoch: 669/1000, batch:   224/ 1052, ite: 175740] train loss: 0.004013, tar: 0.000063 
l0: 0.000055, l1: 0.000058, l2: 0.000087, l3: 0.000159, l4: 0.000439, l5: 0.001383, l6: 0.002138
[epoch: 669/1000, batch:   304/ 1052, ite: 175760] train loss: 0.004012, tar: 0.000063 
l0: 0.000084, l1: 0.000085, l2: 0.000107, l3: 0.000187, l4: 0.000376, l5: 0.001004, l6: 0.002369
[epoch: 669/1000, batch:   384/ 1052, ite: 175780] train loss: 0.004013, tar: 0.000063 
l0: 0.000131, l1: 0.000133, l2: 0.000164, l3: 0.000254, l4: 0.000631, l5: 0.001900, l6: 0.003525
[epoch: 669/1000, batch:   464/ 1052, ite: 175800] train loss: 0.004013, tar: 0.000063 
l0: 0.000044, l1: 0.000044, l2: 0.000083, l3: 0.000220, l4: 0.000600, l5: 0.001479, l6: 0.002547
[epoch: 669/1000, batch:   544/ 1052, ite: 175820] train loss: 0.004014, tar: 0.000063 
l0: 0.000064, l1: 0.000064, l2: 0.000125, l3: 0.000234, l4: 0.000436, l5: 0.001125, l6: 0.001814
[epoch: 669/1000, batch:   624/ 1052, ite: 175840] train loss: 0.004013, tar: 0.000063 
l0: 0.000008, l1: 0.000009, l2: 0.000028, l3: 0.000063, l4: 0.000346, l5: 0.000888, l6: 0.000982
[epoch: 669/1000, batch:   704/ 1052, ite: 175860] train loss: 0.004014, tar: 0.000063 
l0: 0.000150, l1: 0.000156, l2: 0.000161, l3: 0.000285, l4: 0.000541, l5: 0.001342, l6: 0.003398
[epoch: 669/1000, batch:   784/ 1052, ite: 175880] train loss: 0.004014, tar: 0.000063 
l0: 0.000032, l1: 0.000031, l2: 0.000116, l3: 0.000324, l4: 0.000997, l5: 0.001512, l6: 0.002426
[epoch: 669/1000, batch:   864/ 1052, ite: 175900] train loss: 0.004015, tar: 0.000063 
l0: 0.000030, l1: 0.000028, l2: 0.000055, l3: 0.000175, l4: 0.000363, l5: 0.000637, l6: 0.001616
[epoch: 669/1000, batch:   944/ 1052, ite: 175920] train loss: 0.004016, tar: 0.000063 
l0: 0.000033, l1: 0.000036, l2: 0.000036, l3: 0.000116, l4: 0.000394, l5: 0.000758, l6: 0.001277
[epoch: 669/1000, batch:  1024/ 1052, ite: 175940] train loss: 0.004015, tar: 0.000063 
[Epoch 669/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000034, l1: 0.000037, l2: 0.000069, l3: 0.000149, l4: 0.000425, l5: 0.000815, l6: 0.001544
[epoch: 670/1000, batch:    52/ 1052, ite: 175960] train loss: 0.004015, tar: 0.000063 
l0: 0.000032, l1: 0.000031, l2: 0.000080, l3: 0.000164, l4: 0.000480, l5: 0.001250, l6: 0.002130
[epoch: 670/1000, batch:   132/ 1052, ite: 175980] train loss: 0.004015, tar: 0.000063 
l0: 0.000005, l1: 0.000006, l2: 0.000014, l3: 0.000046, l4: 0.000128, l5: 0.000564, l6: 0.000747
[epoch: 670/1000, batch:   212/ 1052, ite: 176000] train loss: 0.004016, tar: 0.000063 
l0: 0.000139, l1: 0.000141, l2: 0.000183, l3: 0.000267, l4: 0.000506, l5: 0.001135, l6: 0.002790
[epoch: 670/1000, batch:   292/ 1052, ite: 176020] train loss: 0.004017, tar: 0.000063 
l0: 0.000065, l1: 0.000066, l2: 0.000098, l3: 0.000172, l4: 0.000300, l5: 0.000957, l6: 0.002126
[epoch: 670/1000, batch:   372/ 1052, ite: 176040] train loss: 0.004018, tar: 0.000063 
l0: 0.000045, l1: 0.000045, l2: 0.000074, l3: 0.000165, l4: 0.000491, l5: 0.000804, l6: 0.001955
[epoch: 670/1000, batch:   452/ 1052, ite: 176060] train loss: 0.004016, tar: 0.000063 
l0: 0.000030, l1: 0.000031, l2: 0.000043, l3: 0.000151, l4: 0.000344, l5: 0.000795, l6: 0.001450
[epoch: 670/1000, batch:   532/ 1052, ite: 176080] train loss: 0.004016, tar: 0.000063 
l0: 0.000071, l1: 0.000072, l2: 0.000100, l3: 0.000181, l4: 0.000553, l5: 0.001156, l6: 0.002223
[epoch: 670/1000, batch:   612/ 1052, ite: 176100] train loss: 0.004016, tar: 0.000063 
l0: 0.000099, l1: 0.000100, l2: 0.000157, l3: 0.000260, l4: 0.000697, l5: 0.001146, l6: 0.002367
[epoch: 670/1000, batch:   692/ 1052, ite: 176120] train loss: 0.004017, tar: 0.000063 
l0: 0.000092, l1: 0.000099, l2: 0.000134, l3: 0.000217, l4: 0.000480, l5: 0.001165, l6: 0.001914
[epoch: 670/1000, batch:   772/ 1052, ite: 176140] train loss: 0.004017, tar: 0.000063 
l0: 0.000141, l1: 0.000142, l2: 0.000273, l3: 0.000499, l4: 0.000892, l5: 0.001468, l6: 0.003385
[epoch: 670/1000, batch:   852/ 1052, ite: 176160] train loss: 0.004017, tar: 0.000063 
l0: 0.000085, l1: 0.000084, l2: 0.000132, l3: 0.000303, l4: 0.000498, l5: 0.000834, l6: 0.001477
[epoch: 670/1000, batch:   932/ 1052, ite: 176180] train loss: 0.004016, tar: 0.000063 
l0: 0.000013, l1: 0.000011, l2: 0.000057, l3: 0.000073, l4: 0.000441, l5: 0.000636, l6: 0.000889
[epoch: 670/1000, batch:  1012/ 1052, ite: 176200] train loss: 0.004016, tar: 0.000063 
[Epoch 670/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000061, l1: 0.000061, l2: 0.000116, l3: 0.000289, l4: 0.000716, l5: 0.001548, l6: 0.003149
[epoch: 671/1000, batch:    40/ 1052, ite: 176220] train loss: 0.004017, tar: 0.000063 
l0: 0.000062, l1: 0.000061, l2: 0.000110, l3: 0.000210, l4: 0.000355, l5: 0.001131, l6: 0.001589
[epoch: 671/1000, batch:   120/ 1052, ite: 176240] train loss: 0.004017, tar: 0.000063 
l0: 0.000065, l1: 0.000068, l2: 0.000187, l3: 0.000330, l4: 0.000773, l5: 0.001209, l6: 0.002764
[epoch: 671/1000, batch:   200/ 1052, ite: 176260] train loss: 0.004017, tar: 0.000063 
l0: 0.000021, l1: 0.000024, l2: 0.000045, l3: 0.000191, l4: 0.000444, l5: 0.001252, l6: 0.001978
[epoch: 671/1000, batch:   280/ 1052, ite: 176280] train loss: 0.004017, tar: 0.000063 
l0: 0.000012, l1: 0.000012, l2: 0.000037, l3: 0.000069, l4: 0.000307, l5: 0.000440, l6: 0.000737
[epoch: 671/1000, batch:   360/ 1052, ite: 176300] train loss: 0.004015, tar: 0.000063 
l0: 0.000111, l1: 0.000110, l2: 0.000167, l3: 0.000268, l4: 0.000598, l5: 0.000833, l6: 0.002923
[epoch: 671/1000, batch:   440/ 1052, ite: 176320] train loss: 0.004015, tar: 0.000063 
l0: 0.000048, l1: 0.000048, l2: 0.000065, l3: 0.000135, l4: 0.000284, l5: 0.000853, l6: 0.001053
[epoch: 671/1000, batch:   520/ 1052, ite: 176340] train loss: 0.004015, tar: 0.000063 
l0: 0.000046, l1: 0.000047, l2: 0.000093, l3: 0.000308, l4: 0.000614, l5: 0.000962, l6: 0.002128
[epoch: 671/1000, batch:   600/ 1052, ite: 176360] train loss: 0.004014, tar: 0.000063 
l0: 0.000053, l1: 0.000053, l2: 0.000062, l3: 0.000130, l4: 0.000324, l5: 0.000601, l6: 0.001470
[epoch: 671/1000, batch:   680/ 1052, ite: 176380] train loss: 0.004013, tar: 0.000063 
l0: 0.000037, l1: 0.000037, l2: 0.000050, l3: 0.000133, l4: 0.000305, l5: 0.001131, l6: 0.002306
[epoch: 671/1000, batch:   760/ 1052, ite: 176400] train loss: 0.004015, tar: 0.000063 
l0: 0.000180, l1: 0.000185, l2: 0.000218, l3: 0.000405, l4: 0.000715, l5: 0.001321, l6: 0.002235
[epoch: 671/1000, batch:   840/ 1052, ite: 176420] train loss: 0.004015, tar: 0.000063 
l0: 0.000029, l1: 0.000028, l2: 0.000046, l3: 0.000129, l4: 0.000260, l5: 0.000970, l6: 0.001556
[epoch: 671/1000, batch:   920/ 1052, ite: 176440] train loss: 0.004015, tar: 0.000063 
l0: 0.000056, l1: 0.000055, l2: 0.000083, l3: 0.000218, l4: 0.000377, l5: 0.000669, l6: 0.001811
[epoch: 671/1000, batch:  1000/ 1052, ite: 176460] train loss: 0.004015, tar: 0.000063 
[Epoch 671/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000120, l1: 0.000117, l2: 0.000170, l3: 0.000220, l4: 0.000391, l5: 0.000796, l6: 0.001513
[epoch: 672/1000, batch:    28/ 1052, ite: 176480] train loss: 0.004013, tar: 0.000063 
l0: 0.000012, l1: 0.000013, l2: 0.000021, l3: 0.000058, l4: 0.000240, l5: 0.000844, l6: 0.001211
[epoch: 672/1000, batch:   108/ 1052, ite: 176500] train loss: 0.004016, tar: 0.000063 
l0: 0.000025, l1: 0.000024, l2: 0.000074, l3: 0.000223, l4: 0.000342, l5: 0.001512, l6: 0.001862
[epoch: 672/1000, batch:   188/ 1052, ite: 176520] train loss: 0.004016, tar: 0.000063 
l0: 0.000016, l1: 0.000014, l2: 0.000074, l3: 0.000316, l4: 0.000536, l5: 0.001040, l6: 0.002634
[epoch: 672/1000, batch:   268/ 1052, ite: 176540] train loss: 0.004016, tar: 0.000063 
l0: 0.000028, l1: 0.000027, l2: 0.000048, l3: 0.000107, l4: 0.000307, l5: 0.000856, l6: 0.000706
[epoch: 672/1000, batch:   348/ 1052, ite: 176560] train loss: 0.004017, tar: 0.000063 
l0: 0.000024, l1: 0.000024, l2: 0.000062, l3: 0.000126, l4: 0.000302, l5: 0.000595, l6: 0.001200
[epoch: 672/1000, batch:   428/ 1052, ite: 176580] train loss: 0.004015, tar: 0.000063 
l0: 0.000053, l1: 0.000050, l2: 0.000139, l3: 0.000325, l4: 0.000637, l5: 0.001845, l6: 0.002586
[epoch: 672/1000, batch:   508/ 1052, ite: 176600] train loss: 0.004016, tar: 0.000063 
l0: 0.000104, l1: 0.000106, l2: 0.000157, l3: 0.000270, l4: 0.000824, l5: 0.001193, l6: 0.003364
[epoch: 672/1000, batch:   588/ 1052, ite: 176620] train loss: 0.004016, tar: 0.000063 
l0: 0.000024, l1: 0.000024, l2: 0.000084, l3: 0.000125, l4: 0.000338, l5: 0.001153, l6: 0.001419
[epoch: 672/1000, batch:   668/ 1052, ite: 176640] train loss: 0.004016, tar: 0.000063 
l0: 0.000134, l1: 0.000136, l2: 0.000173, l3: 0.000284, l4: 0.000735, l5: 0.001283, l6: 0.002553
[epoch: 672/1000, batch:   748/ 1052, ite: 176660] train loss: 0.004016, tar: 0.000063 
l0: 0.000004, l1: 0.000004, l2: 0.000016, l3: 0.000072, l4: 0.000274, l5: 0.000666, l6: 0.001477
[epoch: 672/1000, batch:   828/ 1052, ite: 176680] train loss: 0.004017, tar: 0.000063 
l0: 0.000137, l1: 0.000141, l2: 0.000161, l3: 0.000207, l4: 0.000558, l5: 0.001998, l6: 0.002932
[epoch: 672/1000, batch:   908/ 1052, ite: 176700] train loss: 0.004017, tar: 0.000063 
l0: 0.000074, l1: 0.000074, l2: 0.000113, l3: 0.000168, l4: 0.000387, l5: 0.001363, l6: 0.001625
[epoch: 672/1000, batch:   988/ 1052, ite: 176720] train loss: 0.004016, tar: 0.000063 
[Epoch 672/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000152, l1: 0.000156, l2: 0.000161, l3: 0.000280, l4: 0.000398, l5: 0.001581, l6: 0.002646
[epoch: 673/1000, batch:    16/ 1052, ite: 176740] train loss: 0.004014, tar: 0.000063 
l0: 0.000043, l1: 0.000042, l2: 0.000084, l3: 0.000145, l4: 0.000449, l5: 0.001212, l6: 0.001022
[epoch: 673/1000, batch:    96/ 1052, ite: 176760] train loss: 0.004014, tar: 0.000063 
l0: 0.000079, l1: 0.000078, l2: 0.000140, l3: 0.000221, l4: 0.000447, l5: 0.001106, l6: 0.002361
[epoch: 673/1000, batch:   176/ 1052, ite: 176780] train loss: 0.004013, tar: 0.000063 
l0: 0.000015, l1: 0.000017, l2: 0.000042, l3: 0.000131, l4: 0.000485, l5: 0.001436, l6: 0.001952
[epoch: 673/1000, batch:   256/ 1052, ite: 176800] train loss: 0.004013, tar: 0.000063 
l0: 0.000068, l1: 0.000068, l2: 0.000105, l3: 0.000218, l4: 0.000367, l5: 0.000969, l6: 0.001736
[epoch: 673/1000, batch:   336/ 1052, ite: 176820] train loss: 0.004013, tar: 0.000063 
l0: 0.000197, l1: 0.000201, l2: 0.000217, l3: 0.000394, l4: 0.000859, l5: 0.002550, l6: 0.004291
[epoch: 673/1000, batch:   416/ 1052, ite: 176840] train loss: 0.004013, tar: 0.000063 
l0: 0.000187, l1: 0.000191, l2: 0.000242, l3: 0.000323, l4: 0.000460, l5: 0.001236, l6: 0.002363
[epoch: 673/1000, batch:   496/ 1052, ite: 176860] train loss: 0.004015, tar: 0.000063 
l0: 0.000016, l1: 0.000017, l2: 0.000022, l3: 0.000058, l4: 0.000305, l5: 0.000819, l6: 0.001193
[epoch: 673/1000, batch:   576/ 1052, ite: 176880] train loss: 0.004014, tar: 0.000063 
l0: 0.000145, l1: 0.000149, l2: 0.000203, l3: 0.000328, l4: 0.000559, l5: 0.000862, l6: 0.003397
[epoch: 673/1000, batch:   656/ 1052, ite: 176900] train loss: 0.004014, tar: 0.000063 
l0: 0.000042, l1: 0.000043, l2: 0.000060, l3: 0.000126, l4: 0.000549, l5: 0.000956, l6: 0.001500
[epoch: 673/1000, batch:   736/ 1052, ite: 176920] train loss: 0.004014, tar: 0.000063 
l0: 0.000010, l1: 0.000010, l2: 0.000045, l3: 0.000123, l4: 0.000306, l5: 0.000989, l6: 0.002711
[epoch: 673/1000, batch:   816/ 1052, ite: 176940] train loss: 0.004015, tar: 0.000063 
l0: 0.000038, l1: 0.000040, l2: 0.000043, l3: 0.000107, l4: 0.000291, l5: 0.000933, l6: 0.001448
[epoch: 673/1000, batch:   896/ 1052, ite: 176960] train loss: 0.004015, tar: 0.000063 
l0: 0.000082, l1: 0.000084, l2: 0.000170, l3: 0.000267, l4: 0.000510, l5: 0.001501, l6: 0.003058
[epoch: 673/1000, batch:   976/ 1052, ite: 176980] train loss: 0.004014, tar: 0.000062 
[Epoch 673/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000011, l1: 0.000012, l2: 0.000036, l3: 0.000077, l4: 0.000218, l5: 0.000413, l6: 0.000884
[epoch: 674/1000, batch:     4/ 1052, ite: 177000] train loss: 0.004013, tar: 0.000062 
l0: 0.000023, l1: 0.000023, l2: 0.000087, l3: 0.000201, l4: 0.000413, l5: 0.001061, l6: 0.001570
[epoch: 674/1000, batch:    84/ 1052, ite: 177020] train loss: 0.004013, tar: 0.000062 
l0: 0.000047, l1: 0.000049, l2: 0.000059, l3: 0.000096, l4: 0.000272, l5: 0.000623, l6: 0.001382
[epoch: 674/1000, batch:   164/ 1052, ite: 177040] train loss: 0.004012, tar: 0.000062 
l0: 0.000041, l1: 0.000045, l2: 0.000066, l3: 0.000171, l4: 0.000563, l5: 0.001157, l6: 0.001687
[epoch: 674/1000, batch:   244/ 1052, ite: 177060] train loss: 0.004013, tar: 0.000062 
l0: 0.000158, l1: 0.000155, l2: 0.000155, l3: 0.000280, l4: 0.000608, l5: 0.001199, l6: 0.001927
[epoch: 674/1000, batch:   324/ 1052, ite: 177080] train loss: 0.004014, tar: 0.000062 
l0: 0.000010, l1: 0.000010, l2: 0.000025, l3: 0.000085, l4: 0.000211, l5: 0.000730, l6: 0.001497
[epoch: 674/1000, batch:   404/ 1052, ite: 177100] train loss: 0.004014, tar: 0.000062 
l0: 0.000019, l1: 0.000029, l2: 0.000020, l3: 0.000078, l4: 0.000139, l5: 0.000460, l6: 0.000871
[epoch: 674/1000, batch:   484/ 1052, ite: 177120] train loss: 0.004013, tar: 0.000062 
l0: 0.000090, l1: 0.000092, l2: 0.000126, l3: 0.000166, l4: 0.000298, l5: 0.000590, l6: 0.001376
[epoch: 674/1000, batch:   564/ 1052, ite: 177140] train loss: 0.004012, tar: 0.000062 
l0: 0.000057, l1: 0.000060, l2: 0.000104, l3: 0.000267, l4: 0.000547, l5: 0.001381, l6: 0.002353
[epoch: 674/1000, batch:   644/ 1052, ite: 177160] train loss: 0.004012, tar: 0.000062 
l0: 0.000104, l1: 0.000106, l2: 0.000202, l3: 0.000633, l4: 0.001302, l5: 0.002411, l6: 0.004562
[epoch: 674/1000, batch:   724/ 1052, ite: 177180] train loss: 0.004013, tar: 0.000062 
l0: 0.000101, l1: 0.000100, l2: 0.000176, l3: 0.000411, l4: 0.000736, l5: 0.001449, l6: 0.001968
[epoch: 674/1000, batch:   804/ 1052, ite: 177200] train loss: 0.004014, tar: 0.000062 
l0: 0.000011, l1: 0.000011, l2: 0.000056, l3: 0.000175, l4: 0.000341, l5: 0.000946, l6: 0.001608
[epoch: 674/1000, batch:   884/ 1052, ite: 177220] train loss: 0.004013, tar: 0.000062 
l0: 0.000038, l1: 0.000038, l2: 0.000087, l3: 0.000140, l4: 0.000338, l5: 0.000990, l6: 0.001461
[epoch: 674/1000, batch:   964/ 1052, ite: 177240] train loss: 0.004013, tar: 0.000062 
l0: 0.000154, l1: 0.000155, l2: 0.000200, l3: 0.000306, l4: 0.000610, l5: 0.002096, l6: 0.002842
[epoch: 674/1000, batch:  1044/ 1052, ite: 177260] train loss: 0.004013, tar: 0.000062 
[Epoch 674/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000005, l1: 0.000005, l2: 0.000018, l3: 0.000114, l4: 0.000230, l5: 0.000990, l6: 0.001177
[epoch: 675/1000, batch:    72/ 1052, ite: 177280] train loss: 0.004012, tar: 0.000062 
l0: 0.000086, l1: 0.000086, l2: 0.000139, l3: 0.000234, l4: 0.000468, l5: 0.001437, l6: 0.002310
[epoch: 675/1000, batch:   152/ 1052, ite: 177300] train loss: 0.004014, tar: 0.000062 
l0: 0.000126, l1: 0.000128, l2: 0.000210, l3: 0.000479, l4: 0.000789, l5: 0.001241, l6: 0.003153
[epoch: 675/1000, batch:   232/ 1052, ite: 177320] train loss: 0.004012, tar: 0.000062 
l0: 0.000022, l1: 0.000024, l2: 0.000036, l3: 0.000084, l4: 0.000228, l5: 0.000718, l6: 0.001923
[epoch: 675/1000, batch:   312/ 1052, ite: 177340] train loss: 0.004012, tar: 0.000062 
l0: 0.000039, l1: 0.000039, l2: 0.000083, l3: 0.000301, l4: 0.000941, l5: 0.001860, l6: 0.003312
[epoch: 675/1000, batch:   392/ 1052, ite: 177360] train loss: 0.004013, tar: 0.000062 
l0: 0.000006, l1: 0.000006, l2: 0.000020, l3: 0.000088, l4: 0.000174, l5: 0.000366, l6: 0.001270
[epoch: 675/1000, batch:   472/ 1052, ite: 177380] train loss: 0.004013, tar: 0.000062 
l0: 0.000088, l1: 0.000088, l2: 0.000138, l3: 0.000402, l4: 0.000811, l5: 0.001454, l6: 0.002451
[epoch: 675/1000, batch:   552/ 1052, ite: 177400] train loss: 0.004013, tar: 0.000062 
l0: 0.000036, l1: 0.000036, l2: 0.000101, l3: 0.000378, l4: 0.000954, l5: 0.001793, l6: 0.001881
[epoch: 675/1000, batch:   632/ 1052, ite: 177420] train loss: 0.004013, tar: 0.000062 
l0: 0.000165, l1: 0.000167, l2: 0.000184, l3: 0.000307, l4: 0.000552, l5: 0.001073, l6: 0.003453
[epoch: 675/1000, batch:   712/ 1052, ite: 177440] train loss: 0.004012, tar: 0.000062 
l0: 0.000029, l1: 0.000028, l2: 0.000057, l3: 0.000136, l4: 0.000278, l5: 0.000682, l6: 0.001856
[epoch: 675/1000, batch:   792/ 1052, ite: 177460] train loss: 0.004012, tar: 0.000062 
l0: 0.000147, l1: 0.000148, l2: 0.000190, l3: 0.000263, l4: 0.000437, l5: 0.001361, l6: 0.002761
[epoch: 675/1000, batch:   872/ 1052, ite: 177480] train loss: 0.004012, tar: 0.000062 
l0: 0.000045, l1: 0.000046, l2: 0.000057, l3: 0.000090, l4: 0.000305, l5: 0.001028, l6: 0.001486
[epoch: 675/1000, batch:   952/ 1052, ite: 177500] train loss: 0.004012, tar: 0.000062 
l0: 0.000011, l1: 0.000012, l2: 0.000044, l3: 0.000128, l4: 0.000344, l5: 0.000847, l6: 0.001937
[epoch: 675/1000, batch:  1032/ 1052, ite: 177520] train loss: 0.004013, tar: 0.000062 
[Epoch 675/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000043, l1: 0.000044, l2: 0.000056, l3: 0.000209, l4: 0.000536, l5: 0.000888, l6: 0.001873
[epoch: 676/1000, batch:    60/ 1052, ite: 177540] train loss: 0.004013, tar: 0.000062 
l0: 0.000019, l1: 0.000019, l2: 0.000043, l3: 0.000094, l4: 0.000451, l5: 0.000960, l6: 0.002215
[epoch: 676/1000, batch:   140/ 1052, ite: 177560] train loss: 0.004011, tar: 0.000062 
l0: 0.000065, l1: 0.000065, l2: 0.000110, l3: 0.000178, l4: 0.000454, l5: 0.000962, l6: 0.001833
[epoch: 676/1000, batch:   220/ 1052, ite: 177580] train loss: 0.004011, tar: 0.000062 
l0: 0.000090, l1: 0.000090, l2: 0.000123, l3: 0.000227, l4: 0.000694, l5: 0.002612, l6: 0.004722
[epoch: 676/1000, batch:   300/ 1052, ite: 177600] train loss: 0.004013, tar: 0.000062 
l0: 0.000036, l1: 0.000037, l2: 0.000081, l3: 0.000189, l4: 0.000354, l5: 0.001269, l6: 0.002091
[epoch: 676/1000, batch:   380/ 1052, ite: 177620] train loss: 0.004013, tar: 0.000062 
l0: 0.000026, l1: 0.000027, l2: 0.000039, l3: 0.000055, l4: 0.000314, l5: 0.000784, l6: 0.000972
[epoch: 676/1000, batch:   460/ 1052, ite: 177640] train loss: 0.004013, tar: 0.000062 
l0: 0.000047, l1: 0.000048, l2: 0.000093, l3: 0.000356, l4: 0.000894, l5: 0.001251, l6: 0.002349
[epoch: 676/1000, batch:   540/ 1052, ite: 177660] train loss: 0.004014, tar: 0.000062 
l0: 0.000039, l1: 0.000040, l2: 0.000072, l3: 0.000213, l4: 0.000480, l5: 0.001179, l6: 0.001561
[epoch: 676/1000, batch:   620/ 1052, ite: 177680] train loss: 0.004015, tar: 0.000062 
l0: 0.000021, l1: 0.000022, l2: 0.000048, l3: 0.000195, l4: 0.000426, l5: 0.000836, l6: 0.001376
[epoch: 676/1000, batch:   700/ 1052, ite: 177700] train loss: 0.004013, tar: 0.000062 
l0: 0.000108, l1: 0.000110, l2: 0.000151, l3: 0.000301, l4: 0.000560, l5: 0.001698, l6: 0.003241
[epoch: 676/1000, batch:   780/ 1052, ite: 177720] train loss: 0.004012, tar: 0.000062 
l0: 0.000009, l1: 0.000010, l2: 0.000021, l3: 0.000069, l4: 0.000203, l5: 0.000674, l6: 0.000857
[epoch: 676/1000, batch:   860/ 1052, ite: 177740] train loss: 0.004011, tar: 0.000062 
l0: 0.000005, l1: 0.000005, l2: 0.000021, l3: 0.000067, l4: 0.000270, l5: 0.000715, l6: 0.000873
[epoch: 676/1000, batch:   940/ 1052, ite: 177760] train loss: 0.004011, tar: 0.000062 
l0: 0.000071, l1: 0.000076, l2: 0.000064, l3: 0.000121, l4: 0.000480, l5: 0.000416, l6: 0.000935
[epoch: 676/1000, batch:  1020/ 1052, ite: 177780] train loss: 0.004012, tar: 0.000062 
[Epoch 676/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000071, l1: 0.000074, l2: 0.000095, l3: 0.000201, l4: 0.000553, l5: 0.001099, l6: 0.001765
[epoch: 677/1000, batch:    48/ 1052, ite: 177800] train loss: 0.004013, tar: 0.000062 
l0: 0.000215, l1: 0.000212, l2: 0.000271, l3: 0.000466, l4: 0.001027, l5: 0.002108, l6: 0.003899
[epoch: 677/1000, batch:   128/ 1052, ite: 177820] train loss: 0.004013, tar: 0.000062 
l0: 0.000032, l1: 0.000031, l2: 0.000129, l3: 0.000289, l4: 0.000700, l5: 0.001688, l6: 0.003075
[epoch: 677/1000, batch:   208/ 1052, ite: 177840] train loss: 0.004013, tar: 0.000062 
l0: 0.000009, l1: 0.000008, l2: 0.000054, l3: 0.000267, l4: 0.000580, l5: 0.001671, l6: 0.002835
[epoch: 677/1000, batch:   288/ 1052, ite: 177860] train loss: 0.004014, tar: 0.000062 
l0: 0.000020, l1: 0.000019, l2: 0.000051, l3: 0.000170, l4: 0.000496, l5: 0.000875, l6: 0.001645
[epoch: 677/1000, batch:   368/ 1052, ite: 177880] train loss: 0.004014, tar: 0.000062 
l0: 0.000014, l1: 0.000015, l2: 0.000035, l3: 0.000118, l4: 0.000363, l5: 0.001387, l6: 0.001729
[epoch: 677/1000, batch:   448/ 1052, ite: 177900] train loss: 0.004013, tar: 0.000062 
l0: 0.000118, l1: 0.000122, l2: 0.000123, l3: 0.000170, l4: 0.000345, l5: 0.001171, l6: 0.001987
[epoch: 677/1000, batch:   528/ 1052, ite: 177920] train loss: 0.004013, tar: 0.000062 
l0: 0.000129, l1: 0.000131, l2: 0.000157, l3: 0.000272, l4: 0.000622, l5: 0.001039, l6: 0.002101
[epoch: 677/1000, batch:   608/ 1052, ite: 177940] train loss: 0.004015, tar: 0.000062 
l0: 0.000009, l1: 0.000008, l2: 0.000033, l3: 0.000125, l4: 0.000301, l5: 0.001228, l6: 0.001652
[epoch: 677/1000, batch:   688/ 1052, ite: 177960] train loss: 0.004015, tar: 0.000062 
l0: 0.000016, l1: 0.000016, l2: 0.000058, l3: 0.000279, l4: 0.000775, l5: 0.001697, l6: 0.002430
[epoch: 677/1000, batch:   768/ 1052, ite: 177980] train loss: 0.004014, tar: 0.000062 
l0: 0.000084, l1: 0.000083, l2: 0.000140, l3: 0.000260, l4: 0.000704, l5: 0.001143, l6: 0.002408
[epoch: 677/1000, batch:   848/ 1052, ite: 178000] train loss: 0.004013, tar: 0.000062 
l0: 0.000074, l1: 0.000074, l2: 0.000110, l3: 0.000226, l4: 0.000464, l5: 0.000819, l6: 0.001477
[epoch: 677/1000, batch:   928/ 1052, ite: 178020] train loss: 0.004012, tar: 0.000062 
l0: 0.000059, l1: 0.000063, l2: 0.000068, l3: 0.000152, l4: 0.000500, l5: 0.001049, l6: 0.001873
[epoch: 677/1000, batch:  1008/ 1052, ite: 178040] train loss: 0.004012, tar: 0.000062 
[Epoch 677/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000053, l1: 0.000055, l2: 0.000085, l3: 0.000183, l4: 0.000469, l5: 0.000830, l6: 0.001703
[epoch: 678/1000, batch:    36/ 1052, ite: 178060] train loss: 0.004011, tar: 0.000062 
l0: 0.000047, l1: 0.000048, l2: 0.000072, l3: 0.000145, l4: 0.000534, l5: 0.000456, l6: 0.001851
[epoch: 678/1000, batch:   116/ 1052, ite: 178080] train loss: 0.004011, tar: 0.000062 
l0: 0.000065, l1: 0.000068, l2: 0.000074, l3: 0.000170, l4: 0.000350, l5: 0.000772, l6: 0.001348
[epoch: 678/1000, batch:   196/ 1052, ite: 178100] train loss: 0.004011, tar: 0.000062 
l0: 0.000039, l1: 0.000036, l2: 0.000086, l3: 0.000200, l4: 0.000479, l5: 0.001502, l6: 0.002877
[epoch: 678/1000, batch:   276/ 1052, ite: 178120] train loss: 0.004011, tar: 0.000062 
l0: 0.000065, l1: 0.000070, l2: 0.000065, l3: 0.000122, l4: 0.000383, l5: 0.000827, l6: 0.001522
[epoch: 678/1000, batch:   356/ 1052, ite: 178140] train loss: 0.004012, tar: 0.000062 
l0: 0.000062, l1: 0.000065, l2: 0.000142, l3: 0.000309, l4: 0.000660, l5: 0.002037, l6: 0.002710
[epoch: 678/1000, batch:   436/ 1052, ite: 178160] train loss: 0.004011, tar: 0.000062 
l0: 0.000118, l1: 0.000117, l2: 0.000224, l3: 0.000439, l4: 0.000978, l5: 0.001662, l6: 0.003338
[epoch: 678/1000, batch:   516/ 1052, ite: 178180] train loss: 0.004012, tar: 0.000062 
l0: 0.000028, l1: 0.000030, l2: 0.000036, l3: 0.000128, l4: 0.000213, l5: 0.000885, l6: 0.001495
[epoch: 678/1000, batch:   596/ 1052, ite: 178200] train loss: 0.004012, tar: 0.000062 
l0: 0.000011, l1: 0.000012, l2: 0.000034, l3: 0.000210, l4: 0.000477, l5: 0.001000, l6: 0.001110
[epoch: 678/1000, batch:   676/ 1052, ite: 178220] train loss: 0.004012, tar: 0.000062 
l0: 0.000060, l1: 0.000061, l2: 0.000100, l3: 0.000153, l4: 0.000379, l5: 0.001201, l6: 0.001434
[epoch: 678/1000, batch:   756/ 1052, ite: 178240] train loss: 0.004012, tar: 0.000062 
l0: 0.000036, l1: 0.000035, l2: 0.000087, l3: 0.000182, l4: 0.000510, l5: 0.001369, l6: 0.002355
[epoch: 678/1000, batch:   836/ 1052, ite: 178260] train loss: 0.004012, tar: 0.000062 
l0: 0.000020, l1: 0.000020, l2: 0.000032, l3: 0.000113, l4: 0.000476, l5: 0.001126, l6: 0.001428
[epoch: 678/1000, batch:   916/ 1052, ite: 178280] train loss: 0.004012, tar: 0.000062 
l0: 0.000014, l1: 0.000016, l2: 0.000092, l3: 0.000158, l4: 0.000348, l5: 0.000714, l6: 0.002073
[epoch: 678/1000, batch:   996/ 1052, ite: 178300] train loss: 0.004013, tar: 0.000062 
[Epoch 678/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000024, l1: 0.000025, l2: 0.000034, l3: 0.000077, l4: 0.000393, l5: 0.000894, l6: 0.001669
[epoch: 679/1000, batch:    24/ 1052, ite: 178320] train loss: 0.004012, tar: 0.000062 
l0: 0.000117, l1: 0.000122, l2: 0.000134, l3: 0.000207, l4: 0.000449, l5: 0.001448, l6: 0.003042
[epoch: 679/1000, batch:   104/ 1052, ite: 178340] train loss: 0.004013, tar: 0.000062 
l0: 0.000079, l1: 0.000080, l2: 0.000094, l3: 0.000254, l4: 0.000887, l5: 0.001691, l6: 0.002516
[epoch: 679/1000, batch:   184/ 1052, ite: 178360] train loss: 0.004014, tar: 0.000062 
l0: 0.000078, l1: 0.000078, l2: 0.000120, l3: 0.000189, l4: 0.000649, l5: 0.000952, l6: 0.002016
[epoch: 679/1000, batch:   264/ 1052, ite: 178380] train loss: 0.004014, tar: 0.000062 
l0: 0.000005, l1: 0.000006, l2: 0.000027, l3: 0.000096, l4: 0.000185, l5: 0.000593, l6: 0.000648
[epoch: 679/1000, batch:   344/ 1052, ite: 178400] train loss: 0.004014, tar: 0.000062 
l0: 0.000007, l1: 0.000006, l2: 0.000048, l3: 0.000132, l4: 0.000359, l5: 0.000894, l6: 0.001937
[epoch: 679/1000, batch:   424/ 1052, ite: 178420] train loss: 0.004013, tar: 0.000062 
l0: 0.000081, l1: 0.000083, l2: 0.000104, l3: 0.000250, l4: 0.000569, l5: 0.001436, l6: 0.001528
[epoch: 679/1000, batch:   504/ 1052, ite: 178440] train loss: 0.004013, tar: 0.000062 
l0: 0.000088, l1: 0.000092, l2: 0.000139, l3: 0.000280, l4: 0.000572, l5: 0.001489, l6: 0.002621
[epoch: 679/1000, batch:   584/ 1052, ite: 178460] train loss: 0.004013, tar: 0.000062 
l0: 0.000129, l1: 0.000128, l2: 0.000164, l3: 0.000388, l4: 0.001127, l5: 0.002396, l6: 0.002729
[epoch: 679/1000, batch:   664/ 1052, ite: 178480] train loss: 0.004013, tar: 0.000062 
l0: 0.000014, l1: 0.000012, l2: 0.000044, l3: 0.000114, l4: 0.000515, l5: 0.001010, l6: 0.001939
[epoch: 679/1000, batch:   744/ 1052, ite: 178500] train loss: 0.004013, tar: 0.000062 
l0: 0.000261, l1: 0.000266, l2: 0.000271, l3: 0.000379, l4: 0.000521, l5: 0.001173, l6: 0.002032
[epoch: 679/1000, batch:   824/ 1052, ite: 178520] train loss: 0.004013, tar: 0.000062 
l0: 0.000091, l1: 0.000095, l2: 0.000125, l3: 0.000235, l4: 0.000588, l5: 0.000935, l6: 0.001364
[epoch: 679/1000, batch:   904/ 1052, ite: 178540] train loss: 0.004013, tar: 0.000062 
l0: 0.000111, l1: 0.000116, l2: 0.000152, l3: 0.000311, l4: 0.000914, l5: 0.002013, l6: 0.003628
[epoch: 679/1000, batch:   984/ 1052, ite: 178560] train loss: 0.004013, tar: 0.000062 
[Epoch 679/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000036, l1: 0.000042, l2: 0.000055, l3: 0.000196, l4: 0.000506, l5: 0.001655, l6: 0.002498
[epoch: 680/1000, batch:    12/ 1052, ite: 178580] train loss: 0.004012, tar: 0.000062 
l0: 0.000121, l1: 0.000122, l2: 0.000158, l3: 0.000342, l4: 0.000701, l5: 0.001226, l6: 0.002332
[epoch: 680/1000, batch:    92/ 1052, ite: 178600] train loss: 0.004012, tar: 0.000062 
l0: 0.000016, l1: 0.000017, l2: 0.000023, l3: 0.000045, l4: 0.000079, l5: 0.000616, l6: 0.000780
[epoch: 680/1000, batch:   172/ 1052, ite: 178620] train loss: 0.004012, tar: 0.000062 
l0: 0.000045, l1: 0.000043, l2: 0.000089, l3: 0.000371, l4: 0.000972, l5: 0.001316, l6: 0.002463
[epoch: 680/1000, batch:   252/ 1052, ite: 178640] train loss: 0.004012, tar: 0.000062 
l0: 0.000084, l1: 0.000084, l2: 0.000149, l3: 0.000339, l4: 0.000626, l5: 0.001292, l6: 0.001982
[epoch: 680/1000, batch:   332/ 1052, ite: 178660] train loss: 0.004012, tar: 0.000062 
l0: 0.000022, l1: 0.000021, l2: 0.000067, l3: 0.000208, l4: 0.000705, l5: 0.001499, l6: 0.003281
[epoch: 680/1000, batch:   412/ 1052, ite: 178680] train loss: 0.004012, tar: 0.000062 
l0: 0.000045, l1: 0.000047, l2: 0.000066, l3: 0.000140, l4: 0.000351, l5: 0.001343, l6: 0.002047
[epoch: 680/1000, batch:   492/ 1052, ite: 178700] train loss: 0.004011, tar: 0.000062 
l0: 0.000006, l1: 0.000005, l2: 0.000035, l3: 0.000147, l4: 0.000389, l5: 0.000812, l6: 0.001779
[epoch: 680/1000, batch:   572/ 1052, ite: 178720] train loss: 0.004010, tar: 0.000062 
l0: 0.000071, l1: 0.000072, l2: 0.000118, l3: 0.000223, l4: 0.000656, l5: 0.001394, l6: 0.001661
[epoch: 680/1000, batch:   652/ 1052, ite: 178740] train loss: 0.004010, tar: 0.000062 
l0: 0.000018, l1: 0.000018, l2: 0.000070, l3: 0.000249, l4: 0.000750, l5: 0.001770, l6: 0.003103
[epoch: 680/1000, batch:   732/ 1052, ite: 178760] train loss: 0.004009, tar: 0.000062 
l0: 0.000133, l1: 0.000134, l2: 0.000184, l3: 0.000388, l4: 0.000663, l5: 0.001110, l6: 0.003293
[epoch: 680/1000, batch:   812/ 1052, ite: 178780] train loss: 0.004010, tar: 0.000062 
l0: 0.000149, l1: 0.000158, l2: 0.000182, l3: 0.000240, l4: 0.000510, l5: 0.001420, l6: 0.003690
[epoch: 680/1000, batch:   892/ 1052, ite: 178800] train loss: 0.004010, tar: 0.000062 
l0: 0.000058, l1: 0.000059, l2: 0.000081, l3: 0.000176, l4: 0.000687, l5: 0.000831, l6: 0.001984
[epoch: 680/1000, batch:   972/ 1052, ite: 178820] train loss: 0.004010, tar: 0.000062 
l0: 0.000011, l1: 0.000011, l2: 0.000038, l3: 0.000191, l4: 0.000556, l5: 0.001236, l6: 0.002362
[epoch: 680/1000, batch:  1052/ 1052, ite: 178840] train loss: 0.004011, tar: 0.000062 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_178840.pkl
[Epoch 680/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000059, l1: 0.000058, l2: 0.000108, l3: 0.000152, l4: 0.000458, l5: 0.000988, l6: 0.001303
[epoch: 681/1000, batch:    80/ 1052, ite: 178860] train loss: 0.004189, tar: 0.000062 
l0: 0.000090, l1: 0.000089, l2: 0.000141, l3: 0.000258, l4: 0.000445, l5: 0.000910, l6: 0.001846
[epoch: 681/1000, batch:   160/ 1052, ite: 178880] train loss: 0.004212, tar: 0.000064 
l0: 0.000054, l1: 0.000057, l2: 0.000069, l3: 0.000211, l4: 0.000464, l5: 0.001357, l6: 0.002756
[epoch: 681/1000, batch:   240/ 1052, ite: 178900] train loss: 0.004118, tar: 0.000056 
l0: 0.000030, l1: 0.000031, l2: 0.000067, l3: 0.000182, l4: 0.000349, l5: 0.001330, l6: 0.001407
[epoch: 681/1000, batch:   320/ 1052, ite: 178920] train loss: 0.004033, tar: 0.000056 
l0: 0.000160, l1: 0.000158, l2: 0.000201, l3: 0.000307, l4: 0.000637, l5: 0.001296, l6: 0.002821
[epoch: 681/1000, batch:   400/ 1052, ite: 178940] train loss: 0.004193, tar: 0.000060 
l0: 0.000040, l1: 0.000041, l2: 0.000057, l3: 0.000166, l4: 0.000387, l5: 0.001025, l6: 0.003069
[epoch: 681/1000, batch:   480/ 1052, ite: 178960] train loss: 0.004192, tar: 0.000059 
l0: 0.000067, l1: 0.000072, l2: 0.000096, l3: 0.000167, l4: 0.000298, l5: 0.001088, l6: 0.001443
[epoch: 681/1000, batch:   560/ 1052, ite: 178980] train loss: 0.004136, tar: 0.000058 
l0: 0.000025, l1: 0.000027, l2: 0.000052, l3: 0.000109, l4: 0.000209, l5: 0.000681, l6: 0.000716
[epoch: 681/1000, batch:   640/ 1052, ite: 179000] train loss: 0.004062, tar: 0.000057 
l0: 0.000022, l1: 0.000021, l2: 0.000079, l3: 0.000221, l4: 0.000529, l5: 0.001337, l6: 0.001186
[epoch: 681/1000, batch:   720/ 1052, ite: 179020] train loss: 0.004050, tar: 0.000058 
l0: 0.000037, l1: 0.000037, l2: 0.000053, l3: 0.000071, l4: 0.000278, l5: 0.000662, l6: 0.001294
[epoch: 681/1000, batch:   800/ 1052, ite: 179040] train loss: 0.003975, tar: 0.000056 
l0: 0.000008, l1: 0.000008, l2: 0.000010, l3: 0.000037, l4: 0.000135, l5: 0.000303, l6: 0.000496
[epoch: 681/1000, batch:   880/ 1052, ite: 179060] train loss: 0.003963, tar: 0.000055 
l0: 0.000047, l1: 0.000046, l2: 0.000066, l3: 0.000125, l4: 0.000347, l5: 0.000742, l6: 0.001352
[epoch: 681/1000, batch:   960/ 1052, ite: 179080] train loss: 0.003984, tar: 0.000058 
l0: 0.000016, l1: 0.000016, l2: 0.000022, l3: 0.000081, l4: 0.000254, l5: 0.000679, l6: 0.001647
[epoch: 681/1000, batch:  1040/ 1052, ite: 179100] train loss: 0.003921, tar: 0.000057 
[Epoch 681/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000040, l1: 0.000041, l2: 0.000078, l3: 0.000263, l4: 0.000478, l5: 0.001372, l6: 0.001996
[epoch: 682/1000, batch:    68/ 1052, ite: 179120] train loss: 0.003924, tar: 0.000058 
l0: 0.000087, l1: 0.000085, l2: 0.000235, l3: 0.000439, l4: 0.000847, l5: 0.002151, l6: 0.004187
[epoch: 682/1000, batch:   148/ 1052, ite: 179140] train loss: 0.003938, tar: 0.000058 
l0: 0.000039, l1: 0.000040, l2: 0.000053, l3: 0.000266, l4: 0.000760, l5: 0.002303, l6: 0.003398
[epoch: 682/1000, batch:   228/ 1052, ite: 179160] train loss: 0.003936, tar: 0.000059 
l0: 0.000041, l1: 0.000040, l2: 0.000079, l3: 0.000146, l4: 0.000365, l5: 0.000625, l6: 0.001090
[epoch: 682/1000, batch:   308/ 1052, ite: 179180] train loss: 0.003923, tar: 0.000058 
l0: 0.000051, l1: 0.000052, l2: 0.000075, l3: 0.000126, l4: 0.000349, l5: 0.001270, l6: 0.002049
[epoch: 682/1000, batch:   388/ 1052, ite: 179200] train loss: 0.003941, tar: 0.000058 
l0: 0.000016, l1: 0.000016, l2: 0.000056, l3: 0.000130, l4: 0.000321, l5: 0.000771, l6: 0.001284
[epoch: 682/1000, batch:   468/ 1052, ite: 179220] train loss: 0.003938, tar: 0.000058 
l0: 0.000040, l1: 0.000041, l2: 0.000039, l3: 0.000111, l4: 0.000238, l5: 0.001003, l6: 0.001399
[epoch: 682/1000, batch:   548/ 1052, ite: 179240] train loss: 0.003908, tar: 0.000058 
l0: 0.000040, l1: 0.000042, l2: 0.000066, l3: 0.000293, l4: 0.000544, l5: 0.001306, l6: 0.002009
[epoch: 682/1000, batch:   628/ 1052, ite: 179260] train loss: 0.003934, tar: 0.000058 
l0: 0.000055, l1: 0.000054, l2: 0.000149, l3: 0.000397, l4: 0.000850, l5: 0.001434, l6: 0.002681
[epoch: 682/1000, batch:   708/ 1052, ite: 179280] train loss: 0.003931, tar: 0.000058 
l0: 0.000088, l1: 0.000092, l2: 0.000140, l3: 0.000268, l4: 0.000494, l5: 0.001211, l6: 0.001771
[epoch: 682/1000, batch:   788/ 1052, ite: 179300] train loss: 0.003932, tar: 0.000058 
l0: 0.000020, l1: 0.000019, l2: 0.000067, l3: 0.000142, l4: 0.000246, l5: 0.000819, l6: 0.001753
[epoch: 682/1000, batch:   868/ 1052, ite: 179320] train loss: 0.003943, tar: 0.000058 
l0: 0.000014, l1: 0.000015, l2: 0.000042, l3: 0.000139, l4: 0.000325, l5: 0.000991, l6: 0.001323
[epoch: 682/1000, batch:   948/ 1052, ite: 179340] train loss: 0.003952, tar: 0.000058 
l0: 0.000028, l1: 0.000027, l2: 0.000086, l3: 0.000222, l4: 0.000587, l5: 0.001259, l6: 0.002685
[epoch: 682/1000, batch:  1028/ 1052, ite: 179360] train loss: 0.003981, tar: 0.000058 
[Epoch 682/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000183, l1: 0.000181, l2: 0.000246, l3: 0.000414, l4: 0.001078, l5: 0.002272, l6: 0.003697
[epoch: 683/1000, batch:    56/ 1052, ite: 179380] train loss: 0.003987, tar: 0.000059 
l0: 0.000036, l1: 0.000039, l2: 0.000069, l3: 0.000264, l4: 0.000505, l5: 0.001348, l6: 0.001719
[epoch: 683/1000, batch:   136/ 1052, ite: 179400] train loss: 0.003986, tar: 0.000058 
l0: 0.000163, l1: 0.000160, l2: 0.000248, l3: 0.000377, l4: 0.000855, l5: 0.001249, l6: 0.001797
[epoch: 683/1000, batch:   216/ 1052, ite: 179420] train loss: 0.003992, tar: 0.000059 
l0: 0.000016, l1: 0.000017, l2: 0.000035, l3: 0.000107, l4: 0.000267, l5: 0.000905, l6: 0.001555
[epoch: 683/1000, batch:   296/ 1052, ite: 179440] train loss: 0.003990, tar: 0.000059 
l0: 0.000028, l1: 0.000029, l2: 0.000038, l3: 0.000098, l4: 0.000460, l5: 0.000950, l6: 0.001227
[epoch: 683/1000, batch:   376/ 1052, ite: 179460] train loss: 0.003982, tar: 0.000059 
l0: 0.000117, l1: 0.000118, l2: 0.000167, l3: 0.000267, l4: 0.000637, l5: 0.001236, l6: 0.003047
[epoch: 683/1000, batch:   456/ 1052, ite: 179480] train loss: 0.003980, tar: 0.000059 
l0: 0.000059, l1: 0.000061, l2: 0.000070, l3: 0.000133, l4: 0.000389, l5: 0.000588, l6: 0.001732
[epoch: 683/1000, batch:   536/ 1052, ite: 179500] train loss: 0.003971, tar: 0.000058 
l0: 0.000056, l1: 0.000058, l2: 0.000099, l3: 0.000200, l4: 0.000391, l5: 0.000788, l6: 0.002699
[epoch: 683/1000, batch:   616/ 1052, ite: 179520] train loss: 0.003969, tar: 0.000058 
l0: 0.000055, l1: 0.000053, l2: 0.000073, l3: 0.000154, l4: 0.000226, l5: 0.000934, l6: 0.002434
[epoch: 683/1000, batch:   696/ 1052, ite: 179540] train loss: 0.003965, tar: 0.000058 
l0: 0.000010, l1: 0.000009, l2: 0.000052, l3: 0.000121, l4: 0.000425, l5: 0.001506, l6: 0.001922
[epoch: 683/1000, batch:   776/ 1052, ite: 179560] train loss: 0.003970, tar: 0.000058 
l0: 0.000105, l1: 0.000108, l2: 0.000145, l3: 0.000232, l4: 0.000377, l5: 0.001471, l6: 0.002537
[epoch: 683/1000, batch:   856/ 1052, ite: 179580] train loss: 0.003966, tar: 0.000058 
l0: 0.000103, l1: 0.000104, l2: 0.000206, l3: 0.000331, l4: 0.000565, l5: 0.001153, l6: 0.003169
[epoch: 683/1000, batch:   936/ 1052, ite: 179600] train loss: 0.003990, tar: 0.000058 
l0: 0.000061, l1: 0.000060, l2: 0.000085, l3: 0.000207, l4: 0.000271, l5: 0.000730, l6: 0.001643
[epoch: 683/1000, batch:  1016/ 1052, ite: 179620] train loss: 0.003989, tar: 0.000058 
[Epoch 683/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000034, l1: 0.000034, l2: 0.000039, l3: 0.000087, l4: 0.000215, l5: 0.000767, l6: 0.000880
[epoch: 684/1000, batch:    44/ 1052, ite: 179640] train loss: 0.003983, tar: 0.000058 
l0: 0.000035, l1: 0.000038, l2: 0.000064, l3: 0.000135, l4: 0.000500, l5: 0.001147, l6: 0.002021
[epoch: 684/1000, batch:   124/ 1052, ite: 179660] train loss: 0.003984, tar: 0.000058 
l0: 0.000021, l1: 0.000020, l2: 0.000049, l3: 0.000224, l4: 0.000359, l5: 0.000774, l6: 0.001164
[epoch: 684/1000, batch:   204/ 1052, ite: 179680] train loss: 0.003983, tar: 0.000058 
l0: 0.000015, l1: 0.000015, l2: 0.000031, l3: 0.000148, l4: 0.000223, l5: 0.000762, l6: 0.000745
[epoch: 684/1000, batch:   284/ 1052, ite: 179700] train loss: 0.003984, tar: 0.000058 
l0: 0.000071, l1: 0.000075, l2: 0.000118, l3: 0.000330, l4: 0.000537, l5: 0.001079, l6: 0.003753
[epoch: 684/1000, batch:   364/ 1052, ite: 179720] train loss: 0.003986, tar: 0.000058 
l0: 0.000137, l1: 0.000137, l2: 0.000154, l3: 0.000244, l4: 0.000463, l5: 0.001001, l6: 0.002092
[epoch: 684/1000, batch:   444/ 1052, ite: 179740] train loss: 0.003997, tar: 0.000058 
l0: 0.000062, l1: 0.000062, l2: 0.000118, l3: 0.000273, l4: 0.000557, l5: 0.000819, l6: 0.001740
[epoch: 684/1000, batch:   524/ 1052, ite: 179760] train loss: 0.003999, tar: 0.000058 
l0: 0.000016, l1: 0.000016, l2: 0.000034, l3: 0.000097, l4: 0.000264, l5: 0.000794, l6: 0.001385
[epoch: 684/1000, batch:   604/ 1052, ite: 179780] train loss: 0.004005, tar: 0.000058 
l0: 0.000015, l1: 0.000014, l2: 0.000034, l3: 0.000199, l4: 0.000660, l5: 0.000674, l6: 0.001963
[epoch: 684/1000, batch:   684/ 1052, ite: 179800] train loss: 0.003998, tar: 0.000058 
l0: 0.000118, l1: 0.000123, l2: 0.000204, l3: 0.000382, l4: 0.000823, l5: 0.001942, l6: 0.003108
[epoch: 684/1000, batch:   764/ 1052, ite: 179820] train loss: 0.003999, tar: 0.000058 
l0: 0.000009, l1: 0.000009, l2: 0.000017, l3: 0.000094, l4: 0.000324, l5: 0.000487, l6: 0.001238
[epoch: 684/1000, batch:   844/ 1052, ite: 179840] train loss: 0.004003, tar: 0.000058 
l0: 0.000082, l1: 0.000087, l2: 0.000092, l3: 0.000129, l4: 0.000365, l5: 0.001141, l6: 0.002048
[epoch: 684/1000, batch:   924/ 1052, ite: 179860] train loss: 0.003998, tar: 0.000058 
l0: 0.000127, l1: 0.000128, l2: 0.000181, l3: 0.000339, l4: 0.000862, l5: 0.001455, l6: 0.002298
[epoch: 684/1000, batch:  1004/ 1052, ite: 179880] train loss: 0.004004, tar: 0.000058 
[Epoch 684/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000010, l1: 0.000010, l2: 0.000011, l3: 0.000086, l4: 0.000279, l5: 0.000601, l6: 0.001048
[epoch: 685/1000, batch:    32/ 1052, ite: 179900] train loss: 0.003992, tar: 0.000058 
l0: 0.000074, l1: 0.000075, l2: 0.000188, l3: 0.000316, l4: 0.000606, l5: 0.001996, l6: 0.002636
[epoch: 685/1000, batch:   112/ 1052, ite: 179920] train loss: 0.003995, tar: 0.000059 
l0: 0.000018, l1: 0.000016, l2: 0.000066, l3: 0.000216, l4: 0.000653, l5: 0.001210, l6: 0.002390
[epoch: 685/1000, batch:   192/ 1052, ite: 179940] train loss: 0.003997, tar: 0.000059 
l0: 0.000031, l1: 0.000029, l2: 0.000071, l3: 0.000106, l4: 0.000329, l5: 0.000619, l6: 0.001305
[epoch: 685/1000, batch:   272/ 1052, ite: 179960] train loss: 0.004006, tar: 0.000059 
l0: 0.000139, l1: 0.000140, l2: 0.000183, l3: 0.000281, l4: 0.000833, l5: 0.001196, l6: 0.001453
[epoch: 685/1000, batch:   352/ 1052, ite: 179980] train loss: 0.003994, tar: 0.000059 
l0: 0.000182, l1: 0.000186, l2: 0.000233, l3: 0.000386, l4: 0.000685, l5: 0.001507, l6: 0.003704
[epoch: 685/1000, batch:   432/ 1052, ite: 180000] train loss: 0.003991, tar: 0.000059 
l0: 0.000010, l1: 0.000010, l2: 0.000055, l3: 0.000179, l4: 0.000681, l5: 0.001046, l6: 0.001181
[epoch: 685/1000, batch:   512/ 1052, ite: 180020] train loss: 0.003990, tar: 0.000058 
l0: 0.000066, l1: 0.000068, l2: 0.000118, l3: 0.000278, l4: 0.000615, l5: 0.001369, l6: 0.001876
[epoch: 685/1000, batch:   592/ 1052, ite: 180040] train loss: 0.003992, tar: 0.000059 
l0: 0.000197, l1: 0.000199, l2: 0.000256, l3: 0.000482, l4: 0.000707, l5: 0.001235, l6: 0.002077
[epoch: 685/1000, batch:   672/ 1052, ite: 180060] train loss: 0.003996, tar: 0.000059 
l0: 0.000076, l1: 0.000076, l2: 0.000112, l3: 0.000181, l4: 0.000322, l5: 0.000743, l6: 0.002425
[epoch: 685/1000, batch:   752/ 1052, ite: 180080] train loss: 0.003997, tar: 0.000059 
l0: 0.000016, l1: 0.000016, l2: 0.000044, l3: 0.000162, l4: 0.000533, l5: 0.001257, l6: 0.002090
[epoch: 685/1000, batch:   832/ 1052, ite: 180100] train loss: 0.004000, tar: 0.000058 
l0: 0.000089, l1: 0.000091, l2: 0.000103, l3: 0.000176, l4: 0.000354, l5: 0.001070, l6: 0.002149
[epoch: 685/1000, batch:   912/ 1052, ite: 180120] train loss: 0.003990, tar: 0.000059 
l0: 0.000034, l1: 0.000034, l2: 0.000058, l3: 0.000099, l4: 0.000188, l5: 0.000655, l6: 0.001360
[epoch: 685/1000, batch:   992/ 1052, ite: 180140] train loss: 0.003983, tar: 0.000058 
[Epoch 685/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000014, l1: 0.000014, l2: 0.000039, l3: 0.000128, l4: 0.000351, l5: 0.000610, l6: 0.001021
[epoch: 686/1000, batch:    20/ 1052, ite: 180160] train loss: 0.003986, tar: 0.000059 
l0: 0.000004, l1: 0.000005, l2: 0.000007, l3: 0.000042, l4: 0.000217, l5: 0.000727, l6: 0.001260
[epoch: 686/1000, batch:   100/ 1052, ite: 180180] train loss: 0.003985, tar: 0.000059 
l0: 0.000021, l1: 0.000017, l2: 0.000065, l3: 0.000145, l4: 0.000204, l5: 0.000677, l6: 0.001114
[epoch: 686/1000, batch:   180/ 1052, ite: 180200] train loss: 0.003995, tar: 0.000059 
l0: 0.000097, l1: 0.000093, l2: 0.000152, l3: 0.000264, l4: 0.000620, l5: 0.001605, l6: 0.003110
[epoch: 686/1000, batch:   260/ 1052, ite: 180220] train loss: 0.003992, tar: 0.000059 
l0: 0.000049, l1: 0.000050, l2: 0.000073, l3: 0.000179, l4: 0.000405, l5: 0.000796, l6: 0.001576
[epoch: 686/1000, batch:   340/ 1052, ite: 180240] train loss: 0.003987, tar: 0.000059 
l0: 0.000079, l1: 0.000082, l2: 0.000096, l3: 0.000122, l4: 0.000328, l5: 0.001447, l6: 0.001457
[epoch: 686/1000, batch:   420/ 1052, ite: 180260] train loss: 0.003985, tar: 0.000059 
l0: 0.000100, l1: 0.000102, l2: 0.000143, l3: 0.000174, l4: 0.000443, l5: 0.001034, l6: 0.002403
[epoch: 686/1000, batch:   500/ 1052, ite: 180280] train loss: 0.003988, tar: 0.000059 
l0: 0.000031, l1: 0.000032, l2: 0.000045, l3: 0.000104, l4: 0.000285, l5: 0.000635, l6: 0.001360
[epoch: 686/1000, batch:   580/ 1052, ite: 180300] train loss: 0.003984, tar: 0.000059 
l0: 0.000076, l1: 0.000077, l2: 0.000113, l3: 0.000234, l4: 0.000442, l5: 0.000834, l6: 0.002004
[epoch: 686/1000, batch:   660/ 1052, ite: 180320] train loss: 0.003979, tar: 0.000059 
l0: 0.000057, l1: 0.000058, l2: 0.000096, l3: 0.000106, l4: 0.000294, l5: 0.000701, l6: 0.001711
[epoch: 686/1000, batch:   740/ 1052, ite: 180340] train loss: 0.003973, tar: 0.000059 
l0: 0.000079, l1: 0.000077, l2: 0.000096, l3: 0.000229, l4: 0.000534, l5: 0.000993, l6: 0.002068
[epoch: 686/1000, batch:   820/ 1052, ite: 180360] train loss: 0.003984, tar: 0.000059 
l0: 0.000035, l1: 0.000037, l2: 0.000061, l3: 0.000191, l4: 0.000296, l5: 0.000875, l6: 0.002551
[epoch: 686/1000, batch:   900/ 1052, ite: 180380] train loss: 0.003990, tar: 0.000059 
l0: 0.000178, l1: 0.000183, l2: 0.000198, l3: 0.000278, l4: 0.000369, l5: 0.001415, l6: 0.001578
[epoch: 686/1000, batch:   980/ 1052, ite: 180400] train loss: 0.003991, tar: 0.000059 
[Epoch 686/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000004, l1: 0.000004, l2: 0.000014, l3: 0.000097, l4: 0.000326, l5: 0.000491, l6: 0.000858
[epoch: 687/1000, batch:     8/ 1052, ite: 180420] train loss: 0.003990, tar: 0.000059 
l0: 0.000032, l1: 0.000031, l2: 0.000071, l3: 0.000131, l4: 0.000258, l5: 0.000793, l6: 0.001284
[epoch: 687/1000, batch:    88/ 1052, ite: 180440] train loss: 0.003982, tar: 0.000059 
l0: 0.000055, l1: 0.000053, l2: 0.000063, l3: 0.000118, l4: 0.000449, l5: 0.001335, l6: 0.002477
[epoch: 687/1000, batch:   168/ 1052, ite: 180460] train loss: 0.003982, tar: 0.000059 
l0: 0.000070, l1: 0.000070, l2: 0.000097, l3: 0.000209, l4: 0.000582, l5: 0.000953, l6: 0.002124
[epoch: 687/1000, batch:   248/ 1052, ite: 180480] train loss: 0.003982, tar: 0.000059 
l0: 0.000136, l1: 0.000135, l2: 0.000174, l3: 0.000340, l4: 0.000649, l5: 0.001510, l6: 0.001835
[epoch: 687/1000, batch:   328/ 1052, ite: 180500] train loss: 0.003983, tar: 0.000059 
l0: 0.000007, l1: 0.000007, l2: 0.000022, l3: 0.000051, l4: 0.000280, l5: 0.000809, l6: 0.001181
[epoch: 687/1000, batch:   408/ 1052, ite: 180520] train loss: 0.003990, tar: 0.000059 
l0: 0.000096, l1: 0.000097, l2: 0.000110, l3: 0.000165, l4: 0.000471, l5: 0.001021, l6: 0.001809
[epoch: 687/1000, batch:   488/ 1052, ite: 180540] train loss: 0.003991, tar: 0.000059 
l0: 0.000092, l1: 0.000094, l2: 0.000109, l3: 0.000187, l4: 0.000532, l5: 0.000885, l6: 0.001824
[epoch: 687/1000, batch:   568/ 1052, ite: 180560] train loss: 0.003990, tar: 0.000059 
l0: 0.000035, l1: 0.000035, l2: 0.000075, l3: 0.000135, l4: 0.000398, l5: 0.000837, l6: 0.001044
[epoch: 687/1000, batch:   648/ 1052, ite: 180580] train loss: 0.003995, tar: 0.000059 
l0: 0.000054, l1: 0.000054, l2: 0.000090, l3: 0.000178, l4: 0.000436, l5: 0.001049, l6: 0.001560
[epoch: 687/1000, batch:   728/ 1052, ite: 180600] train loss: 0.003991, tar: 0.000059 
l0: 0.000051, l1: 0.000053, l2: 0.000069, l3: 0.000133, l4: 0.000324, l5: 0.001139, l6: 0.001516
[epoch: 687/1000, batch:   808/ 1052, ite: 180620] train loss: 0.003990, tar: 0.000059 
l0: 0.000045, l1: 0.000046, l2: 0.000084, l3: 0.000208, l4: 0.000634, l5: 0.001286, l6: 0.002202
[epoch: 687/1000, batch:   888/ 1052, ite: 180640] train loss: 0.003986, tar: 0.000059 
l0: 0.000060, l1: 0.000060, l2: 0.000088, l3: 0.000119, l4: 0.000339, l5: 0.000794, l6: 0.001410
[epoch: 687/1000, batch:   968/ 1052, ite: 180660] train loss: 0.003984, tar: 0.000059 
l0: 0.000157, l1: 0.000171, l2: 0.000237, l3: 0.000406, l4: 0.000843, l5: 0.002232, l6: 0.003173
[epoch: 687/1000, batch:  1048/ 1052, ite: 180680] train loss: 0.003998, tar: 0.000060 
[Epoch 687/1000] Test loss: 0.016, Test target loss: 0.002
l0: 0.000037, l1: 0.000039, l2: 0.000045, l3: 0.000106, l4: 0.000388, l5: 0.000966, l6: 0.001567
[epoch: 688/1000, batch:    76/ 1052, ite: 180700] train loss: 0.004001, tar: 0.000060 
l0: 0.000062, l1: 0.000064, l2: 0.000120, l3: 0.000151, l4: 0.000240, l5: 0.000456, l6: 0.001835
[epoch: 688/1000, batch:   156/ 1052, ite: 180720] train loss: 0.003998, tar: 0.000060 
l0: 0.000061, l1: 0.000063, l2: 0.000098, l3: 0.000266, l4: 0.000664, l5: 0.001591, l6: 0.002054
[epoch: 688/1000, batch:   236/ 1052, ite: 180740] train loss: 0.004003, tar: 0.000060 
l0: 0.000177, l1: 0.000182, l2: 0.000225, l3: 0.000456, l4: 0.000981, l5: 0.001141, l6: 0.001929
[epoch: 688/1000, batch:   316/ 1052, ite: 180760] train loss: 0.004007, tar: 0.000061 
l0: 0.000026, l1: 0.000029, l2: 0.000080, l3: 0.000163, l4: 0.000351, l5: 0.000636, l6: 0.001917
[epoch: 688/1000, batch:   396/ 1052, ite: 180780] train loss: 0.004003, tar: 0.000061 
l0: 0.000013, l1: 0.000012, l2: 0.000034, l3: 0.000119, l4: 0.000276, l5: 0.000986, l6: 0.001036
[epoch: 688/1000, batch:   476/ 1052, ite: 180800] train loss: 0.004006, tar: 0.000061 
l0: 0.000142, l1: 0.000145, l2: 0.000164, l3: 0.000226, l4: 0.000663, l5: 0.000796, l6: 0.001872
[epoch: 688/1000, batch:   556/ 1052, ite: 180820] train loss: 0.004005, tar: 0.000061 
l0: 0.000065, l1: 0.000064, l2: 0.000103, l3: 0.000207, l4: 0.000443, l5: 0.000922, l6: 0.001438
[epoch: 688/1000, batch:   636/ 1052, ite: 180840] train loss: 0.004004, tar: 0.000061 
l0: 0.000031, l1: 0.000029, l2: 0.000049, l3: 0.000102, l4: 0.000146, l5: 0.000525, l6: 0.001065
[epoch: 688/1000, batch:   716/ 1052, ite: 180860] train loss: 0.004002, tar: 0.000061 
l0: 0.000075, l1: 0.000076, l2: 0.000088, l3: 0.000137, l4: 0.000474, l5: 0.001175, l6: 0.001639
[epoch: 688/1000, batch:   796/ 1052, ite: 180880] train loss: 0.004002, tar: 0.000061 
l0: 0.000066, l1: 0.000063, l2: 0.000111, l3: 0.000170, l4: 0.000312, l5: 0.000517, l6: 0.001173
[epoch: 688/1000, batch:   876/ 1052, ite: 180900] train loss: 0.004002, tar: 0.000061 
l0: 0.000040, l1: 0.000041, l2: 0.000073, l3: 0.000164, l4: 0.000561, l5: 0.000565, l6: 0.001907
[epoch: 688/1000, batch:   956/ 1052, ite: 180920] train loss: 0.004001, tar: 0.000061 
l0: 0.000034, l1: 0.000036, l2: 0.000104, l3: 0.000230, l4: 0.000405, l5: 0.001022, l6: 0.002742
[epoch: 688/1000, batch:  1036/ 1052, ite: 180940] train loss: 0.004001, tar: 0.000061 
[Epoch 688/1000] Test loss: 0.016, Test target loss: 0.002
l0: 0.000102, l1: 0.000109, l2: 0.000157, l3: 0.000315, l4: 0.000697, l5: 0.001344, l6: 0.002437
[epoch: 689/1000, batch:    64/ 1052, ite: 180960] train loss: 0.004001, tar: 0.000061 
l0: 0.000091, l1: 0.000092, l2: 0.000149, l3: 0.000211, l4: 0.000368, l5: 0.001089, l6: 0.001837
[epoch: 689/1000, batch:   144/ 1052, ite: 180980] train loss: 0.004000, tar: 0.000061 
l0: 0.000023, l1: 0.000024, l2: 0.000066, l3: 0.000149, l4: 0.000266, l5: 0.001247, l6: 0.001942
[epoch: 689/1000, batch:   224/ 1052, ite: 181000] train loss: 0.004004, tar: 0.000061 
l0: 0.000065, l1: 0.000063, l2: 0.000112, l3: 0.000263, l4: 0.000562, l5: 0.001314, l6: 0.002486
[epoch: 689/1000, batch:   304/ 1052, ite: 181020] train loss: 0.004003, tar: 0.000061 
l0: 0.000066, l1: 0.000067, l2: 0.000096, l3: 0.000263, l4: 0.000633, l5: 0.000836, l6: 0.001511
[epoch: 689/1000, batch:   384/ 1052, ite: 181040] train loss: 0.004004, tar: 0.000061 
l0: 0.000089, l1: 0.000085, l2: 0.000114, l3: 0.000206, l4: 0.000469, l5: 0.000996, l6: 0.001549
[epoch: 689/1000, batch:   464/ 1052, ite: 181060] train loss: 0.004002, tar: 0.000061 
l0: 0.000005, l1: 0.000006, l2: 0.000030, l3: 0.000124, l4: 0.000310, l5: 0.000908, l6: 0.002512
[epoch: 689/1000, batch:   544/ 1052, ite: 181080] train loss: 0.004003, tar: 0.000060 
l0: 0.000010, l1: 0.000009, l2: 0.000076, l3: 0.000145, l4: 0.000467, l5: 0.001498, l6: 0.001490
[epoch: 689/1000, batch:   624/ 1052, ite: 181100] train loss: 0.004000, tar: 0.000060 
l0: 0.000085, l1: 0.000088, l2: 0.000085, l3: 0.000100, l4: 0.000180, l5: 0.000498, l6: 0.001043
[epoch: 689/1000, batch:   704/ 1052, ite: 181120] train loss: 0.003997, tar: 0.000060 
l0: 0.000046, l1: 0.000045, l2: 0.000119, l3: 0.000420, l4: 0.000820, l5: 0.002430, l6: 0.003447
[epoch: 689/1000, batch:   784/ 1052, ite: 181140] train loss: 0.003999, tar: 0.000060 
l0: 0.000065, l1: 0.000064, l2: 0.000089, l3: 0.000178, l4: 0.000515, l5: 0.000941, l6: 0.001880
[epoch: 689/1000, batch:   864/ 1052, ite: 181160] train loss: 0.004005, tar: 0.000060 
l0: 0.000003, l1: 0.000003, l2: 0.000012, l3: 0.000132, l4: 0.000371, l5: 0.001496, l6: 0.001789
[epoch: 689/1000, batch:   944/ 1052, ite: 181180] train loss: 0.004005, tar: 0.000060 
l0: 0.000029, l1: 0.000031, l2: 0.000043, l3: 0.000081, l4: 0.000281, l5: 0.001166, l6: 0.000860
[epoch: 689/1000, batch:  1024/ 1052, ite: 181200] train loss: 0.004003, tar: 0.000060 
[Epoch 689/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000006, l1: 0.000006, l2: 0.000022, l3: 0.000102, l4: 0.000291, l5: 0.001065, l6: 0.001082
[epoch: 690/1000, batch:    52/ 1052, ite: 181220] train loss: 0.004004, tar: 0.000060 
l0: 0.000017, l1: 0.000015, l2: 0.000061, l3: 0.000161, l4: 0.000529, l5: 0.000916, l6: 0.002051
[epoch: 690/1000, batch:   132/ 1052, ite: 181240] train loss: 0.004004, tar: 0.000060 
l0: 0.000018, l1: 0.000017, l2: 0.000064, l3: 0.000182, l4: 0.000526, l5: 0.001542, l6: 0.002164
[epoch: 690/1000, batch:   212/ 1052, ite: 181260] train loss: 0.003997, tar: 0.000060 
l0: 0.000067, l1: 0.000067, l2: 0.000118, l3: 0.000263, l4: 0.000449, l5: 0.001243, l6: 0.001878
[epoch: 690/1000, batch:   292/ 1052, ite: 181280] train loss: 0.003993, tar: 0.000060 
l0: 0.000171, l1: 0.000168, l2: 0.000188, l3: 0.000324, l4: 0.000720, l5: 0.001369, l6: 0.002552
[epoch: 690/1000, batch:   372/ 1052, ite: 181300] train loss: 0.003992, tar: 0.000060 
l0: 0.000034, l1: 0.000034, l2: 0.000100, l3: 0.000214, l4: 0.000460, l5: 0.001254, l6: 0.002399
[epoch: 690/1000, batch:   452/ 1052, ite: 181320] train loss: 0.003994, tar: 0.000060 
l0: 0.000055, l1: 0.000056, l2: 0.000092, l3: 0.000194, l4: 0.000359, l5: 0.000927, l6: 0.002398
[epoch: 690/1000, batch:   532/ 1052, ite: 181340] train loss: 0.003996, tar: 0.000060 
l0: 0.000052, l1: 0.000053, l2: 0.000104, l3: 0.000222, l4: 0.000574, l5: 0.001396, l6: 0.002340
[epoch: 690/1000, batch:   612/ 1052, ite: 181360] train loss: 0.004000, tar: 0.000060 
l0: 0.000025, l1: 0.000025, l2: 0.000039, l3: 0.000140, l4: 0.000294, l5: 0.000534, l6: 0.001356
[epoch: 690/1000, batch:   692/ 1052, ite: 181380] train loss: 0.003996, tar: 0.000060 
l0: 0.000019, l1: 0.000018, l2: 0.000054, l3: 0.000091, l4: 0.000400, l5: 0.001144, l6: 0.001115
[epoch: 690/1000, batch:   772/ 1052, ite: 181400] train loss: 0.003999, tar: 0.000060 
l0: 0.000030, l1: 0.000030, l2: 0.000064, l3: 0.000228, l4: 0.000599, l5: 0.000995, l6: 0.002148
[epoch: 690/1000, batch:   852/ 1052, ite: 181420] train loss: 0.003996, tar: 0.000060 
l0: 0.000046, l1: 0.000050, l2: 0.000102, l3: 0.000156, l4: 0.000381, l5: 0.000782, l6: 0.001984
[epoch: 690/1000, batch:   932/ 1052, ite: 181440] train loss: 0.003993, tar: 0.000060 
l0: 0.000022, l1: 0.000021, l2: 0.000112, l3: 0.000254, l4: 0.000587, l5: 0.000914, l6: 0.001697
[epoch: 690/1000, batch:  1012/ 1052, ite: 181460] train loss: 0.003996, tar: 0.000060 
[Epoch 690/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000031, l1: 0.000031, l2: 0.000055, l3: 0.000138, l4: 0.000650, l5: 0.001139, l6: 0.001907
[epoch: 691/1000, batch:    40/ 1052, ite: 181480] train loss: 0.003998, tar: 0.000060 
l0: 0.000266, l1: 0.000274, l2: 0.000358, l3: 0.000628, l4: 0.001264, l5: 0.001750, l6: 0.003679
[epoch: 691/1000, batch:   120/ 1052, ite: 181500] train loss: 0.003997, tar: 0.000060 
l0: 0.000167, l1: 0.000167, l2: 0.000176, l3: 0.000206, l4: 0.000502, l5: 0.001020, l6: 0.001659
[epoch: 691/1000, batch:   200/ 1052, ite: 181520] train loss: 0.003999, tar: 0.000060 
l0: 0.000024, l1: 0.000023, l2: 0.000052, l3: 0.000143, l4: 0.000344, l5: 0.000903, l6: 0.001201
[epoch: 691/1000, batch:   280/ 1052, ite: 181540] train loss: 0.003997, tar: 0.000060 
l0: 0.000130, l1: 0.000143, l2: 0.000121, l3: 0.000261, l4: 0.000762, l5: 0.001352, l6: 0.002820
[epoch: 691/1000, batch:   360/ 1052, ite: 181560] train loss: 0.003996, tar: 0.000060 
l0: 0.000029, l1: 0.000029, l2: 0.000048, l3: 0.000165, l4: 0.000332, l5: 0.001211, l6: 0.001746
[epoch: 691/1000, batch:   440/ 1052, ite: 181580] train loss: 0.003998, tar: 0.000060 
l0: 0.000108, l1: 0.000109, l2: 0.000131, l3: 0.000177, l4: 0.000431, l5: 0.001262, l6: 0.003188
[epoch: 691/1000, batch:   520/ 1052, ite: 181600] train loss: 0.003998, tar: 0.000060 
l0: 0.000028, l1: 0.000027, l2: 0.000071, l3: 0.000185, l4: 0.000411, l5: 0.001273, l6: 0.002090
[epoch: 691/1000, batch:   600/ 1052, ite: 181620] train loss: 0.003999, tar: 0.000060 
l0: 0.000020, l1: 0.000019, l2: 0.000057, l3: 0.000104, l4: 0.000407, l5: 0.000781, l6: 0.002003
[epoch: 691/1000, batch:   680/ 1052, ite: 181640] train loss: 0.004000, tar: 0.000060 
l0: 0.000020, l1: 0.000020, l2: 0.000047, l3: 0.000106, l4: 0.000220, l5: 0.000808, l6: 0.001235
[epoch: 691/1000, batch:   760/ 1052, ite: 181660] train loss: 0.003999, tar: 0.000060 
l0: 0.000012, l1: 0.000013, l2: 0.000018, l3: 0.000055, l4: 0.000137, l5: 0.000460, l6: 0.000793
[epoch: 691/1000, batch:   840/ 1052, ite: 181680] train loss: 0.003994, tar: 0.000059 
l0: 0.000048, l1: 0.000047, l2: 0.000078, l3: 0.000182, l4: 0.000448, l5: 0.000890, l6: 0.001597
[epoch: 691/1000, batch:   920/ 1052, ite: 181700] train loss: 0.003998, tar: 0.000059 
l0: 0.000066, l1: 0.000066, l2: 0.000092, l3: 0.000172, l4: 0.000378, l5: 0.000903, l6: 0.002166
[epoch: 691/1000, batch:  1000/ 1052, ite: 181720] train loss: 0.003999, tar: 0.000059 
[Epoch 691/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000016, l1: 0.000016, l2: 0.000060, l3: 0.000224, l4: 0.000501, l5: 0.001193, l6: 0.002195
[epoch: 692/1000, batch:    28/ 1052, ite: 181740] train loss: 0.003999, tar: 0.000059 
l0: 0.000022, l1: 0.000021, l2: 0.000060, l3: 0.000124, l4: 0.000295, l5: 0.000634, l6: 0.001661
[epoch: 692/1000, batch:   108/ 1052, ite: 181760] train loss: 0.004002, tar: 0.000059 
l0: 0.000018, l1: 0.000021, l2: 0.000040, l3: 0.000201, l4: 0.000538, l5: 0.001181, l6: 0.003291
[epoch: 692/1000, batch:   188/ 1052, ite: 181780] train loss: 0.004002, tar: 0.000059 
l0: 0.000013, l1: 0.000013, l2: 0.000027, l3: 0.000111, l4: 0.000412, l5: 0.000707, l6: 0.001357
[epoch: 692/1000, batch:   268/ 1052, ite: 181800] train loss: 0.004001, tar: 0.000059 
l0: 0.000037, l1: 0.000038, l2: 0.000051, l3: 0.000127, l4: 0.000469, l5: 0.000664, l6: 0.001229
[epoch: 692/1000, batch:   348/ 1052, ite: 181820] train loss: 0.004001, tar: 0.000059 
l0: 0.000041, l1: 0.000043, l2: 0.000038, l3: 0.000060, l4: 0.000196, l5: 0.000592, l6: 0.000797
[epoch: 692/1000, batch:   428/ 1052, ite: 181840] train loss: 0.003998, tar: 0.000059 
l0: 0.000068, l1: 0.000069, l2: 0.000079, l3: 0.000130, l4: 0.000428, l5: 0.000808, l6: 0.001507
[epoch: 692/1000, batch:   508/ 1052, ite: 181860] train loss: 0.003999, tar: 0.000059 
l0: 0.000015, l1: 0.000014, l2: 0.000045, l3: 0.000107, l4: 0.000249, l5: 0.000634, l6: 0.000826
[epoch: 692/1000, batch:   588/ 1052, ite: 181880] train loss: 0.003999, tar: 0.000059 
l0: 0.000043, l1: 0.000044, l2: 0.000099, l3: 0.000350, l4: 0.000851, l5: 0.001242, l6: 0.002381
[epoch: 692/1000, batch:   668/ 1052, ite: 181900] train loss: 0.004000, tar: 0.000059 
l0: 0.000026, l1: 0.000026, l2: 0.000046, l3: 0.000145, l4: 0.000428, l5: 0.000907, l6: 0.001324
[epoch: 692/1000, batch:   748/ 1052, ite: 181920] train loss: 0.004005, tar: 0.000059 
l0: 0.000027, l1: 0.000027, l2: 0.000045, l3: 0.000124, l4: 0.000376, l5: 0.000720, l6: 0.001180
[epoch: 692/1000, batch:   828/ 1052, ite: 181940] train loss: 0.004003, tar: 0.000059 
l0: 0.000010, l1: 0.000012, l2: 0.000034, l3: 0.000131, l4: 0.000317, l5: 0.000604, l6: 0.001589
[epoch: 692/1000, batch:   908/ 1052, ite: 181960] train loss: 0.004000, tar: 0.000059 
l0: 0.000097, l1: 0.000097, l2: 0.000113, l3: 0.000164, l4: 0.000586, l5: 0.001014, l6: 0.002408
[epoch: 692/1000, batch:   988/ 1052, ite: 181980] train loss: 0.004003, tar: 0.000059 
[Epoch 692/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000116, l1: 0.000117, l2: 0.000123, l3: 0.000197, l4: 0.000311, l5: 0.001169, l6: 0.001556
[epoch: 693/1000, batch:    16/ 1052, ite: 182000] train loss: 0.003999, tar: 0.000059 
l0: 0.000021, l1: 0.000021, l2: 0.000083, l3: 0.000276, l4: 0.000453, l5: 0.000951, l6: 0.001886
[epoch: 693/1000, batch:    96/ 1052, ite: 182020] train loss: 0.003997, tar: 0.000059 
l0: 0.000026, l1: 0.000025, l2: 0.000065, l3: 0.000159, l4: 0.000481, l5: 0.001027, l6: 0.001958
[epoch: 693/1000, batch:   176/ 1052, ite: 182040] train loss: 0.003997, tar: 0.000059 
l0: 0.000026, l1: 0.000026, l2: 0.000041, l3: 0.000095, l4: 0.000259, l5: 0.000367, l6: 0.001550
[epoch: 693/1000, batch:   256/ 1052, ite: 182060] train loss: 0.004000, tar: 0.000059 
l0: 0.000038, l1: 0.000037, l2: 0.000073, l3: 0.000137, l4: 0.000422, l5: 0.001054, l6: 0.002137
[epoch: 693/1000, batch:   336/ 1052, ite: 182080] train loss: 0.004002, tar: 0.000059 
l0: 0.000060, l1: 0.000061, l2: 0.000070, l3: 0.000136, l4: 0.000302, l5: 0.000851, l6: 0.001028
[epoch: 693/1000, batch:   416/ 1052, ite: 182100] train loss: 0.004001, tar: 0.000059 
l0: 0.000029, l1: 0.000028, l2: 0.000093, l3: 0.000151, l4: 0.000450, l5: 0.001113, l6: 0.002502
[epoch: 693/1000, batch:   496/ 1052, ite: 182120] train loss: 0.004000, tar: 0.000059 
l0: 0.000054, l1: 0.000056, l2: 0.000134, l3: 0.000237, l4: 0.000706, l5: 0.001139, l6: 0.002363
[epoch: 693/1000, batch:   576/ 1052, ite: 182140] train loss: 0.003999, tar: 0.000059 
l0: 0.000004, l1: 0.000004, l2: 0.000034, l3: 0.000127, l4: 0.000535, l5: 0.000770, l6: 0.001322
[epoch: 693/1000, batch:   656/ 1052, ite: 182160] train loss: 0.003996, tar: 0.000059 
l0: 0.000103, l1: 0.000103, l2: 0.000145, l3: 0.000212, l4: 0.000351, l5: 0.001227, l6: 0.002638
[epoch: 693/1000, batch:   736/ 1052, ite: 182180] train loss: 0.003997, tar: 0.000059 
l0: 0.000124, l1: 0.000127, l2: 0.000150, l3: 0.000241, l4: 0.000439, l5: 0.001354, l6: 0.002830
[epoch: 693/1000, batch:   816/ 1052, ite: 182200] train loss: 0.003997, tar: 0.000059 
l0: 0.000050, l1: 0.000051, l2: 0.000097, l3: 0.000205, l4: 0.000652, l5: 0.001460, l6: 0.001696
[epoch: 693/1000, batch:   896/ 1052, ite: 182220] train loss: 0.004000, tar: 0.000059 
l0: 0.000028, l1: 0.000027, l2: 0.000047, l3: 0.000148, l4: 0.000685, l5: 0.001493, l6: 0.001811
[epoch: 693/1000, batch:   976/ 1052, ite: 182240] train loss: 0.004001, tar: 0.000059 
[Epoch 693/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000081, l1: 0.000083, l2: 0.000116, l3: 0.000162, l4: 0.000432, l5: 0.000801, l6: 0.002048
[epoch: 694/1000, batch:     4/ 1052, ite: 182260] train loss: 0.003999, tar: 0.000059 
l0: 0.000011, l1: 0.000010, l2: 0.000033, l3: 0.000125, l4: 0.000145, l5: 0.000745, l6: 0.001106
[epoch: 694/1000, batch:    84/ 1052, ite: 182280] train loss: 0.003999, tar: 0.000059 
l0: 0.000035, l1: 0.000033, l2: 0.000185, l3: 0.000379, l4: 0.000720, l5: 0.002193, l6: 0.004457
[epoch: 694/1000, batch:   164/ 1052, ite: 182300] train loss: 0.004003, tar: 0.000059 
l0: 0.000048, l1: 0.000050, l2: 0.000090, l3: 0.000181, l4: 0.000346, l5: 0.001633, l6: 0.002803
[epoch: 694/1000, batch:   244/ 1052, ite: 182320] train loss: 0.004006, tar: 0.000059 
l0: 0.000036, l1: 0.000035, l2: 0.000058, l3: 0.000127, l4: 0.000338, l5: 0.000944, l6: 0.000994
[epoch: 694/1000, batch:   324/ 1052, ite: 182340] train loss: 0.004006, tar: 0.000059 
l0: 0.000015, l1: 0.000014, l2: 0.000046, l3: 0.000129, l4: 0.000410, l5: 0.000913, l6: 0.002297
[epoch: 694/1000, batch:   404/ 1052, ite: 182360] train loss: 0.004003, tar: 0.000059 
l0: 0.000006, l1: 0.000006, l2: 0.000038, l3: 0.000151, l4: 0.000438, l5: 0.000799, l6: 0.001345
[epoch: 694/1000, batch:   484/ 1052, ite: 182380] train loss: 0.004003, tar: 0.000059 
l0: 0.000084, l1: 0.000087, l2: 0.000102, l3: 0.000167, l4: 0.000439, l5: 0.000719, l6: 0.001913
[epoch: 694/1000, batch:   564/ 1052, ite: 182400] train loss: 0.004001, tar: 0.000059 
l0: 0.000065, l1: 0.000065, l2: 0.000131, l3: 0.000261, l4: 0.000447, l5: 0.001312, l6: 0.002301
[epoch: 694/1000, batch:   644/ 1052, ite: 182420] train loss: 0.003999, tar: 0.000058 
l0: 0.000054, l1: 0.000054, l2: 0.000053, l3: 0.000139, l4: 0.000525, l5: 0.001530, l6: 0.001052
[epoch: 694/1000, batch:   724/ 1052, ite: 182440] train loss: 0.004000, tar: 0.000059 
l0: 0.000078, l1: 0.000079, l2: 0.000105, l3: 0.000180, l4: 0.000633, l5: 0.001053, l6: 0.002270
[epoch: 694/1000, batch:   804/ 1052, ite: 182460] train loss: 0.003998, tar: 0.000058 
l0: 0.000040, l1: 0.000042, l2: 0.000081, l3: 0.000228, l4: 0.000346, l5: 0.000766, l6: 0.001477
[epoch: 694/1000, batch:   884/ 1052, ite: 182480] train loss: 0.003998, tar: 0.000058 
l0: 0.000041, l1: 0.000041, l2: 0.000095, l3: 0.000176, l4: 0.000506, l5: 0.000903, l6: 0.002521
[epoch: 694/1000, batch:   964/ 1052, ite: 182500] train loss: 0.003997, tar: 0.000058 
l0: 0.000023, l1: 0.000022, l2: 0.000052, l3: 0.000101, l4: 0.000306, l5: 0.001115, l6: 0.001702
[epoch: 694/1000, batch:  1044/ 1052, ite: 182520] train loss: 0.003995, tar: 0.000058 
[Epoch 694/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000033, l1: 0.000032, l2: 0.000064, l3: 0.000110, l4: 0.000389, l5: 0.001022, l6: 0.001334
[epoch: 695/1000, batch:    72/ 1052, ite: 182540] train loss: 0.003995, tar: 0.000058 
l0: 0.000026, l1: 0.000026, l2: 0.000053, l3: 0.000110, l4: 0.000385, l5: 0.000813, l6: 0.001211
[epoch: 695/1000, batch:   152/ 1052, ite: 182560] train loss: 0.003996, tar: 0.000058 
l0: 0.000095, l1: 0.000096, l2: 0.000106, l3: 0.000185, l4: 0.000413, l5: 0.001360, l6: 0.002403
[epoch: 695/1000, batch:   232/ 1052, ite: 182580] train loss: 0.003996, tar: 0.000058 
l0: 0.000112, l1: 0.000112, l2: 0.000140, l3: 0.000208, l4: 0.000655, l5: 0.001314, l6: 0.002508
[epoch: 695/1000, batch:   312/ 1052, ite: 182600] train loss: 0.003996, tar: 0.000058 
l0: 0.000026, l1: 0.000025, l2: 0.000081, l3: 0.000132, l4: 0.000329, l5: 0.001293, l6: 0.002151
[epoch: 695/1000, batch:   392/ 1052, ite: 182620] train loss: 0.003995, tar: 0.000058 
l0: 0.000045, l1: 0.000045, l2: 0.000061, l3: 0.000102, l4: 0.000187, l5: 0.000932, l6: 0.001270
[epoch: 695/1000, batch:   472/ 1052, ite: 182640] train loss: 0.003994, tar: 0.000058 
l0: 0.000011, l1: 0.000012, l2: 0.000029, l3: 0.000112, l4: 0.000257, l5: 0.000780, l6: 0.001254
[epoch: 695/1000, batch:   552/ 1052, ite: 182660] train loss: 0.003993, tar: 0.000058 
l0: 0.000045, l1: 0.000045, l2: 0.000065, l3: 0.000143, l4: 0.000298, l5: 0.000549, l6: 0.002220
[epoch: 695/1000, batch:   632/ 1052, ite: 182680] train loss: 0.003996, tar: 0.000058 
l0: 0.000020, l1: 0.000020, l2: 0.000039, l3: 0.000071, l4: 0.000408, l5: 0.000669, l6: 0.001021
[epoch: 695/1000, batch:   712/ 1052, ite: 182700] train loss: 0.003998, tar: 0.000058 
l0: 0.000046, l1: 0.000046, l2: 0.000064, l3: 0.000214, l4: 0.000490, l5: 0.001000, l6: 0.002085
[epoch: 695/1000, batch:   792/ 1052, ite: 182720] train loss: 0.003996, tar: 0.000058 
l0: 0.000032, l1: 0.000031, l2: 0.000064, l3: 0.000181, l4: 0.000444, l5: 0.001015, l6: 0.001881
[epoch: 695/1000, batch:   872/ 1052, ite: 182740] train loss: 0.003997, tar: 0.000058 
l0: 0.000016, l1: 0.000016, l2: 0.000044, l3: 0.000130, l4: 0.000368, l5: 0.000555, l6: 0.001160
[epoch: 695/1000, batch:   952/ 1052, ite: 182760] train loss: 0.003996, tar: 0.000058 
l0: 0.000071, l1: 0.000071, l2: 0.000089, l3: 0.000130, l4: 0.000240, l5: 0.000875, l6: 0.001954
[epoch: 695/1000, batch:  1032/ 1052, ite: 182780] train loss: 0.003996, tar: 0.000058 
[Epoch 695/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000062, l1: 0.000064, l2: 0.000062, l3: 0.000203, l4: 0.000671, l5: 0.001186, l6: 0.002464
[epoch: 696/1000, batch:    60/ 1052, ite: 182800] train loss: 0.003997, tar: 0.000058 
l0: 0.000025, l1: 0.000026, l2: 0.000052, l3: 0.000125, l4: 0.000242, l5: 0.000743, l6: 0.001699
[epoch: 696/1000, batch:   140/ 1052, ite: 182820] train loss: 0.003997, tar: 0.000058 
l0: 0.000037, l1: 0.000037, l2: 0.000091, l3: 0.000127, l4: 0.000410, l5: 0.000915, l6: 0.001567
[epoch: 696/1000, batch:   220/ 1052, ite: 182840] train loss: 0.003995, tar: 0.000058 
l0: 0.000096, l1: 0.000095, l2: 0.000138, l3: 0.000273, l4: 0.001065, l5: 0.001593, l6: 0.003270
[epoch: 696/1000, batch:   300/ 1052, ite: 182860] train loss: 0.003998, tar: 0.000058 
l0: 0.000038, l1: 0.000038, l2: 0.000062, l3: 0.000132, l4: 0.000281, l5: 0.000997, l6: 0.001612
[epoch: 696/1000, batch:   380/ 1052, ite: 182880] train loss: 0.003994, tar: 0.000058 
l0: 0.000026, l1: 0.000026, l2: 0.000080, l3: 0.000219, l4: 0.000503, l5: 0.001518, l6: 0.003263
[epoch: 696/1000, batch:   460/ 1052, ite: 182900] train loss: 0.003994, tar: 0.000058 
l0: 0.000090, l1: 0.000134, l2: 0.000115, l3: 0.000184, l4: 0.000503, l5: 0.001296, l6: 0.001974
[epoch: 696/1000, batch:   540/ 1052, ite: 182920] train loss: 0.003993, tar: 0.000058 
l0: 0.000162, l1: 0.000164, l2: 0.000201, l3: 0.000434, l4: 0.000848, l5: 0.001551, l6: 0.003551
[epoch: 696/1000, batch:   620/ 1052, ite: 182940] train loss: 0.003993, tar: 0.000058 
l0: 0.000043, l1: 0.000044, l2: 0.000086, l3: 0.000199, l4: 0.000617, l5: 0.002116, l6: 0.002311
[epoch: 696/1000, batch:   700/ 1052, ite: 182960] train loss: 0.003993, tar: 0.000058 
l0: 0.000031, l1: 0.000029, l2: 0.000054, l3: 0.000120, l4: 0.000409, l5: 0.000806, l6: 0.001332
[epoch: 696/1000, batch:   780/ 1052, ite: 182980] train loss: 0.003992, tar: 0.000058 
l0: 0.000019, l1: 0.000019, l2: 0.000065, l3: 0.000122, l4: 0.000326, l5: 0.000648, l6: 0.001114
[epoch: 696/1000, batch:   860/ 1052, ite: 183000] train loss: 0.003991, tar: 0.000058 
l0: 0.000039, l1: 0.000039, l2: 0.000068, l3: 0.000195, l4: 0.000510, l5: 0.001281, l6: 0.001748
[epoch: 696/1000, batch:   940/ 1052, ite: 183020] train loss: 0.003992, tar: 0.000058 
l0: 0.000095, l1: 0.000096, l2: 0.000126, l3: 0.000277, l4: 0.000542, l5: 0.001335, l6: 0.002530
[epoch: 696/1000, batch:  1020/ 1052, ite: 183040] train loss: 0.003993, tar: 0.000058 
[Epoch 696/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000015, l1: 0.000013, l2: 0.000059, l3: 0.000099, l4: 0.000276, l5: 0.000660, l6: 0.001086
[epoch: 697/1000, batch:    48/ 1052, ite: 183060] train loss: 0.003993, tar: 0.000058 
l0: 0.000011, l1: 0.000013, l2: 0.000016, l3: 0.000059, l4: 0.000263, l5: 0.000935, l6: 0.001017
[epoch: 697/1000, batch:   128/ 1052, ite: 183080] train loss: 0.003992, tar: 0.000058 
l0: 0.000028, l1: 0.000029, l2: 0.000048, l3: 0.000112, l4: 0.000337, l5: 0.000471, l6: 0.001113
[epoch: 697/1000, batch:   208/ 1052, ite: 183100] train loss: 0.003992, tar: 0.000058 
l0: 0.000032, l1: 0.000032, l2: 0.000056, l3: 0.000115, l4: 0.000321, l5: 0.001093, l6: 0.001533
[epoch: 697/1000, batch:   288/ 1052, ite: 183120] train loss: 0.003991, tar: 0.000058 
l0: 0.000003, l1: 0.000003, l2: 0.000045, l3: 0.000208, l4: 0.000604, l5: 0.001376, l6: 0.001494
[epoch: 697/1000, batch:   368/ 1052, ite: 183140] train loss: 0.003990, tar: 0.000058 
l0: 0.000113, l1: 0.000115, l2: 0.000167, l3: 0.000329, l4: 0.000657, l5: 0.001457, l6: 0.002185
[epoch: 697/1000, batch:   448/ 1052, ite: 183160] train loss: 0.003994, tar: 0.000058 
l0: 0.000018, l1: 0.000019, l2: 0.000035, l3: 0.000084, l4: 0.000258, l5: 0.000577, l6: 0.000824
[epoch: 697/1000, batch:   528/ 1052, ite: 183180] train loss: 0.003993, tar: 0.000058 
l0: 0.000080, l1: 0.000080, l2: 0.000107, l3: 0.000216, l4: 0.000549, l5: 0.001380, l6: 0.003009
[epoch: 697/1000, batch:   608/ 1052, ite: 183200] train loss: 0.003993, tar: 0.000058 
l0: 0.000026, l1: 0.000025, l2: 0.000065, l3: 0.000128, l4: 0.000329, l5: 0.000890, l6: 0.001475
[epoch: 697/1000, batch:   688/ 1052, ite: 183220] train loss: 0.003993, tar: 0.000058 
l0: 0.000006, l1: 0.000006, l2: 0.000022, l3: 0.000141, l4: 0.000646, l5: 0.000926, l6: 0.001352
[epoch: 697/1000, batch:   768/ 1052, ite: 183240] train loss: 0.003992, tar: 0.000058 
l0: 0.000052, l1: 0.000050, l2: 0.000070, l3: 0.000133, l4: 0.000360, l5: 0.000948, l6: 0.001877
[epoch: 697/1000, batch:   848/ 1052, ite: 183260] train loss: 0.003992, tar: 0.000058 
l0: 0.000022, l1: 0.000023, l2: 0.000032, l3: 0.000156, l4: 0.000408, l5: 0.000786, l6: 0.002083
[epoch: 697/1000, batch:   928/ 1052, ite: 183280] train loss: 0.003990, tar: 0.000058 
l0: 0.000095, l1: 0.000098, l2: 0.000110, l3: 0.000201, l4: 0.000371, l5: 0.000928, l6: 0.002019
[epoch: 697/1000, batch:  1008/ 1052, ite: 183300] train loss: 0.003991, tar: 0.000058 
[Epoch 697/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000100, l1: 0.000102, l2: 0.000135, l3: 0.000317, l4: 0.000550, l5: 0.001247, l6: 0.002670
[epoch: 698/1000, batch:    36/ 1052, ite: 183320] train loss: 0.003990, tar: 0.000058 
l0: 0.000088, l1: 0.000091, l2: 0.000104, l3: 0.000179, l4: 0.000432, l5: 0.001443, l6: 0.001234
[epoch: 698/1000, batch:   116/ 1052, ite: 183340] train loss: 0.003989, tar: 0.000058 
l0: 0.000011, l1: 0.000011, l2: 0.000032, l3: 0.000059, l4: 0.000214, l5: 0.000623, l6: 0.001413
[epoch: 698/1000, batch:   196/ 1052, ite: 183360] train loss: 0.003988, tar: 0.000058 
l0: 0.000050, l1: 0.000050, l2: 0.000073, l3: 0.000189, l4: 0.000426, l5: 0.000772, l6: 0.002024
[epoch: 698/1000, batch:   276/ 1052, ite: 183380] train loss: 0.003990, tar: 0.000058 
l0: 0.000017, l1: 0.000018, l2: 0.000032, l3: 0.000110, l4: 0.000320, l5: 0.000919, l6: 0.001902
[epoch: 698/1000, batch:   356/ 1052, ite: 183400] train loss: 0.003992, tar: 0.000058 
l0: 0.000018, l1: 0.000018, l2: 0.000041, l3: 0.000084, l4: 0.000353, l5: 0.000529, l6: 0.001201
[epoch: 698/1000, batch:   436/ 1052, ite: 183420] train loss: 0.003993, tar: 0.000058 
l0: 0.000038, l1: 0.000038, l2: 0.000058, l3: 0.000098, l4: 0.000187, l5: 0.000843, l6: 0.001236
[epoch: 698/1000, batch:   516/ 1052, ite: 183440] train loss: 0.003992, tar: 0.000058 
l0: 0.000571, l1: 0.000763, l2: 0.000450, l3: 0.000281, l4: 0.000511, l5: 0.001157, l6: 0.003410
[epoch: 698/1000, batch:   596/ 1052, ite: 183460] train loss: 0.003995, tar: 0.000058 
l0: 0.000199, l1: 0.000231, l2: 0.000267, l3: 0.000208, l4: 0.000363, l5: 0.000789, l6: 0.001382
[epoch: 698/1000, batch:   676/ 1052, ite: 183480] train loss: 0.004001, tar: 0.000059 
l0: 0.000395, l1: 0.000436, l2: 0.000471, l3: 0.000666, l4: 0.001051, l5: 0.001888, l6: 0.003194
[epoch: 698/1000, batch:   756/ 1052, ite: 183500] train loss: 0.004007, tar: 0.000060 
l0: 0.000159, l1: 0.000150, l2: 0.000181, l3: 0.000289, l4: 0.000600, l5: 0.001398, l6: 0.002063
[epoch: 698/1000, batch:   836/ 1052, ite: 183520] train loss: 0.004007, tar: 0.000060 
l0: 0.000178, l1: 0.000180, l2: 0.000213, l3: 0.000268, l4: 0.000402, l5: 0.001842, l6: 0.003050
[epoch: 698/1000, batch:   916/ 1052, ite: 183540] train loss: 0.004008, tar: 0.000061 
l0: 0.000145, l1: 0.000160, l2: 0.000259, l3: 0.000261, l4: 0.000495, l5: 0.000918, l6: 0.001848
[epoch: 698/1000, batch:   996/ 1052, ite: 183560] train loss: 0.004011, tar: 0.000061 
[Epoch 698/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000097, l1: 0.000102, l2: 0.000139, l3: 0.000198, l4: 0.000527, l5: 0.001167, l6: 0.001435
[epoch: 699/1000, batch:    24/ 1052, ite: 183580] train loss: 0.004012, tar: 0.000061 
l0: 0.000050, l1: 0.000050, l2: 0.000084, l3: 0.000230, l4: 0.000336, l5: 0.001245, l6: 0.001448
[epoch: 699/1000, batch:   104/ 1052, ite: 183600] train loss: 0.004014, tar: 0.000061 
l0: 0.000059, l1: 0.000060, l2: 0.000111, l3: 0.000133, l4: 0.000373, l5: 0.000401, l6: 0.000881
[epoch: 699/1000, batch:   184/ 1052, ite: 183620] train loss: 0.004016, tar: 0.000061 
l0: 0.000046, l1: 0.000046, l2: 0.000110, l3: 0.000237, l4: 0.000381, l5: 0.000843, l6: 0.002205
[epoch: 699/1000, batch:   264/ 1052, ite: 183640] train loss: 0.004015, tar: 0.000062 
l0: 0.000052, l1: 0.000054, l2: 0.000121, l3: 0.000268, l4: 0.000491, l5: 0.001175, l6: 0.003354
[epoch: 699/1000, batch:   344/ 1052, ite: 183660] train loss: 0.004015, tar: 0.000062 
l0: 0.000061, l1: 0.000061, l2: 0.000093, l3: 0.000227, l4: 0.000405, l5: 0.000731, l6: 0.001341
[epoch: 699/1000, batch:   424/ 1052, ite: 183680] train loss: 0.004018, tar: 0.000062 
l0: 0.000142, l1: 0.000144, l2: 0.000230, l3: 0.000391, l4: 0.000705, l5: 0.001148, l6: 0.002232
[epoch: 699/1000, batch:   504/ 1052, ite: 183700] train loss: 0.004017, tar: 0.000062 
l0: 0.000202, l1: 0.000208, l2: 0.000250, l3: 0.000455, l4: 0.001015, l5: 0.002698, l6: 0.003602
[epoch: 699/1000, batch:   584/ 1052, ite: 183720] train loss: 0.004018, tar: 0.000062 
l0: 0.000008, l1: 0.000007, l2: 0.000018, l3: 0.000057, l4: 0.000116, l5: 0.000856, l6: 0.001717
[epoch: 699/1000, batch:   664/ 1052, ite: 183740] train loss: 0.004017, tar: 0.000062 
l0: 0.000103, l1: 0.000106, l2: 0.000144, l3: 0.000259, l4: 0.000421, l5: 0.000629, l6: 0.001218
[epoch: 699/1000, batch:   744/ 1052, ite: 183760] train loss: 0.004019, tar: 0.000062 
l0: 0.000050, l1: 0.000053, l2: 0.000072, l3: 0.000144, l4: 0.000286, l5: 0.000860, l6: 0.001491
[epoch: 699/1000, batch:   824/ 1052, ite: 183780] train loss: 0.004019, tar: 0.000062 
l0: 0.000085, l1: 0.000084, l2: 0.000172, l3: 0.000304, l4: 0.000804, l5: 0.001530, l6: 0.001877
[epoch: 699/1000, batch:   904/ 1052, ite: 183800] train loss: 0.004021, tar: 0.000062 
l0: 0.000048, l1: 0.000047, l2: 0.000091, l3: 0.000144, l4: 0.000479, l5: 0.001264, l6: 0.001709
[epoch: 699/1000, batch:   984/ 1052, ite: 183820] train loss: 0.004021, tar: 0.000063 
[Epoch 699/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000157, l1: 0.000157, l2: 0.000217, l3: 0.000310, l4: 0.000718, l5: 0.001894, l6: 0.002831
[epoch: 700/1000, batch:    12/ 1052, ite: 183840] train loss: 0.004023, tar: 0.000063 
l0: 0.000049, l1: 0.000050, l2: 0.000063, l3: 0.000139, l4: 0.000316, l5: 0.000772, l6: 0.001701
[epoch: 700/1000, batch:    92/ 1052, ite: 183860] train loss: 0.004022, tar: 0.000063 
l0: 0.000136, l1: 0.000136, l2: 0.000213, l3: 0.000275, l4: 0.000420, l5: 0.001114, l6: 0.002532
[epoch: 700/1000, batch:   172/ 1052, ite: 183880] train loss: 0.004024, tar: 0.000063 
l0: 0.000142, l1: 0.000154, l2: 0.000167, l3: 0.000239, l4: 0.000565, l5: 0.001205, l6: 0.002522
[epoch: 700/1000, batch:   252/ 1052, ite: 183900] train loss: 0.004024, tar: 0.000063 
l0: 0.000073, l1: 0.000076, l2: 0.000089, l3: 0.000131, l4: 0.000340, l5: 0.001013, l6: 0.001914
[epoch: 700/1000, batch:   332/ 1052, ite: 183920] train loss: 0.004023, tar: 0.000063 
l0: 0.000042, l1: 0.000037, l2: 0.000072, l3: 0.000217, l4: 0.000404, l5: 0.001111, l6: 0.001733
[epoch: 700/1000, batch:   412/ 1052, ite: 183940] train loss: 0.004024, tar: 0.000063 
l0: 0.000127, l1: 0.000131, l2: 0.000178, l3: 0.000332, l4: 0.000742, l5: 0.002090, l6: 0.002850
[epoch: 700/1000, batch:   492/ 1052, ite: 183960] train loss: 0.004025, tar: 0.000063 
l0: 0.000060, l1: 0.000058, l2: 0.000136, l3: 0.000225, l4: 0.000516, l5: 0.001330, l6: 0.003335
[epoch: 700/1000, batch:   572/ 1052, ite: 183980] train loss: 0.004026, tar: 0.000063 
l0: 0.000104, l1: 0.000108, l2: 0.000125, l3: 0.000154, l4: 0.000281, l5: 0.001076, l6: 0.001589
[epoch: 700/1000, batch:   652/ 1052, ite: 184000] train loss: 0.004026, tar: 0.000063 
l0: 0.000035, l1: 0.000035, l2: 0.000075, l3: 0.000173, l4: 0.000532, l5: 0.001181, l6: 0.002447
[epoch: 700/1000, batch:   732/ 1052, ite: 184020] train loss: 0.004025, tar: 0.000063 
l0: 0.000018, l1: 0.000018, l2: 0.000031, l3: 0.000103, l4: 0.000253, l5: 0.001084, l6: 0.001685
[epoch: 700/1000, batch:   812/ 1052, ite: 184040] train loss: 0.004026, tar: 0.000063 
l0: 0.000122, l1: 0.000120, l2: 0.000158, l3: 0.000236, l4: 0.000439, l5: 0.001284, l6: 0.001764
[epoch: 700/1000, batch:   892/ 1052, ite: 184060] train loss: 0.004025, tar: 0.000063 
l0: 0.000060, l1: 0.000063, l2: 0.000093, l3: 0.000238, l4: 0.000599, l5: 0.001210, l6: 0.001996
[epoch: 700/1000, batch:   972/ 1052, ite: 184080] train loss: 0.004024, tar: 0.000063 
l0: 0.000052, l1: 0.000059, l2: 0.000063, l3: 0.000185, l4: 0.000363, l5: 0.001624, l6: 0.002539
[epoch: 700/1000, batch:  1052/ 1052, ite: 184100] train loss: 0.004024, tar: 0.000063 
[Epoch 700/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000033, l1: 0.000028, l2: 0.000130, l3: 0.000432, l4: 0.001063, l5: 0.002261, l6: 0.003982
[epoch: 701/1000, batch:    80/ 1052, ite: 184120] train loss: 0.004025, tar: 0.000063 
l0: 0.000023, l1: 0.000024, l2: 0.000046, l3: 0.000108, l4: 0.000256, l5: 0.000991, l6: 0.001729
[epoch: 701/1000, batch:   160/ 1052, ite: 184140] train loss: 0.004026, tar: 0.000063 
l0: 0.000075, l1: 0.000073, l2: 0.000087, l3: 0.000127, l4: 0.000325, l5: 0.000686, l6: 0.001197
[epoch: 701/1000, batch:   240/ 1052, ite: 184160] train loss: 0.004023, tar: 0.000063 
l0: 0.000116, l1: 0.000117, l2: 0.000153, l3: 0.000231, l4: 0.000545, l5: 0.001226, l6: 0.002204
[epoch: 701/1000, batch:   320/ 1052, ite: 184180] train loss: 0.004026, tar: 0.000063 
l0: 0.000013, l1: 0.000014, l2: 0.000036, l3: 0.000151, l4: 0.000389, l5: 0.001177, l6: 0.001338
[epoch: 701/1000, batch:   400/ 1052, ite: 184200] train loss: 0.004026, tar: 0.000063 
l0: 0.000063, l1: 0.000064, l2: 0.000083, l3: 0.000155, l4: 0.000265, l5: 0.001031, l6: 0.001519
[epoch: 701/1000, batch:   480/ 1052, ite: 184220] train loss: 0.004026, tar: 0.000063 
l0: 0.000031, l1: 0.000030, l2: 0.000095, l3: 0.000226, l4: 0.000364, l5: 0.000923, l6: 0.002107
[epoch: 701/1000, batch:   560/ 1052, ite: 184240] train loss: 0.004027, tar: 0.000063 
l0: 0.000030, l1: 0.000030, l2: 0.000061, l3: 0.000135, l4: 0.000300, l5: 0.000977, l6: 0.001346
[epoch: 701/1000, batch:   640/ 1052, ite: 184260] train loss: 0.004027, tar: 0.000063 
l0: 0.000043, l1: 0.000037, l2: 0.000054, l3: 0.000129, l4: 0.000344, l5: 0.000615, l6: 0.001083
[epoch: 701/1000, batch:   720/ 1052, ite: 184280] train loss: 0.004027, tar: 0.000063 
l0: 0.000128, l1: 0.000123, l2: 0.000172, l3: 0.000409, l4: 0.001106, l5: 0.001943, l6: 0.003866
[epoch: 701/1000, batch:   800/ 1052, ite: 184300] train loss: 0.004025, tar: 0.000063 
l0: 0.000079, l1: 0.000080, l2: 0.000072, l3: 0.000158, l4: 0.000512, l5: 0.000932, l6: 0.001275
[epoch: 701/1000, batch:   880/ 1052, ite: 184320] train loss: 0.004026, tar: 0.000063 
l0: 0.000076, l1: 0.000074, l2: 0.000150, l3: 0.000176, l4: 0.000570, l5: 0.001163, l6: 0.001730
[epoch: 701/1000, batch:   960/ 1052, ite: 184340] train loss: 0.004028, tar: 0.000063 
l0: 0.000341, l1: 0.000359, l2: 0.000400, l3: 0.000393, l4: 0.000729, l5: 0.001064, l6: 0.002740
[epoch: 701/1000, batch:  1040/ 1052, ite: 184360] train loss: 0.004029, tar: 0.000064 
[Epoch 701/1000] Test loss: 0.017, Test target loss: 0.003
l0: 0.000168, l1: 0.000181, l2: 0.000174, l3: 0.000274, l4: 0.000429, l5: 0.000852, l6: 0.002296
[epoch: 702/1000, batch:    68/ 1052, ite: 184380] train loss: 0.004030, tar: 0.000064 
l0: 0.000073, l1: 0.000074, l2: 0.000135, l3: 0.000349, l4: 0.000445, l5: 0.001017, l6: 0.001660
[epoch: 702/1000, batch:   148/ 1052, ite: 184400] train loss: 0.004030, tar: 0.000064 
l0: 0.000122, l1: 0.000127, l2: 0.000182, l3: 0.000231, l4: 0.000488, l5: 0.001130, l6: 0.002044
[epoch: 702/1000, batch:   228/ 1052, ite: 184420] train loss: 0.004030, tar: 0.000064 
l0: 0.000020, l1: 0.000020, l2: 0.000054, l3: 0.000102, l4: 0.000361, l5: 0.000451, l6: 0.001025
[epoch: 702/1000, batch:   308/ 1052, ite: 184440] train loss: 0.004030, tar: 0.000064 
l0: 0.000049, l1: 0.000047, l2: 0.000118, l3: 0.000253, l4: 0.000818, l5: 0.001502, l6: 0.002344
[epoch: 702/1000, batch:   388/ 1052, ite: 184460] train loss: 0.004031, tar: 0.000064 
l0: 0.000050, l1: 0.000047, l2: 0.000096, l3: 0.000278, l4: 0.000384, l5: 0.001156, l6: 0.001632
[epoch: 702/1000, batch:   468/ 1052, ite: 184480] train loss: 0.004030, tar: 0.000064 
l0: 0.000147, l1: 0.000147, l2: 0.000207, l3: 0.000313, l4: 0.000619, l5: 0.001345, l6: 0.002472
[epoch: 702/1000, batch:   548/ 1052, ite: 184500] train loss: 0.004030, tar: 0.000064 
l0: 0.000100, l1: 0.000098, l2: 0.000168, l3: 0.000225, l4: 0.000446, l5: 0.000756, l6: 0.001477
[epoch: 702/1000, batch:   628/ 1052, ite: 184520] train loss: 0.004029, tar: 0.000064 
l0: 0.000032, l1: 0.000032, l2: 0.000048, l3: 0.000104, l4: 0.000377, l5: 0.000561, l6: 0.001649
[epoch: 702/1000, batch:   708/ 1052, ite: 184540] train loss: 0.004030, tar: 0.000064 
l0: 0.000028, l1: 0.000026, l2: 0.000032, l3: 0.000074, l4: 0.000257, l5: 0.000776, l6: 0.001120
[epoch: 702/1000, batch:   788/ 1052, ite: 184560] train loss: 0.004030, tar: 0.000064 
l0: 0.000076, l1: 0.000078, l2: 0.000095, l3: 0.000173, l4: 0.000345, l5: 0.001259, l6: 0.001566
[epoch: 702/1000, batch:   868/ 1052, ite: 184580] train loss: 0.004032, tar: 0.000064 
l0: 0.000060, l1: 0.000059, l2: 0.000119, l3: 0.000182, l4: 0.000394, l5: 0.000786, l6: 0.001697
[epoch: 702/1000, batch:   948/ 1052, ite: 184600] train loss: 0.004032, tar: 0.000064 
l0: 0.000189, l1: 0.000193, l2: 0.000218, l3: 0.000340, l4: 0.000621, l5: 0.001537, l6: 0.003616
[epoch: 702/1000, batch:  1028/ 1052, ite: 184620] train loss: 0.004033, tar: 0.000065 
[Epoch 702/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000116, l1: 0.000113, l2: 0.000241, l3: 0.000411, l4: 0.000644, l5: 0.001367, l6: 0.002022
[epoch: 703/1000, batch:    56/ 1052, ite: 184640] train loss: 0.004036, tar: 0.000065 
l0: 0.000117, l1: 0.000124, l2: 0.000164, l3: 0.000276, l4: 0.000455, l5: 0.001284, l6: 0.002040
[epoch: 703/1000, batch:   136/ 1052, ite: 184660] train loss: 0.004035, tar: 0.000065 
l0: 0.000006, l1: 0.000007, l2: 0.000022, l3: 0.000047, l4: 0.000165, l5: 0.000716, l6: 0.001025
[epoch: 703/1000, batch:   216/ 1052, ite: 184680] train loss: 0.004035, tar: 0.000065 
l0: 0.000009, l1: 0.000009, l2: 0.000057, l3: 0.000124, l4: 0.000213, l5: 0.001062, l6: 0.001761
[epoch: 703/1000, batch:   296/ 1052, ite: 184700] train loss: 0.004037, tar: 0.000065 
l0: 0.000022, l1: 0.000024, l2: 0.000073, l3: 0.000119, l4: 0.000290, l5: 0.000778, l6: 0.000844
[epoch: 703/1000, batch:   376/ 1052, ite: 184720] train loss: 0.004035, tar: 0.000065 
l0: 0.000035, l1: 0.000034, l2: 0.000067, l3: 0.000126, l4: 0.000284, l5: 0.000455, l6: 0.000714
[epoch: 703/1000, batch:   456/ 1052, ite: 184740] train loss: 0.004035, tar: 0.000065 
l0: 0.000042, l1: 0.000041, l2: 0.000125, l3: 0.000244, l4: 0.000529, l5: 0.001166, l6: 0.003194
[epoch: 703/1000, batch:   536/ 1052, ite: 184760] train loss: 0.004037, tar: 0.000065 
l0: 0.000063, l1: 0.000060, l2: 0.000134, l3: 0.000349, l4: 0.001111, l5: 0.001355, l6: 0.002149
[epoch: 703/1000, batch:   616/ 1052, ite: 184780] train loss: 0.004039, tar: 0.000065 
l0: 0.000085, l1: 0.000087, l2: 0.000146, l3: 0.000186, l4: 0.000450, l5: 0.001083, l6: 0.001908
[epoch: 703/1000, batch:   696/ 1052, ite: 184800] train loss: 0.004038, tar: 0.000065 
l0: 0.000052, l1: 0.000052, l2: 0.000092, l3: 0.000224, l4: 0.000505, l5: 0.001176, l6: 0.001578
[epoch: 703/1000, batch:   776/ 1052, ite: 184820] train loss: 0.004038, tar: 0.000065 
l0: 0.000155, l1: 0.000159, l2: 0.000163, l3: 0.000225, l4: 0.000530, l5: 0.001147, l6: 0.002294
[epoch: 703/1000, batch:   856/ 1052, ite: 184840] train loss: 0.004035, tar: 0.000065 
l0: 0.000089, l1: 0.000088, l2: 0.000108, l3: 0.000249, l4: 0.000485, l5: 0.000642, l6: 0.001170
[epoch: 703/1000, batch:   936/ 1052, ite: 184860] train loss: 0.004033, tar: 0.000065 
l0: 0.000238, l1: 0.000247, l2: 0.000279, l3: 0.000523, l4: 0.001040, l5: 0.001815, l6: 0.002948
[epoch: 703/1000, batch:  1016/ 1052, ite: 184880] train loss: 0.004033, tar: 0.000065 
[Epoch 703/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000015, l1: 0.000015, l2: 0.000043, l3: 0.000076, l4: 0.000351, l5: 0.000660, l6: 0.001573
[epoch: 704/1000, batch:    44/ 1052, ite: 184900] train loss: 0.004033, tar: 0.000064 
l0: 0.000078, l1: 0.000079, l2: 0.000116, l3: 0.000268, l4: 0.000498, l5: 0.001204, l6: 0.001688
[epoch: 704/1000, batch:   124/ 1052, ite: 184920] train loss: 0.004033, tar: 0.000065 
l0: 0.000058, l1: 0.000058, l2: 0.000096, l3: 0.000249, l4: 0.000617, l5: 0.001356, l6: 0.001663
[epoch: 704/1000, batch:   204/ 1052, ite: 184940] train loss: 0.004033, tar: 0.000065 
l0: 0.000142, l1: 0.000146, l2: 0.000179, l3: 0.000257, l4: 0.000643, l5: 0.001047, l6: 0.002316
[epoch: 704/1000, batch:   284/ 1052, ite: 184960] train loss: 0.004036, tar: 0.000065 
l0: 0.000064, l1: 0.000068, l2: 0.000037, l3: 0.000077, l4: 0.000185, l5: 0.000499, l6: 0.000918
[epoch: 704/1000, batch:   364/ 1052, ite: 184980] train loss: 0.004035, tar: 0.000065 
l0: 0.000064, l1: 0.000065, l2: 0.000124, l3: 0.000330, l4: 0.000543, l5: 0.000881, l6: 0.001258
[epoch: 704/1000, batch:   444/ 1052, ite: 185000] train loss: 0.004035, tar: 0.000065 
l0: 0.000046, l1: 0.000046, l2: 0.000086, l3: 0.000383, l4: 0.000632, l5: 0.000950, l6: 0.001665
[epoch: 704/1000, batch:   524/ 1052, ite: 185020] train loss: 0.004036, tar: 0.000064 
l0: 0.000073, l1: 0.000075, l2: 0.000103, l3: 0.000210, l4: 0.000530, l5: 0.001949, l6: 0.002380
[epoch: 704/1000, batch:   604/ 1052, ite: 185040] train loss: 0.004036, tar: 0.000064 
l0: 0.000081, l1: 0.000084, l2: 0.000095, l3: 0.000228, l4: 0.000393, l5: 0.001215, l6: 0.001411
[epoch: 704/1000, batch:   684/ 1052, ite: 185060] train loss: 0.004034, tar: 0.000064 
l0: 0.000039, l1: 0.000038, l2: 0.000071, l3: 0.000165, l4: 0.000324, l5: 0.000949, l6: 0.001491
[epoch: 704/1000, batch:   764/ 1052, ite: 185080] train loss: 0.004035, tar: 0.000064 
l0: 0.000105, l1: 0.000108, l2: 0.000170, l3: 0.000375, l4: 0.001105, l5: 0.003994, l6: 0.006400
[epoch: 704/1000, batch:   844/ 1052, ite: 185100] train loss: 0.004035, tar: 0.000064 
l0: 0.000086, l1: 0.000087, l2: 0.000103, l3: 0.000191, l4: 0.000470, l5: 0.001203, l6: 0.002383
[epoch: 704/1000, batch:   924/ 1052, ite: 185120] train loss: 0.004035, tar: 0.000064 
l0: 0.000072, l1: 0.000073, l2: 0.000096, l3: 0.000151, l4: 0.000395, l5: 0.000934, l6: 0.001879
[epoch: 704/1000, batch:  1004/ 1052, ite: 185140] train loss: 0.004035, tar: 0.000064 
[Epoch 704/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000011, l1: 0.000010, l2: 0.000052, l3: 0.000166, l4: 0.000473, l5: 0.001059, l6: 0.001866
[epoch: 705/1000, batch:    32/ 1052, ite: 185160] train loss: 0.004035, tar: 0.000064 
l0: 0.000075, l1: 0.000074, l2: 0.000126, l3: 0.000257, l4: 0.000500, l5: 0.000678, l6: 0.001887
[epoch: 705/1000, batch:   112/ 1052, ite: 185180] train loss: 0.004036, tar: 0.000064 
l0: 0.000018, l1: 0.000019, l2: 0.000035, l3: 0.000114, l4: 0.000321, l5: 0.000648, l6: 0.001991
[epoch: 705/1000, batch:   192/ 1052, ite: 185200] train loss: 0.004034, tar: 0.000064 
l0: 0.000107, l1: 0.000105, l2: 0.000117, l3: 0.000268, l4: 0.000540, l5: 0.001683, l6: 0.003620
[epoch: 705/1000, batch:   272/ 1052, ite: 185220] train loss: 0.004034, tar: 0.000064 
l0: 0.000026, l1: 0.000025, l2: 0.000060, l3: 0.000199, l4: 0.000757, l5: 0.001553, l6: 0.002413
[epoch: 705/1000, batch:   352/ 1052, ite: 185240] train loss: 0.004035, tar: 0.000064 
l0: 0.000048, l1: 0.000051, l2: 0.000100, l3: 0.000159, l4: 0.000430, l5: 0.000932, l6: 0.002154
[epoch: 705/1000, batch:   432/ 1052, ite: 185260] train loss: 0.004034, tar: 0.000064 
l0: 0.000095, l1: 0.000097, l2: 0.000118, l3: 0.000205, l4: 0.000518, l5: 0.001593, l6: 0.002593
[epoch: 705/1000, batch:   512/ 1052, ite: 185280] train loss: 0.004036, tar: 0.000064 
l0: 0.000105, l1: 0.000108, l2: 0.000117, l3: 0.000269, l4: 0.000866, l5: 0.001726, l6: 0.002261
[epoch: 705/1000, batch:   592/ 1052, ite: 185300] train loss: 0.004034, tar: 0.000064 
l0: 0.000097, l1: 0.000101, l2: 0.000112, l3: 0.000162, l4: 0.000333, l5: 0.001126, l6: 0.001708
[epoch: 705/1000, batch:   672/ 1052, ite: 185320] train loss: 0.004035, tar: 0.000064 
l0: 0.000038, l1: 0.000040, l2: 0.000055, l3: 0.000134, l4: 0.000287, l5: 0.000970, l6: 0.001263
[epoch: 705/1000, batch:   752/ 1052, ite: 185340] train loss: 0.004035, tar: 0.000064 
l0: 0.000041, l1: 0.000046, l2: 0.000036, l3: 0.000119, l4: 0.000235, l5: 0.000878, l6: 0.001641
[epoch: 705/1000, batch:   832/ 1052, ite: 185360] train loss: 0.004034, tar: 0.000064 
l0: 0.000167, l1: 0.000173, l2: 0.000196, l3: 0.000315, l4: 0.000648, l5: 0.001420, l6: 0.003357
[epoch: 705/1000, batch:   912/ 1052, ite: 185380] train loss: 0.004033, tar: 0.000064 
l0: 0.000052, l1: 0.000057, l2: 0.000083, l3: 0.000161, l4: 0.000380, l5: 0.000814, l6: 0.000990
[epoch: 705/1000, batch:   992/ 1052, ite: 185400] train loss: 0.004033, tar: 0.000064 
[Epoch 705/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000127, l1: 0.000129, l2: 0.000170, l3: 0.000375, l4: 0.000977, l5: 0.001824, l6: 0.002316
[epoch: 706/1000, batch:    20/ 1052, ite: 185420] train loss: 0.004032, tar: 0.000064 
l0: 0.000044, l1: 0.000047, l2: 0.000064, l3: 0.000202, l4: 0.000543, l5: 0.001329, l6: 0.001692
[epoch: 706/1000, batch:   100/ 1052, ite: 185440] train loss: 0.004031, tar: 0.000064 
l0: 0.000080, l1: 0.000081, l2: 0.000111, l3: 0.000184, l4: 0.000466, l5: 0.000916, l6: 0.001407
[epoch: 706/1000, batch:   180/ 1052, ite: 185460] train loss: 0.004030, tar: 0.000064 
l0: 0.000007, l1: 0.000007, l2: 0.000018, l3: 0.000075, l4: 0.000264, l5: 0.001520, l6: 0.002472
[epoch: 706/1000, batch:   260/ 1052, ite: 185480] train loss: 0.004031, tar: 0.000064 
l0: 0.000023, l1: 0.000018, l2: 0.000053, l3: 0.000168, l4: 0.000412, l5: 0.000865, l6: 0.002661
[epoch: 706/1000, batch:   340/ 1052, ite: 185500] train loss: 0.004033, tar: 0.000064 
l0: 0.000018, l1: 0.000016, l2: 0.000052, l3: 0.000133, l4: 0.000418, l5: 0.001095, l6: 0.002213
[epoch: 706/1000, batch:   420/ 1052, ite: 185520] train loss: 0.004032, tar: 0.000064 
l0: 0.000020, l1: 0.000021, l2: 0.000098, l3: 0.000245, l4: 0.000531, l5: 0.001307, l6: 0.003129
[epoch: 706/1000, batch:   500/ 1052, ite: 185540] train loss: 0.004030, tar: 0.000064 
l0: 0.000012, l1: 0.000010, l2: 0.000027, l3: 0.000061, l4: 0.000477, l5: 0.000671, l6: 0.001790
[epoch: 706/1000, batch:   580/ 1052, ite: 185560] train loss: 0.004030, tar: 0.000064 
l0: 0.000100, l1: 0.000100, l2: 0.000111, l3: 0.000159, l4: 0.000475, l5: 0.001279, l6: 0.002343
[epoch: 706/1000, batch:   660/ 1052, ite: 185580] train loss: 0.004028, tar: 0.000064 
l0: 0.000125, l1: 0.000130, l2: 0.000140, l3: 0.000264, l4: 0.000655, l5: 0.001501, l6: 0.003183
[epoch: 706/1000, batch:   740/ 1052, ite: 185600] train loss: 0.004028, tar: 0.000064 
l0: 0.000112, l1: 0.000115, l2: 0.000139, l3: 0.000246, l4: 0.000789, l5: 0.001739, l6: 0.002718
[epoch: 706/1000, batch:   820/ 1052, ite: 185620] train loss: 0.004027, tar: 0.000064 
l0: 0.000094, l1: 0.000099, l2: 0.000128, l3: 0.000251, l4: 0.000539, l5: 0.000860, l6: 0.002000
[epoch: 706/1000, batch:   900/ 1052, ite: 185640] train loss: 0.004026, tar: 0.000064 
l0: 0.000063, l1: 0.000062, l2: 0.000085, l3: 0.000157, l4: 0.000447, l5: 0.000892, l6: 0.001829
[epoch: 706/1000, batch:   980/ 1052, ite: 185660] train loss: 0.004027, tar: 0.000064 
[Epoch 706/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000005, l1: 0.000006, l2: 0.000014, l3: 0.000059, l4: 0.000100, l5: 0.000722, l6: 0.000918
[epoch: 707/1000, batch:     8/ 1052, ite: 185680] train loss: 0.004027, tar: 0.000064 
l0: 0.000028, l1: 0.000029, l2: 0.000049, l3: 0.000094, l4: 0.000246, l5: 0.000523, l6: 0.001294
[epoch: 707/1000, batch:    88/ 1052, ite: 185700] train loss: 0.004026, tar: 0.000063 
l0: 0.000050, l1: 0.000052, l2: 0.000095, l3: 0.000175, l4: 0.000491, l5: 0.001080, l6: 0.000860
[epoch: 707/1000, batch:   168/ 1052, ite: 185720] train loss: 0.004024, tar: 0.000063 
l0: 0.000017, l1: 0.000016, l2: 0.000056, l3: 0.000113, l4: 0.000407, l5: 0.000978, l6: 0.001920
[epoch: 707/1000, batch:   248/ 1052, ite: 185740] train loss: 0.004024, tar: 0.000063 
l0: 0.000004, l1: 0.000004, l2: 0.000032, l3: 0.000079, l4: 0.000289, l5: 0.000866, l6: 0.001819
[epoch: 707/1000, batch:   328/ 1052, ite: 185760] train loss: 0.004023, tar: 0.000063 
l0: 0.000018, l1: 0.000018, l2: 0.000057, l3: 0.000133, l4: 0.000340, l5: 0.000805, l6: 0.001239
[epoch: 707/1000, batch:   408/ 1052, ite: 185780] train loss: 0.004023, tar: 0.000063 
l0: 0.000039, l1: 0.000038, l2: 0.000081, l3: 0.000167, l4: 0.000492, l5: 0.000918, l6: 0.001238
[epoch: 707/1000, batch:   488/ 1052, ite: 185800] train loss: 0.004022, tar: 0.000063 
l0: 0.000035, l1: 0.000036, l2: 0.000058, l3: 0.000150, l4: 0.000369, l5: 0.000813, l6: 0.001145
[epoch: 707/1000, batch:   568/ 1052, ite: 185820] train loss: 0.004021, tar: 0.000063 
l0: 0.000123, l1: 0.000128, l2: 0.000134, l3: 0.000204, l4: 0.000573, l5: 0.001173, l6: 0.002420
[epoch: 707/1000, batch:   648/ 1052, ite: 185840] train loss: 0.004020, tar: 0.000063 
l0: 0.000117, l1: 0.000120, l2: 0.000140, l3: 0.000176, l4: 0.000331, l5: 0.001235, l6: 0.002938
[epoch: 707/1000, batch:   728/ 1052, ite: 185860] train loss: 0.004021, tar: 0.000063 
l0: 0.000042, l1: 0.000042, l2: 0.000088, l3: 0.000202, l4: 0.000620, l5: 0.001020, l6: 0.001820
[epoch: 707/1000, batch:   808/ 1052, ite: 185880] train loss: 0.004023, tar: 0.000063 
l0: 0.000074, l1: 0.000081, l2: 0.000091, l3: 0.000170, l4: 0.000382, l5: 0.001224, l6: 0.002053
[epoch: 707/1000, batch:   888/ 1052, ite: 185900] train loss: 0.004023, tar: 0.000063 
l0: 0.000055, l1: 0.000055, l2: 0.000082, l3: 0.000202, l4: 0.000530, l5: 0.001820, l6: 0.002648
[epoch: 707/1000, batch:   968/ 1052, ite: 185920] train loss: 0.004022, tar: 0.000063 
l0: 0.000030, l1: 0.000029, l2: 0.000052, l3: 0.000134, l4: 0.000471, l5: 0.000827, l6: 0.001241
[epoch: 707/1000, batch:  1048/ 1052, ite: 185940] train loss: 0.004023, tar: 0.000063 
[Epoch 707/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000083, l1: 0.000084, l2: 0.000110, l3: 0.000189, l4: 0.000595, l5: 0.001492, l6: 0.002824
[epoch: 708/1000, batch:    76/ 1052, ite: 185960] train loss: 0.004023, tar: 0.000063 
l0: 0.000022, l1: 0.000021, l2: 0.000098, l3: 0.000254, l4: 0.000693, l5: 0.001174, l6: 0.002421
[epoch: 708/1000, batch:   156/ 1052, ite: 185980] train loss: 0.004024, tar: 0.000063 
l0: 0.000146, l1: 0.000147, l2: 0.000185, l3: 0.000248, l4: 0.000440, l5: 0.001135, l6: 0.002647
[epoch: 708/1000, batch:   236/ 1052, ite: 186000] train loss: 0.004023, tar: 0.000063 
l0: 0.000035, l1: 0.000036, l2: 0.000049, l3: 0.000110, l4: 0.000312, l5: 0.000854, l6: 0.001163
[epoch: 708/1000, batch:   316/ 1052, ite: 186020] train loss: 0.004022, tar: 0.000063 
l0: 0.000129, l1: 0.000131, l2: 0.000205, l3: 0.000352, l4: 0.000575, l5: 0.001785, l6: 0.003213
[epoch: 708/1000, batch:   396/ 1052, ite: 186040] train loss: 0.004022, tar: 0.000063 
l0: 0.000050, l1: 0.000051, l2: 0.000072, l3: 0.000149, l4: 0.000356, l5: 0.000936, l6: 0.001524
[epoch: 708/1000, batch:   476/ 1052, ite: 186060] train loss: 0.004022, tar: 0.000063 
l0: 0.000033, l1: 0.000029, l2: 0.000094, l3: 0.000219, l4: 0.000554, l5: 0.001413, l6: 0.002798
[epoch: 708/1000, batch:   556/ 1052, ite: 186080] train loss: 0.004020, tar: 0.000063 
l0: 0.000138, l1: 0.000134, l2: 0.000175, l3: 0.000336, l4: 0.000610, l5: 0.001465, l6: 0.003089
[epoch: 708/1000, batch:   636/ 1052, ite: 186100] train loss: 0.004020, tar: 0.000063 
l0: 0.000041, l1: 0.000041, l2: 0.000084, l3: 0.000145, l4: 0.000229, l5: 0.000715, l6: 0.001684
[epoch: 708/1000, batch:   716/ 1052, ite: 186120] train loss: 0.004018, tar: 0.000063 
l0: 0.000018, l1: 0.000019, l2: 0.000075, l3: 0.000211, l4: 0.000725, l5: 0.001445, l6: 0.002610
[epoch: 708/1000, batch:   796/ 1052, ite: 186140] train loss: 0.004019, tar: 0.000063 
l0: 0.000084, l1: 0.000090, l2: 0.000103, l3: 0.000258, l4: 0.000608, l5: 0.001548, l6: 0.002749
[epoch: 708/1000, batch:   876/ 1052, ite: 186160] train loss: 0.004020, tar: 0.000063 
l0: 0.000070, l1: 0.000073, l2: 0.000112, l3: 0.000208, l4: 0.000349, l5: 0.001266, l6: 0.002132
[epoch: 708/1000, batch:   956/ 1052, ite: 186180] train loss: 0.004019, tar: 0.000063 
l0: 0.000043, l1: 0.000043, l2: 0.000065, l3: 0.000117, l4: 0.000364, l5: 0.000803, l6: 0.001572
[epoch: 708/1000, batch:  1036/ 1052, ite: 186200] train loss: 0.004019, tar: 0.000063 
[Epoch 708/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000178, l1: 0.000179, l2: 0.000214, l3: 0.000417, l4: 0.000845, l5: 0.001584, l6: 0.003000
[epoch: 709/1000, batch:    64/ 1052, ite: 186220] train loss: 0.004021, tar: 0.000063 
l0: 0.000020, l1: 0.000022, l2: 0.000039, l3: 0.000112, l4: 0.000541, l5: 0.001556, l6: 0.002096
[epoch: 709/1000, batch:   144/ 1052, ite: 186240] train loss: 0.004020, tar: 0.000063 
l0: 0.000103, l1: 0.000103, l2: 0.000146, l3: 0.000292, l4: 0.000483, l5: 0.001343, l6: 0.002523
[epoch: 709/1000, batch:   224/ 1052, ite: 186260] train loss: 0.004020, tar: 0.000063 
l0: 0.000047, l1: 0.000047, l2: 0.000104, l3: 0.000213, l4: 0.000495, l5: 0.001311, l6: 0.001742
[epoch: 709/1000, batch:   304/ 1052, ite: 186280] train loss: 0.004019, tar: 0.000063 
l0: 0.000074, l1: 0.000075, l2: 0.000112, l3: 0.000198, l4: 0.000559, l5: 0.001812, l6: 0.002915
[epoch: 709/1000, batch:   384/ 1052, ite: 186300] train loss: 0.004019, tar: 0.000063 
l0: 0.000025, l1: 0.000025, l2: 0.000047, l3: 0.000172, l4: 0.000749, l5: 0.001622, l6: 0.002599
[epoch: 709/1000, batch:   464/ 1052, ite: 186320] train loss: 0.004019, tar: 0.000062 
l0: 0.000100, l1: 0.000103, l2: 0.000127, l3: 0.000255, l4: 0.000444, l5: 0.001128, l6: 0.002294
[epoch: 709/1000, batch:   544/ 1052, ite: 186340] train loss: 0.004018, tar: 0.000062 
l0: 0.000069, l1: 0.000075, l2: 0.000148, l3: 0.000279, l4: 0.000863, l5: 0.001755, l6: 0.001841
[epoch: 709/1000, batch:   624/ 1052, ite: 186360] train loss: 0.004018, tar: 0.000062 
l0: 0.000024, l1: 0.000023, l2: 0.000044, l3: 0.000163, l4: 0.000529, l5: 0.000693, l6: 0.001447
[epoch: 709/1000, batch:   704/ 1052, ite: 186380] train loss: 0.004017, tar: 0.000062 
l0: 0.000088, l1: 0.000087, l2: 0.000151, l3: 0.000231, l4: 0.000406, l5: 0.000832, l6: 0.002120
[epoch: 709/1000, batch:   784/ 1052, ite: 186400] train loss: 0.004017, tar: 0.000062 
l0: 0.000024, l1: 0.000025, l2: 0.000022, l3: 0.000079, l4: 0.000444, l5: 0.000938, l6: 0.001307
[epoch: 709/1000, batch:   864/ 1052, ite: 186420] train loss: 0.004018, tar: 0.000062 
l0: 0.000008, l1: 0.000007, l2: 0.000023, l3: 0.000105, l4: 0.000428, l5: 0.000685, l6: 0.001442
[epoch: 709/1000, batch:   944/ 1052, ite: 186440] train loss: 0.004018, tar: 0.000062 
l0: 0.000049, l1: 0.000049, l2: 0.000065, l3: 0.000124, l4: 0.000560, l5: 0.001079, l6: 0.002407
[epoch: 709/1000, batch:  1024/ 1052, ite: 186460] train loss: 0.004017, tar: 0.000062 
[Epoch 709/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000021, l1: 0.000022, l2: 0.000070, l3: 0.000103, l4: 0.000345, l5: 0.000734, l6: 0.001302
[epoch: 710/1000, batch:    52/ 1052, ite: 186480] train loss: 0.004017, tar: 0.000062 
l0: 0.000044, l1: 0.000043, l2: 0.000052, l3: 0.000122, l4: 0.000472, l5: 0.000913, l6: 0.001806
[epoch: 710/1000, batch:   132/ 1052, ite: 186500] train loss: 0.004017, tar: 0.000062 
l0: 0.000071, l1: 0.000069, l2: 0.000102, l3: 0.000211, l4: 0.000592, l5: 0.001314, l6: 0.001828
[epoch: 710/1000, batch:   212/ 1052, ite: 186520] train loss: 0.004016, tar: 0.000062 
l0: 0.000050, l1: 0.000050, l2: 0.000052, l3: 0.000112, l4: 0.000586, l5: 0.001051, l6: 0.002234
[epoch: 710/1000, batch:   292/ 1052, ite: 186540] train loss: 0.004015, tar: 0.000062 
l0: 0.000012, l1: 0.000013, l2: 0.000047, l3: 0.000120, l4: 0.000449, l5: 0.001624, l6: 0.003104
[epoch: 710/1000, batch:   372/ 1052, ite: 186560] train loss: 0.004016, tar: 0.000062 
l0: 0.000050, l1: 0.000050, l2: 0.000100, l3: 0.000265, l4: 0.000664, l5: 0.001403, l6: 0.003194
[epoch: 710/1000, batch:   452/ 1052, ite: 186580] train loss: 0.004015, tar: 0.000062 
l0: 0.000201, l1: 0.000198, l2: 0.000182, l3: 0.000286, l4: 0.000519, l5: 0.001055, l6: 0.002040
[epoch: 710/1000, batch:   532/ 1052, ite: 186600] train loss: 0.004015, tar: 0.000062 
l0: 0.000014, l1: 0.000014, l2: 0.000088, l3: 0.000170, l4: 0.000383, l5: 0.000953, l6: 0.001296
[epoch: 710/1000, batch:   612/ 1052, ite: 186620] train loss: 0.004016, tar: 0.000062 
l0: 0.000100, l1: 0.000102, l2: 0.000128, l3: 0.000231, l4: 0.000461, l5: 0.001058, l6: 0.000892
[epoch: 710/1000, batch:   692/ 1052, ite: 186640] train loss: 0.004016, tar: 0.000062 
l0: 0.000022, l1: 0.000022, l2: 0.000045, l3: 0.000098, l4: 0.000387, l5: 0.000851, l6: 0.001016
[epoch: 710/1000, batch:   772/ 1052, ite: 186660] train loss: 0.004015, tar: 0.000062 
l0: 0.000010, l1: 0.000010, l2: 0.000023, l3: 0.000072, l4: 0.000239, l5: 0.000945, l6: 0.001264
[epoch: 710/1000, batch:   852/ 1052, ite: 186680] train loss: 0.004014, tar: 0.000062 
l0: 0.000025, l1: 0.000025, l2: 0.000045, l3: 0.000140, l4: 0.000393, l5: 0.001012, l6: 0.002615
[epoch: 710/1000, batch:   932/ 1052, ite: 186700] train loss: 0.004014, tar: 0.000062 
l0: 0.000057, l1: 0.000059, l2: 0.000071, l3: 0.000132, l4: 0.000354, l5: 0.000688, l6: 0.001294
[epoch: 710/1000, batch:  1012/ 1052, ite: 186720] train loss: 0.004015, tar: 0.000062 
[Epoch 710/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000223, l1: 0.000231, l2: 0.000192, l3: 0.000187, l4: 0.000326, l5: 0.000733, l6: 0.001323
[epoch: 711/1000, batch:    40/ 1052, ite: 186740] train loss: 0.004016, tar: 0.000062 
l0: 0.000069, l1: 0.000070, l2: 0.000080, l3: 0.000142, l4: 0.000401, l5: 0.001151, l6: 0.001723
[epoch: 711/1000, batch:   120/ 1052, ite: 186760] train loss: 0.004015, tar: 0.000062 
l0: 0.000032, l1: 0.000033, l2: 0.000050, l3: 0.000093, l4: 0.000441, l5: 0.000972, l6: 0.001729
[epoch: 711/1000, batch:   200/ 1052, ite: 186780] train loss: 0.004015, tar: 0.000062 
l0: 0.000076, l1: 0.000077, l2: 0.000081, l3: 0.000127, l4: 0.000412, l5: 0.000572, l6: 0.001523
[epoch: 711/1000, batch:   280/ 1052, ite: 186800] train loss: 0.004015, tar: 0.000062 
l0: 0.000034, l1: 0.000034, l2: 0.000050, l3: 0.000124, l4: 0.000275, l5: 0.000447, l6: 0.001006
[epoch: 711/1000, batch:   360/ 1052, ite: 186820] train loss: 0.004014, tar: 0.000062 
l0: 0.000065, l1: 0.000063, l2: 0.000121, l3: 0.000207, l4: 0.000365, l5: 0.000801, l6: 0.002101
[epoch: 711/1000, batch:   440/ 1052, ite: 186840] train loss: 0.004015, tar: 0.000062 
l0: 0.000084, l1: 0.000088, l2: 0.000088, l3: 0.000175, l4: 0.000285, l5: 0.000708, l6: 0.000927
[epoch: 711/1000, batch:   520/ 1052, ite: 186860] train loss: 0.004016, tar: 0.000062 
l0: 0.000039, l1: 0.000039, l2: 0.000075, l3: 0.000143, l4: 0.000467, l5: 0.001148, l6: 0.001675
[epoch: 711/1000, batch:   600/ 1052, ite: 186880] train loss: 0.004015, tar: 0.000062 
l0: 0.000261, l1: 0.000256, l2: 0.000302, l3: 0.000383, l4: 0.000504, l5: 0.001123, l6: 0.001603
[epoch: 711/1000, batch:   680/ 1052, ite: 186900] train loss: 0.004014, tar: 0.000062 
l0: 0.000031, l1: 0.000031, l2: 0.000043, l3: 0.000145, l4: 0.000464, l5: 0.000923, l6: 0.001940
[epoch: 711/1000, batch:   760/ 1052, ite: 186920] train loss: 0.004014, tar: 0.000062 
l0: 0.000054, l1: 0.000055, l2: 0.000082, l3: 0.000108, l4: 0.000232, l5: 0.000659, l6: 0.001259
[epoch: 711/1000, batch:   840/ 1052, ite: 186940] train loss: 0.004016, tar: 0.000062 
l0: 0.000005, l1: 0.000005, l2: 0.000014, l3: 0.000063, l4: 0.000193, l5: 0.000679, l6: 0.001566
[epoch: 711/1000, batch:   920/ 1052, ite: 186960] train loss: 0.004016, tar: 0.000062 
l0: 0.000070, l1: 0.000070, l2: 0.000098, l3: 0.000212, l4: 0.000510, l5: 0.001254, l6: 0.001920
[epoch: 711/1000, batch:  1000/ 1052, ite: 186980] train loss: 0.004015, tar: 0.000061 
[Epoch 711/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000052, l1: 0.000053, l2: 0.000064, l3: 0.000138, l4: 0.000479, l5: 0.000982, l6: 0.002457
[epoch: 712/1000, batch:    28/ 1052, ite: 187000] train loss: 0.004015, tar: 0.000061 
l0: 0.000023, l1: 0.000022, l2: 0.000086, l3: 0.000184, l4: 0.000519, l5: 0.000638, l6: 0.001905
[epoch: 712/1000, batch:   108/ 1052, ite: 187020] train loss: 0.004015, tar: 0.000061 
l0: 0.000061, l1: 0.000061, l2: 0.000078, l3: 0.000181, l4: 0.000382, l5: 0.001185, l6: 0.001167
[epoch: 712/1000, batch:   188/ 1052, ite: 187040] train loss: 0.004015, tar: 0.000061 
l0: 0.000032, l1: 0.000033, l2: 0.000052, l3: 0.000106, l4: 0.000435, l5: 0.000762, l6: 0.001595
[epoch: 712/1000, batch:   268/ 1052, ite: 187060] train loss: 0.004015, tar: 0.000061 
l0: 0.000040, l1: 0.000040, l2: 0.000131, l3: 0.000214, l4: 0.000647, l5: 0.001448, l6: 0.003748
[epoch: 712/1000, batch:   348/ 1052, ite: 187080] train loss: 0.004014, tar: 0.000061 
l0: 0.000109, l1: 0.000114, l2: 0.000165, l3: 0.000230, l4: 0.000369, l5: 0.000841, l6: 0.001103
[epoch: 712/1000, batch:   428/ 1052, ite: 187100] train loss: 0.004015, tar: 0.000061 
l0: 0.000064, l1: 0.000063, l2: 0.000111, l3: 0.000216, l4: 0.000435, l5: 0.001703, l6: 0.001503
[epoch: 712/1000, batch:   508/ 1052, ite: 187120] train loss: 0.004014, tar: 0.000061 
l0: 0.000090, l1: 0.000091, l2: 0.000125, l3: 0.000177, l4: 0.000570, l5: 0.001054, l6: 0.001800
[epoch: 712/1000, batch:   588/ 1052, ite: 187140] train loss: 0.004013, tar: 0.000061 
l0: 0.000131, l1: 0.000129, l2: 0.000194, l3: 0.000356, l4: 0.000622, l5: 0.000909, l6: 0.001583
[epoch: 712/1000, batch:   668/ 1052, ite: 187160] train loss: 0.004013, tar: 0.000061 
l0: 0.000058, l1: 0.000060, l2: 0.000212, l3: 0.000542, l4: 0.001009, l5: 0.001998, l6: 0.003853
[epoch: 712/1000, batch:   748/ 1052, ite: 187180] train loss: 0.004012, tar: 0.000061 
l0: 0.000033, l1: 0.000034, l2: 0.000076, l3: 0.000135, l4: 0.000420, l5: 0.001316, l6: 0.001840
[epoch: 712/1000, batch:   828/ 1052, ite: 187200] train loss: 0.004011, tar: 0.000061 
l0: 0.000029, l1: 0.000030, l2: 0.000057, l3: 0.000137, l4: 0.000443, l5: 0.001967, l6: 0.002776
[epoch: 712/1000, batch:   908/ 1052, ite: 187220] train loss: 0.004013, tar: 0.000061 
l0: 0.000057, l1: 0.000060, l2: 0.000088, l3: 0.000156, l4: 0.000349, l5: 0.000964, l6: 0.002640
[epoch: 712/1000, batch:   988/ 1052, ite: 187240] train loss: 0.004012, tar: 0.000061 
[Epoch 712/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000036, l1: 0.000037, l2: 0.000077, l3: 0.000134, l4: 0.000382, l5: 0.000924, l6: 0.001836
[epoch: 713/1000, batch:    16/ 1052, ite: 187260] train loss: 0.004013, tar: 0.000061 
l0: 0.000063, l1: 0.000062, l2: 0.000080, l3: 0.000149, l4: 0.000335, l5: 0.000633, l6: 0.001387
[epoch: 713/1000, batch:    96/ 1052, ite: 187280] train loss: 0.004013, tar: 0.000061 
l0: 0.000023, l1: 0.000024, l2: 0.000050, l3: 0.000093, l4: 0.000321, l5: 0.001383, l6: 0.002329
[epoch: 713/1000, batch:   176/ 1052, ite: 187300] train loss: 0.004013, tar: 0.000061 
l0: 0.000031, l1: 0.000031, l2: 0.000064, l3: 0.000375, l4: 0.000709, l5: 0.001623, l6: 0.002695
[epoch: 713/1000, batch:   256/ 1052, ite: 187320] train loss: 0.004014, tar: 0.000061 
l0: 0.000056, l1: 0.000058, l2: 0.000098, l3: 0.000220, l4: 0.000509, l5: 0.000977, l6: 0.002581
[epoch: 713/1000, batch:   336/ 1052, ite: 187340] train loss: 0.004015, tar: 0.000061 
l0: 0.000048, l1: 0.000050, l2: 0.000102, l3: 0.000199, l4: 0.000428, l5: 0.001152, l6: 0.001930
[epoch: 713/1000, batch:   416/ 1052, ite: 187360] train loss: 0.004015, tar: 0.000061 
l0: 0.000013, l1: 0.000013, l2: 0.000047, l3: 0.000207, l4: 0.000497, l5: 0.001015, l6: 0.001640
[epoch: 713/1000, batch:   496/ 1052, ite: 187380] train loss: 0.004014, tar: 0.000061 
l0: 0.000063, l1: 0.000063, l2: 0.000099, l3: 0.000198, l4: 0.000371, l5: 0.000888, l6: 0.001342
[epoch: 713/1000, batch:   576/ 1052, ite: 187400] train loss: 0.004014, tar: 0.000061 
l0: 0.000020, l1: 0.000020, l2: 0.000035, l3: 0.000054, l4: 0.000194, l5: 0.000551, l6: 0.001450
[epoch: 713/1000, batch:   656/ 1052, ite: 187420] train loss: 0.004012, tar: 0.000061 
l0: 0.000003, l1: 0.000003, l2: 0.000020, l3: 0.000113, l4: 0.000464, l5: 0.001085, l6: 0.001960
[epoch: 713/1000, batch:   736/ 1052, ite: 187440] train loss: 0.004013, tar: 0.000061 
l0: 0.000033, l1: 0.000034, l2: 0.000081, l3: 0.000205, l4: 0.000648, l5: 0.001388, l6: 0.002805
[epoch: 713/1000, batch:   816/ 1052, ite: 187460] train loss: 0.004013, tar: 0.000061 
l0: 0.000009, l1: 0.000008, l2: 0.000034, l3: 0.000144, l4: 0.000396, l5: 0.000864, l6: 0.002358
[epoch: 713/1000, batch:   896/ 1052, ite: 187480] train loss: 0.004012, tar: 0.000061 
l0: 0.000020, l1: 0.000019, l2: 0.000047, l3: 0.000069, l4: 0.000159, l5: 0.000672, l6: 0.001106
[epoch: 713/1000, batch:   976/ 1052, ite: 187500] train loss: 0.004012, tar: 0.000061 
[Epoch 713/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000014, l1: 0.000013, l2: 0.000021, l3: 0.000079, l4: 0.000449, l5: 0.001196, l6: 0.001670
[epoch: 714/1000, batch:     4/ 1052, ite: 187520] train loss: 0.004011, tar: 0.000061 
l0: 0.000099, l1: 0.000101, l2: 0.000121, l3: 0.000236, l4: 0.000594, l5: 0.001339, l6: 0.002703
[epoch: 714/1000, batch:    84/ 1052, ite: 187540] train loss: 0.004010, tar: 0.000061 
l0: 0.000030, l1: 0.000031, l2: 0.000057, l3: 0.000127, l4: 0.000114, l5: 0.000658, l6: 0.001203
[epoch: 714/1000, batch:   164/ 1052, ite: 187560] train loss: 0.004010, tar: 0.000061 
l0: 0.000063, l1: 0.000063, l2: 0.000125, l3: 0.000195, l4: 0.000422, l5: 0.000777, l6: 0.001151
[epoch: 714/1000, batch:   244/ 1052, ite: 187580] train loss: 0.004010, tar: 0.000061 
l0: 0.000055, l1: 0.000056, l2: 0.000102, l3: 0.000238, l4: 0.000483, l5: 0.000594, l6: 0.001956
[epoch: 714/1000, batch:   324/ 1052, ite: 187600] train loss: 0.004009, tar: 0.000061 
l0: 0.000071, l1: 0.000073, l2: 0.000100, l3: 0.000190, l4: 0.000518, l5: 0.001386, l6: 0.002662
[epoch: 714/1000, batch:   404/ 1052, ite: 187620] train loss: 0.004009, tar: 0.000061 
l0: 0.000079, l1: 0.000081, l2: 0.000114, l3: 0.000279, l4: 0.000489, l5: 0.001294, l6: 0.003347
[epoch: 714/1000, batch:   484/ 1052, ite: 187640] train loss: 0.004008, tar: 0.000061 
l0: 0.000055, l1: 0.000054, l2: 0.000085, l3: 0.000225, l4: 0.000699, l5: 0.001698, l6: 0.002453
[epoch: 714/1000, batch:   564/ 1052, ite: 187660] train loss: 0.004009, tar: 0.000061 
l0: 0.000040, l1: 0.000041, l2: 0.000064, l3: 0.000195, l4: 0.000465, l5: 0.000795, l6: 0.001380
[epoch: 714/1000, batch:   644/ 1052, ite: 187680] train loss: 0.004009, tar: 0.000061 
l0: 0.000016, l1: 0.000017, l2: 0.000041, l3: 0.000107, l4: 0.000288, l5: 0.001166, l6: 0.001623
[epoch: 714/1000, batch:   724/ 1052, ite: 187700] train loss: 0.004011, tar: 0.000061 
l0: 0.000009, l1: 0.000010, l2: 0.000055, l3: 0.000137, l4: 0.000538, l5: 0.001515, l6: 0.002700
[epoch: 714/1000, batch:   804/ 1052, ite: 187720] train loss: 0.004009, tar: 0.000061 
l0: 0.000057, l1: 0.000058, l2: 0.000091, l3: 0.000262, l4: 0.000624, l5: 0.000992, l6: 0.003013
[epoch: 714/1000, batch:   884/ 1052, ite: 187740] train loss: 0.004009, tar: 0.000061 
l0: 0.000105, l1: 0.000107, l2: 0.000145, l3: 0.000339, l4: 0.000920, l5: 0.002460, l6: 0.004028
[epoch: 714/1000, batch:   964/ 1052, ite: 187760] train loss: 0.004010, tar: 0.000061 
l0: 0.000011, l1: 0.000011, l2: 0.000051, l3: 0.000249, l4: 0.000598, l5: 0.001904, l6: 0.002566
[epoch: 714/1000, batch:  1044/ 1052, ite: 187780] train loss: 0.004010, tar: 0.000061 
[Epoch 714/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000040, l1: 0.000041, l2: 0.000091, l3: 0.000277, l4: 0.000814, l5: 0.001538, l6: 0.002796
[epoch: 715/1000, batch:    72/ 1052, ite: 187800] train loss: 0.004010, tar: 0.000061 
l0: 0.000055, l1: 0.000058, l2: 0.000076, l3: 0.000118, l4: 0.000504, l5: 0.001019, l6: 0.002119
[epoch: 715/1000, batch:   152/ 1052, ite: 187820] train loss: 0.004010, tar: 0.000061 
l0: 0.000007, l1: 0.000008, l2: 0.000030, l3: 0.000099, l4: 0.000484, l5: 0.001554, l6: 0.002259
[epoch: 715/1000, batch:   232/ 1052, ite: 187840] train loss: 0.004010, tar: 0.000061 
l0: 0.000048, l1: 0.000047, l2: 0.000097, l3: 0.000288, l4: 0.000510, l5: 0.000882, l6: 0.002189
[epoch: 715/1000, batch:   312/ 1052, ite: 187860] train loss: 0.004010, tar: 0.000061 
l0: 0.000060, l1: 0.000067, l2: 0.000070, l3: 0.000115, l4: 0.000508, l5: 0.000674, l6: 0.001678
[epoch: 715/1000, batch:   392/ 1052, ite: 187880] train loss: 0.004009, tar: 0.000061 
l0: 0.000022, l1: 0.000025, l2: 0.000066, l3: 0.000314, l4: 0.000794, l5: 0.002169, l6: 0.004103
[epoch: 715/1000, batch:   472/ 1052, ite: 187900] train loss: 0.004010, tar: 0.000061 
l0: 0.000019, l1: 0.000018, l2: 0.000049, l3: 0.000086, l4: 0.000244, l5: 0.000638, l6: 0.001233
[epoch: 715/1000, batch:   552/ 1052, ite: 187920] train loss: 0.004011, tar: 0.000061 
l0: 0.000014, l1: 0.000016, l2: 0.000035, l3: 0.000066, l4: 0.000157, l5: 0.000690, l6: 0.001768
[epoch: 715/1000, batch:   632/ 1052, ite: 187940] train loss: 0.004010, tar: 0.000061 
l0: 0.000016, l1: 0.000015, l2: 0.000082, l3: 0.000177, l4: 0.000379, l5: 0.001167, l6: 0.001661
[epoch: 715/1000, batch:   712/ 1052, ite: 187960] train loss: 0.004010, tar: 0.000060 
l0: 0.000070, l1: 0.000072, l2: 0.000097, l3: 0.000205, l4: 0.000430, l5: 0.000876, l6: 0.002343
[epoch: 715/1000, batch:   792/ 1052, ite: 187980] train loss: 0.004009, tar: 0.000060 
l0: 0.000032, l1: 0.000029, l2: 0.000068, l3: 0.000217, l4: 0.000531, l5: 0.000590, l6: 0.001860
[epoch: 715/1000, batch:   872/ 1052, ite: 188000] train loss: 0.004010, tar: 0.000060 
l0: 0.000046, l1: 0.000046, l2: 0.000081, l3: 0.000128, l4: 0.000313, l5: 0.000819, l6: 0.001462
[epoch: 715/1000, batch:   952/ 1052, ite: 188020] train loss: 0.004010, tar: 0.000060 
l0: 0.000005, l1: 0.000011, l2: 0.000011, l3: 0.000031, l4: 0.000203, l5: 0.000682, l6: 0.000863
[epoch: 715/1000, batch:  1032/ 1052, ite: 188040] train loss: 0.004010, tar: 0.000060 
[Epoch 715/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000046, l1: 0.000048, l2: 0.000097, l3: 0.000251, l4: 0.000508, l5: 0.001332, l6: 0.002995
[epoch: 716/1000, batch:    60/ 1052, ite: 188060] train loss: 0.004010, tar: 0.000060 
l0: 0.000045, l1: 0.000045, l2: 0.000088, l3: 0.000182, l4: 0.000691, l5: 0.001398, l6: 0.001994
[epoch: 716/1000, batch:   140/ 1052, ite: 188080] train loss: 0.004010, tar: 0.000060 
l0: 0.000165, l1: 0.000167, l2: 0.000178, l3: 0.000218, l4: 0.000607, l5: 0.001531, l6: 0.001961
[epoch: 716/1000, batch:   220/ 1052, ite: 188100] train loss: 0.004010, tar: 0.000060 
l0: 0.000024, l1: 0.000024, l2: 0.000054, l3: 0.000170, l4: 0.000516, l5: 0.001326, l6: 0.002294
[epoch: 716/1000, batch:   300/ 1052, ite: 188120] train loss: 0.004009, tar: 0.000060 
l0: 0.000096, l1: 0.000099, l2: 0.000157, l3: 0.000280, l4: 0.000424, l5: 0.001015, l6: 0.001533
[epoch: 716/1000, batch:   380/ 1052, ite: 188140] train loss: 0.004008, tar: 0.000060 
l0: 0.000029, l1: 0.000029, l2: 0.000067, l3: 0.000167, l4: 0.000344, l5: 0.001170, l6: 0.002381
[epoch: 716/1000, batch:   460/ 1052, ite: 188160] train loss: 0.004009, tar: 0.000060 
l0: 0.000003, l1: 0.000003, l2: 0.000021, l3: 0.000064, l4: 0.000207, l5: 0.001198, l6: 0.002378
[epoch: 716/1000, batch:   540/ 1052, ite: 188180] train loss: 0.004008, tar: 0.000060 
l0: 0.000004, l1: 0.000004, l2: 0.000011, l3: 0.000078, l4: 0.000236, l5: 0.000903, l6: 0.001442
[epoch: 716/1000, batch:   620/ 1052, ite: 188200] train loss: 0.004009, tar: 0.000060 
l0: 0.000063, l1: 0.000065, l2: 0.000100, l3: 0.000233, l4: 0.000583, l5: 0.001360, l6: 0.001705
[epoch: 716/1000, batch:   700/ 1052, ite: 188220] train loss: 0.004008, tar: 0.000060 
l0: 0.000022, l1: 0.000026, l2: 0.000073, l3: 0.000205, l4: 0.000408, l5: 0.001069, l6: 0.001699
[epoch: 716/1000, batch:   780/ 1052, ite: 188240] train loss: 0.004010, tar: 0.000060 
l0: 0.000046, l1: 0.000048, l2: 0.000067, l3: 0.000179, l4: 0.000455, l5: 0.000687, l6: 0.001098
[epoch: 716/1000, batch:   860/ 1052, ite: 188260] train loss: 0.004009, tar: 0.000060 
l0: 0.000011, l1: 0.000011, l2: 0.000017, l3: 0.000078, l4: 0.000231, l5: 0.000352, l6: 0.000771
[epoch: 716/1000, batch:   940/ 1052, ite: 188280] train loss: 0.004008, tar: 0.000060 
l0: 0.000108, l1: 0.000106, l2: 0.000198, l3: 0.000553, l4: 0.000834, l5: 0.001275, l6: 0.002870
[epoch: 716/1000, batch:  1020/ 1052, ite: 188300] train loss: 0.004008, tar: 0.000060 
[Epoch 716/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000011, l1: 0.000010, l2: 0.000033, l3: 0.000085, l4: 0.000355, l5: 0.000769, l6: 0.001530
[epoch: 717/1000, batch:    48/ 1052, ite: 188320] train loss: 0.004008, tar: 0.000060 
l0: 0.000080, l1: 0.000078, l2: 0.000148, l3: 0.000250, l4: 0.000569, l5: 0.000896, l6: 0.001567
[epoch: 717/1000, batch:   128/ 1052, ite: 188340] train loss: 0.004009, tar: 0.000060 
l0: 0.000033, l1: 0.000033, l2: 0.000048, l3: 0.000115, l4: 0.000378, l5: 0.001180, l6: 0.000822
[epoch: 717/1000, batch:   208/ 1052, ite: 188360] train loss: 0.004008, tar: 0.000060 
l0: 0.000093, l1: 0.000091, l2: 0.000143, l3: 0.000345, l4: 0.000488, l5: 0.001398, l6: 0.001626
[epoch: 717/1000, batch:   288/ 1052, ite: 188380] train loss: 0.004009, tar: 0.000060 
l0: 0.000019, l1: 0.000019, l2: 0.000044, l3: 0.000133, l4: 0.000353, l5: 0.001206, l6: 0.001363
[epoch: 717/1000, batch:   368/ 1052, ite: 188400] train loss: 0.004009, tar: 0.000060 
l0: 0.000065, l1: 0.000067, l2: 0.000084, l3: 0.000222, l4: 0.000495, l5: 0.000814, l6: 0.002349
[epoch: 717/1000, batch:   448/ 1052, ite: 188420] train loss: 0.004010, tar: 0.000060 
l0: 0.000011, l1: 0.000013, l2: 0.000019, l3: 0.000129, l4: 0.000300, l5: 0.000764, l6: 0.001087
[epoch: 717/1000, batch:   528/ 1052, ite: 188440] train loss: 0.004008, tar: 0.000060 
l0: 0.000029, l1: 0.000028, l2: 0.000083, l3: 0.000291, l4: 0.000367, l5: 0.001078, l6: 0.002949
[epoch: 717/1000, batch:   608/ 1052, ite: 188460] train loss: 0.004007, tar: 0.000060 
l0: 0.000055, l1: 0.000061, l2: 0.000093, l3: 0.000177, l4: 0.000398, l5: 0.001123, l6: 0.001836
[epoch: 717/1000, batch:   688/ 1052, ite: 188480] train loss: 0.004007, tar: 0.000060 
l0: 0.000044, l1: 0.000044, l2: 0.000081, l3: 0.000198, l4: 0.000432, l5: 0.000923, l6: 0.002099
[epoch: 717/1000, batch:   768/ 1052, ite: 188500] train loss: 0.004006, tar: 0.000060 
l0: 0.000025, l1: 0.000025, l2: 0.000056, l3: 0.000136, l4: 0.000386, l5: 0.000890, l6: 0.002084
[epoch: 717/1000, batch:   848/ 1052, ite: 188520] train loss: 0.004007, tar: 0.000060 
l0: 0.000015, l1: 0.000016, l2: 0.000035, l3: 0.000097, l4: 0.000296, l5: 0.000769, l6: 0.001924
[epoch: 717/1000, batch:   928/ 1052, ite: 188540] train loss: 0.004007, tar: 0.000060 
l0: 0.000021, l1: 0.000020, l2: 0.000069, l3: 0.000175, l4: 0.000574, l5: 0.001339, l6: 0.002512
[epoch: 717/1000, batch:  1008/ 1052, ite: 188560] train loss: 0.004007, tar: 0.000060 
[Epoch 717/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000148, l1: 0.000150, l2: 0.000227, l3: 0.000404, l4: 0.000638, l5: 0.002046, l6: 0.003780
[epoch: 718/1000, batch:    36/ 1052, ite: 188580] train loss: 0.004007, tar: 0.000060 
l0: 0.000085, l1: 0.000083, l2: 0.000174, l3: 0.000353, l4: 0.000992, l5: 0.001893, l6: 0.004114
[epoch: 718/1000, batch:   116/ 1052, ite: 188600] train loss: 0.004007, tar: 0.000060 
l0: 0.000123, l1: 0.000125, l2: 0.000153, l3: 0.000282, l4: 0.000508, l5: 0.001753, l6: 0.003965
[epoch: 718/1000, batch:   196/ 1052, ite: 188620] train loss: 0.004007, tar: 0.000060 
l0: 0.000126, l1: 0.000127, l2: 0.000095, l3: 0.000239, l4: 0.000391, l5: 0.000924, l6: 0.001368
[epoch: 718/1000, batch:   276/ 1052, ite: 188640] train loss: 0.004007, tar: 0.000060 
l0: 0.000030, l1: 0.000032, l2: 0.000044, l3: 0.000109, l4: 0.000535, l5: 0.000722, l6: 0.002106
[epoch: 718/1000, batch:   356/ 1052, ite: 188660] train loss: 0.004007, tar: 0.000060 
l0: 0.000076, l1: 0.000075, l2: 0.000100, l3: 0.000216, l4: 0.000631, l5: 0.001372, l6: 0.002062
[epoch: 718/1000, batch:   436/ 1052, ite: 188680] train loss: 0.004008, tar: 0.000060 
l0: 0.000052, l1: 0.000051, l2: 0.000090, l3: 0.000214, l4: 0.000703, l5: 0.000960, l6: 0.002485
[epoch: 718/1000, batch:   516/ 1052, ite: 188700] train loss: 0.004007, tar: 0.000060 
l0: 0.000036, l1: 0.000035, l2: 0.000068, l3: 0.000167, l4: 0.000458, l5: 0.001306, l6: 0.002288
[epoch: 718/1000, batch:   596/ 1052, ite: 188720] train loss: 0.004009, tar: 0.000060 
l0: 0.000053, l1: 0.000054, l2: 0.000068, l3: 0.000090, l4: 0.000286, l5: 0.000638, l6: 0.001300
[epoch: 718/1000, batch:   676/ 1052, ite: 188740] train loss: 0.004009, tar: 0.000060 
l0: 0.000029, l1: 0.000030, l2: 0.000061, l3: 0.000131, l4: 0.000414, l5: 0.000820, l6: 0.001681
[epoch: 718/1000, batch:   756/ 1052, ite: 188760] train loss: 0.004008, tar: 0.000060 
l0: 0.000017, l1: 0.000017, l2: 0.000051, l3: 0.000130, l4: 0.000390, l5: 0.000660, l6: 0.001310
[epoch: 718/1000, batch:   836/ 1052, ite: 188780] train loss: 0.004008, tar: 0.000060 
l0: 0.000109, l1: 0.000115, l2: 0.000132, l3: 0.000290, l4: 0.000575, l5: 0.001124, l6: 0.002868
[epoch: 718/1000, batch:   916/ 1052, ite: 188800] train loss: 0.004007, tar: 0.000060 
l0: 0.000056, l1: 0.000056, l2: 0.000097, l3: 0.000240, l4: 0.000563, l5: 0.001536, l6: 0.002243
[epoch: 718/1000, batch:   996/ 1052, ite: 188820] train loss: 0.004006, tar: 0.000060 
[Epoch 718/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000042, l1: 0.000043, l2: 0.000070, l3: 0.000122, l4: 0.000361, l5: 0.001072, l6: 0.001967
[epoch: 719/1000, batch:    24/ 1052, ite: 188840] train loss: 0.004007, tar: 0.000060 
l0: 0.000002, l1: 0.000002, l2: 0.000025, l3: 0.000082, l4: 0.000236, l5: 0.000870, l6: 0.001645
[epoch: 719/1000, batch:   104/ 1052, ite: 188860] train loss: 0.004006, tar: 0.000060 
l0: 0.000039, l1: 0.000040, l2: 0.000101, l3: 0.000221, l4: 0.000659, l5: 0.001416, l6: 0.003182
[epoch: 719/1000, batch:   184/ 1052, ite: 188880] train loss: 0.004006, tar: 0.000060 
l0: 0.000083, l1: 0.000088, l2: 0.000204, l3: 0.000352, l4: 0.000832, l5: 0.001616, l6: 0.005009
[epoch: 719/1000, batch:   264/ 1052, ite: 188900] train loss: 0.004007, tar: 0.000060 
l0: 0.000020, l1: 0.000020, l2: 0.000030, l3: 0.000157, l4: 0.000241, l5: 0.000733, l6: 0.001089
[epoch: 719/1000, batch:   344/ 1052, ite: 188920] train loss: 0.004007, tar: 0.000060 
l0: 0.000041, l1: 0.000042, l2: 0.000069, l3: 0.000134, l4: 0.000272, l5: 0.001188, l6: 0.002159
[epoch: 719/1000, batch:   424/ 1052, ite: 188940] train loss: 0.004007, tar: 0.000060 
l0: 0.000162, l1: 0.000167, l2: 0.000183, l3: 0.000236, l4: 0.000652, l5: 0.001798, l6: 0.002795
[epoch: 719/1000, batch:   504/ 1052, ite: 188960] train loss: 0.004007, tar: 0.000060 
l0: 0.000193, l1: 0.000198, l2: 0.000193, l3: 0.000304, l4: 0.000546, l5: 0.001355, l6: 0.002561
[epoch: 719/1000, batch:   584/ 1052, ite: 188980] train loss: 0.004007, tar: 0.000060 
l0: 0.000053, l1: 0.000052, l2: 0.000084, l3: 0.000131, l4: 0.000456, l5: 0.001140, l6: 0.001161
[epoch: 719/1000, batch:   664/ 1052, ite: 189000] train loss: 0.004007, tar: 0.000060 
l0: 0.000029, l1: 0.000029, l2: 0.000089, l3: 0.000336, l4: 0.000785, l5: 0.002201, l6: 0.003878
[epoch: 719/1000, batch:   744/ 1052, ite: 189020] train loss: 0.004007, tar: 0.000060 
l0: 0.000017, l1: 0.000017, l2: 0.000067, l3: 0.000167, l4: 0.000337, l5: 0.000885, l6: 0.001426
[epoch: 719/1000, batch:   824/ 1052, ite: 189040] train loss: 0.004007, tar: 0.000060 
l0: 0.000029, l1: 0.000030, l2: 0.000068, l3: 0.000267, l4: 0.000651, l5: 0.001634, l6: 0.002994
[epoch: 719/1000, batch:   904/ 1052, ite: 189060] train loss: 0.004006, tar: 0.000060 
l0: 0.000022, l1: 0.000022, l2: 0.000043, l3: 0.000097, l4: 0.000324, l5: 0.000985, l6: 0.001284
[epoch: 719/1000, batch:   984/ 1052, ite: 189080] train loss: 0.004007, tar: 0.000060 
[Epoch 719/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000094, l1: 0.000095, l2: 0.000091, l3: 0.000152, l4: 0.000452, l5: 0.000863, l6: 0.002620
[epoch: 720/1000, batch:    12/ 1052, ite: 189100] train loss: 0.004007, tar: 0.000060 
l0: 0.000044, l1: 0.000046, l2: 0.000057, l3: 0.000131, l4: 0.000420, l5: 0.001145, l6: 0.001683
[epoch: 720/1000, batch:    92/ 1052, ite: 189120] train loss: 0.004006, tar: 0.000060 
l0: 0.000072, l1: 0.000070, l2: 0.000129, l3: 0.000241, l4: 0.000739, l5: 0.001804, l6: 0.003032
[epoch: 720/1000, batch:   172/ 1052, ite: 189140] train loss: 0.004007, tar: 0.000060 
l0: 0.000023, l1: 0.000024, l2: 0.000076, l3: 0.000338, l4: 0.000748, l5: 0.001820, l6: 0.003283
[epoch: 720/1000, batch:   252/ 1052, ite: 189160] train loss: 0.004007, tar: 0.000060 
l0: 0.000043, l1: 0.000043, l2: 0.000091, l3: 0.000156, l4: 0.000455, l5: 0.000775, l6: 0.001211
[epoch: 720/1000, batch:   332/ 1052, ite: 189180] train loss: 0.004008, tar: 0.000060 
l0: 0.000074, l1: 0.000077, l2: 0.000145, l3: 0.000302, l4: 0.000750, l5: 0.001490, l6: 0.002284
[epoch: 720/1000, batch:   412/ 1052, ite: 189200] train loss: 0.004008, tar: 0.000060 
l0: 0.000028, l1: 0.000025, l2: 0.000075, l3: 0.000238, l4: 0.000399, l5: 0.001094, l6: 0.002247
[epoch: 720/1000, batch:   492/ 1052, ite: 189220] train loss: 0.004008, tar: 0.000060 
l0: 0.000025, l1: 0.000027, l2: 0.000040, l3: 0.000105, l4: 0.000320, l5: 0.000801, l6: 0.000764
[epoch: 720/1000, batch:   572/ 1052, ite: 189240] train loss: 0.004008, tar: 0.000060 
l0: 0.000007, l1: 0.000009, l2: 0.000022, l3: 0.000197, l4: 0.000473, l5: 0.000884, l6: 0.001705
[epoch: 720/1000, batch:   652/ 1052, ite: 189260] train loss: 0.004007, tar: 0.000060 
l0: 0.000033, l1: 0.000030, l2: 0.000088, l3: 0.000320, l4: 0.000645, l5: 0.001101, l6: 0.002437
[epoch: 720/1000, batch:   732/ 1052, ite: 189280] train loss: 0.004006, tar: 0.000060 
l0: 0.000035, l1: 0.000033, l2: 0.000084, l3: 0.000120, l4: 0.000385, l5: 0.000885, l6: 0.001408
[epoch: 720/1000, batch:   812/ 1052, ite: 189300] train loss: 0.004006, tar: 0.000060 
l0: 0.000030, l1: 0.000029, l2: 0.000062, l3: 0.000215, l4: 0.000421, l5: 0.000889, l6: 0.001556
[epoch: 720/1000, batch:   892/ 1052, ite: 189320] train loss: 0.004007, tar: 0.000060 
l0: 0.000023, l1: 0.000024, l2: 0.000035, l3: 0.000097, l4: 0.000370, l5: 0.000782, l6: 0.001281
[epoch: 720/1000, batch:   972/ 1052, ite: 189340] train loss: 0.004006, tar: 0.000060 
l0: 0.000016, l1: 0.000015, l2: 0.000053, l3: 0.000148, l4: 0.000443, l5: 0.000839, l6: 0.001700
[epoch: 720/1000, batch:  1052/ 1052, ite: 189360] train loss: 0.004006, tar: 0.000060 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_189360.pkl
[Epoch 720/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000096, l1: 0.000097, l2: 0.000135, l3: 0.000181, l4: 0.000496, l5: 0.001344, l6: 0.002554
[epoch: 721/1000, batch:    80/ 1052, ite: 189380] train loss: 0.004428, tar: 0.000062 
l0: 0.000028, l1: 0.000024, l2: 0.000052, l3: 0.000087, l4: 0.000176, l5: 0.000710, l6: 0.001013
[epoch: 721/1000, batch:   160/ 1052, ite: 189400] train loss: 0.003807, tar: 0.000047 
l0: 0.000043, l1: 0.000048, l2: 0.000089, l3: 0.000348, l4: 0.000866, l5: 0.002310, l6: 0.003426
[epoch: 721/1000, batch:   240/ 1052, ite: 189420] train loss: 0.003887, tar: 0.000048 
l0: 0.000011, l1: 0.000012, l2: 0.000048, l3: 0.000191, l4: 0.000686, l5: 0.001034, l6: 0.001863
[epoch: 721/1000, batch:   320/ 1052, ite: 189440] train loss: 0.003946, tar: 0.000050 
l0: 0.000110, l1: 0.000111, l2: 0.000134, l3: 0.000260, l4: 0.000550, l5: 0.001458, l6: 0.002815
[epoch: 721/1000, batch:   400/ 1052, ite: 189460] train loss: 0.003861, tar: 0.000051 
l0: 0.000009, l1: 0.000008, l2: 0.000024, l3: 0.000124, l4: 0.000541, l5: 0.001462, l6: 0.002167
[epoch: 721/1000, batch:   480/ 1052, ite: 189480] train loss: 0.003923, tar: 0.000054 
l0: 0.000069, l1: 0.000072, l2: 0.000099, l3: 0.000137, l4: 0.000459, l5: 0.001082, l6: 0.002109
[epoch: 721/1000, batch:   560/ 1052, ite: 189500] train loss: 0.003979, tar: 0.000056 
l0: 0.000138, l1: 0.000143, l2: 0.000124, l3: 0.000251, l4: 0.000637, l5: 0.000653, l6: 0.001906
[epoch: 721/1000, batch:   640/ 1052, ite: 189520] train loss: 0.003936, tar: 0.000054 
l0: 0.000077, l1: 0.000076, l2: 0.000135, l3: 0.000266, l4: 0.000439, l5: 0.000938, l6: 0.001220
[epoch: 721/1000, batch:   720/ 1052, ite: 189540] train loss: 0.003956, tar: 0.000055 
l0: 0.000022, l1: 0.000020, l2: 0.000036, l3: 0.000073, l4: 0.000369, l5: 0.000554, l6: 0.001426
[epoch: 721/1000, batch:   800/ 1052, ite: 189560] train loss: 0.003973, tar: 0.000056 
l0: 0.000008, l1: 0.000007, l2: 0.000033, l3: 0.000114, l4: 0.000400, l5: 0.001163, l6: 0.002051
[epoch: 721/1000, batch:   880/ 1052, ite: 189580] train loss: 0.003891, tar: 0.000053 
l0: 0.000010, l1: 0.000010, l2: 0.000030, l3: 0.000068, l4: 0.000333, l5: 0.000743, l6: 0.001050
[epoch: 721/1000, batch:   960/ 1052, ite: 189600] train loss: 0.003914, tar: 0.000053 
l0: 0.000036, l1: 0.000036, l2: 0.000073, l3: 0.000281, l4: 0.000722, l5: 0.001932, l6: 0.004216
[epoch: 721/1000, batch:  1040/ 1052, ite: 189620] train loss: 0.003945, tar: 0.000053 
[Epoch 721/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000083, l1: 0.000084, l2: 0.000117, l3: 0.000215, l4: 0.000373, l5: 0.001191, l6: 0.001489
[epoch: 722/1000, batch:    68/ 1052, ite: 189640] train loss: 0.003945, tar: 0.000055 
l0: 0.000071, l1: 0.000072, l2: 0.000096, l3: 0.000216, l4: 0.000444, l5: 0.001208, l6: 0.002918
[epoch: 722/1000, batch:   148/ 1052, ite: 189660] train loss: 0.003961, tar: 0.000055 
l0: 0.000013, l1: 0.000012, l2: 0.000041, l3: 0.000141, l4: 0.000253, l5: 0.000592, l6: 0.001413
[epoch: 722/1000, batch:   228/ 1052, ite: 189680] train loss: 0.003955, tar: 0.000054 
l0: 0.000076, l1: 0.000077, l2: 0.000098, l3: 0.000171, l4: 0.000501, l5: 0.001040, l6: 0.001785
[epoch: 722/1000, batch:   308/ 1052, ite: 189700] train loss: 0.003960, tar: 0.000055 
l0: 0.000050, l1: 0.000051, l2: 0.000068, l3: 0.000159, l4: 0.000300, l5: 0.001136, l6: 0.001284
[epoch: 722/1000, batch:   388/ 1052, ite: 189720] train loss: 0.003967, tar: 0.000056 
l0: 0.000090, l1: 0.000086, l2: 0.000110, l3: 0.000163, l4: 0.000416, l5: 0.000947, l6: 0.002689
[epoch: 722/1000, batch:   468/ 1052, ite: 189740] train loss: 0.003927, tar: 0.000055 
l0: 0.000099, l1: 0.000101, l2: 0.000175, l3: 0.000363, l4: 0.000942, l5: 0.002254, l6: 0.004249
[epoch: 722/1000, batch:   548/ 1052, ite: 189760] train loss: 0.003932, tar: 0.000054 
l0: 0.000013, l1: 0.000012, l2: 0.000041, l3: 0.000158, l4: 0.000451, l5: 0.000874, l6: 0.001466
[epoch: 722/1000, batch:   628/ 1052, ite: 189780] train loss: 0.003921, tar: 0.000054 
l0: 0.000051, l1: 0.000057, l2: 0.000085, l3: 0.000142, l4: 0.000470, l5: 0.001128, l6: 0.001813
[epoch: 722/1000, batch:   708/ 1052, ite: 189800] train loss: 0.003920, tar: 0.000054 
l0: 0.000002, l1: 0.000002, l2: 0.000009, l3: 0.000100, l4: 0.000283, l5: 0.000870, l6: 0.000977
[epoch: 722/1000, batch:   788/ 1052, ite: 189820] train loss: 0.003939, tar: 0.000054 
l0: 0.000120, l1: 0.000122, l2: 0.000171, l3: 0.000284, l4: 0.000604, l5: 0.001121, l6: 0.003577
[epoch: 722/1000, batch:   868/ 1052, ite: 189840] train loss: 0.003971, tar: 0.000055 
l0: 0.000041, l1: 0.000041, l2: 0.000061, l3: 0.000229, l4: 0.000533, l5: 0.002704, l6: 0.004252
[epoch: 722/1000, batch:   948/ 1052, ite: 189860] train loss: 0.003983, tar: 0.000055 
l0: 0.000048, l1: 0.000047, l2: 0.000093, l3: 0.000252, l4: 0.000646, l5: 0.001442, l6: 0.001777
[epoch: 722/1000, batch:  1028/ 1052, ite: 189880] train loss: 0.003969, tar: 0.000055 
[Epoch 722/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000025, l1: 0.000026, l2: 0.000035, l3: 0.000137, l4: 0.000420, l5: 0.000891, l6: 0.001623
[epoch: 723/1000, batch:    56/ 1052, ite: 189900] train loss: 0.003963, tar: 0.000055 
l0: 0.000034, l1: 0.000033, l2: 0.000057, l3: 0.000123, l4: 0.000484, l5: 0.000779, l6: 0.001318
[epoch: 723/1000, batch:   136/ 1052, ite: 189920] train loss: 0.003941, tar: 0.000054 
l0: 0.000120, l1: 0.000122, l2: 0.000115, l3: 0.000183, l4: 0.000417, l5: 0.000966, l6: 0.002667
[epoch: 723/1000, batch:   216/ 1052, ite: 189940] train loss: 0.003935, tar: 0.000054 
l0: 0.000016, l1: 0.000015, l2: 0.000078, l3: 0.000177, l4: 0.000369, l5: 0.000731, l6: 0.003177
[epoch: 723/1000, batch:   296/ 1052, ite: 189960] train loss: 0.003956, tar: 0.000054 
l0: 0.000131, l1: 0.000132, l2: 0.000192, l3: 0.000366, l4: 0.000901, l5: 0.001709, l6: 0.004465
[epoch: 723/1000, batch:   376/ 1052, ite: 189980] train loss: 0.003953, tar: 0.000054 
l0: 0.000028, l1: 0.000026, l2: 0.000091, l3: 0.000439, l4: 0.000990, l5: 0.001859, l6: 0.002510
[epoch: 723/1000, batch:   456/ 1052, ite: 190000] train loss: 0.003952, tar: 0.000054 
l0: 0.000003, l1: 0.000004, l2: 0.000022, l3: 0.000076, l4: 0.000349, l5: 0.000972, l6: 0.001557
[epoch: 723/1000, batch:   536/ 1052, ite: 190020] train loss: 0.003961, tar: 0.000054 
l0: 0.000012, l1: 0.000011, l2: 0.000076, l3: 0.000172, l4: 0.000452, l5: 0.000825, l6: 0.001963
[epoch: 723/1000, batch:   616/ 1052, ite: 190040] train loss: 0.003965, tar: 0.000055 
l0: 0.000038, l1: 0.000039, l2: 0.000077, l3: 0.000144, l4: 0.000412, l5: 0.001039, l6: 0.001811
[epoch: 723/1000, batch:   696/ 1052, ite: 190060] train loss: 0.003969, tar: 0.000055 
l0: 0.000133, l1: 0.000132, l2: 0.000197, l3: 0.000404, l4: 0.000893, l5: 0.001457, l6: 0.002855
[epoch: 723/1000, batch:   776/ 1052, ite: 190080] train loss: 0.003963, tar: 0.000055 
l0: 0.000015, l1: 0.000015, l2: 0.000053, l3: 0.000226, l4: 0.000463, l5: 0.000546, l6: 0.001300
[epoch: 723/1000, batch:   856/ 1052, ite: 190100] train loss: 0.003957, tar: 0.000055 
l0: 0.000029, l1: 0.000029, l2: 0.000052, l3: 0.000133, l4: 0.000298, l5: 0.000795, l6: 0.001340
[epoch: 723/1000, batch:   936/ 1052, ite: 190120] train loss: 0.003958, tar: 0.000055 
l0: 0.000010, l1: 0.000010, l2: 0.000051, l3: 0.000166, l4: 0.000517, l5: 0.001104, l6: 0.001065
[epoch: 723/1000, batch:  1016/ 1052, ite: 190140] train loss: 0.003956, tar: 0.000054 
[Epoch 723/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000015, l1: 0.000014, l2: 0.000026, l3: 0.000036, l4: 0.000317, l5: 0.000966, l6: 0.001105
[epoch: 724/1000, batch:    44/ 1052, ite: 190160] train loss: 0.003967, tar: 0.000054 
l0: 0.000107, l1: 0.000107, l2: 0.000183, l3: 0.000272, l4: 0.000618, l5: 0.002223, l6: 0.003131
[epoch: 724/1000, batch:   124/ 1052, ite: 190180] train loss: 0.003968, tar: 0.000054 
l0: 0.000102, l1: 0.000103, l2: 0.000134, l3: 0.000289, l4: 0.000522, l5: 0.001212, l6: 0.001929
[epoch: 724/1000, batch:   204/ 1052, ite: 190200] train loss: 0.003993, tar: 0.000055 
l0: 0.000052, l1: 0.000054, l2: 0.000083, l3: 0.000144, l4: 0.000307, l5: 0.000828, l6: 0.001927
[epoch: 724/1000, batch:   284/ 1052, ite: 190220] train loss: 0.003987, tar: 0.000055 
l0: 0.000015, l1: 0.000014, l2: 0.000018, l3: 0.000070, l4: 0.000274, l5: 0.000520, l6: 0.000897
[epoch: 724/1000, batch:   364/ 1052, ite: 190240] train loss: 0.003972, tar: 0.000055 
l0: 0.000079, l1: 0.000078, l2: 0.000112, l3: 0.000255, l4: 0.000533, l5: 0.001308, l6: 0.002114
[epoch: 724/1000, batch:   444/ 1052, ite: 190260] train loss: 0.003966, tar: 0.000055 
l0: 0.000008, l1: 0.000008, l2: 0.000019, l3: 0.000110, l4: 0.000349, l5: 0.001252, l6: 0.001903
[epoch: 724/1000, batch:   524/ 1052, ite: 190280] train loss: 0.003955, tar: 0.000054 
l0: 0.000077, l1: 0.000077, l2: 0.000102, l3: 0.000204, l4: 0.000414, l5: 0.001420, l6: 0.001922
[epoch: 724/1000, batch:   604/ 1052, ite: 190300] train loss: 0.003956, tar: 0.000054 
l0: 0.000003, l1: 0.000004, l2: 0.000020, l3: 0.000078, l4: 0.000283, l5: 0.000518, l6: 0.000947
[epoch: 724/1000, batch:   684/ 1052, ite: 190320] train loss: 0.003964, tar: 0.000054 
l0: 0.000031, l1: 0.000033, l2: 0.000104, l3: 0.000328, l4: 0.000811, l5: 0.001736, l6: 0.003119
[epoch: 724/1000, batch:   764/ 1052, ite: 190340] train loss: 0.003968, tar: 0.000054 
l0: 0.000010, l1: 0.000010, l2: 0.000017, l3: 0.000050, l4: 0.000214, l5: 0.000475, l6: 0.000957
[epoch: 724/1000, batch:   844/ 1052, ite: 190360] train loss: 0.003960, tar: 0.000054 
l0: 0.000023, l1: 0.000025, l2: 0.000079, l3: 0.000174, l4: 0.000385, l5: 0.001250, l6: 0.003990
[epoch: 724/1000, batch:   924/ 1052, ite: 190380] train loss: 0.003965, tar: 0.000054 
l0: 0.000053, l1: 0.000052, l2: 0.000097, l3: 0.000128, l4: 0.000466, l5: 0.000972, l6: 0.001781
[epoch: 724/1000, batch:  1004/ 1052, ite: 190400] train loss: 0.003968, tar: 0.000054 
[Epoch 724/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000004, l1: 0.000004, l2: 0.000029, l3: 0.000138, l4: 0.000297, l5: 0.000762, l6: 0.001641
[epoch: 725/1000, batch:    32/ 1052, ite: 190420] train loss: 0.003974, tar: 0.000055 
l0: 0.000050, l1: 0.000052, l2: 0.000073, l3: 0.000137, l4: 0.000283, l5: 0.000656, l6: 0.001403
[epoch: 725/1000, batch:   112/ 1052, ite: 190440] train loss: 0.003979, tar: 0.000055 
l0: 0.000097, l1: 0.000098, l2: 0.000130, l3: 0.000181, l4: 0.000617, l5: 0.001627, l6: 0.003751
[epoch: 725/1000, batch:   192/ 1052, ite: 190460] train loss: 0.003980, tar: 0.000055 
l0: 0.000007, l1: 0.000007, l2: 0.000036, l3: 0.000083, l4: 0.000305, l5: 0.000682, l6: 0.001441
[epoch: 725/1000, batch:   272/ 1052, ite: 190480] train loss: 0.003979, tar: 0.000055 
l0: 0.000129, l1: 0.000136, l2: 0.000157, l3: 0.000206, l4: 0.000459, l5: 0.001405, l6: 0.002865
[epoch: 725/1000, batch:   352/ 1052, ite: 190500] train loss: 0.003981, tar: 0.000055 
l0: 0.000006, l1: 0.000006, l2: 0.000024, l3: 0.000093, l4: 0.000367, l5: 0.000649, l6: 0.001318
[epoch: 725/1000, batch:   432/ 1052, ite: 190520] train loss: 0.003977, tar: 0.000055 
l0: 0.000015, l1: 0.000015, l2: 0.000058, l3: 0.000203, l4: 0.000403, l5: 0.001316, l6: 0.001929
[epoch: 725/1000, batch:   512/ 1052, ite: 190540] train loss: 0.003987, tar: 0.000055 
l0: 0.000085, l1: 0.000088, l2: 0.000182, l3: 0.000427, l4: 0.001111, l5: 0.002756, l6: 0.004693
[epoch: 725/1000, batch:   592/ 1052, ite: 190560] train loss: 0.003991, tar: 0.000055 
l0: 0.000166, l1: 0.000168, l2: 0.000213, l3: 0.000344, l4: 0.000604, l5: 0.001844, l6: 0.003451
[epoch: 725/1000, batch:   672/ 1052, ite: 190580] train loss: 0.003993, tar: 0.000055 
l0: 0.000070, l1: 0.000070, l2: 0.000094, l3: 0.000222, l4: 0.000557, l5: 0.001537, l6: 0.002173
[epoch: 725/1000, batch:   752/ 1052, ite: 190600] train loss: 0.003983, tar: 0.000055 
l0: 0.000029, l1: 0.000031, l2: 0.000044, l3: 0.000120, l4: 0.000354, l5: 0.001002, l6: 0.001664
[epoch: 725/1000, batch:   832/ 1052, ite: 190620] train loss: 0.003988, tar: 0.000054 
l0: 0.000173, l1: 0.000180, l2: 0.000180, l3: 0.000299, l4: 0.000640, l5: 0.001334, l6: 0.002933
[epoch: 725/1000, batch:   912/ 1052, ite: 190640] train loss: 0.003986, tar: 0.000055 
l0: 0.000090, l1: 0.000090, l2: 0.000123, l3: 0.000239, l4: 0.000617, l5: 0.000894, l6: 0.002028
[epoch: 725/1000, batch:   992/ 1052, ite: 190660] train loss: 0.003980, tar: 0.000055 
[Epoch 725/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000037, l1: 0.000039, l2: 0.000076, l3: 0.000138, l4: 0.000312, l5: 0.000950, l6: 0.002047
[epoch: 726/1000, batch:    20/ 1052, ite: 190680] train loss: 0.003984, tar: 0.000055 
l0: 0.000048, l1: 0.000048, l2: 0.000065, l3: 0.000186, l4: 0.000310, l5: 0.001365, l6: 0.002105
[epoch: 726/1000, batch:   100/ 1052, ite: 190700] train loss: 0.003979, tar: 0.000055 
l0: 0.000033, l1: 0.000031, l2: 0.000076, l3: 0.000165, l4: 0.000620, l5: 0.001270, l6: 0.002631
[epoch: 726/1000, batch:   180/ 1052, ite: 190720] train loss: 0.003980, tar: 0.000054 
l0: 0.000109, l1: 0.000111, l2: 0.000145, l3: 0.000226, l4: 0.000575, l5: 0.001578, l6: 0.002873
[epoch: 726/1000, batch:   260/ 1052, ite: 190740] train loss: 0.003980, tar: 0.000054 
l0: 0.000061, l1: 0.000065, l2: 0.000092, l3: 0.000176, l4: 0.000209, l5: 0.000871, l6: 0.001373
[epoch: 726/1000, batch:   340/ 1052, ite: 190760] train loss: 0.003972, tar: 0.000054 
l0: 0.000120, l1: 0.000126, l2: 0.000123, l3: 0.000190, l4: 0.000484, l5: 0.001193, l6: 0.002285
[epoch: 726/1000, batch:   420/ 1052, ite: 190780] train loss: 0.003976, tar: 0.000054 
l0: 0.000222, l1: 0.000233, l2: 0.000205, l3: 0.000354, l4: 0.000957, l5: 0.001865, l6: 0.004757
[epoch: 726/1000, batch:   500/ 1052, ite: 190800] train loss: 0.003982, tar: 0.000055 
l0: 0.000078, l1: 0.000079, l2: 0.000111, l3: 0.000354, l4: 0.000469, l5: 0.001066, l6: 0.001788
[epoch: 726/1000, batch:   580/ 1052, ite: 190820] train loss: 0.003983, tar: 0.000055 
l0: 0.000010, l1: 0.000009, l2: 0.000058, l3: 0.000236, l4: 0.000488, l5: 0.001031, l6: 0.001200
[epoch: 726/1000, batch:   660/ 1052, ite: 190840] train loss: 0.003983, tar: 0.000055 
l0: 0.000064, l1: 0.000065, l2: 0.000080, l3: 0.000157, l4: 0.000335, l5: 0.001199, l6: 0.002163
[epoch: 726/1000, batch:   740/ 1052, ite: 190860] train loss: 0.003980, tar: 0.000055 
l0: 0.000134, l1: 0.000132, l2: 0.000186, l3: 0.000317, l4: 0.000619, l5: 0.001803, l6: 0.003197
[epoch: 726/1000, batch:   820/ 1052, ite: 190880] train loss: 0.003983, tar: 0.000055 
l0: 0.000032, l1: 0.000034, l2: 0.000068, l3: 0.000125, l4: 0.000510, l5: 0.001061, l6: 0.001527
[epoch: 726/1000, batch:   900/ 1052, ite: 190900] train loss: 0.003984, tar: 0.000055 
l0: 0.000012, l1: 0.000011, l2: 0.000033, l3: 0.000090, l4: 0.000155, l5: 0.000664, l6: 0.000999
[epoch: 726/1000, batch:   980/ 1052, ite: 190920] train loss: 0.003989, tar: 0.000055 
[Epoch 726/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000028, l1: 0.000028, l2: 0.000051, l3: 0.000118, l4: 0.000300, l5: 0.000717, l6: 0.001546
[epoch: 727/1000, batch:     8/ 1052, ite: 190940] train loss: 0.003984, tar: 0.000055 
l0: 0.000017, l1: 0.000018, l2: 0.000043, l3: 0.000125, l4: 0.000257, l5: 0.000584, l6: 0.001579
[epoch: 727/1000, batch:    88/ 1052, ite: 190960] train loss: 0.003980, tar: 0.000055 
l0: 0.000015, l1: 0.000015, l2: 0.000042, l3: 0.000116, l4: 0.000286, l5: 0.000707, l6: 0.001076
[epoch: 727/1000, batch:   168/ 1052, ite: 190980] train loss: 0.003976, tar: 0.000055 
l0: 0.000026, l1: 0.000026, l2: 0.000075, l3: 0.000119, l4: 0.000408, l5: 0.000732, l6: 0.001146
[epoch: 727/1000, batch:   248/ 1052, ite: 191000] train loss: 0.003976, tar: 0.000055 
l0: 0.000019, l1: 0.000018, l2: 0.000069, l3: 0.000171, l4: 0.000573, l5: 0.000916, l6: 0.002025
[epoch: 727/1000, batch:   328/ 1052, ite: 191020] train loss: 0.003987, tar: 0.000055 
l0: 0.000028, l1: 0.000027, l2: 0.000051, l3: 0.000179, l4: 0.000457, l5: 0.001093, l6: 0.001719
[epoch: 727/1000, batch:   408/ 1052, ite: 191040] train loss: 0.003983, tar: 0.000055 
l0: 0.000068, l1: 0.000068, l2: 0.000155, l3: 0.000326, l4: 0.000974, l5: 0.001621, l6: 0.005099
[epoch: 727/1000, batch:   488/ 1052, ite: 191060] train loss: 0.003983, tar: 0.000055 
l0: 0.000040, l1: 0.000040, l2: 0.000064, l3: 0.000148, l4: 0.000289, l5: 0.000993, l6: 0.001234
[epoch: 727/1000, batch:   568/ 1052, ite: 191080] train loss: 0.003985, tar: 0.000054 
l0: 0.000133, l1: 0.000134, l2: 0.000239, l3: 0.000358, l4: 0.000812, l5: 0.001594, l6: 0.003299
[epoch: 727/1000, batch:   648/ 1052, ite: 191100] train loss: 0.003986, tar: 0.000054 
l0: 0.000113, l1: 0.000109, l2: 0.000160, l3: 0.000261, l4: 0.000637, l5: 0.001232, l6: 0.002011
[epoch: 727/1000, batch:   728/ 1052, ite: 191120] train loss: 0.003987, tar: 0.000054 
l0: 0.000014, l1: 0.000013, l2: 0.000048, l3: 0.000156, l4: 0.000429, l5: 0.001346, l6: 0.001990
[epoch: 727/1000, batch:   808/ 1052, ite: 191140] train loss: 0.003986, tar: 0.000054 
l0: 0.000013, l1: 0.000013, l2: 0.000053, l3: 0.000130, l4: 0.000286, l5: 0.001073, l6: 0.002694
[epoch: 727/1000, batch:   888/ 1052, ite: 191160] train loss: 0.003984, tar: 0.000054 
l0: 0.000035, l1: 0.000036, l2: 0.000079, l3: 0.000219, l4: 0.000446, l5: 0.000852, l6: 0.002212
[epoch: 727/1000, batch:   968/ 1052, ite: 191180] train loss: 0.003980, tar: 0.000054 
l0: 0.000015, l1: 0.000014, l2: 0.000041, l3: 0.000087, l4: 0.000364, l5: 0.000746, l6: 0.001041
[epoch: 727/1000, batch:  1048/ 1052, ite: 191200] train loss: 0.003983, tar: 0.000055 
[Epoch 727/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000168, l1: 0.000174, l2: 0.000183, l3: 0.000437, l4: 0.000796, l5: 0.001057, l6: 0.002634
[epoch: 728/1000, batch:    76/ 1052, ite: 191220] train loss: 0.003984, tar: 0.000055 
l0: 0.000045, l1: 0.000047, l2: 0.000058, l3: 0.000113, l4: 0.000261, l5: 0.000785, l6: 0.001120
[epoch: 728/1000, batch:   156/ 1052, ite: 191240] train loss: 0.003977, tar: 0.000055 
l0: 0.000042, l1: 0.000042, l2: 0.000077, l3: 0.000199, l4: 0.000535, l5: 0.001156, l6: 0.002591
[epoch: 728/1000, batch:   236/ 1052, ite: 191260] train loss: 0.003983, tar: 0.000055 
l0: 0.000005, l1: 0.000005, l2: 0.000026, l3: 0.000053, l4: 0.000111, l5: 0.000633, l6: 0.000839
[epoch: 728/1000, batch:   316/ 1052, ite: 191280] train loss: 0.003982, tar: 0.000055 
l0: 0.000030, l1: 0.000030, l2: 0.000051, l3: 0.000105, l4: 0.000487, l5: 0.001193, l6: 0.001700
[epoch: 728/1000, batch:   396/ 1052, ite: 191300] train loss: 0.003981, tar: 0.000055 
l0: 0.000012, l1: 0.000013, l2: 0.000020, l3: 0.000112, l4: 0.000374, l5: 0.000589, l6: 0.000954
[epoch: 728/1000, batch:   476/ 1052, ite: 191320] train loss: 0.003982, tar: 0.000055 
l0: 0.000044, l1: 0.000046, l2: 0.000063, l3: 0.000156, l4: 0.000314, l5: 0.001030, l6: 0.000964
[epoch: 728/1000, batch:   556/ 1052, ite: 191340] train loss: 0.003979, tar: 0.000055 
l0: 0.000047, l1: 0.000046, l2: 0.000080, l3: 0.000158, l4: 0.000449, l5: 0.001194, l6: 0.001890
[epoch: 728/1000, batch:   636/ 1052, ite: 191360] train loss: 0.003982, tar: 0.000055 
l0: 0.000011, l1: 0.000010, l2: 0.000033, l3: 0.000160, l4: 0.000397, l5: 0.000755, l6: 0.002295
[epoch: 728/1000, batch:   716/ 1052, ite: 191380] train loss: 0.003985, tar: 0.000055 
l0: 0.000025, l1: 0.000024, l2: 0.000086, l3: 0.000270, l4: 0.000595, l5: 0.000974, l6: 0.001692
[epoch: 728/1000, batch:   796/ 1052, ite: 191400] train loss: 0.003981, tar: 0.000054 
l0: 0.000005, l1: 0.000005, l2: 0.000021, l3: 0.000092, l4: 0.000461, l5: 0.000756, l6: 0.001989
[epoch: 728/1000, batch:   876/ 1052, ite: 191420] train loss: 0.003977, tar: 0.000054 
l0: 0.000050, l1: 0.000053, l2: 0.000086, l3: 0.000176, l4: 0.000435, l5: 0.000912, l6: 0.001447
[epoch: 728/1000, batch:   956/ 1052, ite: 191440] train loss: 0.003975, tar: 0.000054 
l0: 0.000009, l1: 0.000009, l2: 0.000013, l3: 0.000038, l4: 0.000160, l5: 0.000954, l6: 0.000553
[epoch: 728/1000, batch:  1036/ 1052, ite: 191460] train loss: 0.003976, tar: 0.000054 
[Epoch 728/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000018, l1: 0.000018, l2: 0.000040, l3: 0.000103, l4: 0.000343, l5: 0.000656, l6: 0.001103
[epoch: 729/1000, batch:    64/ 1052, ite: 191480] train loss: 0.003977, tar: 0.000054 
l0: 0.000020, l1: 0.000020, l2: 0.000059, l3: 0.000127, l4: 0.000508, l5: 0.000809, l6: 0.001421
[epoch: 729/1000, batch:   144/ 1052, ite: 191500] train loss: 0.003976, tar: 0.000054 
l0: 0.000011, l1: 0.000010, l2: 0.000022, l3: 0.000157, l4: 0.000704, l5: 0.001201, l6: 0.002541
[epoch: 729/1000, batch:   224/ 1052, ite: 191520] train loss: 0.003975, tar: 0.000054 
l0: 0.000013, l1: 0.000013, l2: 0.000034, l3: 0.000135, l4: 0.000292, l5: 0.000981, l6: 0.001050
[epoch: 729/1000, batch:   304/ 1052, ite: 191540] train loss: 0.003974, tar: 0.000054 
l0: 0.000027, l1: 0.000028, l2: 0.000071, l3: 0.000187, l4: 0.000430, l5: 0.001065, l6: 0.001947
[epoch: 729/1000, batch:   384/ 1052, ite: 191560] train loss: 0.003972, tar: 0.000054 
l0: 0.000062, l1: 0.000065, l2: 0.000081, l3: 0.000147, l4: 0.000299, l5: 0.000761, l6: 0.001264
[epoch: 729/1000, batch:   464/ 1052, ite: 191580] train loss: 0.003972, tar: 0.000054 
l0: 0.000034, l1: 0.000036, l2: 0.000075, l3: 0.000157, l4: 0.000422, l5: 0.000730, l6: 0.002069
[epoch: 729/1000, batch:   544/ 1052, ite: 191600] train loss: 0.003981, tar: 0.000055 
l0: 0.000015, l1: 0.000015, l2: 0.000040, l3: 0.000112, l4: 0.000285, l5: 0.000902, l6: 0.001742
[epoch: 729/1000, batch:   624/ 1052, ite: 191620] train loss: 0.003980, tar: 0.000054 
l0: 0.000042, l1: 0.000043, l2: 0.000052, l3: 0.000153, l4: 0.000360, l5: 0.000770, l6: 0.001555
[epoch: 729/1000, batch:   704/ 1052, ite: 191640] train loss: 0.003973, tar: 0.000054 
l0: 0.000092, l1: 0.000091, l2: 0.000116, l3: 0.000213, l4: 0.000462, l5: 0.001302, l6: 0.001407
[epoch: 729/1000, batch:   784/ 1052, ite: 191660] train loss: 0.003973, tar: 0.000054 
l0: 0.000057, l1: 0.000059, l2: 0.000077, l3: 0.000164, l4: 0.000438, l5: 0.000636, l6: 0.001413
[epoch: 729/1000, batch:   864/ 1052, ite: 191680] train loss: 0.003969, tar: 0.000054 
l0: 0.000125, l1: 0.000128, l2: 0.000176, l3: 0.000280, l4: 0.000521, l5: 0.001208, l6: 0.001819
[epoch: 729/1000, batch:   944/ 1052, ite: 191700] train loss: 0.003970, tar: 0.000054 
l0: 0.000077, l1: 0.000075, l2: 0.000126, l3: 0.000252, l4: 0.000678, l5: 0.000981, l6: 0.002215
[epoch: 729/1000, batch:  1024/ 1052, ite: 191720] train loss: 0.003970, tar: 0.000054 
[Epoch 729/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000041, l1: 0.000041, l2: 0.000062, l3: 0.000096, l4: 0.000370, l5: 0.000819, l6: 0.001143
[epoch: 730/1000, batch:    52/ 1052, ite: 191740] train loss: 0.003975, tar: 0.000055 
l0: 0.000015, l1: 0.000014, l2: 0.000036, l3: 0.000091, l4: 0.000220, l5: 0.000537, l6: 0.001619
[epoch: 730/1000, batch:   132/ 1052, ite: 191760] train loss: 0.003981, tar: 0.000055 
l0: 0.000004, l1: 0.000003, l2: 0.000020, l3: 0.000101, l4: 0.000315, l5: 0.000692, l6: 0.001419
[epoch: 730/1000, batch:   212/ 1052, ite: 191780] train loss: 0.003978, tar: 0.000055 
l0: 0.000055, l1: 0.000056, l2: 0.000076, l3: 0.000175, l4: 0.000585, l5: 0.001131, l6: 0.002032
[epoch: 730/1000, batch:   292/ 1052, ite: 191800] train loss: 0.003978, tar: 0.000054 
l0: 0.000041, l1: 0.000043, l2: 0.000056, l3: 0.000208, l4: 0.000340, l5: 0.001087, l6: 0.002044
[epoch: 730/1000, batch:   372/ 1052, ite: 191820] train loss: 0.003979, tar: 0.000054 
l0: 0.000017, l1: 0.000019, l2: 0.000023, l3: 0.000063, l4: 0.000325, l5: 0.000956, l6: 0.001368
[epoch: 730/1000, batch:   452/ 1052, ite: 191840] train loss: 0.003974, tar: 0.000054 
l0: 0.000035, l1: 0.000036, l2: 0.000065, l3: 0.000204, l4: 0.000492, l5: 0.000988, l6: 0.002111
[epoch: 730/1000, batch:   532/ 1052, ite: 191860] train loss: 0.003972, tar: 0.000055 
l0: 0.000041, l1: 0.000040, l2: 0.000099, l3: 0.000137, l4: 0.000381, l5: 0.001129, l6: 0.001003
[epoch: 730/1000, batch:   612/ 1052, ite: 191880] train loss: 0.003973, tar: 0.000054 
l0: 0.000005, l1: 0.000005, l2: 0.000011, l3: 0.000134, l4: 0.000165, l5: 0.000973, l6: 0.001933
[epoch: 730/1000, batch:   692/ 1052, ite: 191900] train loss: 0.003977, tar: 0.000055 
l0: 0.000137, l1: 0.000138, l2: 0.000208, l3: 0.000369, l4: 0.000668, l5: 0.001315, l6: 0.002764
[epoch: 730/1000, batch:   772/ 1052, ite: 191920] train loss: 0.003976, tar: 0.000055 
l0: 0.000052, l1: 0.000053, l2: 0.000091, l3: 0.000178, l4: 0.000463, l5: 0.000923, l6: 0.001541
[epoch: 730/1000, batch:   852/ 1052, ite: 191940] train loss: 0.003975, tar: 0.000054 
l0: 0.000097, l1: 0.000099, l2: 0.000165, l3: 0.000219, l4: 0.000547, l5: 0.001361, l6: 0.001536
[epoch: 730/1000, batch:   932/ 1052, ite: 191960] train loss: 0.003970, tar: 0.000055 
l0: 0.000016, l1: 0.000016, l2: 0.000021, l3: 0.000088, l4: 0.000426, l5: 0.000560, l6: 0.001210
[epoch: 730/1000, batch:  1012/ 1052, ite: 191980] train loss: 0.003969, tar: 0.000054 
[Epoch 730/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000030, l1: 0.000029, l2: 0.000052, l3: 0.000143, l4: 0.000262, l5: 0.000746, l6: 0.001634
[epoch: 731/1000, batch:    40/ 1052, ite: 192000] train loss: 0.003970, tar: 0.000055 
l0: 0.000086, l1: 0.000082, l2: 0.000142, l3: 0.000309, l4: 0.000410, l5: 0.000957, l6: 0.002191
[epoch: 731/1000, batch:   120/ 1052, ite: 192020] train loss: 0.003974, tar: 0.000055 
l0: 0.000066, l1: 0.000067, l2: 0.000099, l3: 0.000220, l4: 0.000855, l5: 0.001286, l6: 0.002378
[epoch: 731/1000, batch:   200/ 1052, ite: 192040] train loss: 0.003973, tar: 0.000055 
l0: 0.000014, l1: 0.000015, l2: 0.000024, l3: 0.000041, l4: 0.000185, l5: 0.000754, l6: 0.000615
[epoch: 731/1000, batch:   280/ 1052, ite: 192060] train loss: 0.003971, tar: 0.000054 
l0: 0.000078, l1: 0.000075, l2: 0.000125, l3: 0.000202, l4: 0.000341, l5: 0.000910, l6: 0.001657
[epoch: 731/1000, batch:   360/ 1052, ite: 192080] train loss: 0.003968, tar: 0.000054 
l0: 0.000174, l1: 0.000178, l2: 0.000177, l3: 0.000283, l4: 0.000576, l5: 0.001264, l6: 0.002822
[epoch: 731/1000, batch:   440/ 1052, ite: 192100] train loss: 0.003971, tar: 0.000054 
l0: 0.000058, l1: 0.000054, l2: 0.000083, l3: 0.000107, l4: 0.000163, l5: 0.000445, l6: 0.000688
[epoch: 731/1000, batch:   520/ 1052, ite: 192120] train loss: 0.003967, tar: 0.000054 
l0: 0.000037, l1: 0.000036, l2: 0.000078, l3: 0.000270, l4: 0.000425, l5: 0.000887, l6: 0.002107
[epoch: 731/1000, batch:   600/ 1052, ite: 192140] train loss: 0.003967, tar: 0.000054 
l0: 0.000039, l1: 0.000041, l2: 0.000072, l3: 0.000220, l4: 0.000768, l5: 0.001633, l6: 0.003511
[epoch: 731/1000, batch:   680/ 1052, ite: 192160] train loss: 0.003969, tar: 0.000054 
l0: 0.000069, l1: 0.000071, l2: 0.000093, l3: 0.000227, l4: 0.000514, l5: 0.001019, l6: 0.001788
[epoch: 731/1000, batch:   760/ 1052, ite: 192180] train loss: 0.003965, tar: 0.000054 
l0: 0.000007, l1: 0.000006, l2: 0.000026, l3: 0.000100, l4: 0.000441, l5: 0.000906, l6: 0.002064
[epoch: 731/1000, batch:   840/ 1052, ite: 192200] train loss: 0.003966, tar: 0.000054 
l0: 0.000153, l1: 0.000153, l2: 0.000213, l3: 0.000413, l4: 0.000856, l5: 0.001303, l6: 0.003150
[epoch: 731/1000, batch:   920/ 1052, ite: 192220] train loss: 0.003972, tar: 0.000055 
l0: 0.000018, l1: 0.000019, l2: 0.000039, l3: 0.000102, l4: 0.000335, l5: 0.000808, l6: 0.001155
[epoch: 731/1000, batch:  1000/ 1052, ite: 192240] train loss: 0.003968, tar: 0.000055 
[Epoch 731/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000063, l1: 0.000065, l2: 0.000093, l3: 0.000174, l4: 0.000515, l5: 0.001030, l6: 0.002231
[epoch: 732/1000, batch:    28/ 1052, ite: 192260] train loss: 0.003969, tar: 0.000055 
l0: 0.000058, l1: 0.000058, l2: 0.000091, l3: 0.000205, l4: 0.000501, l5: 0.001461, l6: 0.003101
[epoch: 732/1000, batch:   108/ 1052, ite: 192280] train loss: 0.003969, tar: 0.000055 
l0: 0.000037, l1: 0.000037, l2: 0.000066, l3: 0.000292, l4: 0.000702, l5: 0.001522, l6: 0.002649
[epoch: 732/1000, batch:   188/ 1052, ite: 192300] train loss: 0.003967, tar: 0.000055 
l0: 0.000017, l1: 0.000017, l2: 0.000068, l3: 0.000184, l4: 0.000526, l5: 0.001259, l6: 0.001913
[epoch: 732/1000, batch:   268/ 1052, ite: 192320] train loss: 0.003964, tar: 0.000054 
l0: 0.000029, l1: 0.000031, l2: 0.000062, l3: 0.000156, l4: 0.000353, l5: 0.001030, l6: 0.002405
[epoch: 732/1000, batch:   348/ 1052, ite: 192340] train loss: 0.003965, tar: 0.000055 
l0: 0.000103, l1: 0.000102, l2: 0.000144, l3: 0.000292, l4: 0.000444, l5: 0.001079, l6: 0.001587
[epoch: 732/1000, batch:   428/ 1052, ite: 192360] train loss: 0.003964, tar: 0.000055 
l0: 0.000239, l1: 0.000240, l2: 0.000256, l3: 0.000507, l4: 0.001048, l5: 0.001515, l6: 0.002077
[epoch: 732/1000, batch:   508/ 1052, ite: 192380] train loss: 0.003967, tar: 0.000055 
l0: 0.000170, l1: 0.000173, l2: 0.000194, l3: 0.000227, l4: 0.000421, l5: 0.000872, l6: 0.001858
[epoch: 732/1000, batch:   588/ 1052, ite: 192400] train loss: 0.003968, tar: 0.000055 
l0: 0.000047, l1: 0.000045, l2: 0.000077, l3: 0.000170, l4: 0.000499, l5: 0.001255, l6: 0.002225
[epoch: 732/1000, batch:   668/ 1052, ite: 192420] train loss: 0.003970, tar: 0.000055 
l0: 0.000013, l1: 0.000012, l2: 0.000042, l3: 0.000157, l4: 0.000332, l5: 0.000744, l6: 0.001384
[epoch: 732/1000, batch:   748/ 1052, ite: 192440] train loss: 0.003971, tar: 0.000055 
l0: 0.000101, l1: 0.000106, l2: 0.000131, l3: 0.000180, l4: 0.000346, l5: 0.001173, l6: 0.001436
[epoch: 732/1000, batch:   828/ 1052, ite: 192460] train loss: 0.003972, tar: 0.000055 
l0: 0.000086, l1: 0.000092, l2: 0.000094, l3: 0.000125, l4: 0.000384, l5: 0.001118, l6: 0.001740
[epoch: 732/1000, batch:   908/ 1052, ite: 192480] train loss: 0.003970, tar: 0.000055 
l0: 0.000159, l1: 0.000164, l2: 0.000236, l3: 0.000333, l4: 0.000845, l5: 0.001365, l6: 0.003132
[epoch: 732/1000, batch:   988/ 1052, ite: 192500] train loss: 0.003970, tar: 0.000055 
[Epoch 732/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000029, l1: 0.000029, l2: 0.000070, l3: 0.000228, l4: 0.000516, l5: 0.001246, l6: 0.002007
[epoch: 733/1000, batch:    16/ 1052, ite: 192520] train loss: 0.003969, tar: 0.000055 
l0: 0.000066, l1: 0.000066, l2: 0.000085, l3: 0.000341, l4: 0.000506, l5: 0.001503, l6: 0.002679
[epoch: 733/1000, batch:    96/ 1052, ite: 192540] train loss: 0.003969, tar: 0.000055 
l0: 0.000037, l1: 0.000038, l2: 0.000076, l3: 0.000136, l4: 0.000342, l5: 0.000950, l6: 0.001607
[epoch: 733/1000, batch:   176/ 1052, ite: 192560] train loss: 0.003967, tar: 0.000055 
l0: 0.000007, l1: 0.000008, l2: 0.000038, l3: 0.000125, l4: 0.000272, l5: 0.001099, l6: 0.001827
[epoch: 733/1000, batch:   256/ 1052, ite: 192580] train loss: 0.003969, tar: 0.000055 
l0: 0.000069, l1: 0.000067, l2: 0.000088, l3: 0.000136, l4: 0.000412, l5: 0.000963, l6: 0.001608
[epoch: 733/1000, batch:   336/ 1052, ite: 192600] train loss: 0.003967, tar: 0.000054 
l0: 0.000026, l1: 0.000028, l2: 0.000047, l3: 0.000176, l4: 0.000520, l5: 0.001124, l6: 0.001574
[epoch: 733/1000, batch:   416/ 1052, ite: 192620] train loss: 0.003965, tar: 0.000054 
l0: 0.000088, l1: 0.000087, l2: 0.000127, l3: 0.000278, l4: 0.000540, l5: 0.001299, l6: 0.001529
[epoch: 733/1000, batch:   496/ 1052, ite: 192640] train loss: 0.003968, tar: 0.000054 
l0: 0.000006, l1: 0.000006, l2: 0.000033, l3: 0.000079, l4: 0.000257, l5: 0.000660, l6: 0.001803
[epoch: 733/1000, batch:   576/ 1052, ite: 192660] train loss: 0.003971, tar: 0.000055 
l0: 0.000034, l1: 0.000034, l2: 0.000055, l3: 0.000169, l4: 0.000341, l5: 0.000584, l6: 0.001768
[epoch: 733/1000, batch:   656/ 1052, ite: 192680] train loss: 0.003972, tar: 0.000055 
l0: 0.000008, l1: 0.000007, l2: 0.000019, l3: 0.000092, l4: 0.000345, l5: 0.001000, l6: 0.001983
[epoch: 733/1000, batch:   736/ 1052, ite: 192700] train loss: 0.003972, tar: 0.000055 
l0: 0.000125, l1: 0.000128, l2: 0.000171, l3: 0.000241, l4: 0.000407, l5: 0.001116, l6: 0.001785
[epoch: 733/1000, batch:   816/ 1052, ite: 192720] train loss: 0.003972, tar: 0.000055 
l0: 0.000061, l1: 0.000061, l2: 0.000098, l3: 0.000246, l4: 0.000513, l5: 0.001649, l6: 0.003572
[epoch: 733/1000, batch:   896/ 1052, ite: 192740] train loss: 0.003974, tar: 0.000054 
l0: 0.000012, l1: 0.000012, l2: 0.000028, l3: 0.000083, l4: 0.000215, l5: 0.000468, l6: 0.001308
[epoch: 733/1000, batch:   976/ 1052, ite: 192760] train loss: 0.003972, tar: 0.000054 
[Epoch 733/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000027, l1: 0.000025, l2: 0.000068, l3: 0.000279, l4: 0.000795, l5: 0.001531, l6: 0.002572
[epoch: 734/1000, batch:     4/ 1052, ite: 192780] train loss: 0.003971, tar: 0.000054 
l0: 0.000034, l1: 0.000033, l2: 0.000069, l3: 0.000144, l4: 0.000432, l5: 0.001046, l6: 0.001606
[epoch: 734/1000, batch:    84/ 1052, ite: 192800] train loss: 0.003971, tar: 0.000054 
l0: 0.000030, l1: 0.000029, l2: 0.000072, l3: 0.000130, l4: 0.000400, l5: 0.001272, l6: 0.002475
[epoch: 734/1000, batch:   164/ 1052, ite: 192820] train loss: 0.003974, tar: 0.000054 
l0: 0.000073, l1: 0.000074, l2: 0.000097, l3: 0.000171, l4: 0.000308, l5: 0.001013, l6: 0.002451
[epoch: 734/1000, batch:   244/ 1052, ite: 192840] train loss: 0.003973, tar: 0.000054 
l0: 0.000053, l1: 0.000053, l2: 0.000102, l3: 0.000219, l4: 0.000392, l5: 0.000800, l6: 0.001626
[epoch: 734/1000, batch:   324/ 1052, ite: 192860] train loss: 0.003971, tar: 0.000054 
l0: 0.000038, l1: 0.000039, l2: 0.000088, l3: 0.000176, l4: 0.000385, l5: 0.001152, l6: 0.002361
[epoch: 734/1000, batch:   404/ 1052, ite: 192880] train loss: 0.003972, tar: 0.000054 
l0: 0.000029, l1: 0.000029, l2: 0.000081, l3: 0.000206, l4: 0.000322, l5: 0.001153, l6: 0.001552
[epoch: 734/1000, batch:   484/ 1052, ite: 192900] train loss: 0.003969, tar: 0.000054 
l0: 0.000035, l1: 0.000036, l2: 0.000049, l3: 0.000114, l4: 0.000257, l5: 0.000713, l6: 0.001000
[epoch: 734/1000, batch:   564/ 1052, ite: 192920] train loss: 0.003965, tar: 0.000054 
l0: 0.000105, l1: 0.000106, l2: 0.000214, l3: 0.000437, l4: 0.000725, l5: 0.002012, l6: 0.004538
[epoch: 734/1000, batch:   644/ 1052, ite: 192940] train loss: 0.003970, tar: 0.000054 
l0: 0.000137, l1: 0.000168, l2: 0.000155, l3: 0.000329, l4: 0.000749, l5: 0.001535, l6: 0.002628
[epoch: 734/1000, batch:   724/ 1052, ite: 192960] train loss: 0.003971, tar: 0.000054 
l0: 0.000038, l1: 0.000038, l2: 0.000087, l3: 0.000109, l4: 0.000462, l5: 0.000910, l6: 0.000936
[epoch: 734/1000, batch:   804/ 1052, ite: 192980] train loss: 0.003972, tar: 0.000054 
l0: 0.000105, l1: 0.000106, l2: 0.000122, l3: 0.000153, l4: 0.000373, l5: 0.000949, l6: 0.002404
[epoch: 734/1000, batch:   884/ 1052, ite: 193000] train loss: 0.003972, tar: 0.000054 
l0: 0.000026, l1: 0.000026, l2: 0.000031, l3: 0.000067, l4: 0.000142, l5: 0.000376, l6: 0.001112
[epoch: 734/1000, batch:   964/ 1052, ite: 193020] train loss: 0.003973, tar: 0.000054 
l0: 0.000024, l1: 0.000023, l2: 0.000067, l3: 0.000205, l4: 0.000401, l5: 0.000790, l6: 0.001807
[epoch: 734/1000, batch:  1044/ 1052, ite: 193040] train loss: 0.003971, tar: 0.000054 
[Epoch 734/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000010, l1: 0.000010, l2: 0.000038, l3: 0.000126, l4: 0.000334, l5: 0.001619, l6: 0.002578
[epoch: 735/1000, batch:    72/ 1052, ite: 193060] train loss: 0.003971, tar: 0.000054 
l0: 0.000018, l1: 0.000018, l2: 0.000042, l3: 0.000107, l4: 0.000342, l5: 0.000934, l6: 0.001054
[epoch: 735/1000, batch:   152/ 1052, ite: 193080] train loss: 0.003971, tar: 0.000054 
l0: 0.000017, l1: 0.000017, l2: 0.000070, l3: 0.000152, l4: 0.000351, l5: 0.000699, l6: 0.002159
[epoch: 735/1000, batch:   232/ 1052, ite: 193100] train loss: 0.003971, tar: 0.000054 
l0: 0.000017, l1: 0.000016, l2: 0.000049, l3: 0.000113, l4: 0.000327, l5: 0.000880, l6: 0.001967
[epoch: 735/1000, batch:   312/ 1052, ite: 193120] train loss: 0.003971, tar: 0.000054 
l0: 0.000024, l1: 0.000024, l2: 0.000092, l3: 0.000215, l4: 0.000477, l5: 0.000977, l6: 0.001491
[epoch: 735/1000, batch:   392/ 1052, ite: 193140] train loss: 0.003972, tar: 0.000054 
l0: 0.000046, l1: 0.000047, l2: 0.000076, l3: 0.000145, l4: 0.000543, l5: 0.000833, l6: 0.002052
[epoch: 735/1000, batch:   472/ 1052, ite: 193160] train loss: 0.003971, tar: 0.000054 
l0: 0.000091, l1: 0.000090, l2: 0.000136, l3: 0.000219, l4: 0.000443, l5: 0.001114, l6: 0.001973
[epoch: 735/1000, batch:   552/ 1052, ite: 193180] train loss: 0.003970, tar: 0.000054 
l0: 0.000063, l1: 0.000063, l2: 0.000092, l3: 0.000182, l4: 0.000385, l5: 0.000677, l6: 0.002017
[epoch: 735/1000, batch:   632/ 1052, ite: 193200] train loss: 0.003970, tar: 0.000054 
l0: 0.000233, l1: 0.000235, l2: 0.000287, l3: 0.000497, l4: 0.000990, l5: 0.001472, l6: 0.002751
[epoch: 735/1000, batch:   712/ 1052, ite: 193220] train loss: 0.003971, tar: 0.000054 
l0: 0.000048, l1: 0.000048, l2: 0.000104, l3: 0.000303, l4: 0.000749, l5: 0.001344, l6: 0.002391
[epoch: 735/1000, batch:   792/ 1052, ite: 193240] train loss: 0.003970, tar: 0.000054 
l0: 0.000153, l1: 0.000161, l2: 0.000160, l3: 0.000195, l4: 0.000713, l5: 0.001553, l6: 0.002730
[epoch: 735/1000, batch:   872/ 1052, ite: 193260] train loss: 0.003970, tar: 0.000054 
l0: 0.000015, l1: 0.000015, l2: 0.000056, l3: 0.000175, l4: 0.000514, l5: 0.000749, l6: 0.001661
[epoch: 735/1000, batch:   952/ 1052, ite: 193280] train loss: 0.003970, tar: 0.000054 
l0: 0.000008, l1: 0.000009, l2: 0.000015, l3: 0.000097, l4: 0.000258, l5: 0.000582, l6: 0.000977
[epoch: 735/1000, batch:  1032/ 1052, ite: 193300] train loss: 0.003969, tar: 0.000054 
[Epoch 735/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000060, l1: 0.000062, l2: 0.000086, l3: 0.000212, l4: 0.000377, l5: 0.000942, l6: 0.001473
[epoch: 736/1000, batch:    60/ 1052, ite: 193320] train loss: 0.003968, tar: 0.000054 
l0: 0.000011, l1: 0.000012, l2: 0.000044, l3: 0.000193, l4: 0.000309, l5: 0.000482, l6: 0.002008
[epoch: 736/1000, batch:   140/ 1052, ite: 193340] train loss: 0.003971, tar: 0.000054 
l0: 0.000024, l1: 0.000024, l2: 0.000074, l3: 0.000187, l4: 0.000577, l5: 0.001320, l6: 0.002606
[epoch: 736/1000, batch:   220/ 1052, ite: 193360] train loss: 0.003971, tar: 0.000054 
l0: 0.000136, l1: 0.000136, l2: 0.000152, l3: 0.000240, l4: 0.000430, l5: 0.001043, l6: 0.002946
[epoch: 736/1000, batch:   300/ 1052, ite: 193380] train loss: 0.003971, tar: 0.000054 
l0: 0.000151, l1: 0.000150, l2: 0.000190, l3: 0.000391, l4: 0.000711, l5: 0.001950, l6: 0.003319
[epoch: 736/1000, batch:   380/ 1052, ite: 193400] train loss: 0.003970, tar: 0.000054 
l0: 0.000025, l1: 0.000025, l2: 0.000038, l3: 0.000105, l4: 0.000219, l5: 0.000797, l6: 0.001988
[epoch: 736/1000, batch:   460/ 1052, ite: 193420] train loss: 0.003969, tar: 0.000054 
l0: 0.000059, l1: 0.000060, l2: 0.000083, l3: 0.000192, l4: 0.000472, l5: 0.001183, l6: 0.001661
[epoch: 736/1000, batch:   540/ 1052, ite: 193440] train loss: 0.003970, tar: 0.000054 
l0: 0.000078, l1: 0.000078, l2: 0.000097, l3: 0.000233, l4: 0.000504, l5: 0.000644, l6: 0.001314
[epoch: 736/1000, batch:   620/ 1052, ite: 193460] train loss: 0.003971, tar: 0.000054 
l0: 0.000108, l1: 0.000111, l2: 0.000173, l3: 0.000316, l4: 0.000790, l5: 0.001989, l6: 0.002964
[epoch: 736/1000, batch:   700/ 1052, ite: 193480] train loss: 0.003971, tar: 0.000054 
l0: 0.000128, l1: 0.000128, l2: 0.000101, l3: 0.000169, l4: 0.000454, l5: 0.000856, l6: 0.001789
[epoch: 736/1000, batch:   780/ 1052, ite: 193500] train loss: 0.003972, tar: 0.000054 
l0: 0.000019, l1: 0.000020, l2: 0.000037, l3: 0.000111, l4: 0.000228, l5: 0.001046, l6: 0.002385
[epoch: 736/1000, batch:   860/ 1052, ite: 193520] train loss: 0.003970, tar: 0.000054 
l0: 0.000036, l1: 0.000036, l2: 0.000053, l3: 0.000138, l4: 0.000286, l5: 0.000950, l6: 0.001658
[epoch: 736/1000, batch:   940/ 1052, ite: 193540] train loss: 0.003969, tar: 0.000054 
l0: 0.000027, l1: 0.000027, l2: 0.000044, l3: 0.000155, l4: 0.000534, l5: 0.001449, l6: 0.001813
[epoch: 736/1000, batch:  1020/ 1052, ite: 193560] train loss: 0.003970, tar: 0.000054 
[Epoch 736/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000159, l1: 0.000155, l2: 0.000243, l3: 0.000443, l4: 0.000887, l5: 0.002430, l6: 0.004323
[epoch: 737/1000, batch:    48/ 1052, ite: 193580] train loss: 0.003969, tar: 0.000054 
l0: 0.000026, l1: 0.000025, l2: 0.000036, l3: 0.000105, l4: 0.000248, l5: 0.000551, l6: 0.001430
[epoch: 737/1000, batch:   128/ 1052, ite: 193600] train loss: 0.003970, tar: 0.000054 
l0: 0.000071, l1: 0.000073, l2: 0.000080, l3: 0.000111, l4: 0.000707, l5: 0.001080, l6: 0.001322
[epoch: 737/1000, batch:   208/ 1052, ite: 193620] train loss: 0.003969, tar: 0.000054 
l0: 0.000276, l1: 0.000283, l2: 0.000348, l3: 0.000482, l4: 0.001146, l5: 0.001862, l6: 0.004319
[epoch: 737/1000, batch:   288/ 1052, ite: 193640] train loss: 0.003970, tar: 0.000054 
l0: 0.000030, l1: 0.000030, l2: 0.000023, l3: 0.000078, l4: 0.000269, l5: 0.001234, l6: 0.002452
[epoch: 737/1000, batch:   368/ 1052, ite: 193660] train loss: 0.003968, tar: 0.000054 
l0: 0.000025, l1: 0.000025, l2: 0.000043, l3: 0.000095, l4: 0.000687, l5: 0.001610, l6: 0.002172
[epoch: 737/1000, batch:   448/ 1052, ite: 193680] train loss: 0.003971, tar: 0.000054 
l0: 0.000154, l1: 0.000156, l2: 0.000211, l3: 0.000345, l4: 0.000627, l5: 0.001644, l6: 0.003381
[epoch: 737/1000, batch:   528/ 1052, ite: 193700] train loss: 0.003968, tar: 0.000054 
l0: 0.000068, l1: 0.000070, l2: 0.000091, l3: 0.000155, l4: 0.000494, l5: 0.001102, l6: 0.001802
[epoch: 737/1000, batch:   608/ 1052, ite: 193720] train loss: 0.003967, tar: 0.000054 
l0: 0.000050, l1: 0.000049, l2: 0.000098, l3: 0.000286, l4: 0.000798, l5: 0.001753, l6: 0.002687
[epoch: 737/1000, batch:   688/ 1052, ite: 193740] train loss: 0.003967, tar: 0.000054 
l0: 0.000093, l1: 0.000093, l2: 0.000123, l3: 0.000224, l4: 0.000586, l5: 0.001004, l6: 0.002508
[epoch: 737/1000, batch:   768/ 1052, ite: 193760] train loss: 0.003969, tar: 0.000054 
l0: 0.000024, l1: 0.000022, l2: 0.000068, l3: 0.000089, l4: 0.000334, l5: 0.000935, l6: 0.001636
[epoch: 737/1000, batch:   848/ 1052, ite: 193780] train loss: 0.003971, tar: 0.000054 
l0: 0.000076, l1: 0.000074, l2: 0.000114, l3: 0.000193, l4: 0.000481, l5: 0.001123, l6: 0.003026
[epoch: 737/1000, batch:   928/ 1052, ite: 193800] train loss: 0.003972, tar: 0.000054 
l0: 0.000007, l1: 0.000007, l2: 0.000033, l3: 0.000121, l4: 0.000311, l5: 0.000806, l6: 0.000741
[epoch: 737/1000, batch:  1008/ 1052, ite: 193820] train loss: 0.003970, tar: 0.000054 
[Epoch 737/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000059, l1: 0.000057, l2: 0.000127, l3: 0.000390, l4: 0.000982, l5: 0.001572, l6: 0.002703
[epoch: 738/1000, batch:    36/ 1052, ite: 193840] train loss: 0.003972, tar: 0.000054 
l0: 0.000021, l1: 0.000021, l2: 0.000054, l3: 0.000124, l4: 0.000413, l5: 0.001140, l6: 0.001930
[epoch: 738/1000, batch:   116/ 1052, ite: 193860] train loss: 0.003973, tar: 0.000054 
l0: 0.000058, l1: 0.000060, l2: 0.000145, l3: 0.000353, l4: 0.000782, l5: 0.001642, l6: 0.003616
[epoch: 738/1000, batch:   196/ 1052, ite: 193880] train loss: 0.003974, tar: 0.000054 
l0: 0.000134, l1: 0.000134, l2: 0.000186, l3: 0.000305, l4: 0.000700, l5: 0.001390, l6: 0.002787
[epoch: 738/1000, batch:   276/ 1052, ite: 193900] train loss: 0.003978, tar: 0.000054 
l0: 0.000026, l1: 0.000025, l2: 0.000038, l3: 0.000192, l4: 0.000468, l5: 0.000771, l6: 0.001562
[epoch: 738/1000, batch:   356/ 1052, ite: 193920] train loss: 0.003976, tar: 0.000054 
l0: 0.000025, l1: 0.000023, l2: 0.000091, l3: 0.000184, l4: 0.000430, l5: 0.000590, l6: 0.001993
[epoch: 738/1000, batch:   436/ 1052, ite: 193940] train loss: 0.003977, tar: 0.000054 
l0: 0.000029, l1: 0.000028, l2: 0.000047, l3: 0.000155, l4: 0.000304, l5: 0.000367, l6: 0.001304
[epoch: 738/1000, batch:   516/ 1052, ite: 193960] train loss: 0.003974, tar: 0.000054 
l0: 0.000108, l1: 0.000110, l2: 0.000142, l3: 0.000253, l4: 0.000451, l5: 0.001432, l6: 0.002422
[epoch: 738/1000, batch:   596/ 1052, ite: 193980] train loss: 0.003975, tar: 0.000054 
l0: 0.000101, l1: 0.000106, l2: 0.000106, l3: 0.000181, l4: 0.000442, l5: 0.000895, l6: 0.001153
[epoch: 738/1000, batch:   676/ 1052, ite: 194000] train loss: 0.003974, tar: 0.000054 
l0: 0.000107, l1: 0.000108, l2: 0.000132, l3: 0.000272, l4: 0.000604, l5: 0.001185, l6: 0.001834
[epoch: 738/1000, batch:   756/ 1052, ite: 194020] train loss: 0.003972, tar: 0.000054 
l0: 0.000028, l1: 0.000027, l2: 0.000055, l3: 0.000137, l4: 0.000340, l5: 0.000712, l6: 0.001223
[epoch: 738/1000, batch:   836/ 1052, ite: 194040] train loss: 0.003974, tar: 0.000054 
l0: 0.000091, l1: 0.000090, l2: 0.000155, l3: 0.000232, l4: 0.000720, l5: 0.001238, l6: 0.002426
[epoch: 738/1000, batch:   916/ 1052, ite: 194060] train loss: 0.003971, tar: 0.000054 
l0: 0.000020, l1: 0.000020, l2: 0.000062, l3: 0.000122, l4: 0.000267, l5: 0.000757, l6: 0.001500
[epoch: 738/1000, batch:   996/ 1052, ite: 194080] train loss: 0.003971, tar: 0.000054 
[Epoch 738/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000109, l1: 0.000109, l2: 0.000155, l3: 0.000186, l4: 0.000466, l5: 0.001142, l6: 0.002413
[epoch: 739/1000, batch:    24/ 1052, ite: 194100] train loss: 0.003970, tar: 0.000054 
l0: 0.000104, l1: 0.000105, l2: 0.000203, l3: 0.000325, l4: 0.000527, l5: 0.000765, l6: 0.001876
[epoch: 739/1000, batch:   104/ 1052, ite: 194120] train loss: 0.003969, tar: 0.000054 
l0: 0.000092, l1: 0.000091, l2: 0.000161, l3: 0.000249, l4: 0.000502, l5: 0.000844, l6: 0.000504
[epoch: 739/1000, batch:   184/ 1052, ite: 194140] train loss: 0.003969, tar: 0.000054 
l0: 0.000023, l1: 0.000025, l2: 0.000028, l3: 0.000110, l4: 0.000284, l5: 0.000543, l6: 0.000584
[epoch: 739/1000, batch:   264/ 1052, ite: 194160] train loss: 0.003967, tar: 0.000054 
l0: 0.000150, l1: 0.000150, l2: 0.000190, l3: 0.000235, l4: 0.000475, l5: 0.001155, l6: 0.001616
[epoch: 739/1000, batch:   344/ 1052, ite: 194180] train loss: 0.003969, tar: 0.000054 
l0: 0.000071, l1: 0.000073, l2: 0.000090, l3: 0.000143, l4: 0.000387, l5: 0.000985, l6: 0.001864
[epoch: 739/1000, batch:   424/ 1052, ite: 194200] train loss: 0.003971, tar: 0.000054 
l0: 0.000095, l1: 0.000097, l2: 0.000129, l3: 0.000338, l4: 0.000991, l5: 0.002111, l6: 0.003306
[epoch: 739/1000, batch:   504/ 1052, ite: 194220] train loss: 0.003973, tar: 0.000054 
l0: 0.000034, l1: 0.000034, l2: 0.000048, l3: 0.000125, l4: 0.000393, l5: 0.000731, l6: 0.002095
[epoch: 739/1000, batch:   584/ 1052, ite: 194240] train loss: 0.003973, tar: 0.000054 
l0: 0.000012, l1: 0.000011, l2: 0.000040, l3: 0.000112, l4: 0.000210, l5: 0.001151, l6: 0.003169
[epoch: 739/1000, batch:   664/ 1052, ite: 194260] train loss: 0.003972, tar: 0.000054 
l0: 0.000043, l1: 0.000046, l2: 0.000063, l3: 0.000112, l4: 0.000323, l5: 0.001070, l6: 0.001433
[epoch: 739/1000, batch:   744/ 1052, ite: 194280] train loss: 0.003971, tar: 0.000054 
l0: 0.000041, l1: 0.000041, l2: 0.000080, l3: 0.000143, l4: 0.000397, l5: 0.001122, l6: 0.002358
[epoch: 739/1000, batch:   824/ 1052, ite: 194300] train loss: 0.003968, tar: 0.000054 
l0: 0.000019, l1: 0.000019, l2: 0.000067, l3: 0.000165, l4: 0.000438, l5: 0.001075, l6: 0.002069
[epoch: 739/1000, batch:   904/ 1052, ite: 194320] train loss: 0.003968, tar: 0.000054 
l0: 0.000065, l1: 0.000069, l2: 0.000111, l3: 0.000247, l4: 0.000499, l5: 0.001121, l6: 0.001150
[epoch: 739/1000, batch:   984/ 1052, ite: 194340] train loss: 0.003970, tar: 0.000054 
[Epoch 739/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000024, l1: 0.000026, l2: 0.000057, l3: 0.000152, l4: 0.000286, l5: 0.000775, l6: 0.001115
[epoch: 740/1000, batch:    12/ 1052, ite: 194360] train loss: 0.003970, tar: 0.000054 
l0: 0.000104, l1: 0.000109, l2: 0.000123, l3: 0.000207, l4: 0.000512, l5: 0.001186, l6: 0.001923
[epoch: 740/1000, batch:    92/ 1052, ite: 194380] train loss: 0.003969, tar: 0.000054 
l0: 0.000049, l1: 0.000048, l2: 0.000072, l3: 0.000116, l4: 0.000277, l5: 0.000791, l6: 0.001898
[epoch: 740/1000, batch:   172/ 1052, ite: 194400] train loss: 0.003967, tar: 0.000054 
l0: 0.000132, l1: 0.000134, l2: 0.000208, l3: 0.000339, l4: 0.000613, l5: 0.000628, l6: 0.001686
[epoch: 740/1000, batch:   252/ 1052, ite: 194420] train loss: 0.003968, tar: 0.000054 
l0: 0.000033, l1: 0.000032, l2: 0.000103, l3: 0.000201, l4: 0.000472, l5: 0.001162, l6: 0.002196
[epoch: 740/1000, batch:   332/ 1052, ite: 194440] train loss: 0.003968, tar: 0.000054 
l0: 0.000116, l1: 0.000113, l2: 0.000172, l3: 0.000224, l4: 0.000422, l5: 0.001139, l6: 0.002163
[epoch: 740/1000, batch:   412/ 1052, ite: 194460] train loss: 0.003969, tar: 0.000054 
l0: 0.000042, l1: 0.000041, l2: 0.000064, l3: 0.000169, l4: 0.000550, l5: 0.000474, l6: 0.001844
[epoch: 740/1000, batch:   492/ 1052, ite: 194480] train loss: 0.003969, tar: 0.000054 
l0: 0.000044, l1: 0.000045, l2: 0.000072, l3: 0.000113, l4: 0.000378, l5: 0.000607, l6: 0.001128
[epoch: 740/1000, batch:   572/ 1052, ite: 194500] train loss: 0.003971, tar: 0.000054 
l0: 0.000019, l1: 0.000019, l2: 0.000057, l3: 0.000195, l4: 0.000642, l5: 0.001309, l6: 0.001348
[epoch: 740/1000, batch:   652/ 1052, ite: 194520] train loss: 0.003971, tar: 0.000054 
l0: 0.000005, l1: 0.000005, l2: 0.000024, l3: 0.000094, l4: 0.000260, l5: 0.000733, l6: 0.001764
[epoch: 740/1000, batch:   732/ 1052, ite: 194540] train loss: 0.003970, tar: 0.000054 
l0: 0.000078, l1: 0.000082, l2: 0.000132, l3: 0.000260, l4: 0.000500, l5: 0.001219, l6: 0.001631
[epoch: 740/1000, batch:   812/ 1052, ite: 194560] train loss: 0.003973, tar: 0.000054 
l0: 0.000100, l1: 0.000098, l2: 0.000159, l3: 0.000432, l4: 0.000583, l5: 0.001127, l6: 0.003901
[epoch: 740/1000, batch:   892/ 1052, ite: 194580] train loss: 0.003972, tar: 0.000054 
l0: 0.000048, l1: 0.000049, l2: 0.000056, l3: 0.000102, l4: 0.000335, l5: 0.000901, l6: 0.001407
[epoch: 740/1000, batch:   972/ 1052, ite: 194600] train loss: 0.003972, tar: 0.000054 
l0: 0.000105, l1: 0.000106, l2: 0.000128, l3: 0.000233, l4: 0.000538, l5: 0.001252, l6: 0.002536
[epoch: 740/1000, batch:  1052/ 1052, ite: 194620] train loss: 0.003971, tar: 0.000054 
[Epoch 740/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000019, l1: 0.000020, l2: 0.000037, l3: 0.000083, l4: 0.000251, l5: 0.000508, l6: 0.001592
[epoch: 741/1000, batch:    80/ 1052, ite: 194640] train loss: 0.003970, tar: 0.000054 
l0: 0.000006, l1: 0.000006, l2: 0.000016, l3: 0.000076, l4: 0.000290, l5: 0.000968, l6: 0.001368
[epoch: 741/1000, batch:   160/ 1052, ite: 194660] train loss: 0.003968, tar: 0.000054 
l0: 0.000045, l1: 0.000046, l2: 0.000086, l3: 0.000188, l4: 0.000587, l5: 0.001303, l6: 0.002684
[epoch: 741/1000, batch:   240/ 1052, ite: 194680] train loss: 0.003970, tar: 0.000054 
l0: 0.000004, l1: 0.000004, l2: 0.000036, l3: 0.000170, l4: 0.000430, l5: 0.000956, l6: 0.002503
[epoch: 741/1000, batch:   320/ 1052, ite: 194700] train loss: 0.003969, tar: 0.000054 
l0: 0.000031, l1: 0.000034, l2: 0.000044, l3: 0.000131, l4: 0.000618, l5: 0.000992, l6: 0.002200
[epoch: 741/1000, batch:   400/ 1052, ite: 194720] train loss: 0.003971, tar: 0.000054 
l0: 0.000014, l1: 0.000014, l2: 0.000044, l3: 0.000128, l4: 0.000418, l5: 0.000789, l6: 0.001279
[epoch: 741/1000, batch:   480/ 1052, ite: 194740] train loss: 0.003970, tar: 0.000054 
l0: 0.000005, l1: 0.000005, l2: 0.000050, l3: 0.000164, l4: 0.000615, l5: 0.001317, l6: 0.002481
[epoch: 741/1000, batch:   560/ 1052, ite: 194760] train loss: 0.003969, tar: 0.000054 
l0: 0.000142, l1: 0.000144, l2: 0.000191, l3: 0.000224, l4: 0.000608, l5: 0.002131, l6: 0.002910
[epoch: 741/1000, batch:   640/ 1052, ite: 194780] train loss: 0.003969, tar: 0.000054 
l0: 0.000066, l1: 0.000067, l2: 0.000098, l3: 0.000191, l4: 0.000405, l5: 0.001417, l6: 0.003016
[epoch: 741/1000, batch:   720/ 1052, ite: 194800] train loss: 0.003968, tar: 0.000054 
l0: 0.000031, l1: 0.000031, l2: 0.000073, l3: 0.000240, l4: 0.000546, l5: 0.000994, l6: 0.002219
[epoch: 741/1000, batch:   800/ 1052, ite: 194820] train loss: 0.003967, tar: 0.000054 
l0: 0.000012, l1: 0.000013, l2: 0.000045, l3: 0.000152, l4: 0.000378, l5: 0.000786, l6: 0.001767
[epoch: 741/1000, batch:   880/ 1052, ite: 194840] train loss: 0.003969, tar: 0.000054 
l0: 0.000006, l1: 0.000005, l2: 0.000036, l3: 0.000100, l4: 0.000299, l5: 0.000804, l6: 0.001189
[epoch: 741/1000, batch:   960/ 1052, ite: 194860] train loss: 0.003969, tar: 0.000054 
l0: 0.000080, l1: 0.000083, l2: 0.000109, l3: 0.000226, l4: 0.000564, l5: 0.001069, l6: 0.003193
[epoch: 741/1000, batch:  1040/ 1052, ite: 194880] train loss: 0.003970, tar: 0.000054 
[Epoch 741/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000002, l1: 0.000002, l2: 0.000011, l3: 0.000076, l4: 0.000283, l5: 0.000991, l6: 0.001358
[epoch: 742/1000, batch:    68/ 1052, ite: 194900] train loss: 0.003967, tar: 0.000054 
l0: 0.000026, l1: 0.000028, l2: 0.000081, l3: 0.000292, l4: 0.001216, l5: 0.002226, l6: 0.003011
[epoch: 742/1000, batch:   148/ 1052, ite: 194920] train loss: 0.003968, tar: 0.000054 
l0: 0.000012, l1: 0.000012, l2: 0.000027, l3: 0.000057, l4: 0.000333, l5: 0.001262, l6: 0.001992
[epoch: 742/1000, batch:   228/ 1052, ite: 194940] train loss: 0.003970, tar: 0.000054 
l0: 0.000009, l1: 0.000008, l2: 0.000060, l3: 0.000192, l4: 0.000581, l5: 0.001589, l6: 0.002579
[epoch: 742/1000, batch:   308/ 1052, ite: 194960] train loss: 0.003970, tar: 0.000054 
l0: 0.000044, l1: 0.000046, l2: 0.000065, l3: 0.000173, l4: 0.000469, l5: 0.001090, l6: 0.002037
[epoch: 742/1000, batch:   388/ 1052, ite: 194980] train loss: 0.003970, tar: 0.000054 
l0: 0.000171, l1: 0.000178, l2: 0.000183, l3: 0.000256, l4: 0.000597, l5: 0.001313, l6: 0.001205
[epoch: 742/1000, batch:   468/ 1052, ite: 195000] train loss: 0.003971, tar: 0.000054 
l0: 0.000095, l1: 0.000095, l2: 0.000127, l3: 0.000141, l4: 0.000466, l5: 0.000902, l6: 0.001651
[epoch: 742/1000, batch:   548/ 1052, ite: 195020] train loss: 0.003972, tar: 0.000054 
l0: 0.000021, l1: 0.000021, l2: 0.000040, l3: 0.000093, l4: 0.000319, l5: 0.000807, l6: 0.001198
[epoch: 742/1000, batch:   628/ 1052, ite: 195040] train loss: 0.003973, tar: 0.000054 
l0: 0.000052, l1: 0.000051, l2: 0.000086, l3: 0.000198, l4: 0.000480, l5: 0.001228, l6: 0.001228
[epoch: 742/1000, batch:   708/ 1052, ite: 195060] train loss: 0.003973, tar: 0.000054 
l0: 0.000032, l1: 0.000031, l2: 0.000059, l3: 0.000106, l4: 0.000498, l5: 0.000715, l6: 0.001670
[epoch: 742/1000, batch:   788/ 1052, ite: 195080] train loss: 0.003974, tar: 0.000054 
l0: 0.000053, l1: 0.000053, l2: 0.000089, l3: 0.000193, l4: 0.000690, l5: 0.001348, l6: 0.002699
[epoch: 742/1000, batch:   868/ 1052, ite: 195100] train loss: 0.003972, tar: 0.000054 
l0: 0.000125, l1: 0.000130, l2: 0.000143, l3: 0.000214, l4: 0.000432, l5: 0.001411, l6: 0.003607
[epoch: 742/1000, batch:   948/ 1052, ite: 195120] train loss: 0.003972, tar: 0.000054 
l0: 0.000040, l1: 0.000039, l2: 0.000085, l3: 0.000205, l4: 0.000561, l5: 0.001335, l6: 0.002648
[epoch: 742/1000, batch:  1028/ 1052, ite: 195140] train loss: 0.003970, tar: 0.000054 
[Epoch 742/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000083, l1: 0.000084, l2: 0.000095, l3: 0.000226, l4: 0.000263, l5: 0.000834, l6: 0.002650
[epoch: 743/1000, batch:    56/ 1052, ite: 195160] train loss: 0.003969, tar: 0.000054 
l0: 0.000073, l1: 0.000074, l2: 0.000104, l3: 0.000201, l4: 0.000442, l5: 0.000965, l6: 0.001095
[epoch: 743/1000, batch:   136/ 1052, ite: 195180] train loss: 0.003969, tar: 0.000054 
l0: 0.000176, l1: 0.000180, l2: 0.000240, l3: 0.000393, l4: 0.000718, l5: 0.001128, l6: 0.003061
[epoch: 743/1000, batch:   216/ 1052, ite: 195200] train loss: 0.003968, tar: 0.000054 
l0: 0.000011, l1: 0.000012, l2: 0.000045, l3: 0.000108, l4: 0.000392, l5: 0.000529, l6: 0.001561
[epoch: 743/1000, batch:   296/ 1052, ite: 195220] train loss: 0.003967, tar: 0.000054 
l0: 0.000073, l1: 0.000072, l2: 0.000100, l3: 0.000193, l4: 0.000408, l5: 0.001061, l6: 0.002509
[epoch: 743/1000, batch:   376/ 1052, ite: 195240] train loss: 0.003969, tar: 0.000054 
l0: 0.000017, l1: 0.000017, l2: 0.000040, l3: 0.000132, l4: 0.000287, l5: 0.000714, l6: 0.002100
[epoch: 743/1000, batch:   456/ 1052, ite: 195260] train loss: 0.003969, tar: 0.000054 
l0: 0.000069, l1: 0.000070, l2: 0.000095, l3: 0.000182, l4: 0.000503, l5: 0.001062, l6: 0.002115
[epoch: 743/1000, batch:   536/ 1052, ite: 195280] train loss: 0.003970, tar: 0.000054 
l0: 0.000014, l1: 0.000016, l2: 0.000031, l3: 0.000071, l4: 0.000194, l5: 0.000878, l6: 0.001686
[epoch: 743/1000, batch:   616/ 1052, ite: 195300] train loss: 0.003970, tar: 0.000054 
l0: 0.000055, l1: 0.000054, l2: 0.000074, l3: 0.000129, l4: 0.000220, l5: 0.000908, l6: 0.001114
[epoch: 743/1000, batch:   696/ 1052, ite: 195320] train loss: 0.003970, tar: 0.000054 
l0: 0.000091, l1: 0.000091, l2: 0.000120, l3: 0.000233, l4: 0.000730, l5: 0.001143, l6: 0.001927
[epoch: 743/1000, batch:   776/ 1052, ite: 195340] train loss: 0.003970, tar: 0.000054 
l0: 0.000061, l1: 0.000062, l2: 0.000089, l3: 0.000240, l4: 0.000622, l5: 0.001104, l6: 0.002188
[epoch: 743/1000, batch:   856/ 1052, ite: 195360] train loss: 0.003972, tar: 0.000054 
l0: 0.000029, l1: 0.000028, l2: 0.000090, l3: 0.000285, l4: 0.000550, l5: 0.001697, l6: 0.002018
[epoch: 743/1000, batch:   936/ 1052, ite: 195380] train loss: 0.003970, tar: 0.000054 
l0: 0.000007, l1: 0.000006, l2: 0.000043, l3: 0.000110, l4: 0.000244, l5: 0.001164, l6: 0.001680
[epoch: 743/1000, batch:  1016/ 1052, ite: 195400] train loss: 0.003968, tar: 0.000054 
[Epoch 743/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000039, l1: 0.000039, l2: 0.000035, l3: 0.000119, l4: 0.000337, l5: 0.000592, l6: 0.000547
[epoch: 744/1000, batch:    44/ 1052, ite: 195420] train loss: 0.003968, tar: 0.000054 
l0: 0.000065, l1: 0.000065, l2: 0.000080, l3: 0.000186, l4: 0.000482, l5: 0.001259, l6: 0.001274
[epoch: 744/1000, batch:   124/ 1052, ite: 195440] train loss: 0.003968, tar: 0.000054 
l0: 0.000006, l1: 0.000007, l2: 0.000021, l3: 0.000170, l4: 0.000394, l5: 0.000724, l6: 0.002040
[epoch: 744/1000, batch:   204/ 1052, ite: 195460] train loss: 0.003967, tar: 0.000054 
l0: 0.000010, l1: 0.000010, l2: 0.000055, l3: 0.000123, l4: 0.000327, l5: 0.000896, l6: 0.003089
[epoch: 744/1000, batch:   284/ 1052, ite: 195480] train loss: 0.003967, tar: 0.000054 
l0: 0.000009, l1: 0.000008, l2: 0.000033, l3: 0.000129, l4: 0.000287, l5: 0.000847, l6: 0.001827
[epoch: 744/1000, batch:   364/ 1052, ite: 195500] train loss: 0.003966, tar: 0.000054 
l0: 0.000053, l1: 0.000055, l2: 0.000067, l3: 0.000100, l4: 0.000364, l5: 0.000751, l6: 0.001473
[epoch: 744/1000, batch:   444/ 1052, ite: 195520] train loss: 0.003967, tar: 0.000054 
l0: 0.000022, l1: 0.000024, l2: 0.000037, l3: 0.000127, l4: 0.000297, l5: 0.001039, l6: 0.001423
[epoch: 744/1000, batch:   524/ 1052, ite: 195540] train loss: 0.003966, tar: 0.000054 
l0: 0.000148, l1: 0.000147, l2: 0.000191, l3: 0.000371, l4: 0.000590, l5: 0.001069, l6: 0.002656
[epoch: 744/1000, batch:   604/ 1052, ite: 195560] train loss: 0.003968, tar: 0.000054 
l0: 0.000048, l1: 0.000051, l2: 0.000115, l3: 0.000273, l4: 0.000540, l5: 0.001229, l6: 0.001337
[epoch: 744/1000, batch:   684/ 1052, ite: 195580] train loss: 0.003966, tar: 0.000054 
l0: 0.000005, l1: 0.000005, l2: 0.000026, l3: 0.000078, l4: 0.000186, l5: 0.000333, l6: 0.001437
[epoch: 744/1000, batch:   764/ 1052, ite: 195600] train loss: 0.003965, tar: 0.000054 
l0: 0.000018, l1: 0.000018, l2: 0.000069, l3: 0.000294, l4: 0.000754, l5: 0.001495, l6: 0.002285
[epoch: 744/1000, batch:   844/ 1052, ite: 195620] train loss: 0.003966, tar: 0.000054 
l0: 0.000033, l1: 0.000033, l2: 0.000058, l3: 0.000245, l4: 0.000583, l5: 0.001719, l6: 0.002259
[epoch: 744/1000, batch:   924/ 1052, ite: 195640] train loss: 0.003967, tar: 0.000054 
l0: 0.000062, l1: 0.000062, l2: 0.000095, l3: 0.000179, l4: 0.000465, l5: 0.001219, l6: 0.001194
[epoch: 744/1000, batch:  1004/ 1052, ite: 195660] train loss: 0.003966, tar: 0.000054 
[Epoch 744/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000021, l1: 0.000021, l2: 0.000038, l3: 0.000127, l4: 0.000344, l5: 0.001300, l6: 0.001946
[epoch: 745/1000, batch:    32/ 1052, ite: 195680] train loss: 0.003964, tar: 0.000054 
l0: 0.000016, l1: 0.000017, l2: 0.000026, l3: 0.000062, l4: 0.000402, l5: 0.000803, l6: 0.001009
[epoch: 745/1000, batch:   112/ 1052, ite: 195700] train loss: 0.003964, tar: 0.000054 
l0: 0.000054, l1: 0.000054, l2: 0.000095, l3: 0.000251, l4: 0.000538, l5: 0.001145, l6: 0.002873
[epoch: 745/1000, batch:   192/ 1052, ite: 195720] train loss: 0.003964, tar: 0.000054 
l0: 0.000060, l1: 0.000060, l2: 0.000081, l3: 0.000134, l4: 0.000245, l5: 0.001041, l6: 0.001088
[epoch: 745/1000, batch:   272/ 1052, ite: 195740] train loss: 0.003962, tar: 0.000054 
l0: 0.000025, l1: 0.000027, l2: 0.000025, l3: 0.000122, l4: 0.000493, l5: 0.001112, l6: 0.002390
[epoch: 745/1000, batch:   352/ 1052, ite: 195760] train loss: 0.003962, tar: 0.000054 
l0: 0.000026, l1: 0.000025, l2: 0.000050, l3: 0.000106, l4: 0.000363, l5: 0.001092, l6: 0.001933
[epoch: 745/1000, batch:   432/ 1052, ite: 195780] train loss: 0.003961, tar: 0.000054 
l0: 0.000089, l1: 0.000091, l2: 0.000132, l3: 0.000289, l4: 0.000707, l5: 0.001454, l6: 0.003303
[epoch: 745/1000, batch:   512/ 1052, ite: 195800] train loss: 0.003962, tar: 0.000054 
l0: 0.000104, l1: 0.000105, l2: 0.000147, l3: 0.000244, l4: 0.000465, l5: 0.001322, l6: 0.002122
[epoch: 745/1000, batch:   592/ 1052, ite: 195820] train loss: 0.003961, tar: 0.000054 
l0: 0.000052, l1: 0.000051, l2: 0.000067, l3: 0.000124, l4: 0.000367, l5: 0.001100, l6: 0.001562
[epoch: 745/1000, batch:   672/ 1052, ite: 195840] train loss: 0.003962, tar: 0.000054 
l0: 0.000024, l1: 0.000025, l2: 0.000040, l3: 0.000094, l4: 0.000294, l5: 0.000540, l6: 0.001127
[epoch: 745/1000, batch:   752/ 1052, ite: 195860] train loss: 0.003961, tar: 0.000054 
l0: 0.000015, l1: 0.000014, l2: 0.000026, l3: 0.000086, l4: 0.000520, l5: 0.000972, l6: 0.001886
[epoch: 745/1000, batch:   832/ 1052, ite: 195880] train loss: 0.003962, tar: 0.000054 
l0: 0.000071, l1: 0.000069, l2: 0.000133, l3: 0.000224, l4: 0.000419, l5: 0.000921, l6: 0.001743
[epoch: 745/1000, batch:   912/ 1052, ite: 195900] train loss: 0.003963, tar: 0.000054 
l0: 0.000046, l1: 0.000044, l2: 0.000064, l3: 0.000142, l4: 0.000289, l5: 0.000888, l6: 0.001165
[epoch: 745/1000, batch:   992/ 1052, ite: 195920] train loss: 0.003964, tar: 0.000054 
[Epoch 745/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000040, l1: 0.000039, l2: 0.000053, l3: 0.000083, l4: 0.000301, l5: 0.000529, l6: 0.001296
[epoch: 746/1000, batch:    20/ 1052, ite: 195940] train loss: 0.003963, tar: 0.000054 
l0: 0.000132, l1: 0.000132, l2: 0.000184, l3: 0.000382, l4: 0.000666, l5: 0.001363, l6: 0.002328
[epoch: 746/1000, batch:   100/ 1052, ite: 195960] train loss: 0.003964, tar: 0.000054 
l0: 0.000007, l1: 0.000008, l2: 0.000017, l3: 0.000086, l4: 0.000318, l5: 0.000805, l6: 0.000933
[epoch: 746/1000, batch:   180/ 1052, ite: 195980] train loss: 0.003965, tar: 0.000054 
l0: 0.000156, l1: 0.000158, l2: 0.000204, l3: 0.000326, l4: 0.000646, l5: 0.000873, l6: 0.003205
[epoch: 746/1000, batch:   260/ 1052, ite: 196000] train loss: 0.003965, tar: 0.000054 
l0: 0.000050, l1: 0.000051, l2: 0.000079, l3: 0.000216, l4: 0.000486, l5: 0.001283, l6: 0.004030
[epoch: 746/1000, batch:   340/ 1052, ite: 196020] train loss: 0.003965, tar: 0.000054 
l0: 0.000021, l1: 0.000022, l2: 0.000089, l3: 0.000239, l4: 0.000475, l5: 0.001134, l6: 0.002101
[epoch: 746/1000, batch:   420/ 1052, ite: 196040] train loss: 0.003962, tar: 0.000054 
l0: 0.000108, l1: 0.000108, l2: 0.000142, l3: 0.000232, l4: 0.000376, l5: 0.000758, l6: 0.002031
[epoch: 746/1000, batch:   500/ 1052, ite: 196060] train loss: 0.003961, tar: 0.000053 
l0: 0.000003, l1: 0.000003, l2: 0.000012, l3: 0.000085, l4: 0.000238, l5: 0.000617, l6: 0.001365
[epoch: 746/1000, batch:   580/ 1052, ite: 196080] train loss: 0.003963, tar: 0.000054 
l0: 0.000019, l1: 0.000018, l2: 0.000053, l3: 0.000198, l4: 0.000303, l5: 0.001147, l6: 0.001867
[epoch: 746/1000, batch:   660/ 1052, ite: 196100] train loss: 0.003963, tar: 0.000053 
l0: 0.000052, l1: 0.000051, l2: 0.000082, l3: 0.000184, l4: 0.000497, l5: 0.001094, l6: 0.001622
[epoch: 746/1000, batch:   740/ 1052, ite: 196120] train loss: 0.003963, tar: 0.000053 
l0: 0.000016, l1: 0.000015, l2: 0.000059, l3: 0.000187, l4: 0.000423, l5: 0.001051, l6: 0.001964
[epoch: 746/1000, batch:   820/ 1052, ite: 196140] train loss: 0.003962, tar: 0.000054 
l0: 0.000057, l1: 0.000058, l2: 0.000097, l3: 0.000214, l4: 0.000585, l5: 0.001523, l6: 0.002547
[epoch: 746/1000, batch:   900/ 1052, ite: 196160] train loss: 0.003963, tar: 0.000053 
l0: 0.000014, l1: 0.000014, l2: 0.000053, l3: 0.000230, l4: 0.000460, l5: 0.001006, l6: 0.002155
[epoch: 746/1000, batch:   980/ 1052, ite: 196180] train loss: 0.003964, tar: 0.000054 
[Epoch 746/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000040, l1: 0.000042, l2: 0.000053, l3: 0.000137, l4: 0.000417, l5: 0.000988, l6: 0.001619
[epoch: 747/1000, batch:     8/ 1052, ite: 196200] train loss: 0.003964, tar: 0.000054 
l0: 0.000036, l1: 0.000037, l2: 0.000064, l3: 0.000187, l4: 0.000505, l5: 0.000670, l6: 0.001829
[epoch: 747/1000, batch:    88/ 1052, ite: 196220] train loss: 0.003965, tar: 0.000054 
l0: 0.000078, l1: 0.000079, l2: 0.000101, l3: 0.000174, l4: 0.000507, l5: 0.001293, l6: 0.002464
[epoch: 747/1000, batch:   168/ 1052, ite: 196240] train loss: 0.003964, tar: 0.000054 
l0: 0.000170, l1: 0.000168, l2: 0.000224, l3: 0.000421, l4: 0.000818, l5: 0.001384, l6: 0.003293
[epoch: 747/1000, batch:   248/ 1052, ite: 196260] train loss: 0.003964, tar: 0.000054 
l0: 0.000058, l1: 0.000057, l2: 0.000099, l3: 0.000199, l4: 0.000490, l5: 0.001099, l6: 0.002301
[epoch: 747/1000, batch:   328/ 1052, ite: 196280] train loss: 0.003964, tar: 0.000054 
l0: 0.000031, l1: 0.000032, l2: 0.000061, l3: 0.000160, l4: 0.000632, l5: 0.000652, l6: 0.001695
[epoch: 747/1000, batch:   408/ 1052, ite: 196300] train loss: 0.003963, tar: 0.000054 
l0: 0.000169, l1: 0.000170, l2: 0.000215, l3: 0.000373, l4: 0.000560, l5: 0.001760, l6: 0.006156
[epoch: 747/1000, batch:   488/ 1052, ite: 196320] train loss: 0.003964, tar: 0.000054 
l0: 0.000042, l1: 0.000042, l2: 0.000063, l3: 0.000150, l4: 0.000369, l5: 0.000808, l6: 0.001487
[epoch: 747/1000, batch:   568/ 1052, ite: 196340] train loss: 0.003964, tar: 0.000054 
l0: 0.000063, l1: 0.000065, l2: 0.000090, l3: 0.000164, l4: 0.000446, l5: 0.000963, l6: 0.003322
[epoch: 747/1000, batch:   648/ 1052, ite: 196360] train loss: 0.003964, tar: 0.000054 
l0: 0.000033, l1: 0.000035, l2: 0.000042, l3: 0.000121, l4: 0.000335, l5: 0.001327, l6: 0.002197
[epoch: 747/1000, batch:   728/ 1052, ite: 196380] train loss: 0.003964, tar: 0.000054 
l0: 0.000116, l1: 0.000115, l2: 0.000169, l3: 0.000556, l4: 0.001240, l5: 0.001706, l6: 0.003390
[epoch: 747/1000, batch:   808/ 1052, ite: 196400] train loss: 0.003964, tar: 0.000054 
l0: 0.000060, l1: 0.000060, l2: 0.000053, l3: 0.000173, l4: 0.000420, l5: 0.001428, l6: 0.001956
[epoch: 747/1000, batch:   888/ 1052, ite: 196420] train loss: 0.003964, tar: 0.000054 
l0: 0.000022, l1: 0.000021, l2: 0.000035, l3: 0.000078, l4: 0.000178, l5: 0.000428, l6: 0.000315
[epoch: 747/1000, batch:   968/ 1052, ite: 196440] train loss: 0.003963, tar: 0.000053 
l0: 0.000081, l1: 0.000083, l2: 0.000105, l3: 0.000167, l4: 0.000390, l5: 0.001935, l6: 0.003236
[epoch: 747/1000, batch:  1048/ 1052, ite: 196460] train loss: 0.003964, tar: 0.000053 
[Epoch 747/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000069, l1: 0.000069, l2: 0.000088, l3: 0.000190, l4: 0.000481, l5: 0.001081, l6: 0.001984
[epoch: 748/1000, batch:    76/ 1052, ite: 196480] train loss: 0.003963, tar: 0.000053 
l0: 0.000078, l1: 0.000077, l2: 0.000109, l3: 0.000237, l4: 0.000389, l5: 0.000926, l6: 0.001295
[epoch: 748/1000, batch:   156/ 1052, ite: 196500] train loss: 0.003964, tar: 0.000053 
l0: 0.000031, l1: 0.000031, l2: 0.000086, l3: 0.000245, l4: 0.000518, l5: 0.002024, l6: 0.002814
[epoch: 748/1000, batch:   236/ 1052, ite: 196520] train loss: 0.003964, tar: 0.000053 
l0: 0.000180, l1: 0.000180, l2: 0.000224, l3: 0.000376, l4: 0.000473, l5: 0.000895, l6: 0.002961
[epoch: 748/1000, batch:   316/ 1052, ite: 196540] train loss: 0.003962, tar: 0.000053 
l0: 0.000031, l1: 0.000034, l2: 0.000065, l3: 0.000140, l4: 0.000349, l5: 0.000815, l6: 0.001379
[epoch: 748/1000, batch:   396/ 1052, ite: 196560] train loss: 0.003963, tar: 0.000053 
l0: 0.000008, l1: 0.000008, l2: 0.000031, l3: 0.000082, l4: 0.000249, l5: 0.000756, l6: 0.000930
[epoch: 748/1000, batch:   476/ 1052, ite: 196580] train loss: 0.003963, tar: 0.000053 
l0: 0.000150, l1: 0.000145, l2: 0.000187, l3: 0.000332, l4: 0.000462, l5: 0.001240, l6: 0.002394
[epoch: 748/1000, batch:   556/ 1052, ite: 196600] train loss: 0.003963, tar: 0.000053 
l0: 0.000020, l1: 0.000020, l2: 0.000038, l3: 0.000108, l4: 0.000374, l5: 0.000755, l6: 0.001271
[epoch: 748/1000, batch:   636/ 1052, ite: 196620] train loss: 0.003963, tar: 0.000053 
l0: 0.000030, l1: 0.000029, l2: 0.000145, l3: 0.000434, l4: 0.000802, l5: 0.002386, l6: 0.004723
[epoch: 748/1000, batch:   716/ 1052, ite: 196640] train loss: 0.003962, tar: 0.000053 
l0: 0.000043, l1: 0.000043, l2: 0.000066, l3: 0.000168, l4: 0.000520, l5: 0.001077, l6: 0.001920
[epoch: 748/1000, batch:   796/ 1052, ite: 196660] train loss: 0.003962, tar: 0.000053 
l0: 0.000033, l1: 0.000035, l2: 0.000034, l3: 0.000353, l4: 0.000811, l5: 0.000976, l6: 0.001883
[epoch: 748/1000, batch:   876/ 1052, ite: 196680] train loss: 0.003963, tar: 0.000053 
l0: 0.000025, l1: 0.000027, l2: 0.000078, l3: 0.000265, l4: 0.000779, l5: 0.001192, l6: 0.002584
[epoch: 748/1000, batch:   956/ 1052, ite: 196700] train loss: 0.003962, tar: 0.000053 
l0: 0.000066, l1: 0.000068, l2: 0.000107, l3: 0.000200, l4: 0.000369, l5: 0.001147, l6: 0.001472
[epoch: 748/1000, batch:  1036/ 1052, ite: 196720] train loss: 0.003961, tar: 0.000053 
[Epoch 748/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000016, l1: 0.000015, l2: 0.000034, l3: 0.000106, l4: 0.000227, l5: 0.000617, l6: 0.001735
[epoch: 749/1000, batch:    64/ 1052, ite: 196740] train loss: 0.003961, tar: 0.000053 
l0: 0.000014, l1: 0.000014, l2: 0.000047, l3: 0.000097, l4: 0.000293, l5: 0.001004, l6: 0.002080
[epoch: 749/1000, batch:   144/ 1052, ite: 196760] train loss: 0.003961, tar: 0.000053 
l0: 0.000054, l1: 0.000055, l2: 0.000087, l3: 0.000157, l4: 0.000370, l5: 0.001084, l6: 0.001523
[epoch: 749/1000, batch:   224/ 1052, ite: 196780] train loss: 0.003962, tar: 0.000053 
l0: 0.000014, l1: 0.000013, l2: 0.000062, l3: 0.000098, l4: 0.000302, l5: 0.000697, l6: 0.001370
[epoch: 749/1000, batch:   304/ 1052, ite: 196800] train loss: 0.003961, tar: 0.000053 
l0: 0.000075, l1: 0.000078, l2: 0.000093, l3: 0.000172, l4: 0.000422, l5: 0.001147, l6: 0.001855
[epoch: 749/1000, batch:   384/ 1052, ite: 196820] train loss: 0.003960, tar: 0.000053 
l0: 0.000066, l1: 0.000066, l2: 0.000072, l3: 0.000165, l4: 0.000512, l5: 0.000887, l6: 0.001133
[epoch: 749/1000, batch:   464/ 1052, ite: 196840] train loss: 0.003959, tar: 0.000053 
l0: 0.000051, l1: 0.000050, l2: 0.000122, l3: 0.000285, l4: 0.000664, l5: 0.001411, l6: 0.002775
[epoch: 749/1000, batch:   544/ 1052, ite: 196860] train loss: 0.003959, tar: 0.000053 
l0: 0.000174, l1: 0.000173, l2: 0.000212, l3: 0.000368, l4: 0.000810, l5: 0.002031, l6: 0.003499
[epoch: 749/1000, batch:   624/ 1052, ite: 196880] train loss: 0.003960, tar: 0.000053 
l0: 0.000008, l1: 0.000007, l2: 0.000028, l3: 0.000060, l4: 0.000252, l5: 0.000353, l6: 0.000925
[epoch: 749/1000, batch:   704/ 1052, ite: 196900] train loss: 0.003961, tar: 0.000053 
l0: 0.000018, l1: 0.000019, l2: 0.000032, l3: 0.000125, l4: 0.000397, l5: 0.001000, l6: 0.002354
[epoch: 749/1000, batch:   784/ 1052, ite: 196920] train loss: 0.003961, tar: 0.000053 
l0: 0.000041, l1: 0.000044, l2: 0.000079, l3: 0.000192, l4: 0.000463, l5: 0.001544, l6: 0.002330
[epoch: 749/1000, batch:   864/ 1052, ite: 196940] train loss: 0.003962, tar: 0.000053 
l0: 0.000013, l1: 0.000013, l2: 0.000025, l3: 0.000079, l4: 0.000369, l5: 0.000982, l6: 0.001780
[epoch: 749/1000, batch:   944/ 1052, ite: 196960] train loss: 0.003962, tar: 0.000053 
l0: 0.000071, l1: 0.000071, l2: 0.000101, l3: 0.000177, l4: 0.000431, l5: 0.000743, l6: 0.001328
[epoch: 749/1000, batch:  1024/ 1052, ite: 196980] train loss: 0.003961, tar: 0.000053 
[Epoch 749/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000028, l1: 0.000029, l2: 0.000037, l3: 0.000074, l4: 0.000268, l5: 0.000394, l6: 0.000689
[epoch: 750/1000, batch:    52/ 1052, ite: 197000] train loss: 0.003960, tar: 0.000053 
l0: 0.000004, l1: 0.000004, l2: 0.000028, l3: 0.000140, l4: 0.000623, l5: 0.001043, l6: 0.001638
[epoch: 750/1000, batch:   132/ 1052, ite: 197020] train loss: 0.003959, tar: 0.000053 
l0: 0.000204, l1: 0.000208, l2: 0.000254, l3: 0.000406, l4: 0.000808, l5: 0.001757, l6: 0.002839
[epoch: 750/1000, batch:   212/ 1052, ite: 197040] train loss: 0.003962, tar: 0.000053 
l0: 0.000048, l1: 0.000051, l2: 0.000079, l3: 0.000247, l4: 0.000513, l5: 0.001529, l6: 0.003208
[epoch: 750/1000, batch:   292/ 1052, ite: 197060] train loss: 0.003960, tar: 0.000053 
l0: 0.000023, l1: 0.000023, l2: 0.000028, l3: 0.000090, l4: 0.000388, l5: 0.000868, l6: 0.000969
[epoch: 750/1000, batch:   372/ 1052, ite: 197080] train loss: 0.003960, tar: 0.000053 
l0: 0.000052, l1: 0.000051, l2: 0.000083, l3: 0.000156, l4: 0.000321, l5: 0.000528, l6: 0.001058
[epoch: 750/1000, batch:   452/ 1052, ite: 197100] train loss: 0.003960, tar: 0.000053 
l0: 0.000180, l1: 0.000183, l2: 0.000276, l3: 0.000542, l4: 0.000900, l5: 0.001992, l6: 0.003199
[epoch: 750/1000, batch:   532/ 1052, ite: 197120] train loss: 0.003961, tar: 0.000053 
l0: 0.000040, l1: 0.000041, l2: 0.000052, l3: 0.000109, l4: 0.000261, l5: 0.000962, l6: 0.002050
[epoch: 750/1000, batch:   612/ 1052, ite: 197140] train loss: 0.003962, tar: 0.000053 
l0: 0.000017, l1: 0.000017, l2: 0.000061, l3: 0.000157, l4: 0.000561, l5: 0.001490, l6: 0.003154
[epoch: 750/1000, batch:   692/ 1052, ite: 197160] train loss: 0.003962, tar: 0.000053 
l0: 0.000037, l1: 0.000039, l2: 0.000079, l3: 0.000153, l4: 0.000513, l5: 0.001060, l6: 0.001982
[epoch: 750/1000, batch:   772/ 1052, ite: 197180] train loss: 0.003962, tar: 0.000053 
l0: 0.000089, l1: 0.000091, l2: 0.000145, l3: 0.000240, l4: 0.000429, l5: 0.000889, l6: 0.001429
[epoch: 750/1000, batch:   852/ 1052, ite: 197200] train loss: 0.003961, tar: 0.000053 
l0: 0.000019, l1: 0.000019, l2: 0.000075, l3: 0.000173, l4: 0.000579, l5: 0.001118, l6: 0.001606
[epoch: 750/1000, batch:   932/ 1052, ite: 197220] train loss: 0.003961, tar: 0.000053 
l0: 0.000065, l1: 0.000066, l2: 0.000115, l3: 0.000257, l4: 0.000539, l5: 0.001046, l6: 0.004130
[epoch: 750/1000, batch:  1012/ 1052, ite: 197240] train loss: 0.003962, tar: 0.000053 
[Epoch 750/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000007, l1: 0.000008, l2: 0.000040, l3: 0.000081, l4: 0.000202, l5: 0.000895, l6: 0.001288
[epoch: 751/1000, batch:    40/ 1052, ite: 197260] train loss: 0.003960, tar: 0.000053 
l0: 0.000111, l1: 0.000114, l2: 0.000181, l3: 0.000398, l4: 0.000945, l5: 0.001809, l6: 0.003763
[epoch: 751/1000, batch:   120/ 1052, ite: 197280] train loss: 0.003961, tar: 0.000053 
l0: 0.000013, l1: 0.000013, l2: 0.000028, l3: 0.000122, l4: 0.000421, l5: 0.001384, l6: 0.002554
[epoch: 751/1000, batch:   200/ 1052, ite: 197300] train loss: 0.003962, tar: 0.000053 
l0: 0.000145, l1: 0.000148, l2: 0.000170, l3: 0.000251, l4: 0.000493, l5: 0.001369, l6: 0.003395
[epoch: 751/1000, batch:   280/ 1052, ite: 197320] train loss: 0.003962, tar: 0.000053 
l0: 0.000026, l1: 0.000026, l2: 0.000052, l3: 0.000349, l4: 0.000692, l5: 0.001697, l6: 0.003248
[epoch: 751/1000, batch:   360/ 1052, ite: 197340] train loss: 0.003962, tar: 0.000053 
l0: 0.000021, l1: 0.000023, l2: 0.000097, l3: 0.000280, l4: 0.000610, l5: 0.001580, l6: 0.003849
[epoch: 751/1000, batch:   440/ 1052, ite: 197360] train loss: 0.003962, tar: 0.000053 
l0: 0.000073, l1: 0.000071, l2: 0.000094, l3: 0.000214, l4: 0.000419, l5: 0.000747, l6: 0.001365
[epoch: 751/1000, batch:   520/ 1052, ite: 197380] train loss: 0.003962, tar: 0.000053 
l0: 0.000026, l1: 0.000026, l2: 0.000052, l3: 0.000114, l4: 0.000397, l5: 0.000676, l6: 0.001144
[epoch: 751/1000, batch:   600/ 1052, ite: 197400] train loss: 0.003962, tar: 0.000053 
l0: 0.000020, l1: 0.000021, l2: 0.000037, l3: 0.000081, l4: 0.000375, l5: 0.000895, l6: 0.001434
[epoch: 751/1000, batch:   680/ 1052, ite: 197420] train loss: 0.003962, tar: 0.000053 
l0: 0.000041, l1: 0.000040, l2: 0.000090, l3: 0.000221, l4: 0.000442, l5: 0.001142, l6: 0.002209
[epoch: 751/1000, batch:   760/ 1052, ite: 197440] train loss: 0.003961, tar: 0.000053 
l0: 0.000038, l1: 0.000040, l2: 0.000079, l3: 0.000239, l4: 0.000488, l5: 0.001276, l6: 0.002330
[epoch: 751/1000, batch:   840/ 1052, ite: 197460] train loss: 0.003961, tar: 0.000053 
l0: 0.000010, l1: 0.000010, l2: 0.000040, l3: 0.000155, l4: 0.000459, l5: 0.000504, l6: 0.001621
[epoch: 751/1000, batch:   920/ 1052, ite: 197480] train loss: 0.003963, tar: 0.000053 
l0: 0.000012, l1: 0.000011, l2: 0.000043, l3: 0.000116, l4: 0.000438, l5: 0.001027, l6: 0.001281
[epoch: 751/1000, batch:  1000/ 1052, ite: 197500] train loss: 0.003963, tar: 0.000053 
[Epoch 751/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000000, l1: 0.000000, l2: 0.000002, l3: 0.000044, l4: 0.000234, l5: 0.000411, l6: 0.001129
[epoch: 752/1000, batch:    28/ 1052, ite: 197520] train loss: 0.003961, tar: 0.000053 
l0: 0.000009, l1: 0.000009, l2: 0.000032, l3: 0.000131, l4: 0.000309, l5: 0.001531, l6: 0.001501
[epoch: 752/1000, batch:   108/ 1052, ite: 197540] train loss: 0.003962, tar: 0.000053 
l0: 0.000003, l1: 0.000002, l2: 0.000030, l3: 0.000096, l4: 0.000303, l5: 0.000881, l6: 0.001693
[epoch: 752/1000, batch:   188/ 1052, ite: 197560] train loss: 0.003964, tar: 0.000053 
l0: 0.000021, l1: 0.000019, l2: 0.000063, l3: 0.000154, l4: 0.000288, l5: 0.001057, l6: 0.002210
[epoch: 752/1000, batch:   268/ 1052, ite: 197580] train loss: 0.003963, tar: 0.000053 
l0: 0.000058, l1: 0.000060, l2: 0.000056, l3: 0.000089, l4: 0.000438, l5: 0.000986, l6: 0.001628
[epoch: 752/1000, batch:   348/ 1052, ite: 197600] train loss: 0.003963, tar: 0.000053 
l0: 0.000126, l1: 0.000114, l2: 0.000142, l3: 0.000230, l4: 0.000671, l5: 0.000992, l6: 0.001080
[epoch: 752/1000, batch:   428/ 1052, ite: 197620] train loss: 0.003962, tar: 0.000053 
l0: 0.000044, l1: 0.000047, l2: 0.000077, l3: 0.000174, l4: 0.000481, l5: 0.000898, l6: 0.001096
[epoch: 752/1000, batch:   508/ 1052, ite: 197640] train loss: 0.003961, tar: 0.000053 
l0: 0.000035, l1: 0.000035, l2: 0.000059, l3: 0.000186, l4: 0.000463, l5: 0.001101, l6: 0.001563
[epoch: 752/1000, batch:   588/ 1052, ite: 197660] train loss: 0.003961, tar: 0.000053 
l0: 0.000028, l1: 0.000027, l2: 0.000075, l3: 0.000172, l4: 0.000654, l5: 0.001163, l6: 0.002499
[epoch: 752/1000, batch:   668/ 1052, ite: 197680] train loss: 0.003963, tar: 0.000053 
l0: 0.000020, l1: 0.000020, l2: 0.000056, l3: 0.000113, l4: 0.000465, l5: 0.000801, l6: 0.001623
[epoch: 752/1000, batch:   748/ 1052, ite: 197700] train loss: 0.003963, tar: 0.000053 
l0: 0.000039, l1: 0.000044, l2: 0.000036, l3: 0.000085, l4: 0.000258, l5: 0.000772, l6: 0.001029
[epoch: 752/1000, batch:   828/ 1052, ite: 197720] train loss: 0.003962, tar: 0.000053 
l0: 0.000012, l1: 0.000012, l2: 0.000033, l3: 0.000148, l4: 0.000328, l5: 0.000988, l6: 0.001169
[epoch: 752/1000, batch:   908/ 1052, ite: 197740] train loss: 0.003962, tar: 0.000053 
l0: 0.000009, l1: 0.000009, l2: 0.000023, l3: 0.000062, l4: 0.000349, l5: 0.000667, l6: 0.001375
[epoch: 752/1000, batch:   988/ 1052, ite: 197760] train loss: 0.003961, tar: 0.000053 
[Epoch 752/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000009, l1: 0.000008, l2: 0.000018, l3: 0.000095, l4: 0.000289, l5: 0.000474, l6: 0.000567
[epoch: 753/1000, batch:    16/ 1052, ite: 197780] train loss: 0.003960, tar: 0.000053 
l0: 0.000009, l1: 0.000008, l2: 0.000048, l3: 0.000190, l4: 0.000471, l5: 0.000702, l6: 0.002752
[epoch: 753/1000, batch:    96/ 1052, ite: 197800] train loss: 0.003961, tar: 0.000053 
l0: 0.000174, l1: 0.000178, l2: 0.000192, l3: 0.000286, l4: 0.000648, l5: 0.001372, l6: 0.003874
[epoch: 753/1000, batch:   176/ 1052, ite: 197820] train loss: 0.003962, tar: 0.000053 
l0: 0.000042, l1: 0.000043, l2: 0.000101, l3: 0.000256, l4: 0.000981, l5: 0.001520, l6: 0.002671
[epoch: 753/1000, batch:   256/ 1052, ite: 197840] train loss: 0.003963, tar: 0.000053 
l0: 0.000012, l1: 0.000012, l2: 0.000027, l3: 0.000083, l4: 0.000417, l5: 0.000727, l6: 0.001234
[epoch: 753/1000, batch:   336/ 1052, ite: 197860] train loss: 0.003962, tar: 0.000053 
l0: 0.000065, l1: 0.000066, l2: 0.000098, l3: 0.000145, l4: 0.000250, l5: 0.000676, l6: 0.001110
[epoch: 753/1000, batch:   416/ 1052, ite: 197880] train loss: 0.003962, tar: 0.000053 
l0: 0.000014, l1: 0.000015, l2: 0.000062, l3: 0.000195, l4: 0.000336, l5: 0.000658, l6: 0.001666
[epoch: 753/1000, batch:   496/ 1052, ite: 197900] train loss: 0.003962, tar: 0.000053 
l0: 0.000026, l1: 0.000025, l2: 0.000054, l3: 0.000172, l4: 0.000493, l5: 0.000867, l6: 0.002466
[epoch: 753/1000, batch:   576/ 1052, ite: 197920] train loss: 0.003961, tar: 0.000053 
l0: 0.000006, l1: 0.000006, l2: 0.000039, l3: 0.000250, l4: 0.000762, l5: 0.001657, l6: 0.002073
[epoch: 753/1000, batch:   656/ 1052, ite: 197940] train loss: 0.003961, tar: 0.000053 
l0: 0.000038, l1: 0.000040, l2: 0.000054, l3: 0.000095, l4: 0.000356, l5: 0.000742, l6: 0.001004
[epoch: 753/1000, batch:   736/ 1052, ite: 197960] train loss: 0.003961, tar: 0.000053 
l0: 0.000056, l1: 0.000057, l2: 0.000095, l3: 0.000257, l4: 0.000516, l5: 0.001155, l6: 0.002017
[epoch: 753/1000, batch:   816/ 1052, ite: 197980] train loss: 0.003961, tar: 0.000053 
l0: 0.000012, l1: 0.000010, l2: 0.000034, l3: 0.000141, l4: 0.000364, l5: 0.000610, l6: 0.001128
[epoch: 753/1000, batch:   896/ 1052, ite: 198000] train loss: 0.003960, tar: 0.000053 
l0: 0.000057, l1: 0.000058, l2: 0.000103, l3: 0.000205, l4: 0.000430, l5: 0.000818, l6: 0.001294
[epoch: 753/1000, batch:   976/ 1052, ite: 198020] train loss: 0.003961, tar: 0.000053 
[Epoch 753/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000097, l1: 0.000099, l2: 0.000092, l3: 0.000204, l4: 0.000386, l5: 0.000769, l6: 0.001486
[epoch: 754/1000, batch:     4/ 1052, ite: 198040] train loss: 0.003960, tar: 0.000053 
l0: 0.000010, l1: 0.000009, l2: 0.000038, l3: 0.000189, l4: 0.000507, l5: 0.001570, l6: 0.002324
[epoch: 754/1000, batch:    84/ 1052, ite: 198060] train loss: 0.003959, tar: 0.000053 
l0: 0.000001, l1: 0.000001, l2: 0.000010, l3: 0.000058, l4: 0.000161, l5: 0.000663, l6: 0.001095
[epoch: 754/1000, batch:   164/ 1052, ite: 198080] train loss: 0.003960, tar: 0.000053 
l0: 0.000068, l1: 0.000067, l2: 0.000131, l3: 0.000262, l4: 0.000603, l5: 0.000911, l6: 0.001820
[epoch: 754/1000, batch:   244/ 1052, ite: 198100] train loss: 0.003960, tar: 0.000053 
l0: 0.000015, l1: 0.000016, l2: 0.000040, l3: 0.000074, l4: 0.000327, l5: 0.001123, l6: 0.001432
[epoch: 754/1000, batch:   324/ 1052, ite: 198120] train loss: 0.003960, tar: 0.000053 
l0: 0.000007, l1: 0.000006, l2: 0.000033, l3: 0.000098, l4: 0.000186, l5: 0.000981, l6: 0.001346
[epoch: 754/1000, batch:   404/ 1052, ite: 198140] train loss: 0.003961, tar: 0.000053 
l0: 0.000067, l1: 0.000070, l2: 0.000088, l3: 0.000162, l4: 0.000402, l5: 0.000540, l6: 0.001010
[epoch: 754/1000, batch:   484/ 1052, ite: 198160] train loss: 0.003960, tar: 0.000053 
l0: 0.000077, l1: 0.000077, l2: 0.000142, l3: 0.000278, l4: 0.000593, l5: 0.001148, l6: 0.001571
[epoch: 754/1000, batch:   564/ 1052, ite: 198180] train loss: 0.003959, tar: 0.000053 
l0: 0.000027, l1: 0.000026, l2: 0.000052, l3: 0.000130, l4: 0.000500, l5: 0.000886, l6: 0.002272
[epoch: 754/1000, batch:   644/ 1052, ite: 198200] train loss: 0.003959, tar: 0.000053 
l0: 0.000158, l1: 0.000164, l2: 0.000237, l3: 0.000393, l4: 0.000765, l5: 0.001736, l6: 0.003097
[epoch: 754/1000, batch:   724/ 1052, ite: 198220] train loss: 0.003960, tar: 0.000053 
l0: 0.000052, l1: 0.000052, l2: 0.000101, l3: 0.000256, l4: 0.000688, l5: 0.001299, l6: 0.002190
[epoch: 754/1000, batch:   804/ 1052, ite: 198240] train loss: 0.003961, tar: 0.000053 
l0: 0.000060, l1: 0.000059, l2: 0.000092, l3: 0.000130, l4: 0.000288, l5: 0.000732, l6: 0.001160
[epoch: 754/1000, batch:   884/ 1052, ite: 198260] train loss: 0.003960, tar: 0.000053 
l0: 0.000032, l1: 0.000031, l2: 0.000069, l3: 0.000192, l4: 0.000336, l5: 0.000618, l6: 0.002355
[epoch: 754/1000, batch:   964/ 1052, ite: 198280] train loss: 0.003960, tar: 0.000053 
l0: 0.000004, l1: 0.000004, l2: 0.000026, l3: 0.000114, l4: 0.000379, l5: 0.000793, l6: 0.001464
[epoch: 754/1000, batch:  1044/ 1052, ite: 198300] train loss: 0.003960, tar: 0.000053 
[Epoch 754/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000056, l1: 0.000056, l2: 0.000091, l3: 0.000224, l4: 0.000657, l5: 0.001151, l6: 0.002308
[epoch: 755/1000, batch:    72/ 1052, ite: 198320] train loss: 0.003959, tar: 0.000053 
l0: 0.000010, l1: 0.000008, l2: 0.000035, l3: 0.000109, l4: 0.000669, l5: 0.000896, l6: 0.002001
[epoch: 755/1000, batch:   152/ 1052, ite: 198340] train loss: 0.003958, tar: 0.000053 
l0: 0.000017, l1: 0.000018, l2: 0.000026, l3: 0.000058, l4: 0.000223, l5: 0.000773, l6: 0.001032
[epoch: 755/1000, batch:   232/ 1052, ite: 198360] train loss: 0.003958, tar: 0.000053 
l0: 0.000044, l1: 0.000047, l2: 0.000097, l3: 0.000151, l4: 0.000390, l5: 0.001010, l6: 0.001913
[epoch: 755/1000, batch:   312/ 1052, ite: 198380] train loss: 0.003957, tar: 0.000053 
l0: 0.000084, l1: 0.000083, l2: 0.000091, l3: 0.000179, l4: 0.000418, l5: 0.000481, l6: 0.001875
[epoch: 755/1000, batch:   392/ 1052, ite: 198400] train loss: 0.003957, tar: 0.000053 
l0: 0.000200, l1: 0.000205, l2: 0.000229, l3: 0.000386, l4: 0.000711, l5: 0.001492, l6: 0.004616
[epoch: 755/1000, batch:   472/ 1052, ite: 198420] train loss: 0.003958, tar: 0.000053 
l0: 0.000011, l1: 0.000009, l2: 0.000036, l3: 0.000095, l4: 0.000199, l5: 0.000505, l6: 0.000639
[epoch: 755/1000, batch:   552/ 1052, ite: 198440] train loss: 0.003958, tar: 0.000053 
l0: 0.000139, l1: 0.000141, l2: 0.000165, l3: 0.000204, l4: 0.000531, l5: 0.001432, l6: 0.001927
[epoch: 755/1000, batch:   632/ 1052, ite: 198460] train loss: 0.003958, tar: 0.000053 
l0: 0.000011, l1: 0.000013, l2: 0.000039, l3: 0.000158, l4: 0.000692, l5: 0.000973, l6: 0.002183
[epoch: 755/1000, batch:   712/ 1052, ite: 198480] train loss: 0.003957, tar: 0.000053 
l0: 0.000060, l1: 0.000059, l2: 0.000100, l3: 0.000200, l4: 0.000394, l5: 0.001068, l6: 0.001668
[epoch: 755/1000, batch:   792/ 1052, ite: 198500] train loss: 0.003958, tar: 0.000053 
l0: 0.000082, l1: 0.000082, l2: 0.000172, l3: 0.000375, l4: 0.001042, l5: 0.001691, l6: 0.002312
[epoch: 755/1000, batch:   872/ 1052, ite: 198520] train loss: 0.003959, tar: 0.000053 
l0: 0.000038, l1: 0.000038, l2: 0.000064, l3: 0.000196, l4: 0.000533, l5: 0.002041, l6: 0.002755
[epoch: 755/1000, batch:   952/ 1052, ite: 198540] train loss: 0.003958, tar: 0.000053 
l0: 0.000088, l1: 0.000086, l2: 0.000132, l3: 0.000210, l4: 0.000444, l5: 0.001132, l6: 0.001941
[epoch: 755/1000, batch:  1032/ 1052, ite: 198560] train loss: 0.003958, tar: 0.000053 
[Epoch 755/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000030, l1: 0.000030, l2: 0.000050, l3: 0.000096, l4: 0.000361, l5: 0.000396, l6: 0.001100
[epoch: 756/1000, batch:    60/ 1052, ite: 198580] train loss: 0.003958, tar: 0.000053 
l0: 0.000002, l1: 0.000002, l2: 0.000010, l3: 0.000050, l4: 0.000311, l5: 0.000491, l6: 0.001014
[epoch: 756/1000, batch:   140/ 1052, ite: 198600] train loss: 0.003956, tar: 0.000053 
l0: 0.000031, l1: 0.000029, l2: 0.000035, l3: 0.000070, l4: 0.000359, l5: 0.000852, l6: 0.001224
[epoch: 756/1000, batch:   220/ 1052, ite: 198620] train loss: 0.003957, tar: 0.000053 
l0: 0.000007, l1: 0.000008, l2: 0.000011, l3: 0.000068, l4: 0.000141, l5: 0.000486, l6: 0.000810
[epoch: 756/1000, batch:   300/ 1052, ite: 198640] train loss: 0.003958, tar: 0.000053 
l0: 0.000011, l1: 0.000010, l2: 0.000046, l3: 0.000170, l4: 0.000399, l5: 0.000997, l6: 0.001591
[epoch: 756/1000, batch:   380/ 1052, ite: 198660] train loss: 0.003956, tar: 0.000053 
l0: 0.000064, l1: 0.000062, l2: 0.000123, l3: 0.000301, l4: 0.000510, l5: 0.001279, l6: 0.002576
[epoch: 756/1000, batch:   460/ 1052, ite: 198680] train loss: 0.003957, tar: 0.000053 
l0: 0.000012, l1: 0.000012, l2: 0.000038, l3: 0.000181, l4: 0.000308, l5: 0.001324, l6: 0.002040
[epoch: 756/1000, batch:   540/ 1052, ite: 198700] train loss: 0.003958, tar: 0.000053 
l0: 0.000073, l1: 0.000075, l2: 0.000113, l3: 0.000221, l4: 0.000657, l5: 0.001878, l6: 0.003860
[epoch: 756/1000, batch:   620/ 1052, ite: 198720] train loss: 0.003959, tar: 0.000053 
l0: 0.000058, l1: 0.000058, l2: 0.000072, l3: 0.000134, l4: 0.000296, l5: 0.000600, l6: 0.001463
[epoch: 756/1000, batch:   700/ 1052, ite: 198740] train loss: 0.003959, tar: 0.000053 
l0: 0.000049, l1: 0.000051, l2: 0.000067, l3: 0.000141, l4: 0.000431, l5: 0.000909, l6: 0.001565
[epoch: 756/1000, batch:   780/ 1052, ite: 198760] train loss: 0.003959, tar: 0.000053 
l0: 0.000028, l1: 0.000027, l2: 0.000049, l3: 0.000155, l4: 0.000441, l5: 0.000859, l6: 0.001596
[epoch: 756/1000, batch:   860/ 1052, ite: 198780] train loss: 0.003960, tar: 0.000053 
l0: 0.000047, l1: 0.000046, l2: 0.000068, l3: 0.000282, l4: 0.000422, l5: 0.001248, l6: 0.002382
[epoch: 756/1000, batch:   940/ 1052, ite: 198800] train loss: 0.003960, tar: 0.000053 
l0: 0.000141, l1: 0.000142, l2: 0.000170, l3: 0.000313, l4: 0.000528, l5: 0.001010, l6: 0.002485
[epoch: 756/1000, batch:  1020/ 1052, ite: 198820] train loss: 0.003960, tar: 0.000053 
[Epoch 756/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000104, l1: 0.000101, l2: 0.000161, l3: 0.000329, l4: 0.000890, l5: 0.001964, l6: 0.002904
[epoch: 757/1000, batch:    48/ 1052, ite: 198840] train loss: 0.003961, tar: 0.000053 
l0: 0.000020, l1: 0.000020, l2: 0.000040, l3: 0.000214, l4: 0.000550, l5: 0.000785, l6: 0.002443
[epoch: 757/1000, batch:   128/ 1052, ite: 198860] train loss: 0.003961, tar: 0.000053 
l0: 0.000040, l1: 0.000040, l2: 0.000065, l3: 0.000184, l4: 0.000612, l5: 0.001294, l6: 0.002192
[epoch: 757/1000, batch:   208/ 1052, ite: 198880] train loss: 0.003962, tar: 0.000053 
l0: 0.000022, l1: 0.000022, l2: 0.000047, l3: 0.000165, l4: 0.000425, l5: 0.001257, l6: 0.002826
[epoch: 757/1000, batch:   288/ 1052, ite: 198900] train loss: 0.003962, tar: 0.000053 
l0: 0.000042, l1: 0.000044, l2: 0.000038, l3: 0.000109, l4: 0.000454, l5: 0.000624, l6: 0.001196
[epoch: 757/1000, batch:   368/ 1052, ite: 198920] train loss: 0.003962, tar: 0.000053 
l0: 0.000014, l1: 0.000015, l2: 0.000043, l3: 0.000148, l4: 0.000495, l5: 0.000886, l6: 0.002006
[epoch: 757/1000, batch:   448/ 1052, ite: 198940] train loss: 0.003960, tar: 0.000053 
l0: 0.000012, l1: 0.000018, l2: 0.000034, l3: 0.000305, l4: 0.000599, l5: 0.000926, l6: 0.002015
[epoch: 757/1000, batch:   528/ 1052, ite: 198960] train loss: 0.003960, tar: 0.000053 
l0: 0.000022, l1: 0.000022, l2: 0.000045, l3: 0.000134, l4: 0.000292, l5: 0.001181, l6: 0.001831
[epoch: 757/1000, batch:   608/ 1052, ite: 198980] train loss: 0.003961, tar: 0.000053 
l0: 0.000126, l1: 0.000130, l2: 0.000168, l3: 0.000278, l4: 0.000637, l5: 0.000935, l6: 0.001787
[epoch: 757/1000, batch:   688/ 1052, ite: 199000] train loss: 0.003962, tar: 0.000053 
l0: 0.000033, l1: 0.000032, l2: 0.000075, l3: 0.000334, l4: 0.000916, l5: 0.001709, l6: 0.003604
[epoch: 757/1000, batch:   768/ 1052, ite: 199020] train loss: 0.003962, tar: 0.000053 
l0: 0.000026, l1: 0.000025, l2: 0.000050, l3: 0.000166, l4: 0.000351, l5: 0.001316, l6: 0.002930
[epoch: 757/1000, batch:   848/ 1052, ite: 199040] train loss: 0.003961, tar: 0.000053 
l0: 0.000176, l1: 0.000177, l2: 0.000290, l3: 0.000499, l4: 0.000849, l5: 0.001580, l6: 0.002971
[epoch: 757/1000, batch:   928/ 1052, ite: 199060] train loss: 0.003960, tar: 0.000053 
l0: 0.000130, l1: 0.000108, l2: 0.000163, l3: 0.000279, l4: 0.000548, l5: 0.001165, l6: 0.001820
[epoch: 757/1000, batch:  1008/ 1052, ite: 199080] train loss: 0.003963, tar: 0.000054 
[Epoch 757/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000076, l1: 0.000081, l2: 0.000079, l3: 0.000138, l4: 0.000297, l5: 0.000843, l6: 0.001204
[epoch: 758/1000, batch:    36/ 1052, ite: 199100] train loss: 0.003965, tar: 0.000054 
l0: 0.000141, l1: 0.000146, l2: 0.000155, l3: 0.000277, l4: 0.000337, l5: 0.000969, l6: 0.001094
[epoch: 758/1000, batch:   116/ 1052, ite: 199120] train loss: 0.003964, tar: 0.000054 
l0: 0.000185, l1: 0.000189, l2: 0.000277, l3: 0.000329, l4: 0.000606, l5: 0.001416, l6: 0.002563
[epoch: 758/1000, batch:   196/ 1052, ite: 199140] train loss: 0.003966, tar: 0.000055 
l0: 0.000367, l1: 0.000402, l2: 0.000640, l3: 0.000535, l4: 0.000792, l5: 0.001207, l6: 0.001847
[epoch: 758/1000, batch:   276/ 1052, ite: 199160] train loss: 0.003969, tar: 0.000055 
l0: 0.000196, l1: 0.000210, l2: 0.000314, l3: 0.000357, l4: 0.000569, l5: 0.001290, l6: 0.002040
[epoch: 758/1000, batch:   356/ 1052, ite: 199180] train loss: 0.003971, tar: 0.000056 
l0: 0.000270, l1: 0.000273, l2: 0.000290, l3: 0.000455, l4: 0.000604, l5: 0.001381, l6: 0.001188
[epoch: 758/1000, batch:   436/ 1052, ite: 199200] train loss: 0.003973, tar: 0.000056 
l0: 0.000305, l1: 0.000306, l2: 0.000326, l3: 0.000613, l4: 0.001033, l5: 0.001657, l6: 0.003781
[epoch: 758/1000, batch:   516/ 1052, ite: 199220] train loss: 0.003975, tar: 0.000056 
l0: 0.000113, l1: 0.000110, l2: 0.000178, l3: 0.000487, l4: 0.000938, l5: 0.001939, l6: 0.002578
[epoch: 758/1000, batch:   596/ 1052, ite: 199240] train loss: 0.003976, tar: 0.000056 
l0: 0.000041, l1: 0.000036, l2: 0.000040, l3: 0.000126, l4: 0.000222, l5: 0.000874, l6: 0.001401
[epoch: 758/1000, batch:   676/ 1052, ite: 199260] train loss: 0.003978, tar: 0.000057 
l0: 0.000111, l1: 0.000141, l2: 0.000161, l3: 0.000222, l4: 0.000458, l5: 0.000997, l6: 0.002017
[epoch: 758/1000, batch:   756/ 1052, ite: 199280] train loss: 0.003980, tar: 0.000057 
l0: 0.000028, l1: 0.000026, l2: 0.000053, l3: 0.000120, l4: 0.000171, l5: 0.000876, l6: 0.001251
[epoch: 758/1000, batch:   836/ 1052, ite: 199300] train loss: 0.003980, tar: 0.000057 
l0: 0.000206, l1: 0.000207, l2: 0.000214, l3: 0.000304, l4: 0.000496, l5: 0.001022, l6: 0.001093
[epoch: 758/1000, batch:   916/ 1052, ite: 199320] train loss: 0.003982, tar: 0.000057 
l0: 0.000072, l1: 0.000088, l2: 0.000125, l3: 0.000143, l4: 0.000214, l5: 0.001041, l6: 0.001401
[epoch: 758/1000, batch:   996/ 1052, ite: 199340] train loss: 0.003985, tar: 0.000058 
[Epoch 758/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000262, l1: 0.000277, l2: 0.000265, l3: 0.000434, l4: 0.001325, l5: 0.001259, l6: 0.003183
[epoch: 759/1000, batch:    24/ 1052, ite: 199360] train loss: 0.003989, tar: 0.000058 
l0: 0.000052, l1: 0.000053, l2: 0.000069, l3: 0.000111, l4: 0.000251, l5: 0.000639, l6: 0.000941
[epoch: 759/1000, batch:   104/ 1052, ite: 199380] train loss: 0.003991, tar: 0.000059 
l0: 0.000033, l1: 0.000032, l2: 0.000056, l3: 0.000114, l4: 0.000356, l5: 0.000547, l6: 0.001377
[epoch: 759/1000, batch:   184/ 1052, ite: 199400] train loss: 0.003992, tar: 0.000059 
l0: 0.000018, l1: 0.000015, l2: 0.000042, l3: 0.000135, l4: 0.000247, l5: 0.000618, l6: 0.001110
[epoch: 759/1000, batch:   264/ 1052, ite: 199420] train loss: 0.003992, tar: 0.000059 
l0: 0.000608, l1: 0.000606, l2: 0.000719, l3: 0.000457, l4: 0.001308, l5: 0.003267, l6: 0.005980
[epoch: 759/1000, batch:   344/ 1052, ite: 199440] train loss: 0.003993, tar: 0.000059 
l0: 0.000065, l1: 0.000062, l2: 0.000111, l3: 0.000168, l4: 0.000440, l5: 0.001238, l6: 0.002304
[epoch: 759/1000, batch:   424/ 1052, ite: 199460] train loss: 0.003993, tar: 0.000059 
l0: 0.000120, l1: 0.000129, l2: 0.000203, l3: 0.000245, l4: 0.000598, l5: 0.001130, l6: 0.001950
[epoch: 759/1000, batch:   504/ 1052, ite: 199480] train loss: 0.003995, tar: 0.000060 
l0: 0.000054, l1: 0.000053, l2: 0.000060, l3: 0.000123, l4: 0.000359, l5: 0.000987, l6: 0.001218
[epoch: 759/1000, batch:   584/ 1052, ite: 199500] train loss: 0.003995, tar: 0.000060 
l0: 0.000120, l1: 0.000125, l2: 0.000178, l3: 0.000215, l4: 0.000593, l5: 0.001073, l6: 0.001741
[epoch: 759/1000, batch:   664/ 1052, ite: 199520] train loss: 0.003998, tar: 0.000060 
l0: 0.000125, l1: 0.000128, l2: 0.000143, l3: 0.000277, l4: 0.000569, l5: 0.001130, l6: 0.001980
[epoch: 759/1000, batch:   744/ 1052, ite: 199540] train loss: 0.004001, tar: 0.000061 
l0: 0.000053, l1: 0.000057, l2: 0.000069, l3: 0.000105, l4: 0.000308, l5: 0.000797, l6: 0.001242
[epoch: 759/1000, batch:   824/ 1052, ite: 199560] train loss: 0.004003, tar: 0.000061 
l0: 0.000259, l1: 0.000249, l2: 0.000300, l3: 0.000434, l4: 0.001023, l5: 0.001683, l6: 0.002875
[epoch: 759/1000, batch:   904/ 1052, ite: 199580] train loss: 0.004004, tar: 0.000061 
l0: 0.000105, l1: 0.000112, l2: 0.000126, l3: 0.000196, l4: 0.000437, l5: 0.000946, l6: 0.001015
[epoch: 759/1000, batch:   984/ 1052, ite: 199600] train loss: 0.004004, tar: 0.000061 
[Epoch 759/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000056, l1: 0.000061, l2: 0.000141, l3: 0.000169, l4: 0.000331, l5: 0.000774, l6: 0.001997
[epoch: 760/1000, batch:    12/ 1052, ite: 199620] train loss: 0.004006, tar: 0.000061 
l0: 0.000070, l1: 0.000072, l2: 0.000088, l3: 0.000251, l4: 0.000423, l5: 0.000948, l6: 0.001597
[epoch: 760/1000, batch:    92/ 1052, ite: 199640] train loss: 0.004006, tar: 0.000061 
l0: 0.000099, l1: 0.000101, l2: 0.000158, l3: 0.000259, l4: 0.000689, l5: 0.001165, l6: 0.003721
[epoch: 760/1000, batch:   172/ 1052, ite: 199660] train loss: 0.004007, tar: 0.000061 
l0: 0.000019, l1: 0.000020, l2: 0.000033, l3: 0.000076, l4: 0.000300, l5: 0.000772, l6: 0.001908
[epoch: 760/1000, batch:   252/ 1052, ite: 199680] train loss: 0.004008, tar: 0.000061 
l0: 0.000085, l1: 0.000090, l2: 0.000128, l3: 0.000246, l4: 0.000489, l5: 0.000861, l6: 0.001363
[epoch: 760/1000, batch:   332/ 1052, ite: 199700] train loss: 0.004008, tar: 0.000062 
l0: 0.000042, l1: 0.000040, l2: 0.000088, l3: 0.000178, l4: 0.000534, l5: 0.001154, l6: 0.001992
[epoch: 760/1000, batch:   412/ 1052, ite: 199720] train loss: 0.004009, tar: 0.000062 
l0: 0.000063, l1: 0.000061, l2: 0.000132, l3: 0.000279, l4: 0.000646, l5: 0.001521, l6: 0.002844
[epoch: 760/1000, batch:   492/ 1052, ite: 199740] train loss: 0.004009, tar: 0.000062 
l0: 0.000056, l1: 0.000061, l2: 0.000074, l3: 0.000179, l4: 0.000507, l5: 0.001177, l6: 0.001312
[epoch: 760/1000, batch:   572/ 1052, ite: 199760] train loss: 0.004009, tar: 0.000062 
l0: 0.000164, l1: 0.000169, l2: 0.000208, l3: 0.000312, l4: 0.000752, l5: 0.001139, l6: 0.002869
[epoch: 760/1000, batch:   652/ 1052, ite: 199780] train loss: 0.004010, tar: 0.000062 
l0: 0.000031, l1: 0.000034, l2: 0.000073, l3: 0.000145, l4: 0.000250, l5: 0.001010, l6: 0.002268
[epoch: 760/1000, batch:   732/ 1052, ite: 199800] train loss: 0.004010, tar: 0.000062 
l0: 0.000070, l1: 0.000073, l2: 0.000097, l3: 0.000167, l4: 0.000495, l5: 0.000655, l6: 0.001153
[epoch: 760/1000, batch:   812/ 1052, ite: 199820] train loss: 0.004010, tar: 0.000062 
l0: 0.000171, l1: 0.000168, l2: 0.000244, l3: 0.000426, l4: 0.000923, l5: 0.001576, l6: 0.002545
[epoch: 760/1000, batch:   892/ 1052, ite: 199840] train loss: 0.004009, tar: 0.000062 
l0: 0.000169, l1: 0.000170, l2: 0.000226, l3: 0.000348, l4: 0.000615, l5: 0.001032, l6: 0.002916
[epoch: 760/1000, batch:   972/ 1052, ite: 199860] train loss: 0.004009, tar: 0.000062 
l0: 0.000072, l1: 0.000073, l2: 0.000082, l3: 0.000164, l4: 0.000295, l5: 0.000780, l6: 0.001364
[epoch: 760/1000, batch:  1052/ 1052, ite: 199880] train loss: 0.004010, tar: 0.000062 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_199880.pkl
[Epoch 760/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000044, l1: 0.000038, l2: 0.000084, l3: 0.000204, l4: 0.000362, l5: 0.000946, l6: 0.003069
[epoch: 761/1000, batch:    80/ 1052, ite: 199900] train loss: 0.004132, tar: 0.000076 
l0: 0.000049, l1: 0.000048, l2: 0.000098, l3: 0.000166, l4: 0.000430, l5: 0.001329, l6: 0.002055
[epoch: 761/1000, batch:   160/ 1052, ite: 199920] train loss: 0.004078, tar: 0.000072 
l0: 0.000067, l1: 0.000068, l2: 0.000106, l3: 0.000250, l4: 0.000638, l5: 0.000853, l6: 0.002347
[epoch: 761/1000, batch:   240/ 1052, ite: 199940] train loss: 0.004431, tar: 0.000086 
l0: 0.000109, l1: 0.000116, l2: 0.000141, l3: 0.000266, l4: 0.000403, l5: 0.001027, l6: 0.001841
[epoch: 761/1000, batch:   320/ 1052, ite: 199960] train loss: 0.004288, tar: 0.000083 
l0: 0.000020, l1: 0.000022, l2: 0.000044, l3: 0.000104, l4: 0.000249, l5: 0.000823, l6: 0.001560
[epoch: 761/1000, batch:   400/ 1052, ite: 199980] train loss: 0.004133, tar: 0.000078 
l0: 0.000026, l1: 0.000028, l2: 0.000066, l3: 0.000178, l4: 0.000517, l5: 0.001170, l6: 0.002100
[epoch: 761/1000, batch:   480/ 1052, ite: 200000] train loss: 0.004106, tar: 0.000075 
l0: 0.000062, l1: 0.000062, l2: 0.000101, l3: 0.000237, l4: 0.000630, l5: 0.001541, l6: 0.002912
[epoch: 761/1000, batch:   560/ 1052, ite: 200020] train loss: 0.004139, tar: 0.000073 
l0: 0.000049, l1: 0.000063, l2: 0.000069, l3: 0.000099, l4: 0.000221, l5: 0.000738, l6: 0.000917
[epoch: 761/1000, batch:   640/ 1052, ite: 200040] train loss: 0.004077, tar: 0.000070 
l0: 0.000125, l1: 0.000125, l2: 0.000154, l3: 0.000259, l4: 0.000455, l5: 0.001447, l6: 0.001706
[epoch: 761/1000, batch:   720/ 1052, ite: 200060] train loss: 0.004115, tar: 0.000070 
l0: 0.000020, l1: 0.000021, l2: 0.000038, l3: 0.000121, l4: 0.000311, l5: 0.000521, l6: 0.001133
[epoch: 761/1000, batch:   800/ 1052, ite: 200080] train loss: 0.004113, tar: 0.000071 
l0: 0.000041, l1: 0.000039, l2: 0.000080, l3: 0.000114, l4: 0.000243, l5: 0.000863, l6: 0.000876
[epoch: 761/1000, batch:   880/ 1052, ite: 200100] train loss: 0.004097, tar: 0.000071 
l0: 0.000013, l1: 0.000014, l2: 0.000025, l3: 0.000092, l4: 0.000318, l5: 0.000655, l6: 0.001003
[epoch: 761/1000, batch:   960/ 1052, ite: 200120] train loss: 0.004077, tar: 0.000070 
l0: 0.000021, l1: 0.000021, l2: 0.000032, l3: 0.000103, l4: 0.000334, l5: 0.000928, l6: 0.001133
[epoch: 761/1000, batch:  1040/ 1052, ite: 200140] train loss: 0.004080, tar: 0.000071 
[Epoch 761/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000006, l1: 0.000006, l2: 0.000022, l3: 0.000090, l4: 0.000197, l5: 0.000843, l6: 0.001089
[epoch: 762/1000, batch:    68/ 1052, ite: 200160] train loss: 0.004064, tar: 0.000070 
l0: 0.000111, l1: 0.000115, l2: 0.000144, l3: 0.000246, l4: 0.000739, l5: 0.001639, l6: 0.003192
[epoch: 762/1000, batch:   148/ 1052, ite: 200180] train loss: 0.004063, tar: 0.000069 
l0: 0.000038, l1: 0.000036, l2: 0.000104, l3: 0.000158, l4: 0.000443, l5: 0.001183, l6: 0.001276
[epoch: 762/1000, batch:   228/ 1052, ite: 200200] train loss: 0.004087, tar: 0.000070 
l0: 0.000067, l1: 0.000069, l2: 0.000085, l3: 0.000239, l4: 0.000475, l5: 0.001327, l6: 0.001987
[epoch: 762/1000, batch:   308/ 1052, ite: 200220] train loss: 0.004087, tar: 0.000069 
l0: 0.000100, l1: 0.000101, l2: 0.000125, l3: 0.000192, l4: 0.000405, l5: 0.001366, l6: 0.003404
[epoch: 762/1000, batch:   388/ 1052, ite: 200240] train loss: 0.004093, tar: 0.000068 
l0: 0.000026, l1: 0.000025, l2: 0.000051, l3: 0.000121, l4: 0.000259, l5: 0.001167, l6: 0.001661
[epoch: 762/1000, batch:   468/ 1052, ite: 200260] train loss: 0.004117, tar: 0.000068 
l0: 0.000142, l1: 0.000145, l2: 0.000192, l3: 0.000301, l4: 0.000547, l5: 0.002265, l6: 0.003367
[epoch: 762/1000, batch:   548/ 1052, ite: 200280] train loss: 0.004114, tar: 0.000068 
l0: 0.000088, l1: 0.000090, l2: 0.000110, l3: 0.000212, l4: 0.000354, l5: 0.001159, l6: 0.002234
[epoch: 762/1000, batch:   628/ 1052, ite: 200300] train loss: 0.004109, tar: 0.000067 
l0: 0.000012, l1: 0.000013, l2: 0.000041, l3: 0.000180, l4: 0.000561, l5: 0.001114, l6: 0.001705
[epoch: 762/1000, batch:   708/ 1052, ite: 200320] train loss: 0.004130, tar: 0.000068 
l0: 0.000032, l1: 0.000036, l2: 0.000035, l3: 0.000078, l4: 0.000360, l5: 0.000633, l6: 0.001656
[epoch: 762/1000, batch:   788/ 1052, ite: 200340] train loss: 0.004126, tar: 0.000068 
l0: 0.000041, l1: 0.000039, l2: 0.000058, l3: 0.000161, l4: 0.000331, l5: 0.000729, l6: 0.000843
[epoch: 762/1000, batch:   868/ 1052, ite: 200360] train loss: 0.004109, tar: 0.000067 
l0: 0.000006, l1: 0.000005, l2: 0.000015, l3: 0.000077, l4: 0.000168, l5: 0.001271, l6: 0.001427
[epoch: 762/1000, batch:   948/ 1052, ite: 200380] train loss: 0.004112, tar: 0.000068 
l0: 0.000019, l1: 0.000020, l2: 0.000071, l3: 0.000141, l4: 0.000421, l5: 0.000880, l6: 0.001800
[epoch: 762/1000, batch:  1028/ 1052, ite: 200400] train loss: 0.004079, tar: 0.000067 
[Epoch 762/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000044, l1: 0.000046, l2: 0.000052, l3: 0.000159, l4: 0.000910, l5: 0.001467, l6: 0.002385
[epoch: 763/1000, batch:    56/ 1052, ite: 200420] train loss: 0.004101, tar: 0.000068 
l0: 0.000169, l1: 0.000184, l2: 0.000232, l3: 0.000267, l4: 0.000679, l5: 0.001288, l6: 0.003522
[epoch: 763/1000, batch:   136/ 1052, ite: 200440] train loss: 0.004113, tar: 0.000072 
l0: 0.000032, l1: 0.000028, l2: 0.000071, l3: 0.000180, l4: 0.000518, l5: 0.000927, l6: 0.002135
[epoch: 763/1000, batch:   216/ 1052, ite: 200460] train loss: 0.004110, tar: 0.000072 
l0: 0.000065, l1: 0.000054, l2: 0.000175, l3: 0.000476, l4: 0.000827, l5: 0.001298, l6: 0.002735
[epoch: 763/1000, batch:   296/ 1052, ite: 200480] train loss: 0.004111, tar: 0.000072 
l0: 0.000074, l1: 0.000075, l2: 0.000134, l3: 0.000204, l4: 0.000461, l5: 0.000945, l6: 0.002004
[epoch: 763/1000, batch:   376/ 1052, ite: 200500] train loss: 0.004114, tar: 0.000073 
l0: 0.000035, l1: 0.000037, l2: 0.000049, l3: 0.000099, l4: 0.000367, l5: 0.000616, l6: 0.001164
[epoch: 763/1000, batch:   456/ 1052, ite: 200520] train loss: 0.004120, tar: 0.000073 
l0: 0.000078, l1: 0.000075, l2: 0.000135, l3: 0.000329, l4: 0.000603, l5: 0.001012, l6: 0.002140
[epoch: 763/1000, batch:   536/ 1052, ite: 200540] train loss: 0.004130, tar: 0.000074 
l0: 0.000024, l1: 0.000025, l2: 0.000036, l3: 0.000063, l4: 0.000196, l5: 0.000410, l6: 0.000649
[epoch: 763/1000, batch:   616/ 1052, ite: 200560] train loss: 0.004123, tar: 0.000074 
l0: 0.000049, l1: 0.000051, l2: 0.000091, l3: 0.000270, l4: 0.000887, l5: 0.001229, l6: 0.002415
[epoch: 763/1000, batch:   696/ 1052, ite: 200580] train loss: 0.004125, tar: 0.000074 
l0: 0.000064, l1: 0.000064, l2: 0.000103, l3: 0.000193, l4: 0.000399, l5: 0.001428, l6: 0.002671
[epoch: 763/1000, batch:   776/ 1052, ite: 200600] train loss: 0.004132, tar: 0.000074 
l0: 0.000015, l1: 0.000016, l2: 0.000037, l3: 0.000083, l4: 0.000215, l5: 0.000770, l6: 0.001391
[epoch: 763/1000, batch:   856/ 1052, ite: 200620] train loss: 0.004132, tar: 0.000074 
l0: 0.000085, l1: 0.000087, l2: 0.000151, l3: 0.000259, l4: 0.000502, l5: 0.001349, l6: 0.002173
[epoch: 763/1000, batch:   936/ 1052, ite: 200640] train loss: 0.004116, tar: 0.000073 
l0: 0.000122, l1: 0.000120, l2: 0.000167, l3: 0.000417, l4: 0.000567, l5: 0.001210, l6: 0.001951
[epoch: 763/1000, batch:  1016/ 1052, ite: 200660] train loss: 0.004113, tar: 0.000073 
[Epoch 763/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000091, l1: 0.000090, l2: 0.000160, l3: 0.000492, l4: 0.001046, l5: 0.002151, l6: 0.002939
[epoch: 764/1000, batch:    44/ 1052, ite: 200680] train loss: 0.004119, tar: 0.000073 
l0: 0.000140, l1: 0.000142, l2: 0.000244, l3: 0.000400, l4: 0.000768, l5: 0.001261, l6: 0.002395
[epoch: 764/1000, batch:   124/ 1052, ite: 200700] train loss: 0.004121, tar: 0.000073 
l0: 0.000051, l1: 0.000049, l2: 0.000087, l3: 0.000176, l4: 0.000382, l5: 0.000637, l6: 0.002320
[epoch: 764/1000, batch:   204/ 1052, ite: 200720] train loss: 0.004119, tar: 0.000073 
l0: 0.000014, l1: 0.000014, l2: 0.000019, l3: 0.000084, l4: 0.000172, l5: 0.000592, l6: 0.000901
[epoch: 764/1000, batch:   284/ 1052, ite: 200740] train loss: 0.004115, tar: 0.000073 
l0: 0.000054, l1: 0.000050, l2: 0.000117, l3: 0.000258, l4: 0.000832, l5: 0.001536, l6: 0.003201
[epoch: 764/1000, batch:   364/ 1052, ite: 200760] train loss: 0.004111, tar: 0.000072 
l0: 0.000011, l1: 0.000010, l2: 0.000022, l3: 0.000061, l4: 0.000177, l5: 0.000581, l6: 0.000551
[epoch: 764/1000, batch:   444/ 1052, ite: 200780] train loss: 0.004115, tar: 0.000072 
l0: 0.000031, l1: 0.000031, l2: 0.000058, l3: 0.000111, l4: 0.000276, l5: 0.000708, l6: 0.001227
[epoch: 764/1000, batch:   524/ 1052, ite: 200800] train loss: 0.004120, tar: 0.000072 
l0: 0.000095, l1: 0.000098, l2: 0.000121, l3: 0.000257, l4: 0.000507, l5: 0.001523, l6: 0.002479
[epoch: 764/1000, batch:   604/ 1052, ite: 200820] train loss: 0.004127, tar: 0.000072 
l0: 0.000015, l1: 0.000012, l2: 0.000051, l3: 0.000137, l4: 0.000436, l5: 0.001153, l6: 0.002607
[epoch: 764/1000, batch:   684/ 1052, ite: 200840] train loss: 0.004116, tar: 0.000071 
l0: 0.000084, l1: 0.000089, l2: 0.000095, l3: 0.000153, l4: 0.000276, l5: 0.001115, l6: 0.001596
[epoch: 764/1000, batch:   764/ 1052, ite: 200860] train loss: 0.004108, tar: 0.000070 
l0: 0.000026, l1: 0.000021, l2: 0.000062, l3: 0.000155, l4: 0.000332, l5: 0.001080, l6: 0.001705
[epoch: 764/1000, batch:   844/ 1052, ite: 200880] train loss: 0.004103, tar: 0.000070 
l0: 0.000037, l1: 0.000035, l2: 0.000071, l3: 0.000220, l4: 0.000531, l5: 0.001060, l6: 0.001611
[epoch: 764/1000, batch:   924/ 1052, ite: 200900] train loss: 0.004102, tar: 0.000070 
l0: 0.000021, l1: 0.000024, l2: 0.000033, l3: 0.000067, l4: 0.000279, l5: 0.000517, l6: 0.000799
[epoch: 764/1000, batch:  1004/ 1052, ite: 200920] train loss: 0.004094, tar: 0.000069 
[Epoch 764/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000037, l1: 0.000036, l2: 0.000060, l3: 0.000090, l4: 0.000277, l5: 0.000423, l6: 0.001511
[epoch: 765/1000, batch:    32/ 1052, ite: 200940] train loss: 0.004082, tar: 0.000069 
l0: 0.000063, l1: 0.000063, l2: 0.000109, l3: 0.000178, l4: 0.000501, l5: 0.001372, l6: 0.003413
[epoch: 765/1000, batch:   112/ 1052, ite: 200960] train loss: 0.004087, tar: 0.000069 
l0: 0.000034, l1: 0.000035, l2: 0.000054, l3: 0.000128, l4: 0.000252, l5: 0.000857, l6: 0.001116
[epoch: 765/1000, batch:   192/ 1052, ite: 200980] train loss: 0.004072, tar: 0.000068 
l0: 0.000064, l1: 0.000065, l2: 0.000081, l3: 0.000161, l4: 0.000325, l5: 0.000580, l6: 0.001680
[epoch: 765/1000, batch:   272/ 1052, ite: 201000] train loss: 0.004071, tar: 0.000068 
l0: 0.000052, l1: 0.000049, l2: 0.000083, l3: 0.000233, l4: 0.000302, l5: 0.001260, l6: 0.001715
[epoch: 765/1000, batch:   352/ 1052, ite: 201020] train loss: 0.004065, tar: 0.000068 
l0: 0.000024, l1: 0.000023, l2: 0.000084, l3: 0.000226, l4: 0.000676, l5: 0.001837, l6: 0.002396
[epoch: 765/1000, batch:   432/ 1052, ite: 201040] train loss: 0.004068, tar: 0.000068 
l0: 0.000004, l1: 0.000004, l2: 0.000023, l3: 0.000097, l4: 0.000322, l5: 0.001031, l6: 0.001781
[epoch: 765/1000, batch:   512/ 1052, ite: 201060] train loss: 0.004074, tar: 0.000067 
l0: 0.000079, l1: 0.000082, l2: 0.000131, l3: 0.000365, l4: 0.000673, l5: 0.001119, l6: 0.001738
[epoch: 765/1000, batch:   592/ 1052, ite: 201080] train loss: 0.004075, tar: 0.000067 
l0: 0.000090, l1: 0.000093, l2: 0.000115, l3: 0.000264, l4: 0.000820, l5: 0.001194, l6: 0.002400
[epoch: 765/1000, batch:   672/ 1052, ite: 201100] train loss: 0.004076, tar: 0.000067 
l0: 0.000044, l1: 0.000051, l2: 0.000045, l3: 0.000102, l4: 0.000811, l5: 0.001050, l6: 0.001801
[epoch: 765/1000, batch:   752/ 1052, ite: 201120] train loss: 0.004085, tar: 0.000068 
l0: 0.000082, l1: 0.000083, l2: 0.000103, l3: 0.000208, l4: 0.000403, l5: 0.001163, l6: 0.001992
[epoch: 765/1000, batch:   832/ 1052, ite: 201140] train loss: 0.004082, tar: 0.000068 
l0: 0.000026, l1: 0.000023, l2: 0.000073, l3: 0.000235, l4: 0.000633, l5: 0.000963, l6: 0.001838
[epoch: 765/1000, batch:   912/ 1052, ite: 201160] train loss: 0.004081, tar: 0.000067 
l0: 0.000019, l1: 0.000021, l2: 0.000033, l3: 0.000107, l4: 0.000297, l5: 0.001059, l6: 0.002237
[epoch: 765/1000, batch:   992/ 1052, ite: 201180] train loss: 0.004086, tar: 0.000067 
[Epoch 765/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000006, l1: 0.000007, l2: 0.000014, l3: 0.000036, l4: 0.000277, l5: 0.000723, l6: 0.001135
[epoch: 766/1000, batch:    20/ 1052, ite: 201200] train loss: 0.004083, tar: 0.000067 
l0: 0.000033, l1: 0.000032, l2: 0.000062, l3: 0.000132, l4: 0.000270, l5: 0.001102, l6: 0.001820
[epoch: 766/1000, batch:   100/ 1052, ite: 201220] train loss: 0.004078, tar: 0.000067 
l0: 0.000144, l1: 0.000141, l2: 0.000225, l3: 0.000353, l4: 0.000719, l5: 0.001514, l6: 0.002542
[epoch: 766/1000, batch:   180/ 1052, ite: 201240] train loss: 0.004077, tar: 0.000066 
l0: 0.000068, l1: 0.000068, l2: 0.000095, l3: 0.000243, l4: 0.000529, l5: 0.001288, l6: 0.001339
[epoch: 766/1000, batch:   260/ 1052, ite: 201260] train loss: 0.004075, tar: 0.000066 
l0: 0.000040, l1: 0.000040, l2: 0.000079, l3: 0.000226, l4: 0.000758, l5: 0.001292, l6: 0.001903
[epoch: 766/1000, batch:   340/ 1052, ite: 201280] train loss: 0.004071, tar: 0.000066 
l0: 0.000035, l1: 0.000035, l2: 0.000056, l3: 0.000206, l4: 0.000321, l5: 0.001325, l6: 0.002391
[epoch: 766/1000, batch:   420/ 1052, ite: 201300] train loss: 0.004073, tar: 0.000066 
l0: 0.000013, l1: 0.000011, l2: 0.000081, l3: 0.000145, l4: 0.000197, l5: 0.000473, l6: 0.001325
[epoch: 766/1000, batch:   500/ 1052, ite: 201320] train loss: 0.004065, tar: 0.000066 
l0: 0.000021, l1: 0.000022, l2: 0.000047, l3: 0.000118, l4: 0.000333, l5: 0.001003, l6: 0.001506
[epoch: 766/1000, batch:   580/ 1052, ite: 201340] train loss: 0.004066, tar: 0.000066 
l0: 0.000036, l1: 0.000037, l2: 0.000060, l3: 0.000122, l4: 0.000361, l5: 0.000815, l6: 0.001616
[epoch: 766/1000, batch:   660/ 1052, ite: 201360] train loss: 0.004065, tar: 0.000065 
l0: 0.000067, l1: 0.000066, l2: 0.000104, l3: 0.000253, l4: 0.000427, l5: 0.000959, l6: 0.002145
[epoch: 766/1000, batch:   740/ 1052, ite: 201380] train loss: 0.004060, tar: 0.000065 
l0: 0.000038, l1: 0.000039, l2: 0.000078, l3: 0.000149, l4: 0.000333, l5: 0.000941, l6: 0.001120
[epoch: 766/1000, batch:   820/ 1052, ite: 201400] train loss: 0.004062, tar: 0.000065 
l0: 0.000043, l1: 0.000042, l2: 0.000108, l3: 0.000295, l4: 0.000708, l5: 0.001325, l6: 0.001889
[epoch: 766/1000, batch:   900/ 1052, ite: 201420] train loss: 0.004063, tar: 0.000065 
l0: 0.000001, l1: 0.000001, l2: 0.000012, l3: 0.000084, l4: 0.000290, l5: 0.000572, l6: 0.001233
[epoch: 766/1000, batch:   980/ 1052, ite: 201440] train loss: 0.004059, tar: 0.000065 
[Epoch 766/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000033, l1: 0.000034, l2: 0.000100, l3: 0.000255, l4: 0.000454, l5: 0.001153, l6: 0.003637
[epoch: 767/1000, batch:     8/ 1052, ite: 201460] train loss: 0.004058, tar: 0.000064 
l0: 0.000031, l1: 0.000032, l2: 0.000063, l3: 0.000109, l4: 0.000432, l5: 0.000840, l6: 0.001802
[epoch: 767/1000, batch:    88/ 1052, ite: 201480] train loss: 0.004053, tar: 0.000064 
l0: 0.000057, l1: 0.000058, l2: 0.000082, l3: 0.000164, l4: 0.000603, l5: 0.000926, l6: 0.001584
[epoch: 767/1000, batch:   168/ 1052, ite: 201500] train loss: 0.004055, tar: 0.000064 
l0: 0.000070, l1: 0.000071, l2: 0.000087, l3: 0.000191, l4: 0.000510, l5: 0.001332, l6: 0.001915
[epoch: 767/1000, batch:   248/ 1052, ite: 201520] train loss: 0.004057, tar: 0.000064 
l0: 0.000012, l1: 0.000011, l2: 0.000055, l3: 0.000152, l4: 0.000343, l5: 0.001386, l6: 0.002073
[epoch: 767/1000, batch:   328/ 1052, ite: 201540] train loss: 0.004062, tar: 0.000064 
l0: 0.000057, l1: 0.000054, l2: 0.000110, l3: 0.000281, l4: 0.000927, l5: 0.003171, l6: 0.004372
[epoch: 767/1000, batch:   408/ 1052, ite: 201560] train loss: 0.004059, tar: 0.000064 
l0: 0.000087, l1: 0.000084, l2: 0.000180, l3: 0.000365, l4: 0.000775, l5: 0.002783, l6: 0.005094
[epoch: 767/1000, batch:   488/ 1052, ite: 201580] train loss: 0.004058, tar: 0.000063 
l0: 0.000004, l1: 0.000003, l2: 0.000038, l3: 0.000122, l4: 0.000355, l5: 0.001176, l6: 0.001144
[epoch: 767/1000, batch:   568/ 1052, ite: 201600] train loss: 0.004049, tar: 0.000063 
l0: 0.000011, l1: 0.000010, l2: 0.000058, l3: 0.000090, l4: 0.000486, l5: 0.000696, l6: 0.002190
[epoch: 767/1000, batch:   648/ 1052, ite: 201620] train loss: 0.004044, tar: 0.000063 
l0: 0.000262, l1: 0.000265, l2: 0.000307, l3: 0.000421, l4: 0.000609, l5: 0.001250, l6: 0.002884
[epoch: 767/1000, batch:   728/ 1052, ite: 201640] train loss: 0.004042, tar: 0.000063 
l0: 0.000028, l1: 0.000027, l2: 0.000040, l3: 0.000096, l4: 0.000410, l5: 0.001361, l6: 0.002499
[epoch: 767/1000, batch:   808/ 1052, ite: 201660] train loss: 0.004038, tar: 0.000063 
l0: 0.000021, l1: 0.000019, l2: 0.000060, l3: 0.000165, l4: 0.000645, l5: 0.000855, l6: 0.001409
[epoch: 767/1000, batch:   888/ 1052, ite: 201680] train loss: 0.004035, tar: 0.000062 
l0: 0.000050, l1: 0.000050, l2: 0.000069, l3: 0.000208, l4: 0.000563, l5: 0.001281, l6: 0.002111
[epoch: 767/1000, batch:   968/ 1052, ite: 201700] train loss: 0.004036, tar: 0.000062 
l0: 0.000053, l1: 0.000054, l2: 0.000092, l3: 0.000175, l4: 0.000364, l5: 0.000837, l6: 0.001759
[epoch: 767/1000, batch:  1048/ 1052, ite: 201720] train loss: 0.004038, tar: 0.000062 
[Epoch 767/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000016, l1: 0.000014, l2: 0.000075, l3: 0.000194, l4: 0.000577, l5: 0.001225, l6: 0.002878
[epoch: 768/1000, batch:    76/ 1052, ite: 201740] train loss: 0.004037, tar: 0.000062 
l0: 0.000024, l1: 0.000019, l2: 0.000084, l3: 0.000296, l4: 0.000562, l5: 0.000935, l6: 0.002745
[epoch: 768/1000, batch:   156/ 1052, ite: 201760] train loss: 0.004031, tar: 0.000062 
l0: 0.000154, l1: 0.000159, l2: 0.000178, l3: 0.000306, l4: 0.000520, l5: 0.000987, l6: 0.003057
[epoch: 768/1000, batch:   236/ 1052, ite: 201780] train loss: 0.004038, tar: 0.000062 
l0: 0.000018, l1: 0.000019, l2: 0.000034, l3: 0.000088, l4: 0.000438, l5: 0.001119, l6: 0.001334
[epoch: 768/1000, batch:   316/ 1052, ite: 201800] train loss: 0.004038, tar: 0.000062 
l0: 0.000040, l1: 0.000041, l2: 0.000069, l3: 0.000182, l4: 0.000431, l5: 0.000906, l6: 0.001540
[epoch: 768/1000, batch:   396/ 1052, ite: 201820] train loss: 0.004030, tar: 0.000062 
l0: 0.000025, l1: 0.000026, l2: 0.000032, l3: 0.000125, l4: 0.000399, l5: 0.000982, l6: 0.002097
[epoch: 768/1000, batch:   476/ 1052, ite: 201840] train loss: 0.004025, tar: 0.000061 
l0: 0.000130, l1: 0.000129, l2: 0.000168, l3: 0.000356, l4: 0.000682, l5: 0.001744, l6: 0.001720
[epoch: 768/1000, batch:   556/ 1052, ite: 201860] train loss: 0.004026, tar: 0.000061 
l0: 0.000117, l1: 0.000115, l2: 0.000175, l3: 0.000269, l4: 0.000481, l5: 0.001116, l6: 0.001622
[epoch: 768/1000, batch:   636/ 1052, ite: 201880] train loss: 0.004025, tar: 0.000061 
l0: 0.000055, l1: 0.000054, l2: 0.000077, l3: 0.000160, l4: 0.000443, l5: 0.001005, l6: 0.002199
[epoch: 768/1000, batch:   716/ 1052, ite: 201900] train loss: 0.004027, tar: 0.000061 
l0: 0.000056, l1: 0.000056, l2: 0.000079, l3: 0.000166, l4: 0.000557, l5: 0.001432, l6: 0.002325
[epoch: 768/1000, batch:   796/ 1052, ite: 201920] train loss: 0.004028, tar: 0.000061 
l0: 0.000054, l1: 0.000055, l2: 0.000065, l3: 0.000133, l4: 0.000409, l5: 0.000826, l6: 0.001608
[epoch: 768/1000, batch:   876/ 1052, ite: 201940] train loss: 0.004027, tar: 0.000061 
l0: 0.000108, l1: 0.000106, l2: 0.000157, l3: 0.000384, l4: 0.000940, l5: 0.001835, l6: 0.004099
[epoch: 768/1000, batch:   956/ 1052, ite: 201960] train loss: 0.004025, tar: 0.000060 
l0: 0.000021, l1: 0.000019, l2: 0.000051, l3: 0.000203, l4: 0.000402, l5: 0.000592, l6: 0.001625
[epoch: 768/1000, batch:  1036/ 1052, ite: 201980] train loss: 0.004025, tar: 0.000060 
[Epoch 768/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000012, l1: 0.000013, l2: 0.000028, l3: 0.000115, l4: 0.000291, l5: 0.001109, l6: 0.001818
[epoch: 769/1000, batch:    64/ 1052, ite: 202000] train loss: 0.004023, tar: 0.000060 
l0: 0.000011, l1: 0.000010, l2: 0.000043, l3: 0.000208, l4: 0.000518, l5: 0.001183, l6: 0.002208
[epoch: 769/1000, batch:   144/ 1052, ite: 202020] train loss: 0.004024, tar: 0.000060 
l0: 0.000045, l1: 0.000045, l2: 0.000097, l3: 0.000187, l4: 0.000564, l5: 0.000787, l6: 0.001977
[epoch: 769/1000, batch:   224/ 1052, ite: 202040] train loss: 0.004025, tar: 0.000060 
l0: 0.000012, l1: 0.000013, l2: 0.000019, l3: 0.000077, l4: 0.000220, l5: 0.000652, l6: 0.001174
[epoch: 769/1000, batch:   304/ 1052, ite: 202060] train loss: 0.004022, tar: 0.000060 
l0: 0.000005, l1: 0.000004, l2: 0.000022, l3: 0.000100, l4: 0.000376, l5: 0.001087, l6: 0.001966
[epoch: 769/1000, batch:   384/ 1052, ite: 202080] train loss: 0.004022, tar: 0.000060 
l0: 0.000057, l1: 0.000057, l2: 0.000069, l3: 0.000149, l4: 0.000419, l5: 0.001183, l6: 0.001591
[epoch: 769/1000, batch:   464/ 1052, ite: 202100] train loss: 0.004019, tar: 0.000060 
l0: 0.000011, l1: 0.000011, l2: 0.000038, l3: 0.000153, l4: 0.000464, l5: 0.000840, l6: 0.001819
[epoch: 769/1000, batch:   544/ 1052, ite: 202120] train loss: 0.004013, tar: 0.000059 
l0: 0.000042, l1: 0.000044, l2: 0.000040, l3: 0.000196, l4: 0.000453, l5: 0.000810, l6: 0.001517
[epoch: 769/1000, batch:   624/ 1052, ite: 202140] train loss: 0.004013, tar: 0.000059 
l0: 0.000052, l1: 0.000056, l2: 0.000071, l3: 0.000214, l4: 0.000725, l5: 0.000651, l6: 0.001823
[epoch: 769/1000, batch:   704/ 1052, ite: 202160] train loss: 0.004016, tar: 0.000059 
l0: 0.000052, l1: 0.000052, l2: 0.000100, l3: 0.000260, l4: 0.000878, l5: 0.001814, l6: 0.003547
[epoch: 769/1000, batch:   784/ 1052, ite: 202180] train loss: 0.004017, tar: 0.000059 
l0: 0.000074, l1: 0.000073, l2: 0.000141, l3: 0.000198, l4: 0.000234, l5: 0.000725, l6: 0.000790
[epoch: 769/1000, batch:   864/ 1052, ite: 202200] train loss: 0.004015, tar: 0.000059 
l0: 0.000007, l1: 0.000007, l2: 0.000014, l3: 0.000068, l4: 0.000280, l5: 0.000647, l6: 0.001209
[epoch: 769/1000, batch:   944/ 1052, ite: 202220] train loss: 0.004016, tar: 0.000059 
l0: 0.000005, l1: 0.000005, l2: 0.000040, l3: 0.000122, l4: 0.000331, l5: 0.000838, l6: 0.001079
[epoch: 769/1000, batch:  1024/ 1052, ite: 202240] train loss: 0.004013, tar: 0.000058 
[Epoch 769/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000069, l1: 0.000070, l2: 0.000104, l3: 0.000227, l4: 0.000644, l5: 0.001239, l6: 0.002063
[epoch: 770/1000, batch:    52/ 1052, ite: 202260] train loss: 0.004015, tar: 0.000058 
l0: 0.000042, l1: 0.000041, l2: 0.000073, l3: 0.000278, l4: 0.000452, l5: 0.000807, l6: 0.001503
[epoch: 770/1000, batch:   132/ 1052, ite: 202280] train loss: 0.004015, tar: 0.000058 
l0: 0.000009, l1: 0.000009, l2: 0.000033, l3: 0.000153, l4: 0.000357, l5: 0.000943, l6: 0.001882
[epoch: 770/1000, batch:   212/ 1052, ite: 202300] train loss: 0.004017, tar: 0.000058 
l0: 0.000044, l1: 0.000049, l2: 0.000070, l3: 0.000140, l4: 0.000319, l5: 0.000963, l6: 0.001974
[epoch: 770/1000, batch:   292/ 1052, ite: 202320] train loss: 0.004016, tar: 0.000058 
l0: 0.000005, l1: 0.000003, l2: 0.000023, l3: 0.000119, l4: 0.000357, l5: 0.000996, l6: 0.001522
[epoch: 770/1000, batch:   372/ 1052, ite: 202340] train loss: 0.004014, tar: 0.000058 
l0: 0.000075, l1: 0.000080, l2: 0.000094, l3: 0.000201, l4: 0.000578, l5: 0.001204, l6: 0.002250
[epoch: 770/1000, batch:   452/ 1052, ite: 202360] train loss: 0.004017, tar: 0.000058 
l0: 0.000127, l1: 0.000128, l2: 0.000175, l3: 0.000365, l4: 0.000820, l5: 0.001664, l6: 0.002554
[epoch: 770/1000, batch:   532/ 1052, ite: 202380] train loss: 0.004020, tar: 0.000058 
l0: 0.000003, l1: 0.000003, l2: 0.000021, l3: 0.000061, l4: 0.000270, l5: 0.000835, l6: 0.001742
[epoch: 770/1000, batch:   612/ 1052, ite: 202400] train loss: 0.004019, tar: 0.000058 
l0: 0.000063, l1: 0.000065, l2: 0.000080, l3: 0.000149, l4: 0.000444, l5: 0.001597, l6: 0.001641
[epoch: 770/1000, batch:   692/ 1052, ite: 202420] train loss: 0.004015, tar: 0.000058 
l0: 0.000003, l1: 0.000003, l2: 0.000029, l3: 0.000148, l4: 0.000402, l5: 0.001039, l6: 0.001569
[epoch: 770/1000, batch:   772/ 1052, ite: 202440] train loss: 0.004013, tar: 0.000058 
l0: 0.000010, l1: 0.000010, l2: 0.000027, l3: 0.000078, l4: 0.000237, l5: 0.000660, l6: 0.001176
[epoch: 770/1000, batch:   852/ 1052, ite: 202460] train loss: 0.004010, tar: 0.000058 
l0: 0.000008, l1: 0.000008, l2: 0.000041, l3: 0.000162, l4: 0.000390, l5: 0.000629, l6: 0.001489
[epoch: 770/1000, batch:   932/ 1052, ite: 202480] train loss: 0.004007, tar: 0.000058 
l0: 0.000021, l1: 0.000019, l2: 0.000063, l3: 0.000170, l4: 0.000384, l5: 0.000948, l6: 0.001910
[epoch: 770/1000, batch:  1012/ 1052, ite: 202500] train loss: 0.004006, tar: 0.000058 
[Epoch 770/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000014, l1: 0.000013, l2: 0.000038, l3: 0.000214, l4: 0.000342, l5: 0.000544, l6: 0.001475
[epoch: 771/1000, batch:    40/ 1052, ite: 202520] train loss: 0.004004, tar: 0.000057 
l0: 0.000031, l1: 0.000031, l2: 0.000077, l3: 0.000387, l4: 0.000843, l5: 0.001826, l6: 0.003154
[epoch: 771/1000, batch:   120/ 1052, ite: 202540] train loss: 0.004006, tar: 0.000057 
l0: 0.000062, l1: 0.000063, l2: 0.000076, l3: 0.000216, l4: 0.000850, l5: 0.001636, l6: 0.002149
[epoch: 771/1000, batch:   200/ 1052, ite: 202560] train loss: 0.004004, tar: 0.000057 
l0: 0.000009, l1: 0.000007, l2: 0.000023, l3: 0.000063, l4: 0.000319, l5: 0.000740, l6: 0.001787
[epoch: 771/1000, batch:   280/ 1052, ite: 202580] train loss: 0.004001, tar: 0.000057 
l0: 0.000044, l1: 0.000045, l2: 0.000080, l3: 0.000196, l4: 0.000466, l5: 0.000877, l6: 0.001430
[epoch: 771/1000, batch:   360/ 1052, ite: 202600] train loss: 0.004002, tar: 0.000057 
l0: 0.000010, l1: 0.000011, l2: 0.000008, l3: 0.000109, l4: 0.000222, l5: 0.000546, l6: 0.000442
[epoch: 771/1000, batch:   440/ 1052, ite: 202620] train loss: 0.004002, tar: 0.000057 
l0: 0.000021, l1: 0.000022, l2: 0.000086, l3: 0.000411, l4: 0.001119, l5: 0.001401, l6: 0.003045
[epoch: 771/1000, batch:   520/ 1052, ite: 202640] train loss: 0.004001, tar: 0.000057 
l0: 0.000105, l1: 0.000107, l2: 0.000114, l3: 0.000199, l4: 0.000385, l5: 0.001508, l6: 0.003055
[epoch: 771/1000, batch:   600/ 1052, ite: 202660] train loss: 0.004000, tar: 0.000057 
l0: 0.000048, l1: 0.000048, l2: 0.000079, l3: 0.000262, l4: 0.000684, l5: 0.001960, l6: 0.003815
[epoch: 771/1000, batch:   680/ 1052, ite: 202680] train loss: 0.004001, tar: 0.000057 
l0: 0.000013, l1: 0.000013, l2: 0.000035, l3: 0.000122, l4: 0.000264, l5: 0.001017, l6: 0.001359
[epoch: 771/1000, batch:   760/ 1052, ite: 202700] train loss: 0.003996, tar: 0.000057 
l0: 0.000021, l1: 0.000021, l2: 0.000046, l3: 0.000104, l4: 0.000271, l5: 0.000883, l6: 0.001362
[epoch: 771/1000, batch:   840/ 1052, ite: 202720] train loss: 0.003992, tar: 0.000057 
l0: 0.000005, l1: 0.000007, l2: 0.000018, l3: 0.000141, l4: 0.000269, l5: 0.000610, l6: 0.000823
[epoch: 771/1000, batch:   920/ 1052, ite: 202740] train loss: 0.003993, tar: 0.000057 
l0: 0.000034, l1: 0.000034, l2: 0.000061, l3: 0.000154, l4: 0.000296, l5: 0.000865, l6: 0.000659
[epoch: 771/1000, batch:  1000/ 1052, ite: 202760] train loss: 0.003992, tar: 0.000056 
[Epoch 771/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000019, l1: 0.000018, l2: 0.000054, l3: 0.000339, l4: 0.000690, l5: 0.001790, l6: 0.002559
[epoch: 772/1000, batch:    28/ 1052, ite: 202780] train loss: 0.003994, tar: 0.000056 
l0: 0.000030, l1: 0.000033, l2: 0.000039, l3: 0.000062, l4: 0.000370, l5: 0.000746, l6: 0.000947
[epoch: 772/1000, batch:   108/ 1052, ite: 202800] train loss: 0.003991, tar: 0.000056 
l0: 0.000065, l1: 0.000064, l2: 0.000094, l3: 0.000145, l4: 0.000295, l5: 0.001046, l6: 0.001500
[epoch: 772/1000, batch:   188/ 1052, ite: 202820] train loss: 0.003990, tar: 0.000056 
l0: 0.000001, l1: 0.000001, l2: 0.000008, l3: 0.000076, l4: 0.000202, l5: 0.000753, l6: 0.000809
[epoch: 772/1000, batch:   268/ 1052, ite: 202840] train loss: 0.003989, tar: 0.000056 
l0: 0.000105, l1: 0.000109, l2: 0.000140, l3: 0.000307, l4: 0.000630, l5: 0.001552, l6: 0.003340
[epoch: 772/1000, batch:   348/ 1052, ite: 202860] train loss: 0.003989, tar: 0.000056 
l0: 0.000092, l1: 0.000092, l2: 0.000123, l3: 0.000252, l4: 0.000673, l5: 0.001721, l6: 0.002632
[epoch: 772/1000, batch:   428/ 1052, ite: 202880] train loss: 0.003993, tar: 0.000056 
l0: 0.000031, l1: 0.000031, l2: 0.000057, l3: 0.000174, l4: 0.000350, l5: 0.000611, l6: 0.001278
[epoch: 772/1000, batch:   508/ 1052, ite: 202900] train loss: 0.003992, tar: 0.000056 
l0: 0.000018, l1: 0.000016, l2: 0.000063, l3: 0.000124, l4: 0.000484, l5: 0.001133, l6: 0.001867
[epoch: 772/1000, batch:   588/ 1052, ite: 202920] train loss: 0.003990, tar: 0.000056 
l0: 0.000045, l1: 0.000048, l2: 0.000058, l3: 0.000127, l4: 0.000434, l5: 0.001076, l6: 0.001804
[epoch: 772/1000, batch:   668/ 1052, ite: 202940] train loss: 0.003988, tar: 0.000056 
l0: 0.000038, l1: 0.000038, l2: 0.000070, l3: 0.000158, l4: 0.000340, l5: 0.000949, l6: 0.002417
[epoch: 772/1000, batch:   748/ 1052, ite: 202960] train loss: 0.003993, tar: 0.000056 
l0: 0.000027, l1: 0.000027, l2: 0.000042, l3: 0.000118, l4: 0.000358, l5: 0.000538, l6: 0.001458
[epoch: 772/1000, batch:   828/ 1052, ite: 202980] train loss: 0.003992, tar: 0.000056 
l0: 0.000008, l1: 0.000009, l2: 0.000032, l3: 0.000156, l4: 0.000540, l5: 0.000859, l6: 0.002571
[epoch: 772/1000, batch:   908/ 1052, ite: 203000] train loss: 0.003992, tar: 0.000056 
l0: 0.000068, l1: 0.000070, l2: 0.000087, l3: 0.000140, l4: 0.000384, l5: 0.000735, l6: 0.001216
[epoch: 772/1000, batch:   988/ 1052, ite: 203020] train loss: 0.003988, tar: 0.000056 
[Epoch 772/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000022, l1: 0.000020, l2: 0.000105, l3: 0.000319, l4: 0.000616, l5: 0.002336, l6: 0.003986
[epoch: 773/1000, batch:    16/ 1052, ite: 203040] train loss: 0.003989, tar: 0.000056 
l0: 0.000071, l1: 0.000071, l2: 0.000155, l3: 0.000269, l4: 0.000522, l5: 0.000809, l6: 0.001794
[epoch: 773/1000, batch:    96/ 1052, ite: 203060] train loss: 0.003989, tar: 0.000056 
l0: 0.000117, l1: 0.000119, l2: 0.000173, l3: 0.000315, l4: 0.000772, l5: 0.001692, l6: 0.002317
[epoch: 773/1000, batch:   176/ 1052, ite: 203080] train loss: 0.003994, tar: 0.000056 
l0: 0.000015, l1: 0.000014, l2: 0.000031, l3: 0.000107, l4: 0.000412, l5: 0.001191, l6: 0.001857
[epoch: 773/1000, batch:   256/ 1052, ite: 203100] train loss: 0.003991, tar: 0.000055 
l0: 0.000094, l1: 0.000088, l2: 0.000144, l3: 0.000278, l4: 0.000618, l5: 0.001227, l6: 0.003395
[epoch: 773/1000, batch:   336/ 1052, ite: 203120] train loss: 0.003990, tar: 0.000055 
l0: 0.000110, l1: 0.000111, l2: 0.000202, l3: 0.000361, l4: 0.000713, l5: 0.002420, l6: 0.003761
[epoch: 773/1000, batch:   416/ 1052, ite: 203140] train loss: 0.003991, tar: 0.000055 
l0: 0.000099, l1: 0.000100, l2: 0.000145, l3: 0.000295, l4: 0.000780, l5: 0.000527, l6: 0.002182
[epoch: 773/1000, batch:   496/ 1052, ite: 203160] train loss: 0.003992, tar: 0.000055 
l0: 0.000037, l1: 0.000036, l2: 0.000083, l3: 0.000184, l4: 0.000337, l5: 0.001383, l6: 0.001460
[epoch: 773/1000, batch:   576/ 1052, ite: 203180] train loss: 0.003989, tar: 0.000055 
l0: 0.000031, l1: 0.000031, l2: 0.000054, l3: 0.000113, l4: 0.000363, l5: 0.000671, l6: 0.001774
[epoch: 773/1000, batch:   656/ 1052, ite: 203200] train loss: 0.003988, tar: 0.000055 
l0: 0.000060, l1: 0.000059, l2: 0.000107, l3: 0.000215, l4: 0.000626, l5: 0.001649, l6: 0.002508
[epoch: 773/1000, batch:   736/ 1052, ite: 203220] train loss: 0.003988, tar: 0.000055 
l0: 0.000170, l1: 0.000177, l2: 0.000184, l3: 0.000222, l4: 0.000430, l5: 0.001062, l6: 0.003070
[epoch: 773/1000, batch:   816/ 1052, ite: 203240] train loss: 0.003986, tar: 0.000055 
l0: 0.000056, l1: 0.000059, l2: 0.000086, l3: 0.000192, l4: 0.000617, l5: 0.001462, l6: 0.001688
[epoch: 773/1000, batch:   896/ 1052, ite: 203260] train loss: 0.003985, tar: 0.000055 
l0: 0.000118, l1: 0.000119, l2: 0.000139, l3: 0.000205, l4: 0.000419, l5: 0.001273, l6: 0.002606
[epoch: 773/1000, batch:   976/ 1052, ite: 203280] train loss: 0.003983, tar: 0.000055 
[Epoch 773/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000078, l1: 0.000080, l2: 0.000098, l3: 0.000207, l4: 0.000326, l5: 0.000603, l6: 0.002029
[epoch: 774/1000, batch:     4/ 1052, ite: 203300] train loss: 0.003983, tar: 0.000055 
l0: 0.000011, l1: 0.000012, l2: 0.000070, l3: 0.000197, l4: 0.000413, l5: 0.000936, l6: 0.001243
[epoch: 774/1000, batch:    84/ 1052, ite: 203320] train loss: 0.003984, tar: 0.000055 
l0: 0.000008, l1: 0.000008, l2: 0.000037, l3: 0.000163, l4: 0.000518, l5: 0.001820, l6: 0.002689
[epoch: 774/1000, batch:   164/ 1052, ite: 203340] train loss: 0.003981, tar: 0.000055 
l0: 0.000028, l1: 0.000030, l2: 0.000031, l3: 0.000064, l4: 0.000295, l5: 0.000584, l6: 0.001091
[epoch: 774/1000, batch:   244/ 1052, ite: 203360] train loss: 0.003982, tar: 0.000055 
l0: 0.000036, l1: 0.000037, l2: 0.000047, l3: 0.000159, l4: 0.000307, l5: 0.000605, l6: 0.000802
[epoch: 774/1000, batch:   324/ 1052, ite: 203380] train loss: 0.003981, tar: 0.000055 
l0: 0.000006, l1: 0.000007, l2: 0.000014, l3: 0.000112, l4: 0.000324, l5: 0.000506, l6: 0.001250
[epoch: 774/1000, batch:   404/ 1052, ite: 203400] train loss: 0.003983, tar: 0.000055 
l0: 0.000109, l1: 0.000110, l2: 0.000162, l3: 0.000321, l4: 0.000737, l5: 0.002077, l6: 0.003292
[epoch: 774/1000, batch:   484/ 1052, ite: 203420] train loss: 0.003984, tar: 0.000055 
l0: 0.000016, l1: 0.000017, l2: 0.000050, l3: 0.000116, l4: 0.000641, l5: 0.000867, l6: 0.002204
[epoch: 774/1000, batch:   564/ 1052, ite: 203440] train loss: 0.003983, tar: 0.000054 
l0: 0.000011, l1: 0.000011, l2: 0.000025, l3: 0.000098, l4: 0.000183, l5: 0.001105, l6: 0.001418
[epoch: 774/1000, batch:   644/ 1052, ite: 203460] train loss: 0.003978, tar: 0.000054 
l0: 0.000014, l1: 0.000017, l2: 0.000024, l3: 0.000095, l4: 0.000437, l5: 0.000646, l6: 0.001185
[epoch: 774/1000, batch:   724/ 1052, ite: 203480] train loss: 0.003975, tar: 0.000054 
l0: 0.000056, l1: 0.000055, l2: 0.000091, l3: 0.000247, l4: 0.000537, l5: 0.001248, l6: 0.002504
[epoch: 774/1000, batch:   804/ 1052, ite: 203500] train loss: 0.003976, tar: 0.000054 
l0: 0.000011, l1: 0.000012, l2: 0.000033, l3: 0.000075, l4: 0.000291, l5: 0.000800, l6: 0.002192
[epoch: 774/1000, batch:   884/ 1052, ite: 203520] train loss: 0.003977, tar: 0.000054 
l0: 0.000003, l1: 0.000003, l2: 0.000019, l3: 0.000078, l4: 0.000289, l5: 0.000755, l6: 0.001224
[epoch: 774/1000, batch:   964/ 1052, ite: 203540] train loss: 0.003978, tar: 0.000054 
l0: 0.000030, l1: 0.000031, l2: 0.000066, l3: 0.000232, l4: 0.000582, l5: 0.001273, l6: 0.002089
[epoch: 774/1000, batch:  1044/ 1052, ite: 203560] train loss: 0.003975, tar: 0.000054 
[Epoch 774/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000020, l1: 0.000021, l2: 0.000033, l3: 0.000073, l4: 0.000385, l5: 0.000662, l6: 0.001224
[epoch: 775/1000, batch:    72/ 1052, ite: 203580] train loss: 0.003974, tar: 0.000054 
l0: 0.000046, l1: 0.000046, l2: 0.000088, l3: 0.000247, l4: 0.000478, l5: 0.001277, l6: 0.002051
[epoch: 775/1000, batch:   152/ 1052, ite: 203600] train loss: 0.003975, tar: 0.000054 
l0: 0.000008, l1: 0.000008, l2: 0.000041, l3: 0.000154, l4: 0.000330, l5: 0.000484, l6: 0.001733
[epoch: 775/1000, batch:   232/ 1052, ite: 203620] train loss: 0.003972, tar: 0.000054 
l0: 0.000018, l1: 0.000019, l2: 0.000048, l3: 0.000158, l4: 0.000468, l5: 0.001386, l6: 0.002864
[epoch: 775/1000, batch:   312/ 1052, ite: 203640] train loss: 0.003973, tar: 0.000054 
l0: 0.000084, l1: 0.000083, l2: 0.000117, l3: 0.000229, l4: 0.000483, l5: 0.001688, l6: 0.002618
[epoch: 775/1000, batch:   392/ 1052, ite: 203660] train loss: 0.003972, tar: 0.000054 
l0: 0.000001, l1: 0.000001, l2: 0.000009, l3: 0.000040, l4: 0.000229, l5: 0.000818, l6: 0.001016
[epoch: 775/1000, batch:   472/ 1052, ite: 203680] train loss: 0.003970, tar: 0.000054 
l0: 0.000066, l1: 0.000065, l2: 0.000142, l3: 0.000312, l4: 0.000908, l5: 0.001977, l6: 0.003385
[epoch: 775/1000, batch:   552/ 1052, ite: 203700] train loss: 0.003970, tar: 0.000054 
l0: 0.000021, l1: 0.000021, l2: 0.000054, l3: 0.000151, l4: 0.000338, l5: 0.001065, l6: 0.001816
[epoch: 775/1000, batch:   632/ 1052, ite: 203720] train loss: 0.003967, tar: 0.000053 
l0: 0.000004, l1: 0.000004, l2: 0.000029, l3: 0.000116, l4: 0.000309, l5: 0.000934, l6: 0.001153
[epoch: 775/1000, batch:   712/ 1052, ite: 203740] train loss: 0.003967, tar: 0.000054 
l0: 0.000053, l1: 0.000053, l2: 0.000079, l3: 0.000161, l4: 0.000388, l5: 0.000934, l6: 0.001226
[epoch: 775/1000, batch:   792/ 1052, ite: 203760] train loss: 0.003969, tar: 0.000054 
l0: 0.000048, l1: 0.000047, l2: 0.000049, l3: 0.000124, l4: 0.000328, l5: 0.000646, l6: 0.001544
[epoch: 775/1000, batch:   872/ 1052, ite: 203780] train loss: 0.003967, tar: 0.000053 
l0: 0.000171, l1: 0.000170, l2: 0.000205, l3: 0.000376, l4: 0.000949, l5: 0.001463, l6: 0.003452
[epoch: 775/1000, batch:   952/ 1052, ite: 203800] train loss: 0.003967, tar: 0.000053 
l0: 0.000078, l1: 0.000078, l2: 0.000143, l3: 0.000319, l4: 0.000516, l5: 0.000991, l6: 0.002041
[epoch: 775/1000, batch:  1032/ 1052, ite: 203820] train loss: 0.003969, tar: 0.000053 
[Epoch 775/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000032, l1: 0.000032, l2: 0.000048, l3: 0.000212, l4: 0.000606, l5: 0.001148, l6: 0.002116
[epoch: 776/1000, batch:    60/ 1052, ite: 203840] train loss: 0.003970, tar: 0.000053 
l0: 0.000041, l1: 0.000042, l2: 0.000122, l3: 0.000259, l4: 0.000553, l5: 0.001265, l6: 0.002911
[epoch: 776/1000, batch:   140/ 1052, ite: 203860] train loss: 0.003970, tar: 0.000053 
l0: 0.000047, l1: 0.000048, l2: 0.000059, l3: 0.000128, l4: 0.000360, l5: 0.000956, l6: 0.001174
[epoch: 776/1000, batch:   220/ 1052, ite: 203880] train loss: 0.003967, tar: 0.000053 
l0: 0.000047, l1: 0.000047, l2: 0.000086, l3: 0.000173, l4: 0.000495, l5: 0.000940, l6: 0.002138
[epoch: 776/1000, batch:   300/ 1052, ite: 203900] train loss: 0.003966, tar: 0.000053 
l0: 0.000174, l1: 0.000180, l2: 0.000205, l3: 0.000355, l4: 0.000834, l5: 0.002908, l6: 0.005838
[epoch: 776/1000, batch:   380/ 1052, ite: 203920] train loss: 0.003970, tar: 0.000053 
l0: 0.000014, l1: 0.000013, l2: 0.000033, l3: 0.000087, l4: 0.000244, l5: 0.000801, l6: 0.001271
[epoch: 776/1000, batch:   460/ 1052, ite: 203940] train loss: 0.003969, tar: 0.000053 
l0: 0.000025, l1: 0.000027, l2: 0.000045, l3: 0.000091, l4: 0.000424, l5: 0.001056, l6: 0.001354
[epoch: 776/1000, batch:   540/ 1052, ite: 203960] train loss: 0.003969, tar: 0.000053 
l0: 0.000047, l1: 0.000048, l2: 0.000075, l3: 0.000202, l4: 0.000600, l5: 0.001192, l6: 0.003077
[epoch: 776/1000, batch:   620/ 1052, ite: 203980] train loss: 0.003968, tar: 0.000053 
l0: 0.000019, l1: 0.000019, l2: 0.000056, l3: 0.000155, l4: 0.000350, l5: 0.000953, l6: 0.001352
[epoch: 776/1000, batch:   700/ 1052, ite: 204000] train loss: 0.003966, tar: 0.000053 
l0: 0.000053, l1: 0.000054, l2: 0.000085, l3: 0.000214, l4: 0.000441, l5: 0.001094, l6: 0.001753
[epoch: 776/1000, batch:   780/ 1052, ite: 204020] train loss: 0.003966, tar: 0.000053 
l0: 0.000037, l1: 0.000038, l2: 0.000109, l3: 0.000246, l4: 0.000738, l5: 0.001903, l6: 0.002687
[epoch: 776/1000, batch:   860/ 1052, ite: 204040] train loss: 0.003966, tar: 0.000053 
l0: 0.000057, l1: 0.000056, l2: 0.000100, l3: 0.000157, l4: 0.000475, l5: 0.000939, l6: 0.000869
[epoch: 776/1000, batch:   940/ 1052, ite: 204060] train loss: 0.003967, tar: 0.000053 
l0: 0.000016, l1: 0.000016, l2: 0.000060, l3: 0.000157, l4: 0.000464, l5: 0.000640, l6: 0.001659
[epoch: 776/1000, batch:  1020/ 1052, ite: 204080] train loss: 0.003964, tar: 0.000053 
[Epoch 776/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000046, l1: 0.000042, l2: 0.000160, l3: 0.000451, l4: 0.000983, l5: 0.002266, l6: 0.004571
[epoch: 777/1000, batch:    48/ 1052, ite: 204100] train loss: 0.003966, tar: 0.000053 
l0: 0.000021, l1: 0.000020, l2: 0.000055, l3: 0.000126, l4: 0.000369, l5: 0.000600, l6: 0.001186
[epoch: 777/1000, batch:   128/ 1052, ite: 204120] train loss: 0.003968, tar: 0.000053 
l0: 0.000059, l1: 0.000059, l2: 0.000083, l3: 0.000135, l4: 0.000348, l5: 0.001110, l6: 0.002678
[epoch: 777/1000, batch:   208/ 1052, ite: 204140] train loss: 0.003966, tar: 0.000053 
l0: 0.000047, l1: 0.000048, l2: 0.000098, l3: 0.000170, l4: 0.000411, l5: 0.000856, l6: 0.001744
[epoch: 777/1000, batch:   288/ 1052, ite: 204160] train loss: 0.003966, tar: 0.000053 
l0: 0.000031, l1: 0.000031, l2: 0.000059, l3: 0.000156, l4: 0.000438, l5: 0.000300, l6: 0.001263
[epoch: 777/1000, batch:   368/ 1052, ite: 204180] train loss: 0.003969, tar: 0.000053 
l0: 0.000086, l1: 0.000089, l2: 0.000106, l3: 0.000232, l4: 0.000703, l5: 0.001220, l6: 0.002231
[epoch: 777/1000, batch:   448/ 1052, ite: 204200] train loss: 0.003968, tar: 0.000053 
l0: 0.000005, l1: 0.000006, l2: 0.000024, l3: 0.000092, l4: 0.000404, l5: 0.001003, l6: 0.001987
[epoch: 777/1000, batch:   528/ 1052, ite: 204220] train loss: 0.003967, tar: 0.000053 
l0: 0.000040, l1: 0.000040, l2: 0.000076, l3: 0.000219, l4: 0.000801, l5: 0.001183, l6: 0.002591
[epoch: 777/1000, batch:   608/ 1052, ite: 204240] train loss: 0.003966, tar: 0.000053 
l0: 0.000006, l1: 0.000007, l2: 0.000029, l3: 0.000222, l4: 0.000414, l5: 0.000650, l6: 0.001454
[epoch: 777/1000, batch:   688/ 1052, ite: 204260] train loss: 0.003965, tar: 0.000053 
l0: 0.000019, l1: 0.000020, l2: 0.000037, l3: 0.000095, l4: 0.000361, l5: 0.000815, l6: 0.002290
[epoch: 777/1000, batch:   768/ 1052, ite: 204280] train loss: 0.003962, tar: 0.000053 
l0: 0.000052, l1: 0.000053, l2: 0.000098, l3: 0.000328, l4: 0.000635, l5: 0.001928, l6: 0.002387
[epoch: 777/1000, batch:   848/ 1052, ite: 204300] train loss: 0.003960, tar: 0.000052 
l0: 0.000042, l1: 0.000040, l2: 0.000133, l3: 0.000381, l4: 0.000744, l5: 0.001604, l6: 0.003562
[epoch: 777/1000, batch:   928/ 1052, ite: 204320] train loss: 0.003961, tar: 0.000053 
l0: 0.000069, l1: 0.000069, l2: 0.000099, l3: 0.000263, l4: 0.000827, l5: 0.002147, l6: 0.002768
[epoch: 777/1000, batch:  1008/ 1052, ite: 204340] train loss: 0.003960, tar: 0.000053 
[Epoch 777/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000034, l1: 0.000034, l2: 0.000090, l3: 0.000223, l4: 0.000443, l5: 0.001123, l6: 0.002382
[epoch: 778/1000, batch:    36/ 1052, ite: 204360] train loss: 0.003961, tar: 0.000053 
l0: 0.000080, l1: 0.000087, l2: 0.000094, l3: 0.000115, l4: 0.000240, l5: 0.001039, l6: 0.001021
[epoch: 778/1000, batch:   116/ 1052, ite: 204380] train loss: 0.003964, tar: 0.000053 
l0: 0.000010, l1: 0.000011, l2: 0.000032, l3: 0.000098, l4: 0.000235, l5: 0.001110, l6: 0.001026
[epoch: 778/1000, batch:   196/ 1052, ite: 204400] train loss: 0.003965, tar: 0.000053 
l0: 0.000005, l1: 0.000004, l2: 0.000037, l3: 0.000146, l4: 0.000402, l5: 0.001225, l6: 0.002390
[epoch: 778/1000, batch:   276/ 1052, ite: 204420] train loss: 0.003962, tar: 0.000052 
l0: 0.000029, l1: 0.000031, l2: 0.000024, l3: 0.000091, l4: 0.000259, l5: 0.000497, l6: 0.001334
[epoch: 778/1000, batch:   356/ 1052, ite: 204440] train loss: 0.003961, tar: 0.000052 
l0: 0.000079, l1: 0.000080, l2: 0.000103, l3: 0.000174, l4: 0.000471, l5: 0.001898, l6: 0.003478
[epoch: 778/1000, batch:   436/ 1052, ite: 204460] train loss: 0.003961, tar: 0.000052 
l0: 0.000125, l1: 0.000128, l2: 0.000145, l3: 0.000378, l4: 0.000665, l5: 0.001845, l6: 0.002343
[epoch: 778/1000, batch:   516/ 1052, ite: 204480] train loss: 0.003961, tar: 0.000052 
l0: 0.000007, l1: 0.000007, l2: 0.000042, l3: 0.000074, l4: 0.000253, l5: 0.000993, l6: 0.001423
[epoch: 778/1000, batch:   596/ 1052, ite: 204500] train loss: 0.003960, tar: 0.000052 
l0: 0.000041, l1: 0.000042, l2: 0.000121, l3: 0.000347, l4: 0.000578, l5: 0.001073, l6: 0.001960
[epoch: 778/1000, batch:   676/ 1052, ite: 204520] train loss: 0.003958, tar: 0.000052 
l0: 0.000010, l1: 0.000011, l2: 0.000040, l3: 0.000120, l4: 0.000282, l5: 0.000788, l6: 0.001314
[epoch: 778/1000, batch:   756/ 1052, ite: 204540] train loss: 0.003956, tar: 0.000052 
l0: 0.000057, l1: 0.000056, l2: 0.000083, l3: 0.000254, l4: 0.000609, l5: 0.001790, l6: 0.005498
[epoch: 778/1000, batch:   836/ 1052, ite: 204560] train loss: 0.003958, tar: 0.000052 
l0: 0.000002, l1: 0.000002, l2: 0.000015, l3: 0.000090, l4: 0.000324, l5: 0.000838, l6: 0.002262
[epoch: 778/1000, batch:   916/ 1052, ite: 204580] train loss: 0.003958, tar: 0.000052 
l0: 0.000046, l1: 0.000047, l2: 0.000081, l3: 0.000208, l4: 0.000452, l5: 0.000533, l6: 0.002091
[epoch: 778/1000, batch:   996/ 1052, ite: 204600] train loss: 0.003958, tar: 0.000052 
[Epoch 778/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000041, l1: 0.000042, l2: 0.000079, l3: 0.000251, l4: 0.000849, l5: 0.002243, l6: 0.003954
[epoch: 779/1000, batch:    24/ 1052, ite: 204620] train loss: 0.003959, tar: 0.000052 
l0: 0.000120, l1: 0.000119, l2: 0.000167, l3: 0.000236, l4: 0.000493, l5: 0.001061, l6: 0.001525
[epoch: 779/1000, batch:   104/ 1052, ite: 204640] train loss: 0.003962, tar: 0.000052 
l0: 0.000052, l1: 0.000052, l2: 0.000068, l3: 0.000154, l4: 0.000211, l5: 0.000412, l6: 0.001263
[epoch: 779/1000, batch:   184/ 1052, ite: 204660] train loss: 0.003961, tar: 0.000052 
l0: 0.000031, l1: 0.000033, l2: 0.000085, l3: 0.000242, l4: 0.001419, l5: 0.001823, l6: 0.003256
[epoch: 779/1000, batch:   264/ 1052, ite: 204680] train loss: 0.003962, tar: 0.000052 
l0: 0.000008, l1: 0.000009, l2: 0.000037, l3: 0.000128, l4: 0.000343, l5: 0.000965, l6: 0.001051
[epoch: 779/1000, batch:   344/ 1052, ite: 204700] train loss: 0.003963, tar: 0.000052 
l0: 0.000129, l1: 0.000129, l2: 0.000172, l3: 0.000231, l4: 0.000626, l5: 0.001051, l6: 0.001982
[epoch: 779/1000, batch:   424/ 1052, ite: 204720] train loss: 0.003961, tar: 0.000052 
l0: 0.000039, l1: 0.000039, l2: 0.000104, l3: 0.000304, l4: 0.000552, l5: 0.001933, l6: 0.003162
[epoch: 779/1000, batch:   504/ 1052, ite: 204740] train loss: 0.003960, tar: 0.000052 
l0: 0.000021, l1: 0.000023, l2: 0.000055, l3: 0.000155, l4: 0.000447, l5: 0.001195, l6: 0.003153
[epoch: 779/1000, batch:   584/ 1052, ite: 204760] train loss: 0.003958, tar: 0.000052 
l0: 0.000054, l1: 0.000053, l2: 0.000100, l3: 0.000228, l4: 0.000356, l5: 0.000565, l6: 0.001341
[epoch: 779/1000, batch:   664/ 1052, ite: 204780] train loss: 0.003958, tar: 0.000052 
l0: 0.000026, l1: 0.000026, l2: 0.000070, l3: 0.000126, l4: 0.000222, l5: 0.000680, l6: 0.001309
[epoch: 779/1000, batch:   744/ 1052, ite: 204800] train loss: 0.003956, tar: 0.000052 
l0: 0.000018, l1: 0.000017, l2: 0.000096, l3: 0.000261, l4: 0.000633, l5: 0.001334, l6: 0.003249
[epoch: 779/1000, batch:   824/ 1052, ite: 204820] train loss: 0.003957, tar: 0.000052 
l0: 0.000058, l1: 0.000059, l2: 0.000081, l3: 0.000169, l4: 0.000502, l5: 0.000800, l6: 0.002066
[epoch: 779/1000, batch:   904/ 1052, ite: 204840] train loss: 0.003957, tar: 0.000052 
l0: 0.000013, l1: 0.000012, l2: 0.000020, l3: 0.000091, l4: 0.000340, l5: 0.000674, l6: 0.001439
[epoch: 779/1000, batch:   984/ 1052, ite: 204860] train loss: 0.003957, tar: 0.000052 
[Epoch 779/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000012, l1: 0.000011, l2: 0.000051, l3: 0.000145, l4: 0.000495, l5: 0.000661, l6: 0.001250
[epoch: 780/1000, batch:    12/ 1052, ite: 204880] train loss: 0.003957, tar: 0.000052 
l0: 0.000012, l1: 0.000012, l2: 0.000049, l3: 0.000118, l4: 0.000367, l5: 0.001110, l6: 0.002108
[epoch: 780/1000, batch:    92/ 1052, ite: 204900] train loss: 0.003956, tar: 0.000052 
l0: 0.000043, l1: 0.000043, l2: 0.000055, l3: 0.000105, l4: 0.000350, l5: 0.000834, l6: 0.001644
[epoch: 780/1000, batch:   172/ 1052, ite: 204920] train loss: 0.003956, tar: 0.000052 
l0: 0.000025, l1: 0.000025, l2: 0.000047, l3: 0.000137, l4: 0.000306, l5: 0.000601, l6: 0.000702
[epoch: 780/1000, batch:   252/ 1052, ite: 204940] train loss: 0.003955, tar: 0.000052 
l0: 0.000007, l1: 0.000007, l2: 0.000052, l3: 0.000099, l4: 0.000365, l5: 0.000997, l6: 0.001358
[epoch: 780/1000, batch:   332/ 1052, ite: 204960] train loss: 0.003953, tar: 0.000052 
l0: 0.000050, l1: 0.000053, l2: 0.000117, l3: 0.000227, l4: 0.000461, l5: 0.001236, l6: 0.003136
[epoch: 780/1000, batch:   412/ 1052, ite: 204980] train loss: 0.003954, tar: 0.000052 
l0: 0.000025, l1: 0.000025, l2: 0.000050, l3: 0.000153, l4: 0.000516, l5: 0.001529, l6: 0.002931
[epoch: 780/1000, batch:   492/ 1052, ite: 205000] train loss: 0.003956, tar: 0.000052 
l0: 0.000067, l1: 0.000069, l2: 0.000099, l3: 0.000182, l4: 0.000575, l5: 0.001244, l6: 0.002068
[epoch: 780/1000, batch:   572/ 1052, ite: 205020] train loss: 0.003955, tar: 0.000052 
l0: 0.000025, l1: 0.000027, l2: 0.000066, l3: 0.000255, l4: 0.000775, l5: 0.001279, l6: 0.002642
[epoch: 780/1000, batch:   652/ 1052, ite: 205040] train loss: 0.003954, tar: 0.000052 
l0: 0.000091, l1: 0.000093, l2: 0.000149, l3: 0.000263, l4: 0.000781, l5: 0.001564, l6: 0.002803
[epoch: 780/1000, batch:   732/ 1052, ite: 205060] train loss: 0.003955, tar: 0.000052 
l0: 0.000008, l1: 0.000009, l2: 0.000024, l3: 0.000052, l4: 0.000308, l5: 0.000504, l6: 0.000921
[epoch: 780/1000, batch:   812/ 1052, ite: 205080] train loss: 0.003957, tar: 0.000052 
l0: 0.000010, l1: 0.000011, l2: 0.000020, l3: 0.000117, l4: 0.000248, l5: 0.000595, l6: 0.001109
[epoch: 780/1000, batch:   892/ 1052, ite: 205100] train loss: 0.003956, tar: 0.000052 
l0: 0.000090, l1: 0.000091, l2: 0.000118, l3: 0.000242, l4: 0.000437, l5: 0.001132, l6: 0.001419
[epoch: 780/1000, batch:   972/ 1052, ite: 205120] train loss: 0.003957, tar: 0.000052 
l0: 0.000023, l1: 0.000021, l2: 0.000052, l3: 0.000101, l4: 0.000300, l5: 0.000833, l6: 0.000885
[epoch: 780/1000, batch:  1052/ 1052, ite: 205140] train loss: 0.003956, tar: 0.000052 
[Epoch 780/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000018, l1: 0.000016, l2: 0.000041, l3: 0.000094, l4: 0.000198, l5: 0.000769, l6: 0.001165
[epoch: 781/1000, batch:    80/ 1052, ite: 205160] train loss: 0.003955, tar: 0.000052 
l0: 0.000067, l1: 0.000066, l2: 0.000092, l3: 0.000165, l4: 0.000431, l5: 0.000636, l6: 0.001949
[epoch: 781/1000, batch:   160/ 1052, ite: 205180] train loss: 0.003956, tar: 0.000052 
l0: 0.000002, l1: 0.000002, l2: 0.000010, l3: 0.000095, l4: 0.000400, l5: 0.000410, l6: 0.001329
[epoch: 781/1000, batch:   240/ 1052, ite: 205200] train loss: 0.003958, tar: 0.000052 
l0: 0.000049, l1: 0.000051, l2: 0.000051, l3: 0.000161, l4: 0.000542, l5: 0.000918, l6: 0.001604
[epoch: 781/1000, batch:   320/ 1052, ite: 205220] train loss: 0.003959, tar: 0.000052 
l0: 0.000057, l1: 0.000053, l2: 0.000110, l3: 0.000249, l4: 0.000587, l5: 0.001457, l6: 0.002215
[epoch: 781/1000, batch:   400/ 1052, ite: 205240] train loss: 0.003959, tar: 0.000052 
l0: 0.000033, l1: 0.000036, l2: 0.000034, l3: 0.000110, l4: 0.000374, l5: 0.000940, l6: 0.001313
[epoch: 781/1000, batch:   480/ 1052, ite: 205260] train loss: 0.003958, tar: 0.000052 
l0: 0.000039, l1: 0.000043, l2: 0.000038, l3: 0.000061, l4: 0.000276, l5: 0.000421, l6: 0.001283
[epoch: 781/1000, batch:   560/ 1052, ite: 205280] train loss: 0.003958, tar: 0.000052 
l0: 0.000036, l1: 0.000035, l2: 0.000063, l3: 0.000187, l4: 0.000524, l5: 0.001129, l6: 0.002131
[epoch: 781/1000, batch:   640/ 1052, ite: 205300] train loss: 0.003959, tar: 0.000052 
l0: 0.000012, l1: 0.000012, l2: 0.000043, l3: 0.000111, l4: 0.000383, l5: 0.001142, l6: 0.001313
[epoch: 781/1000, batch:   720/ 1052, ite: 205320] train loss: 0.003957, tar: 0.000052 
l0: 0.000018, l1: 0.000016, l2: 0.000022, l3: 0.000091, l4: 0.000224, l5: 0.000531, l6: 0.000654
[epoch: 781/1000, batch:   800/ 1052, ite: 205340] train loss: 0.003955, tar: 0.000052 
l0: 0.000016, l1: 0.000015, l2: 0.000071, l3: 0.000139, l4: 0.000384, l5: 0.001125, l6: 0.000829
[epoch: 781/1000, batch:   880/ 1052, ite: 205360] train loss: 0.003955, tar: 0.000052 
l0: 0.000014, l1: 0.000012, l2: 0.000081, l3: 0.000261, l4: 0.000625, l5: 0.000993, l6: 0.002046
[epoch: 781/1000, batch:   960/ 1052, ite: 205380] train loss: 0.003954, tar: 0.000052 
l0: 0.000031, l1: 0.000030, l2: 0.000057, l3: 0.000128, l4: 0.000288, l5: 0.000560, l6: 0.002083
[epoch: 781/1000, batch:  1040/ 1052, ite: 205400] train loss: 0.003954, tar: 0.000052 
[Epoch 781/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000006, l1: 0.000006, l2: 0.000059, l3: 0.000205, l4: 0.000487, l5: 0.001005, l6: 0.001314
[epoch: 782/1000, batch:    68/ 1052, ite: 205420] train loss: 0.003955, tar: 0.000052 
l0: 0.000008, l1: 0.000007, l2: 0.000029, l3: 0.000123, l4: 0.000436, l5: 0.000780, l6: 0.001937
[epoch: 782/1000, batch:   148/ 1052, ite: 205440] train loss: 0.003953, tar: 0.000052 
l0: 0.000028, l1: 0.000028, l2: 0.000033, l3: 0.000091, l4: 0.000244, l5: 0.000579, l6: 0.001356
[epoch: 782/1000, batch:   228/ 1052, ite: 205460] train loss: 0.003953, tar: 0.000052 
l0: 0.000017, l1: 0.000017, l2: 0.000040, l3: 0.000159, l4: 0.000516, l5: 0.000924, l6: 0.002396
[epoch: 782/1000, batch:   308/ 1052, ite: 205480] train loss: 0.003953, tar: 0.000052 
l0: 0.000043, l1: 0.000045, l2: 0.000063, l3: 0.000165, l4: 0.000293, l5: 0.000852, l6: 0.001706
[epoch: 782/1000, batch:   388/ 1052, ite: 205500] train loss: 0.003951, tar: 0.000052 
l0: 0.000028, l1: 0.000028, l2: 0.000038, l3: 0.000131, l4: 0.000461, l5: 0.000907, l6: 0.002463
[epoch: 782/1000, batch:   468/ 1052, ite: 205520] train loss: 0.003954, tar: 0.000052 
l0: 0.000002, l1: 0.000002, l2: 0.000006, l3: 0.000031, l4: 0.000290, l5: 0.000718, l6: 0.000994
[epoch: 782/1000, batch:   548/ 1052, ite: 205540] train loss: 0.003954, tar: 0.000052 
l0: 0.000064, l1: 0.000063, l2: 0.000086, l3: 0.000230, l4: 0.000476, l5: 0.001376, l6: 0.002516
[epoch: 782/1000, batch:   628/ 1052, ite: 205560] train loss: 0.003953, tar: 0.000052 
l0: 0.000122, l1: 0.000122, l2: 0.000202, l3: 0.000484, l4: 0.001283, l5: 0.002371, l6: 0.004075
[epoch: 782/1000, batch:   708/ 1052, ite: 205580] train loss: 0.003953, tar: 0.000052 
l0: 0.000041, l1: 0.000044, l2: 0.000063, l3: 0.000131, l4: 0.000448, l5: 0.001318, l6: 0.002872
[epoch: 782/1000, batch:   788/ 1052, ite: 205600] train loss: 0.003954, tar: 0.000052 
l0: 0.000025, l1: 0.000024, l2: 0.000040, l3: 0.000189, l4: 0.000284, l5: 0.000654, l6: 0.001326
[epoch: 782/1000, batch:   868/ 1052, ite: 205620] train loss: 0.003952, tar: 0.000052 
l0: 0.000029, l1: 0.000031, l2: 0.000033, l3: 0.000069, l4: 0.000358, l5: 0.000644, l6: 0.001213
[epoch: 782/1000, batch:   948/ 1052, ite: 205640] train loss: 0.003950, tar: 0.000052 
l0: 0.000090, l1: 0.000091, l2: 0.000128, l3: 0.000203, l4: 0.000780, l5: 0.001256, l6: 0.001965
[epoch: 782/1000, batch:  1028/ 1052, ite: 205660] train loss: 0.003953, tar: 0.000052 
[Epoch 782/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000041, l1: 0.000040, l2: 0.000079, l3: 0.000185, l4: 0.000627, l5: 0.001446, l6: 0.002045
[epoch: 783/1000, batch:    56/ 1052, ite: 205680] train loss: 0.003952, tar: 0.000052 
l0: 0.000046, l1: 0.000051, l2: 0.000074, l3: 0.000160, l4: 0.000272, l5: 0.000976, l6: 0.002169
[epoch: 783/1000, batch:   136/ 1052, ite: 205700] train loss: 0.003951, tar: 0.000052 
l0: 0.000050, l1: 0.000051, l2: 0.000078, l3: 0.000177, l4: 0.000633, l5: 0.000968, l6: 0.002231
[epoch: 783/1000, batch:   216/ 1052, ite: 205720] train loss: 0.003950, tar: 0.000052 
l0: 0.000092, l1: 0.000094, l2: 0.000125, l3: 0.000234, l4: 0.000637, l5: 0.001218, l6: 0.002003
[epoch: 783/1000, batch:   296/ 1052, ite: 205740] train loss: 0.003950, tar: 0.000052 
l0: 0.000224, l1: 0.000228, l2: 0.000260, l3: 0.000389, l4: 0.000912, l5: 0.001951, l6: 0.004250
[epoch: 783/1000, batch:   376/ 1052, ite: 205760] train loss: 0.003952, tar: 0.000052 
l0: 0.000048, l1: 0.000047, l2: 0.000111, l3: 0.000275, l4: 0.000648, l5: 0.001371, l6: 0.002436
[epoch: 783/1000, batch:   456/ 1052, ite: 205780] train loss: 0.003951, tar: 0.000052 
l0: 0.000006, l1: 0.000006, l2: 0.000035, l3: 0.000151, l4: 0.000301, l5: 0.000769, l6: 0.001675
[epoch: 783/1000, batch:   536/ 1052, ite: 205800] train loss: 0.003954, tar: 0.000052 
l0: 0.000010, l1: 0.000009, l2: 0.000034, l3: 0.000129, l4: 0.000245, l5: 0.000878, l6: 0.001118
[epoch: 783/1000, batch:   616/ 1052, ite: 205820] train loss: 0.003954, tar: 0.000052 
l0: 0.000018, l1: 0.000017, l2: 0.000072, l3: 0.000182, l4: 0.000460, l5: 0.000926, l6: 0.001656
[epoch: 783/1000, batch:   696/ 1052, ite: 205840] train loss: 0.003954, tar: 0.000052 
l0: 0.000028, l1: 0.000027, l2: 0.000064, l3: 0.000223, l4: 0.000473, l5: 0.001057, l6: 0.002365
[epoch: 783/1000, batch:   776/ 1052, ite: 205860] train loss: 0.003954, tar: 0.000052 
l0: 0.000003, l1: 0.000004, l2: 0.000020, l3: 0.000181, l4: 0.000544, l5: 0.000935, l6: 0.002281
[epoch: 783/1000, batch:   856/ 1052, ite: 205880] train loss: 0.003954, tar: 0.000052 
l0: 0.000013, l1: 0.000013, l2: 0.000022, l3: 0.000070, l4: 0.000258, l5: 0.000949, l6: 0.001171
[epoch: 783/1000, batch:   936/ 1052, ite: 205900] train loss: 0.003952, tar: 0.000052 
l0: 0.000113, l1: 0.000113, l2: 0.000168, l3: 0.000426, l4: 0.000797, l5: 0.001680, l6: 0.003075
[epoch: 783/1000, batch:  1016/ 1052, ite: 205920] train loss: 0.003952, tar: 0.000052 
[Epoch 783/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000014, l1: 0.000014, l2: 0.000057, l3: 0.000160, l4: 0.000398, l5: 0.000887, l6: 0.001782
[epoch: 784/1000, batch:    44/ 1052, ite: 205940] train loss: 0.003953, tar: 0.000052 
l0: 0.000046, l1: 0.000044, l2: 0.000061, l3: 0.000127, l4: 0.000289, l5: 0.000670, l6: 0.000791
[epoch: 784/1000, batch:   124/ 1052, ite: 205960] train loss: 0.003955, tar: 0.000052 
l0: 0.000069, l1: 0.000068, l2: 0.000138, l3: 0.000292, l4: 0.000814, l5: 0.001284, l6: 0.001499
[epoch: 784/1000, batch:   204/ 1052, ite: 205980] train loss: 0.003954, tar: 0.000052 
l0: 0.000031, l1: 0.000031, l2: 0.000054, l3: 0.000107, l4: 0.000470, l5: 0.000698, l6: 0.000933
[epoch: 784/1000, batch:   284/ 1052, ite: 206000] train loss: 0.003955, tar: 0.000052 
l0: 0.000003, l1: 0.000003, l2: 0.000008, l3: 0.000067, l4: 0.000222, l5: 0.000725, l6: 0.001114
[epoch: 784/1000, batch:   364/ 1052, ite: 206020] train loss: 0.003953, tar: 0.000052 
l0: 0.000009, l1: 0.000008, l2: 0.000051, l3: 0.000160, l4: 0.000318, l5: 0.001432, l6: 0.002831
[epoch: 784/1000, batch:   444/ 1052, ite: 206040] train loss: 0.003952, tar: 0.000052 
l0: 0.000001, l1: 0.000001, l2: 0.000011, l3: 0.000037, l4: 0.000342, l5: 0.000769, l6: 0.001187
[epoch: 784/1000, batch:   524/ 1052, ite: 206060] train loss: 0.003952, tar: 0.000052 
l0: 0.000071, l1: 0.000073, l2: 0.000099, l3: 0.000142, l4: 0.000377, l5: 0.000883, l6: 0.001552
[epoch: 784/1000, batch:   604/ 1052, ite: 206080] train loss: 0.003951, tar: 0.000052 
l0: 0.000023, l1: 0.000024, l2: 0.000054, l3: 0.000094, l4: 0.000337, l5: 0.000437, l6: 0.000990
[epoch: 784/1000, batch:   684/ 1052, ite: 206100] train loss: 0.003951, tar: 0.000052 
l0: 0.000089, l1: 0.000089, l2: 0.000115, l3: 0.000317, l4: 0.000802, l5: 0.001748, l6: 0.003471
[epoch: 784/1000, batch:   764/ 1052, ite: 206120] train loss: 0.003952, tar: 0.000052 
l0: 0.000035, l1: 0.000036, l2: 0.000084, l3: 0.000173, l4: 0.000437, l5: 0.000868, l6: 0.001032
[epoch: 784/1000, batch:   844/ 1052, ite: 206140] train loss: 0.003954, tar: 0.000052 
l0: 0.000119, l1: 0.000121, l2: 0.000228, l3: 0.000248, l4: 0.000559, l5: 0.001217, l6: 0.001583
[epoch: 784/1000, batch:   924/ 1052, ite: 206160] train loss: 0.003952, tar: 0.000052 
l0: 0.000037, l1: 0.000038, l2: 0.000071, l3: 0.000171, l4: 0.000656, l5: 0.001150, l6: 0.001675
[epoch: 784/1000, batch:  1004/ 1052, ite: 206180] train loss: 0.003952, tar: 0.000052 
[Epoch 784/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000061, l1: 0.000059, l2: 0.000120, l3: 0.000193, l4: 0.000413, l5: 0.000957, l6: 0.001491
[epoch: 785/1000, batch:    32/ 1052, ite: 206200] train loss: 0.003952, tar: 0.000052 
l0: 0.000067, l1: 0.000067, l2: 0.000109, l3: 0.000216, l4: 0.000544, l5: 0.001231, l6: 0.002490
[epoch: 785/1000, batch:   112/ 1052, ite: 206220] train loss: 0.003953, tar: 0.000052 
l0: 0.000079, l1: 0.000082, l2: 0.000132, l3: 0.000248, l4: 0.000717, l5: 0.001054, l6: 0.001525
[epoch: 785/1000, batch:   192/ 1052, ite: 206240] train loss: 0.003953, tar: 0.000052 
l0: 0.000144, l1: 0.000126, l2: 0.000138, l3: 0.000273, l4: 0.000515, l5: 0.001089, l6: 0.001136
[epoch: 785/1000, batch:   272/ 1052, ite: 206260] train loss: 0.003953, tar: 0.000052 
l0: 0.000047, l1: 0.000048, l2: 0.000058, l3: 0.000115, l4: 0.000304, l5: 0.000771, l6: 0.001463
[epoch: 785/1000, batch:   352/ 1052, ite: 206280] train loss: 0.003953, tar: 0.000052 
l0: 0.000005, l1: 0.000005, l2: 0.000013, l3: 0.000069, l4: 0.000384, l5: 0.000648, l6: 0.001209
[epoch: 785/1000, batch:   432/ 1052, ite: 206300] train loss: 0.003953, tar: 0.000052 
l0: 0.000061, l1: 0.000063, l2: 0.000117, l3: 0.000167, l4: 0.000584, l5: 0.000939, l6: 0.002009
[epoch: 785/1000, batch:   512/ 1052, ite: 206320] train loss: 0.003951, tar: 0.000052 
l0: 0.000008, l1: 0.000008, l2: 0.000041, l3: 0.000187, l4: 0.000886, l5: 0.001883, l6: 0.003110
[epoch: 785/1000, batch:   592/ 1052, ite: 206340] train loss: 0.003951, tar: 0.000051 
l0: 0.000031, l1: 0.000030, l2: 0.000052, l3: 0.000142, l4: 0.000301, l5: 0.000926, l6: 0.001734
[epoch: 785/1000, batch:   672/ 1052, ite: 206360] train loss: 0.003952, tar: 0.000052 
l0: 0.000021, l1: 0.000019, l2: 0.000080, l3: 0.000216, l4: 0.000652, l5: 0.001778, l6: 0.004105
[epoch: 785/1000, batch:   752/ 1052, ite: 206380] train loss: 0.003952, tar: 0.000051 
l0: 0.000022, l1: 0.000023, l2: 0.000041, l3: 0.000138, l4: 0.000391, l5: 0.001254, l6: 0.001572
[epoch: 785/1000, batch:   832/ 1052, ite: 206400] train loss: 0.003952, tar: 0.000051 
l0: 0.000015, l1: 0.000015, l2: 0.000029, l3: 0.000091, l4: 0.000259, l5: 0.000698, l6: 0.001601
[epoch: 785/1000, batch:   912/ 1052, ite: 206420] train loss: 0.003951, tar: 0.000051 
l0: 0.000049, l1: 0.000050, l2: 0.000058, l3: 0.000115, l4: 0.000354, l5: 0.000791, l6: 0.001492
[epoch: 785/1000, batch:   992/ 1052, ite: 206440] train loss: 0.003952, tar: 0.000051 
[Epoch 785/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000053, l1: 0.000053, l2: 0.000074, l3: 0.000223, l4: 0.000384, l5: 0.000638, l6: 0.001554
[epoch: 786/1000, batch:    20/ 1052, ite: 206460] train loss: 0.003953, tar: 0.000052 
l0: 0.000046, l1: 0.000046, l2: 0.000074, l3: 0.000139, l4: 0.000343, l5: 0.000854, l6: 0.001184
[epoch: 786/1000, batch:   100/ 1052, ite: 206480] train loss: 0.003953, tar: 0.000052 
l0: 0.000076, l1: 0.000075, l2: 0.000127, l3: 0.000334, l4: 0.000390, l5: 0.001154, l6: 0.002634
[epoch: 786/1000, batch:   180/ 1052, ite: 206500] train loss: 0.003952, tar: 0.000051 
l0: 0.000077, l1: 0.000078, l2: 0.000089, l3: 0.000270, l4: 0.000753, l5: 0.002026, l6: 0.002644
[epoch: 786/1000, batch:   260/ 1052, ite: 206520] train loss: 0.003952, tar: 0.000051 
l0: 0.000084, l1: 0.000087, l2: 0.000124, l3: 0.000340, l4: 0.000793, l5: 0.001244, l6: 0.002783
[epoch: 786/1000, batch:   340/ 1052, ite: 206540] train loss: 0.003953, tar: 0.000051 
l0: 0.000093, l1: 0.000094, l2: 0.000114, l3: 0.000204, l4: 0.000400, l5: 0.001085, l6: 0.002292
[epoch: 786/1000, batch:   420/ 1052, ite: 206560] train loss: 0.003953, tar: 0.000052 
l0: 0.000027, l1: 0.000030, l2: 0.000033, l3: 0.000173, l4: 0.000449, l5: 0.001101, l6: 0.001564
[epoch: 786/1000, batch:   500/ 1052, ite: 206580] train loss: 0.003953, tar: 0.000052 
l0: 0.000027, l1: 0.000028, l2: 0.000064, l3: 0.000209, l4: 0.000460, l5: 0.000898, l6: 0.001341
[epoch: 786/1000, batch:   580/ 1052, ite: 206600] train loss: 0.003951, tar: 0.000051 
l0: 0.000028, l1: 0.000025, l2: 0.000056, l3: 0.000391, l4: 0.000691, l5: 0.001651, l6: 0.002114
[epoch: 786/1000, batch:   660/ 1052, ite: 206620] train loss: 0.003951, tar: 0.000051 
l0: 0.000019, l1: 0.000017, l2: 0.000093, l3: 0.000214, l4: 0.000593, l5: 0.001663, l6: 0.002984
[epoch: 786/1000, batch:   740/ 1052, ite: 206640] train loss: 0.003950, tar: 0.000051 
l0: 0.000041, l1: 0.000040, l2: 0.000063, l3: 0.000126, l4: 0.000408, l5: 0.000911, l6: 0.002010
[epoch: 786/1000, batch:   820/ 1052, ite: 206660] train loss: 0.003951, tar: 0.000051 
l0: 0.000066, l1: 0.000067, l2: 0.000091, l3: 0.000179, l4: 0.000454, l5: 0.000979, l6: 0.001549
[epoch: 786/1000, batch:   900/ 1052, ite: 206680] train loss: 0.003952, tar: 0.000051 
l0: 0.000041, l1: 0.000043, l2: 0.000101, l3: 0.000292, l4: 0.000744, l5: 0.001372, l6: 0.002712
[epoch: 786/1000, batch:   980/ 1052, ite: 206700] train loss: 0.003952, tar: 0.000051 
[Epoch 786/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000093, l1: 0.000103, l2: 0.000135, l3: 0.000230, l4: 0.000481, l5: 0.001032, l6: 0.001269
[epoch: 787/1000, batch:     8/ 1052, ite: 206720] train loss: 0.003952, tar: 0.000051 
l0: 0.000112, l1: 0.000114, l2: 0.000137, l3: 0.000274, l4: 0.000431, l5: 0.000790, l6: 0.001327
[epoch: 787/1000, batch:    88/ 1052, ite: 206740] train loss: 0.003952, tar: 0.000051 
l0: 0.000017, l1: 0.000019, l2: 0.000075, l3: 0.000186, l4: 0.000395, l5: 0.000897, l6: 0.002288
[epoch: 787/1000, batch:   168/ 1052, ite: 206760] train loss: 0.003952, tar: 0.000051 
l0: 0.000019, l1: 0.000020, l2: 0.000057, l3: 0.000190, l4: 0.000635, l5: 0.001045, l6: 0.001957
[epoch: 787/1000, batch:   248/ 1052, ite: 206780] train loss: 0.003955, tar: 0.000051 
l0: 0.000103, l1: 0.000106, l2: 0.000130, l3: 0.000229, l4: 0.000604, l5: 0.001081, l6: 0.002200
[epoch: 787/1000, batch:   328/ 1052, ite: 206800] train loss: 0.003955, tar: 0.000052 
l0: 0.000018, l1: 0.000018, l2: 0.000042, l3: 0.000091, l4: 0.000185, l5: 0.000714, l6: 0.000467
[epoch: 787/1000, batch:   408/ 1052, ite: 206820] train loss: 0.003955, tar: 0.000052 
l0: 0.000044, l1: 0.000045, l2: 0.000061, l3: 0.000140, l4: 0.000249, l5: 0.000908, l6: 0.001525
[epoch: 787/1000, batch:   488/ 1052, ite: 206840] train loss: 0.003955, tar: 0.000052 
l0: 0.000171, l1: 0.000174, l2: 0.000183, l3: 0.000242, l4: 0.000466, l5: 0.000586, l6: 0.001392
[epoch: 787/1000, batch:   568/ 1052, ite: 206860] train loss: 0.003955, tar: 0.000052 
l0: 0.000022, l1: 0.000022, l2: 0.000046, l3: 0.000125, l4: 0.000329, l5: 0.000510, l6: 0.001659
[epoch: 787/1000, batch:   648/ 1052, ite: 206880] train loss: 0.003954, tar: 0.000052 
l0: 0.000026, l1: 0.000028, l2: 0.000045, l3: 0.000144, l4: 0.000485, l5: 0.001258, l6: 0.002820
[epoch: 787/1000, batch:   728/ 1052, ite: 206900] train loss: 0.003954, tar: 0.000051 
l0: 0.000077, l1: 0.000075, l2: 0.000104, l3: 0.000213, l4: 0.000487, l5: 0.001065, l6: 0.002345
[epoch: 787/1000, batch:   808/ 1052, ite: 206920] train loss: 0.003954, tar: 0.000051 
l0: 0.000017, l1: 0.000017, l2: 0.000056, l3: 0.000212, l4: 0.000565, l5: 0.000992, l6: 0.001350
[epoch: 787/1000, batch:   888/ 1052, ite: 206940] train loss: 0.003954, tar: 0.000051 
l0: 0.000048, l1: 0.000050, l2: 0.000067, l3: 0.000094, l4: 0.000315, l5: 0.000609, l6: 0.001417
[epoch: 787/1000, batch:   968/ 1052, ite: 206960] train loss: 0.003954, tar: 0.000051 
l0: 0.000009, l1: 0.000010, l2: 0.000025, l3: 0.000097, l4: 0.000300, l5: 0.000920, l6: 0.001424
[epoch: 787/1000, batch:  1048/ 1052, ite: 206980] train loss: 0.003953, tar: 0.000051 
[Epoch 787/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000004, l1: 0.000004, l2: 0.000007, l3: 0.000044, l4: 0.000206, l5: 0.000631, l6: 0.001237
[epoch: 788/1000, batch:    76/ 1052, ite: 207000] train loss: 0.003954, tar: 0.000051 
l0: 0.000042, l1: 0.000047, l2: 0.000040, l3: 0.000121, l4: 0.000487, l5: 0.001081, l6: 0.001866
[epoch: 788/1000, batch:   156/ 1052, ite: 207020] train loss: 0.003953, tar: 0.000051 
l0: 0.000025, l1: 0.000026, l2: 0.000024, l3: 0.000080, l4: 0.000218, l5: 0.000601, l6: 0.001531
[epoch: 788/1000, batch:   236/ 1052, ite: 207040] train loss: 0.003953, tar: 0.000051 
l0: 0.000022, l1: 0.000022, l2: 0.000037, l3: 0.000054, l4: 0.000286, l5: 0.000881, l6: 0.001037
[epoch: 788/1000, batch:   316/ 1052, ite: 207060] train loss: 0.003954, tar: 0.000051 
l0: 0.000042, l1: 0.000045, l2: 0.000092, l3: 0.000211, l4: 0.000332, l5: 0.000846, l6: 0.001873
[epoch: 788/1000, batch:   396/ 1052, ite: 207080] train loss: 0.003954, tar: 0.000051 
l0: 0.000018, l1: 0.000019, l2: 0.000043, l3: 0.000161, l4: 0.000441, l5: 0.000709, l6: 0.001207
[epoch: 788/1000, batch:   476/ 1052, ite: 207100] train loss: 0.003953, tar: 0.000051 
l0: 0.000120, l1: 0.000120, l2: 0.000194, l3: 0.000298, l4: 0.000606, l5: 0.001567, l6: 0.002161
[epoch: 788/1000, batch:   556/ 1052, ite: 207120] train loss: 0.003954, tar: 0.000051 
l0: 0.000066, l1: 0.000067, l2: 0.000089, l3: 0.000157, l4: 0.000314, l5: 0.000678, l6: 0.001400
[epoch: 788/1000, batch:   636/ 1052, ite: 207140] train loss: 0.003953, tar: 0.000051 
l0: 0.000028, l1: 0.000029, l2: 0.000052, l3: 0.000123, l4: 0.000178, l5: 0.000716, l6: 0.001384
[epoch: 788/1000, batch:   716/ 1052, ite: 207160] train loss: 0.003953, tar: 0.000051 
l0: 0.000016, l1: 0.000015, l2: 0.000024, l3: 0.000112, l4: 0.000455, l5: 0.001160, l6: 0.001771
[epoch: 788/1000, batch:   796/ 1052, ite: 207180] train loss: 0.003953, tar: 0.000051 
l0: 0.000001, l1: 0.000001, l2: 0.000003, l3: 0.000119, l4: 0.000322, l5: 0.000638, l6: 0.001218
[epoch: 788/1000, batch:   876/ 1052, ite: 207200] train loss: 0.003952, tar: 0.000051 
l0: 0.000017, l1: 0.000017, l2: 0.000030, l3: 0.000098, l4: 0.000340, l5: 0.000352, l6: 0.000921
[epoch: 788/1000, batch:   956/ 1052, ite: 207220] train loss: 0.003952, tar: 0.000051 
l0: 0.000053, l1: 0.000052, l2: 0.000106, l3: 0.000169, l4: 0.000448, l5: 0.000961, l6: 0.002036
[epoch: 788/1000, batch:  1036/ 1052, ite: 207240] train loss: 0.003952, tar: 0.000051 
[Epoch 788/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000002, l1: 0.000002, l2: 0.000010, l3: 0.000064, l4: 0.000245, l5: 0.000768, l6: 0.001328
[epoch: 789/1000, batch:    64/ 1052, ite: 207260] train loss: 0.003952, tar: 0.000051 
l0: 0.000012, l1: 0.000011, l2: 0.000046, l3: 0.000120, l4: 0.000314, l5: 0.000722, l6: 0.002485
[epoch: 789/1000, batch:   144/ 1052, ite: 207280] train loss: 0.003952, tar: 0.000051 
l0: 0.000054, l1: 0.000056, l2: 0.000085, l3: 0.000167, l4: 0.000522, l5: 0.001220, l6: 0.002328
[epoch: 789/1000, batch:   224/ 1052, ite: 207300] train loss: 0.003952, tar: 0.000051 
l0: 0.000049, l1: 0.000049, l2: 0.000066, l3: 0.000177, l4: 0.000344, l5: 0.000786, l6: 0.001341
[epoch: 789/1000, batch:   304/ 1052, ite: 207320] train loss: 0.003952, tar: 0.000051 
l0: 0.000046, l1: 0.000047, l2: 0.000068, l3: 0.000109, l4: 0.000209, l5: 0.001208, l6: 0.002422
[epoch: 789/1000, batch:   384/ 1052, ite: 207340] train loss: 0.003952, tar: 0.000051 
l0: 0.000004, l1: 0.000004, l2: 0.000010, l3: 0.000073, l4: 0.000284, l5: 0.000325, l6: 0.000836
[epoch: 789/1000, batch:   464/ 1052, ite: 207360] train loss: 0.003952, tar: 0.000051 
l0: 0.000009, l1: 0.000010, l2: 0.000050, l3: 0.000130, l4: 0.000409, l5: 0.001094, l6: 0.002225
[epoch: 789/1000, batch:   544/ 1052, ite: 207380] train loss: 0.003952, tar: 0.000051 
l0: 0.000073, l1: 0.000075, l2: 0.000088, l3: 0.000141, l4: 0.000338, l5: 0.000768, l6: 0.001417
[epoch: 789/1000, batch:   624/ 1052, ite: 207400] train loss: 0.003953, tar: 0.000051 
l0: 0.000036, l1: 0.000038, l2: 0.000041, l3: 0.000120, l4: 0.000282, l5: 0.000240, l6: 0.000707
[epoch: 789/1000, batch:   704/ 1052, ite: 207420] train loss: 0.003952, tar: 0.000051 
l0: 0.000022, l1: 0.000021, l2: 0.000032, l3: 0.000133, l4: 0.000392, l5: 0.000806, l6: 0.000827
[epoch: 789/1000, batch:   784/ 1052, ite: 207440] train loss: 0.003951, tar: 0.000051 
l0: 0.000013, l1: 0.000013, l2: 0.000052, l3: 0.000185, l4: 0.000358, l5: 0.001244, l6: 0.002161
[epoch: 789/1000, batch:   864/ 1052, ite: 207460] train loss: 0.003950, tar: 0.000051 
l0: 0.000049, l1: 0.000051, l2: 0.000089, l3: 0.000145, l4: 0.000229, l5: 0.000874, l6: 0.001103
[epoch: 789/1000, batch:   944/ 1052, ite: 207480] train loss: 0.003950, tar: 0.000051 
l0: 0.000020, l1: 0.000020, l2: 0.000045, l3: 0.000061, l4: 0.000396, l5: 0.000777, l6: 0.000960
[epoch: 789/1000, batch:  1024/ 1052, ite: 207500] train loss: 0.003950, tar: 0.000051 
[Epoch 789/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000051, l1: 0.000050, l2: 0.000113, l3: 0.000258, l4: 0.000428, l5: 0.001392, l6: 0.002723
[epoch: 790/1000, batch:    52/ 1052, ite: 207520] train loss: 0.003951, tar: 0.000051 
l0: 0.000022, l1: 0.000021, l2: 0.000056, l3: 0.000145, l4: 0.000563, l5: 0.001368, l6: 0.002091
[epoch: 790/1000, batch:   132/ 1052, ite: 207540] train loss: 0.003950, tar: 0.000051 
l0: 0.000089, l1: 0.000091, l2: 0.000104, l3: 0.000195, l4: 0.000389, l5: 0.001171, l6: 0.001907
[epoch: 790/1000, batch:   212/ 1052, ite: 207560] train loss: 0.003950, tar: 0.000051 
l0: 0.000027, l1: 0.000027, l2: 0.000036, l3: 0.000124, l4: 0.000274, l5: 0.000729, l6: 0.000802
[epoch: 790/1000, batch:   292/ 1052, ite: 207580] train loss: 0.003950, tar: 0.000051 
l0: 0.000005, l1: 0.000005, l2: 0.000018, l3: 0.000085, l4: 0.000430, l5: 0.000779, l6: 0.001381
[epoch: 790/1000, batch:   372/ 1052, ite: 207600] train loss: 0.003950, tar: 0.000051 
l0: 0.000054, l1: 0.000055, l2: 0.000089, l3: 0.000153, l4: 0.000292, l5: 0.000681, l6: 0.001297
[epoch: 790/1000, batch:   452/ 1052, ite: 207620] train loss: 0.003948, tar: 0.000051 
l0: 0.000008, l1: 0.000009, l2: 0.000021, l3: 0.000197, l4: 0.000406, l5: 0.000575, l6: 0.000800
[epoch: 790/1000, batch:   532/ 1052, ite: 207640] train loss: 0.003947, tar: 0.000051 
l0: 0.000046, l1: 0.000046, l2: 0.000070, l3: 0.000248, l4: 0.000372, l5: 0.000865, l6: 0.001234
[epoch: 790/1000, batch:   612/ 1052, ite: 207660] train loss: 0.003947, tar: 0.000051 
l0: 0.000049, l1: 0.000049, l2: 0.000088, l3: 0.000209, l4: 0.000419, l5: 0.001129, l6: 0.001473
[epoch: 790/1000, batch:   692/ 1052, ite: 207680] train loss: 0.003948, tar: 0.000051 
l0: 0.000075, l1: 0.000078, l2: 0.000100, l3: 0.000195, l4: 0.000499, l5: 0.001116, l6: 0.001925
[epoch: 790/1000, batch:   772/ 1052, ite: 207700] train loss: 0.003949, tar: 0.000051 
l0: 0.000046, l1: 0.000047, l2: 0.000064, l3: 0.000082, l4: 0.000257, l5: 0.000814, l6: 0.001570
[epoch: 790/1000, batch:   852/ 1052, ite: 207720] train loss: 0.003949, tar: 0.000051 
l0: 0.000022, l1: 0.000022, l2: 0.000069, l3: 0.000200, l4: 0.000396, l5: 0.001296, l6: 0.001493
[epoch: 790/1000, batch:   932/ 1052, ite: 207740] train loss: 0.003949, tar: 0.000051 
l0: 0.000019, l1: 0.000020, l2: 0.000026, l3: 0.000055, l4: 0.000301, l5: 0.000785, l6: 0.001301
[epoch: 790/1000, batch:  1012/ 1052, ite: 207760] train loss: 0.003949, tar: 0.000051 
[Epoch 790/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000014, l1: 0.000014, l2: 0.000032, l3: 0.000186, l4: 0.000505, l5: 0.000758, l6: 0.002836
[epoch: 791/1000, batch:    40/ 1052, ite: 207780] train loss: 0.003950, tar: 0.000051 
l0: 0.000096, l1: 0.000095, l2: 0.000111, l3: 0.000199, l4: 0.000615, l5: 0.001080, l6: 0.001597
[epoch: 791/1000, batch:   120/ 1052, ite: 207800] train loss: 0.003949, tar: 0.000051 
l0: 0.000016, l1: 0.000016, l2: 0.000062, l3: 0.000195, l4: 0.000435, l5: 0.000756, l6: 0.002217
[epoch: 791/1000, batch:   200/ 1052, ite: 207820] train loss: 0.003950, tar: 0.000051 
l0: 0.000052, l1: 0.000052, l2: 0.000096, l3: 0.000192, l4: 0.000398, l5: 0.001139, l6: 0.002445
[epoch: 791/1000, batch:   280/ 1052, ite: 207840] train loss: 0.003950, tar: 0.000051 
l0: 0.000012, l1: 0.000015, l2: 0.000036, l3: 0.000095, l4: 0.000190, l5: 0.000715, l6: 0.001303
[epoch: 791/1000, batch:   360/ 1052, ite: 207860] train loss: 0.003951, tar: 0.000051 
l0: 0.000119, l1: 0.000121, l2: 0.000164, l3: 0.000223, l4: 0.000599, l5: 0.001037, l6: 0.002319
[epoch: 791/1000, batch:   440/ 1052, ite: 207880] train loss: 0.003950, tar: 0.000051 
l0: 0.000060, l1: 0.000060, l2: 0.000103, l3: 0.000251, l4: 0.000656, l5: 0.001093, l6: 0.002117
[epoch: 791/1000, batch:   520/ 1052, ite: 207900] train loss: 0.003952, tar: 0.000051 
l0: 0.000017, l1: 0.000016, l2: 0.000037, l3: 0.000090, l4: 0.000319, l5: 0.001235, l6: 0.001682
[epoch: 791/1000, batch:   600/ 1052, ite: 207920] train loss: 0.003951, tar: 0.000051 
l0: 0.000079, l1: 0.000077, l2: 0.000126, l3: 0.000273, l4: 0.000985, l5: 0.001438, l6: 0.002945
[epoch: 791/1000, batch:   680/ 1052, ite: 207940] train loss: 0.003950, tar: 0.000051 
l0: 0.000042, l1: 0.000042, l2: 0.000061, l3: 0.000223, l4: 0.000653, l5: 0.001470, l6: 0.002245
[epoch: 791/1000, batch:   760/ 1052, ite: 207960] train loss: 0.003949, tar: 0.000051 
l0: 0.000027, l1: 0.000030, l2: 0.000091, l3: 0.000216, l4: 0.000417, l5: 0.000938, l6: 0.002272
[epoch: 791/1000, batch:   840/ 1052, ite: 207980] train loss: 0.003950, tar: 0.000051 
l0: 0.000049, l1: 0.000048, l2: 0.000103, l3: 0.000253, l4: 0.000546, l5: 0.002201, l6: 0.003243
[epoch: 791/1000, batch:   920/ 1052, ite: 208000] train loss: 0.003949, tar: 0.000051 
l0: 0.000004, l1: 0.000005, l2: 0.000015, l3: 0.000017, l4: 0.000099, l5: 0.000408, l6: 0.000767
[epoch: 791/1000, batch:  1000/ 1052, ite: 208020] train loss: 0.003948, tar: 0.000051 
[Epoch 791/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000019, l1: 0.000018, l2: 0.000041, l3: 0.000161, l4: 0.000754, l5: 0.000881, l6: 0.001702
[epoch: 792/1000, batch:    28/ 1052, ite: 208040] train loss: 0.003949, tar: 0.000051 
l0: 0.000027, l1: 0.000024, l2: 0.000067, l3: 0.000162, l4: 0.000228, l5: 0.000959, l6: 0.001241
[epoch: 792/1000, batch:   108/ 1052, ite: 208060] train loss: 0.003950, tar: 0.000051 
l0: 0.000062, l1: 0.000064, l2: 0.000080, l3: 0.000156, l4: 0.000532, l5: 0.000775, l6: 0.001649
[epoch: 792/1000, batch:   188/ 1052, ite: 208080] train loss: 0.003949, tar: 0.000051 
l0: 0.000115, l1: 0.000116, l2: 0.000142, l3: 0.000220, l4: 0.000448, l5: 0.001171, l6: 0.002527
[epoch: 792/1000, batch:   268/ 1052, ite: 208100] train loss: 0.003949, tar: 0.000051 
l0: 0.000059, l1: 0.000059, l2: 0.000093, l3: 0.000219, l4: 0.000426, l5: 0.000576, l6: 0.001147
[epoch: 792/1000, batch:   348/ 1052, ite: 208120] train loss: 0.003950, tar: 0.000051 
l0: 0.000058, l1: 0.000060, l2: 0.000063, l3: 0.000120, l4: 0.000458, l5: 0.000918, l6: 0.001305
[epoch: 792/1000, batch:   428/ 1052, ite: 208140] train loss: 0.003950, tar: 0.000051 
l0: 0.000064, l1: 0.000062, l2: 0.000136, l3: 0.000243, l4: 0.000500, l5: 0.000997, l6: 0.002224
[epoch: 792/1000, batch:   508/ 1052, ite: 208160] train loss: 0.003949, tar: 0.000051 
l0: 0.000025, l1: 0.000027, l2: 0.000030, l3: 0.000078, l4: 0.000403, l5: 0.000961, l6: 0.000902
[epoch: 792/1000, batch:   588/ 1052, ite: 208180] train loss: 0.003951, tar: 0.000051 
l0: 0.000073, l1: 0.000071, l2: 0.000131, l3: 0.000325, l4: 0.000759, l5: 0.002199, l6: 0.003825
[epoch: 792/1000, batch:   668/ 1052, ite: 208200] train loss: 0.003952, tar: 0.000051 
l0: 0.000047, l1: 0.000046, l2: 0.000064, l3: 0.000117, l4: 0.000289, l5: 0.000455, l6: 0.001458
[epoch: 792/1000, batch:   748/ 1052, ite: 208220] train loss: 0.003950, tar: 0.000051 
l0: 0.000044, l1: 0.000044, l2: 0.000063, l3: 0.000142, l4: 0.000560, l5: 0.001120, l6: 0.001898
[epoch: 792/1000, batch:   828/ 1052, ite: 208240] train loss: 0.003950, tar: 0.000051 
l0: 0.000066, l1: 0.000068, l2: 0.000107, l3: 0.000220, l4: 0.000712, l5: 0.001773, l6: 0.002699
[epoch: 792/1000, batch:   908/ 1052, ite: 208260] train loss: 0.003950, tar: 0.000051 
l0: 0.000011, l1: 0.000012, l2: 0.000037, l3: 0.000154, l4: 0.000529, l5: 0.001281, l6: 0.002730
[epoch: 792/1000, batch:   988/ 1052, ite: 208280] train loss: 0.003950, tar: 0.000051 
[Epoch 792/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000071, l1: 0.000069, l2: 0.000147, l3: 0.000294, l4: 0.000797, l5: 0.002629, l6: 0.003537
[epoch: 793/1000, batch:    16/ 1052, ite: 208300] train loss: 0.003950, tar: 0.000051 
l0: 0.000034, l1: 0.000035, l2: 0.000050, l3: 0.000186, l4: 0.000462, l5: 0.000955, l6: 0.002252
[epoch: 793/1000, batch:    96/ 1052, ite: 208320] train loss: 0.003950, tar: 0.000051 
l0: 0.000011, l1: 0.000012, l2: 0.000024, l3: 0.000089, l4: 0.000121, l5: 0.000903, l6: 0.001377
[epoch: 793/1000, batch:   176/ 1052, ite: 208340] train loss: 0.003950, tar: 0.000051 
l0: 0.000009, l1: 0.000008, l2: 0.000041, l3: 0.000129, l4: 0.000583, l5: 0.000905, l6: 0.002005
[epoch: 793/1000, batch:   256/ 1052, ite: 208360] train loss: 0.003950, tar: 0.000051 
l0: 0.000046, l1: 0.000047, l2: 0.000070, l3: 0.000166, l4: 0.000357, l5: 0.000758, l6: 0.001251
[epoch: 793/1000, batch:   336/ 1052, ite: 208380] train loss: 0.003951, tar: 0.000051 
l0: 0.000016, l1: 0.000016, l2: 0.000023, l3: 0.000109, l4: 0.000384, l5: 0.001304, l6: 0.002136
[epoch: 793/1000, batch:   416/ 1052, ite: 208400] train loss: 0.003951, tar: 0.000051 
l0: 0.000101, l1: 0.000101, l2: 0.000138, l3: 0.000195, l4: 0.000426, l5: 0.000976, l6: 0.001614
[epoch: 793/1000, batch:   496/ 1052, ite: 208420] train loss: 0.003951, tar: 0.000051 
l0: 0.000058, l1: 0.000056, l2: 0.000098, l3: 0.000143, l4: 0.000359, l5: 0.000768, l6: 0.001560
[epoch: 793/1000, batch:   576/ 1052, ite: 208440] train loss: 0.003951, tar: 0.000051 
l0: 0.000266, l1: 0.000269, l2: 0.000339, l3: 0.000430, l4: 0.000683, l5: 0.001411, l6: 0.002178
[epoch: 793/1000, batch:   656/ 1052, ite: 208460] train loss: 0.003950, tar: 0.000051 
l0: 0.000022, l1: 0.000023, l2: 0.000037, l3: 0.000136, l4: 0.000247, l5: 0.000965, l6: 0.001706
[epoch: 793/1000, batch:   736/ 1052, ite: 208480] train loss: 0.003950, tar: 0.000051 
l0: 0.000133, l1: 0.000132, l2: 0.000147, l3: 0.000192, l4: 0.000543, l5: 0.001129, l6: 0.002377
[epoch: 793/1000, batch:   816/ 1052, ite: 208500] train loss: 0.003949, tar: 0.000051 
l0: 0.000096, l1: 0.000098, l2: 0.000139, l3: 0.000303, l4: 0.000732, l5: 0.001777, l6: 0.002384
[epoch: 793/1000, batch:   896/ 1052, ite: 208520] train loss: 0.003949, tar: 0.000051 
l0: 0.000012, l1: 0.000012, l2: 0.000025, l3: 0.000097, l4: 0.000276, l5: 0.000428, l6: 0.002332
[epoch: 793/1000, batch:   976/ 1052, ite: 208540] train loss: 0.003949, tar: 0.000051 
[Epoch 793/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000003, l1: 0.000003, l2: 0.000024, l3: 0.000110, l4: 0.000332, l5: 0.000813, l6: 0.001572
[epoch: 794/1000, batch:     4/ 1052, ite: 208560] train loss: 0.003949, tar: 0.000051 
l0: 0.000013, l1: 0.000014, l2: 0.000029, l3: 0.000080, l4: 0.000224, l5: 0.001056, l6: 0.002312
[epoch: 794/1000, batch:    84/ 1052, ite: 208580] train loss: 0.003948, tar: 0.000051 
l0: 0.000047, l1: 0.000047, l2: 0.000065, l3: 0.000149, l4: 0.000358, l5: 0.001397, l6: 0.001503
[epoch: 794/1000, batch:   164/ 1052, ite: 208600] train loss: 0.003948, tar: 0.000051 
l0: 0.000013, l1: 0.000013, l2: 0.000051, l3: 0.000151, l4: 0.000300, l5: 0.000817, l6: 0.002118
[epoch: 794/1000, batch:   244/ 1052, ite: 208620] train loss: 0.003948, tar: 0.000051 
l0: 0.000008, l1: 0.000008, l2: 0.000029, l3: 0.000099, l4: 0.000258, l5: 0.001082, l6: 0.002099
[epoch: 794/1000, batch:   324/ 1052, ite: 208640] train loss: 0.003949, tar: 0.000051 
l0: 0.000009, l1: 0.000008, l2: 0.000031, l3: 0.000141, l4: 0.000396, l5: 0.001070, l6: 0.001471
[epoch: 794/1000, batch:   404/ 1052, ite: 208660] train loss: 0.003948, tar: 0.000051 
l0: 0.000103, l1: 0.000104, l2: 0.000144, l3: 0.000255, l4: 0.000576, l5: 0.001137, l6: 0.002581
[epoch: 794/1000, batch:   484/ 1052, ite: 208680] train loss: 0.003948, tar: 0.000051 
l0: 0.000037, l1: 0.000038, l2: 0.000080, l3: 0.000241, l4: 0.000647, l5: 0.001654, l6: 0.002112
[epoch: 794/1000, batch:   564/ 1052, ite: 208700] train loss: 0.003947, tar: 0.000051 
l0: 0.000033, l1: 0.000032, l2: 0.000150, l3: 0.000295, l4: 0.000897, l5: 0.001935, l6: 0.002747
[epoch: 794/1000, batch:   644/ 1052, ite: 208720] train loss: 0.003947, tar: 0.000051 
l0: 0.000104, l1: 0.000106, l2: 0.000132, l3: 0.000231, l4: 0.000355, l5: 0.001027, l6: 0.003009
[epoch: 794/1000, batch:   724/ 1052, ite: 208740] train loss: 0.003949, tar: 0.000051 
l0: 0.000018, l1: 0.000016, l2: 0.000046, l3: 0.000121, l4: 0.000303, l5: 0.001160, l6: 0.001744
[epoch: 794/1000, batch:   804/ 1052, ite: 208760] train loss: 0.003948, tar: 0.000051 
l0: 0.000058, l1: 0.000060, l2: 0.000097, l3: 0.000203, l4: 0.000496, l5: 0.000941, l6: 0.001681
[epoch: 794/1000, batch:   884/ 1052, ite: 208780] train loss: 0.003948, tar: 0.000051 
l0: 0.000035, l1: 0.000035, l2: 0.000041, l3: 0.000071, l4: 0.000326, l5: 0.000586, l6: 0.001139
[epoch: 794/1000, batch:   964/ 1052, ite: 208800] train loss: 0.003947, tar: 0.000051 
l0: 0.000100, l1: 0.000104, l2: 0.000139, l3: 0.000239, l4: 0.000810, l5: 0.001619, l6: 0.001931
[epoch: 794/1000, batch:  1044/ 1052, ite: 208820] train loss: 0.003948, tar: 0.000051 
[Epoch 794/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000043, l1: 0.000043, l2: 0.000066, l3: 0.000183, l4: 0.000286, l5: 0.000680, l6: 0.001170
[epoch: 795/1000, batch:    72/ 1052, ite: 208840] train loss: 0.003948, tar: 0.000051 
l0: 0.000010, l1: 0.000010, l2: 0.000047, l3: 0.000160, l4: 0.000353, l5: 0.000993, l6: 0.001696
[epoch: 795/1000, batch:   152/ 1052, ite: 208860] train loss: 0.003947, tar: 0.000051 
l0: 0.000042, l1: 0.000045, l2: 0.000064, l3: 0.000123, l4: 0.000344, l5: 0.000935, l6: 0.002086
[epoch: 795/1000, batch:   232/ 1052, ite: 208880] train loss: 0.003948, tar: 0.000051 
l0: 0.000055, l1: 0.000057, l2: 0.000066, l3: 0.000183, l4: 0.000377, l5: 0.000844, l6: 0.001706
[epoch: 795/1000, batch:   312/ 1052, ite: 208900] train loss: 0.003947, tar: 0.000051 
l0: 0.000115, l1: 0.000117, l2: 0.000139, l3: 0.000206, l4: 0.000375, l5: 0.001127, l6: 0.001526
[epoch: 795/1000, batch:   392/ 1052, ite: 208920] train loss: 0.003947, tar: 0.000051 
l0: 0.000025, l1: 0.000024, l2: 0.000080, l3: 0.000097, l4: 0.000286, l5: 0.000491, l6: 0.001182
[epoch: 795/1000, batch:   472/ 1052, ite: 208940] train loss: 0.003948, tar: 0.000051 
l0: 0.000039, l1: 0.000039, l2: 0.000045, l3: 0.000093, l4: 0.000267, l5: 0.000497, l6: 0.001530
[epoch: 795/1000, batch:   552/ 1052, ite: 208960] train loss: 0.003947, tar: 0.000051 
l0: 0.000217, l1: 0.000221, l2: 0.000278, l3: 0.000436, l4: 0.000890, l5: 0.001920, l6: 0.003165
[epoch: 795/1000, batch:   632/ 1052, ite: 208980] train loss: 0.003948, tar: 0.000051 
l0: 0.000026, l1: 0.000026, l2: 0.000070, l3: 0.000256, l4: 0.000546, l5: 0.001735, l6: 0.002027
[epoch: 795/1000, batch:   712/ 1052, ite: 209000] train loss: 0.003947, tar: 0.000051 
l0: 0.000002, l1: 0.000002, l2: 0.000033, l3: 0.000107, l4: 0.000286, l5: 0.000461, l6: 0.001025
[epoch: 795/1000, batch:   792/ 1052, ite: 209020] train loss: 0.003947, tar: 0.000051 
l0: 0.000090, l1: 0.000089, l2: 0.000119, l3: 0.000224, l4: 0.000691, l5: 0.002170, l6: 0.003502
[epoch: 795/1000, batch:   872/ 1052, ite: 209040] train loss: 0.003948, tar: 0.000051 
l0: 0.000019, l1: 0.000019, l2: 0.000038, l3: 0.000099, l4: 0.000308, l5: 0.000724, l6: 0.001280
[epoch: 795/1000, batch:   952/ 1052, ite: 209060] train loss: 0.003947, tar: 0.000051 
l0: 0.000022, l1: 0.000023, l2: 0.000028, l3: 0.000061, l4: 0.000442, l5: 0.000755, l6: 0.001372
[epoch: 795/1000, batch:  1032/ 1052, ite: 209080] train loss: 0.003947, tar: 0.000051 
[Epoch 795/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000023, l1: 0.000023, l2: 0.000034, l3: 0.000082, l4: 0.000275, l5: 0.001467, l6: 0.002165
[epoch: 796/1000, batch:    60/ 1052, ite: 209100] train loss: 0.003947, tar: 0.000051 
l0: 0.000010, l1: 0.000010, l2: 0.000017, l3: 0.000068, l4: 0.000146, l5: 0.000799, l6: 0.001138
[epoch: 796/1000, batch:   140/ 1052, ite: 209120] train loss: 0.003947, tar: 0.000051 
l0: 0.000118, l1: 0.000116, l2: 0.000181, l3: 0.000489, l4: 0.000935, l5: 0.002444, l6: 0.003858
[epoch: 796/1000, batch:   220/ 1052, ite: 209140] train loss: 0.003948, tar: 0.000051 
l0: 0.000034, l1: 0.000033, l2: 0.000068, l3: 0.000264, l4: 0.000571, l5: 0.001135, l6: 0.002197
[epoch: 796/1000, batch:   300/ 1052, ite: 209160] train loss: 0.003947, tar: 0.000051 
l0: 0.000050, l1: 0.000051, l2: 0.000097, l3: 0.000235, l4: 0.000437, l5: 0.000814, l6: 0.001516
[epoch: 796/1000, batch:   380/ 1052, ite: 209180] train loss: 0.003947, tar: 0.000051 
l0: 0.000077, l1: 0.000078, l2: 0.000143, l3: 0.000262, l4: 0.000459, l5: 0.002032, l6: 0.002487
[epoch: 796/1000, batch:   460/ 1052, ite: 209200] train loss: 0.003948, tar: 0.000051 
l0: 0.000035, l1: 0.000036, l2: 0.000071, l3: 0.000176, l4: 0.000433, l5: 0.001206, l6: 0.002100
[epoch: 796/1000, batch:   540/ 1052, ite: 209220] train loss: 0.003948, tar: 0.000051 
l0: 0.000085, l1: 0.000084, l2: 0.000113, l3: 0.000124, l4: 0.000418, l5: 0.000585, l6: 0.001828
[epoch: 796/1000, batch:   620/ 1052, ite: 209240] train loss: 0.003947, tar: 0.000051 
l0: 0.000013, l1: 0.000013, l2: 0.000063, l3: 0.000145, l4: 0.000275, l5: 0.001018, l6: 0.002926
[epoch: 796/1000, batch:   700/ 1052, ite: 209260] train loss: 0.003947, tar: 0.000051 
l0: 0.000018, l1: 0.000017, l2: 0.000034, l3: 0.000095, l4: 0.000253, l5: 0.000569, l6: 0.001114
[epoch: 796/1000, batch:   780/ 1052, ite: 209280] train loss: 0.003946, tar: 0.000051 
l0: 0.000024, l1: 0.000024, l2: 0.000112, l3: 0.000161, l4: 0.000324, l5: 0.001176, l6: 0.002442
[epoch: 796/1000, batch:   860/ 1052, ite: 209300] train loss: 0.003947, tar: 0.000051 
l0: 0.000060, l1: 0.000060, l2: 0.000079, l3: 0.000194, l4: 0.000406, l5: 0.001026, l6: 0.001729
[epoch: 796/1000, batch:   940/ 1052, ite: 209320] train loss: 0.003947, tar: 0.000051 
l0: 0.000075, l1: 0.000075, l2: 0.000109, l3: 0.000211, l4: 0.000553, l5: 0.001246, l6: 0.002207
[epoch: 796/1000, batch:  1020/ 1052, ite: 209340] train loss: 0.003947, tar: 0.000051 
[Epoch 796/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000005, l1: 0.000004, l2: 0.000019, l3: 0.000112, l4: 0.000216, l5: 0.000513, l6: 0.001223
[epoch: 797/1000, batch:    48/ 1052, ite: 209360] train loss: 0.003946, tar: 0.000051 
l0: 0.000019, l1: 0.000019, l2: 0.000053, l3: 0.000222, l4: 0.000463, l5: 0.000809, l6: 0.002185
[epoch: 797/1000, batch:   128/ 1052, ite: 209380] train loss: 0.003947, tar: 0.000051 
l0: 0.000031, l1: 0.000030, l2: 0.000051, l3: 0.000085, l4: 0.000248, l5: 0.000567, l6: 0.001617
[epoch: 797/1000, batch:   208/ 1052, ite: 209400] train loss: 0.003946, tar: 0.000051 
l0: 0.000046, l1: 0.000045, l2: 0.000084, l3: 0.000212, l4: 0.000378, l5: 0.000692, l6: 0.002284
[epoch: 797/1000, batch:   288/ 1052, ite: 209420] train loss: 0.003945, tar: 0.000051 
l0: 0.000082, l1: 0.000081, l2: 0.000122, l3: 0.000316, l4: 0.000639, l5: 0.001026, l6: 0.002629
[epoch: 797/1000, batch:   368/ 1052, ite: 209440] train loss: 0.003945, tar: 0.000051 
l0: 0.000029, l1: 0.000030, l2: 0.000074, l3: 0.000187, l4: 0.000409, l5: 0.000673, l6: 0.001567
[epoch: 797/1000, batch:   448/ 1052, ite: 209460] train loss: 0.003945, tar: 0.000051 
l0: 0.000019, l1: 0.000019, l2: 0.000066, l3: 0.000209, l4: 0.000496, l5: 0.001493, l6: 0.002771
[epoch: 797/1000, batch:   528/ 1052, ite: 209480] train loss: 0.003946, tar: 0.000051 
l0: 0.000015, l1: 0.000014, l2: 0.000061, l3: 0.000171, l4: 0.000507, l5: 0.000683, l6: 0.001993
[epoch: 797/1000, batch:   608/ 1052, ite: 209500] train loss: 0.003946, tar: 0.000051 
l0: 0.000016, l1: 0.000016, l2: 0.000041, l3: 0.000156, l4: 0.000392, l5: 0.000908, l6: 0.001678
[epoch: 797/1000, batch:   688/ 1052, ite: 209520] train loss: 0.003945, tar: 0.000051 
l0: 0.000034, l1: 0.000036, l2: 0.000069, l3: 0.000185, l4: 0.000448, l5: 0.001032, l6: 0.001525
[epoch: 797/1000, batch:   768/ 1052, ite: 209540] train loss: 0.003944, tar: 0.000051 
l0: 0.000062, l1: 0.000063, l2: 0.000084, l3: 0.000218, l4: 0.000715, l5: 0.002967, l6: 0.004406
[epoch: 797/1000, batch:   848/ 1052, ite: 209560] train loss: 0.003945, tar: 0.000051 
l0: 0.000083, l1: 0.000083, l2: 0.000114, l3: 0.000214, l4: 0.000500, l5: 0.001020, l6: 0.001589
[epoch: 797/1000, batch:   928/ 1052, ite: 209580] train loss: 0.003946, tar: 0.000051 
l0: 0.000088, l1: 0.000088, l2: 0.000114, l3: 0.000265, l4: 0.000547, l5: 0.001280, l6: 0.002599
[epoch: 797/1000, batch:  1008/ 1052, ite: 209600] train loss: 0.003946, tar: 0.000051 
[Epoch 797/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000025, l1: 0.000025, l2: 0.000063, l3: 0.000251, l4: 0.000840, l5: 0.001293, l6: 0.002554
[epoch: 798/1000, batch:    36/ 1052, ite: 209620] train loss: 0.003947, tar: 0.000051 
l0: 0.000136, l1: 0.000140, l2: 0.000150, l3: 0.000211, l4: 0.000566, l5: 0.000967, l6: 0.002269
[epoch: 798/1000, batch:   116/ 1052, ite: 209640] train loss: 0.003947, tar: 0.000051 
l0: 0.000120, l1: 0.000121, l2: 0.000178, l3: 0.000260, l4: 0.000682, l5: 0.001471, l6: 0.002198
[epoch: 798/1000, batch:   196/ 1052, ite: 209660] train loss: 0.003947, tar: 0.000051 
l0: 0.000226, l1: 0.000233, l2: 0.000262, l3: 0.000340, l4: 0.000706, l5: 0.001282, l6: 0.002385
[epoch: 798/1000, batch:   276/ 1052, ite: 209680] train loss: 0.003948, tar: 0.000051 
l0: 0.000004, l1: 0.000004, l2: 0.000012, l3: 0.000044, l4: 0.000240, l5: 0.000310, l6: 0.000953
[epoch: 798/1000, batch:   356/ 1052, ite: 209700] train loss: 0.003948, tar: 0.000051 
l0: 0.000086, l1: 0.000086, l2: 0.000128, l3: 0.000320, l4: 0.000465, l5: 0.001186, l6: 0.003969
[epoch: 798/1000, batch:   436/ 1052, ite: 209720] train loss: 0.003949, tar: 0.000051 
l0: 0.000019, l1: 0.000017, l2: 0.000054, l3: 0.000178, l4: 0.000430, l5: 0.001187, l6: 0.002117
[epoch: 798/1000, batch:   516/ 1052, ite: 209740] train loss: 0.003949, tar: 0.000051 
l0: 0.000041, l1: 0.000040, l2: 0.000081, l3: 0.000205, l4: 0.000522, l5: 0.000973, l6: 0.001662
[epoch: 798/1000, batch:   596/ 1052, ite: 209760] train loss: 0.003949, tar: 0.000051 
l0: 0.000057, l1: 0.000058, l2: 0.000112, l3: 0.000241, l4: 0.000417, l5: 0.001115, l6: 0.002440
[epoch: 798/1000, batch:   676/ 1052, ite: 209780] train loss: 0.003949, tar: 0.000051 
l0: 0.000016, l1: 0.000017, l2: 0.000040, l3: 0.000129, l4: 0.000451, l5: 0.000733, l6: 0.001797
[epoch: 798/1000, batch:   756/ 1052, ite: 209800] train loss: 0.003949, tar: 0.000051 
l0: 0.000017, l1: 0.000017, l2: 0.000036, l3: 0.000136, l4: 0.000417, l5: 0.001321, l6: 0.001466
[epoch: 798/1000, batch:   836/ 1052, ite: 209820] train loss: 0.003948, tar: 0.000051 
l0: 0.000039, l1: 0.000039, l2: 0.000066, l3: 0.000187, l4: 0.000673, l5: 0.001015, l6: 0.002247
[epoch: 798/1000, batch:   916/ 1052, ite: 209840] train loss: 0.003947, tar: 0.000051 
l0: 0.000010, l1: 0.000008, l2: 0.000065, l3: 0.000154, l4: 0.000455, l5: 0.001210, l6: 0.001828
[epoch: 798/1000, batch:   996/ 1052, ite: 209860] train loss: 0.003947, tar: 0.000051 
[Epoch 798/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000018, l1: 0.000018, l2: 0.000028, l3: 0.000104, l4: 0.000337, l5: 0.000483, l6: 0.000678
[epoch: 799/1000, batch:    24/ 1052, ite: 209880] train loss: 0.003947, tar: 0.000051 
l0: 0.000028, l1: 0.000028, l2: 0.000038, l3: 0.000138, l4: 0.000221, l5: 0.000663, l6: 0.001098
[epoch: 799/1000, batch:   104/ 1052, ite: 209900] train loss: 0.003946, tar: 0.000051 
l0: 0.000039, l1: 0.000038, l2: 0.000040, l3: 0.000137, l4: 0.000234, l5: 0.000753, l6: 0.001221
[epoch: 799/1000, batch:   184/ 1052, ite: 209920] train loss: 0.003946, tar: 0.000051 
l0: 0.000020, l1: 0.000019, l2: 0.000051, l3: 0.000148, l4: 0.000337, l5: 0.000362, l6: 0.001561
[epoch: 799/1000, batch:   264/ 1052, ite: 209940] train loss: 0.003947, tar: 0.000051 
l0: 0.000058, l1: 0.000058, l2: 0.000073, l3: 0.000127, l4: 0.000301, l5: 0.001142, l6: 0.001052
[epoch: 799/1000, batch:   344/ 1052, ite: 209960] train loss: 0.003947, tar: 0.000051 
l0: 0.000070, l1: 0.000071, l2: 0.000089, l3: 0.000140, l4: 0.000603, l5: 0.000978, l6: 0.001884
[epoch: 799/1000, batch:   424/ 1052, ite: 209980] train loss: 0.003946, tar: 0.000051 
l0: 0.000006, l1: 0.000005, l2: 0.000040, l3: 0.000097, l4: 0.000441, l5: 0.000965, l6: 0.001271
[epoch: 799/1000, batch:   504/ 1052, ite: 210000] train loss: 0.003947, tar: 0.000051 
l0: 0.000016, l1: 0.000015, l2: 0.000045, l3: 0.000100, l4: 0.000228, l5: 0.000958, l6: 0.002478
[epoch: 799/1000, batch:   584/ 1052, ite: 210020] train loss: 0.003947, tar: 0.000051 
l0: 0.000020, l1: 0.000022, l2: 0.000030, l3: 0.000052, l4: 0.000375, l5: 0.000601, l6: 0.001384
[epoch: 799/1000, batch:   664/ 1052, ite: 210040] train loss: 0.003946, tar: 0.000051 
l0: 0.000077, l1: 0.000078, l2: 0.000084, l3: 0.000196, l4: 0.000391, l5: 0.000582, l6: 0.001047
[epoch: 799/1000, batch:   744/ 1052, ite: 210060] train loss: 0.003947, tar: 0.000051 
l0: 0.000021, l1: 0.000020, l2: 0.000053, l3: 0.000262, l4: 0.000671, l5: 0.001117, l6: 0.001754
[epoch: 799/1000, batch:   824/ 1052, ite: 210080] train loss: 0.003947, tar: 0.000051 
l0: 0.000015, l1: 0.000015, l2: 0.000083, l3: 0.000235, l4: 0.000454, l5: 0.001083, l6: 0.002124
[epoch: 799/1000, batch:   904/ 1052, ite: 210100] train loss: 0.003947, tar: 0.000051 
l0: 0.000044, l1: 0.000043, l2: 0.000074, l3: 0.000131, l4: 0.000337, l5: 0.000947, l6: 0.001461
[epoch: 799/1000, batch:   984/ 1052, ite: 210120] train loss: 0.003946, tar: 0.000050 
[Epoch 799/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000012, l1: 0.000012, l2: 0.000046, l3: 0.000176, l4: 0.000437, l5: 0.000792, l6: 0.002162
[epoch: 800/1000, batch:    12/ 1052, ite: 210140] train loss: 0.003947, tar: 0.000051 
l0: 0.000005, l1: 0.000005, l2: 0.000032, l3: 0.000078, l4: 0.000457, l5: 0.000860, l6: 0.001133
[epoch: 800/1000, batch:    92/ 1052, ite: 210160] train loss: 0.003946, tar: 0.000051 
l0: 0.000008, l1: 0.000009, l2: 0.000035, l3: 0.000114, l4: 0.000349, l5: 0.001012, l6: 0.001196
[epoch: 800/1000, batch:   172/ 1052, ite: 210180] train loss: 0.003946, tar: 0.000050 
l0: 0.000002, l1: 0.000002, l2: 0.000020, l3: 0.000101, l4: 0.000433, l5: 0.001541, l6: 0.001490
[epoch: 800/1000, batch:   252/ 1052, ite: 210200] train loss: 0.003947, tar: 0.000051 
l0: 0.000011, l1: 0.000011, l2: 0.000022, l3: 0.000084, l4: 0.000217, l5: 0.000772, l6: 0.001646
[epoch: 800/1000, batch:   332/ 1052, ite: 210220] train loss: 0.003946, tar: 0.000051 
l0: 0.000004, l1: 0.000004, l2: 0.000012, l3: 0.000056, l4: 0.000194, l5: 0.001108, l6: 0.001399
[epoch: 800/1000, batch:   412/ 1052, ite: 210240] train loss: 0.003945, tar: 0.000050 
l0: 0.000020, l1: 0.000021, l2: 0.000024, l3: 0.000082, l4: 0.000343, l5: 0.000800, l6: 0.001002
[epoch: 800/1000, batch:   492/ 1052, ite: 210260] train loss: 0.003945, tar: 0.000050 
l0: 0.000025, l1: 0.000024, l2: 0.000076, l3: 0.000147, l4: 0.000497, l5: 0.001053, l6: 0.001893
[epoch: 800/1000, batch:   572/ 1052, ite: 210280] train loss: 0.003944, tar: 0.000050 
l0: 0.000030, l1: 0.000029, l2: 0.000064, l3: 0.000124, l4: 0.000411, l5: 0.001166, l6: 0.002676
[epoch: 800/1000, batch:   652/ 1052, ite: 210300] train loss: 0.003944, tar: 0.000050 
l0: 0.000011, l1: 0.000008, l2: 0.000017, l3: 0.000059, l4: 0.000205, l5: 0.000525, l6: 0.000955
[epoch: 800/1000, batch:   732/ 1052, ite: 210320] train loss: 0.003943, tar: 0.000050 
l0: 0.000029, l1: 0.000030, l2: 0.000078, l3: 0.000142, l4: 0.000502, l5: 0.000939, l6: 0.002597
[epoch: 800/1000, batch:   812/ 1052, ite: 210340] train loss: 0.003944, tar: 0.000050 
l0: 0.000020, l1: 0.000020, l2: 0.000039, l3: 0.000094, l4: 0.000262, l5: 0.000835, l6: 0.000892
[epoch: 800/1000, batch:   892/ 1052, ite: 210360] train loss: 0.003944, tar: 0.000050 
l0: 0.000117, l1: 0.000119, l2: 0.000152, l3: 0.000236, l4: 0.000423, l5: 0.001454, l6: 0.002155
[epoch: 800/1000, batch:   972/ 1052, ite: 210380] train loss: 0.003945, tar: 0.000050 
l0: 0.000013, l1: 0.000012, l2: 0.000043, l3: 0.000127, l4: 0.000446, l5: 0.001266, l6: 0.002859
[epoch: 800/1000, batch:  1052/ 1052, ite: 210400] train loss: 0.003945, tar: 0.000050 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_210400.pkl
[Epoch 800/1000] Test loss: 0.016, Test target loss: 0.002
l0: 0.000011, l1: 0.000010, l2: 0.000022, l3: 0.000137, l4: 0.000485, l5: 0.000794, l6: 0.002025
[epoch: 801/1000, batch:    80/ 1052, ite: 210420] train loss: 0.003592, tar: 0.000045 
l0: 0.000119, l1: 0.000119, l2: 0.000155, l3: 0.000260, l4: 0.000726, l5: 0.001104, l6: 0.002144
[epoch: 801/1000, batch:   160/ 1052, ite: 210440] train loss: 0.003673, tar: 0.000045 
l0: 0.000009, l1: 0.000008, l2: 0.000062, l3: 0.000143, l4: 0.000300, l5: 0.001177, l6: 0.001355
[epoch: 801/1000, batch:   240/ 1052, ite: 210460] train loss: 0.003830, tar: 0.000047 
l0: 0.000107, l1: 0.000105, l2: 0.000146, l3: 0.000268, l4: 0.000604, l5: 0.001323, l6: 0.002949
[epoch: 801/1000, batch:   320/ 1052, ite: 210480] train loss: 0.003828, tar: 0.000046 
l0: 0.000007, l1: 0.000006, l2: 0.000041, l3: 0.000121, l4: 0.000251, l5: 0.001110, l6: 0.001740
[epoch: 801/1000, batch:   400/ 1052, ite: 210500] train loss: 0.003971, tar: 0.000048 
l0: 0.000041, l1: 0.000041, l2: 0.000070, l3: 0.000163, l4: 0.000403, l5: 0.000634, l6: 0.002250
[epoch: 801/1000, batch:   480/ 1052, ite: 210520] train loss: 0.003922, tar: 0.000048 
l0: 0.000008, l1: 0.000008, l2: 0.000035, l3: 0.000222, l4: 0.000445, l5: 0.000922, l6: 0.002319
[epoch: 801/1000, batch:   560/ 1052, ite: 210540] train loss: 0.003957, tar: 0.000047 
l0: 0.000025, l1: 0.000025, l2: 0.000056, l3: 0.000123, l4: 0.000370, l5: 0.000666, l6: 0.002130
[epoch: 801/1000, batch:   640/ 1052, ite: 210560] train loss: 0.003967, tar: 0.000048 
l0: 0.000098, l1: 0.000099, l2: 0.000118, l3: 0.000209, l4: 0.000432, l5: 0.001096, l6: 0.002539
[epoch: 801/1000, batch:   720/ 1052, ite: 210580] train loss: 0.003951, tar: 0.000047 
l0: 0.000053, l1: 0.000054, l2: 0.000065, l3: 0.000202, l4: 0.000531, l5: 0.000853, l6: 0.001453
[epoch: 801/1000, batch:   800/ 1052, ite: 210600] train loss: 0.003957, tar: 0.000047 
l0: 0.000023, l1: 0.000023, l2: 0.000060, l3: 0.000130, l4: 0.000366, l5: 0.001061, l6: 0.001593
[epoch: 801/1000, batch:   880/ 1052, ite: 210620] train loss: 0.003936, tar: 0.000047 
l0: 0.000021, l1: 0.000022, l2: 0.000028, l3: 0.000059, l4: 0.000178, l5: 0.000604, l6: 0.001172
[epoch: 801/1000, batch:   960/ 1052, ite: 210640] train loss: 0.003914, tar: 0.000046 
l0: 0.000137, l1: 0.000137, l2: 0.000190, l3: 0.000379, l4: 0.000724, l5: 0.001853, l6: 0.002722
[epoch: 801/1000, batch:  1040/ 1052, ite: 210660] train loss: 0.003951, tar: 0.000047 
[Epoch 801/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000057, l1: 0.000055, l2: 0.000095, l3: 0.000188, l4: 0.000396, l5: 0.000879, l6: 0.001621
[epoch: 802/1000, batch:    68/ 1052, ite: 210680] train loss: 0.003919, tar: 0.000047 
l0: 0.000027, l1: 0.000028, l2: 0.000031, l3: 0.000089, l4: 0.000641, l5: 0.000669, l6: 0.001700
[epoch: 802/1000, batch:   148/ 1052, ite: 210700] train loss: 0.003916, tar: 0.000046 
l0: 0.000053, l1: 0.000055, l2: 0.000063, l3: 0.000108, l4: 0.000190, l5: 0.000718, l6: 0.001129
[epoch: 802/1000, batch:   228/ 1052, ite: 210720] train loss: 0.003887, tar: 0.000047 
l0: 0.000003, l1: 0.000004, l2: 0.000015, l3: 0.000092, l4: 0.000372, l5: 0.001486, l6: 0.001225
[epoch: 802/1000, batch:   308/ 1052, ite: 210740] train loss: 0.003911, tar: 0.000046 
l0: 0.000074, l1: 0.000074, l2: 0.000105, l3: 0.000149, l4: 0.000273, l5: 0.000753, l6: 0.001316
[epoch: 802/1000, batch:   388/ 1052, ite: 210760] train loss: 0.003917, tar: 0.000047 
l0: 0.000024, l1: 0.000023, l2: 0.000073, l3: 0.000232, l4: 0.000572, l5: 0.001217, l6: 0.002156
[epoch: 802/1000, batch:   468/ 1052, ite: 210780] train loss: 0.003908, tar: 0.000046 
l0: 0.000049, l1: 0.000052, l2: 0.000129, l3: 0.000366, l4: 0.001008, l5: 0.001557, l6: 0.002191
[epoch: 802/1000, batch:   548/ 1052, ite: 210800] train loss: 0.003920, tar: 0.000047 
l0: 0.000038, l1: 0.000039, l2: 0.000102, l3: 0.000213, l4: 0.000483, l5: 0.001340, l6: 0.001964
[epoch: 802/1000, batch:   628/ 1052, ite: 210820] train loss: 0.003925, tar: 0.000048 
l0: 0.000036, l1: 0.000035, l2: 0.000062, l3: 0.000164, l4: 0.000399, l5: 0.000799, l6: 0.001881
[epoch: 802/1000, batch:   708/ 1052, ite: 210840] train loss: 0.003933, tar: 0.000048 
l0: 0.000007, l1: 0.000006, l2: 0.000044, l3: 0.000162, l4: 0.000330, l5: 0.001906, l6: 0.003225
[epoch: 802/1000, batch:   788/ 1052, ite: 210860] train loss: 0.003931, tar: 0.000047 
l0: 0.000046, l1: 0.000049, l2: 0.000040, l3: 0.000122, l4: 0.000369, l5: 0.000947, l6: 0.002407
[epoch: 802/1000, batch:   868/ 1052, ite: 210880] train loss: 0.003919, tar: 0.000047 
l0: 0.000007, l1: 0.000007, l2: 0.000019, l3: 0.000095, l4: 0.000341, l5: 0.000861, l6: 0.001451
[epoch: 802/1000, batch:   948/ 1052, ite: 210900] train loss: 0.003935, tar: 0.000048 
l0: 0.000030, l1: 0.000032, l2: 0.000045, l3: 0.000098, l4: 0.000341, l5: 0.000755, l6: 0.000876
[epoch: 802/1000, batch:  1028/ 1052, ite: 210920] train loss: 0.003921, tar: 0.000047 
[Epoch 802/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000005, l1: 0.000004, l2: 0.000046, l3: 0.000350, l4: 0.000749, l5: 0.001562, l6: 0.002733
[epoch: 803/1000, batch:    56/ 1052, ite: 210940] train loss: 0.003905, tar: 0.000047 
l0: 0.000027, l1: 0.000028, l2: 0.000049, l3: 0.000130, l4: 0.000615, l5: 0.001049, l6: 0.001766
[epoch: 803/1000, batch:   136/ 1052, ite: 210960] train loss: 0.003909, tar: 0.000047 
l0: 0.000066, l1: 0.000067, l2: 0.000087, l3: 0.000208, l4: 0.000416, l5: 0.000894, l6: 0.001370
[epoch: 803/1000, batch:   216/ 1052, ite: 210980] train loss: 0.003921, tar: 0.000048 
l0: 0.000043, l1: 0.000045, l2: 0.000072, l3: 0.000174, l4: 0.000548, l5: 0.001295, l6: 0.002908
[epoch: 803/1000, batch:   296/ 1052, ite: 211000] train loss: 0.003916, tar: 0.000048 
l0: 0.000030, l1: 0.000031, l2: 0.000041, l3: 0.000135, l4: 0.000362, l5: 0.000776, l6: 0.001775
[epoch: 803/1000, batch:   376/ 1052, ite: 211020] train loss: 0.003929, tar: 0.000048 
l0: 0.000003, l1: 0.000004, l2: 0.000014, l3: 0.000074, l4: 0.000263, l5: 0.000485, l6: 0.000613
[epoch: 803/1000, batch:   456/ 1052, ite: 211040] train loss: 0.003928, tar: 0.000048 
l0: 0.000008, l1: 0.000007, l2: 0.000036, l3: 0.000064, l4: 0.000190, l5: 0.000468, l6: 0.001392
[epoch: 803/1000, batch:   536/ 1052, ite: 211060] train loss: 0.003925, tar: 0.000048 
l0: 0.000034, l1: 0.000036, l2: 0.000051, l3: 0.000116, l4: 0.000277, l5: 0.000830, l6: 0.002657
[epoch: 803/1000, batch:   616/ 1052, ite: 211080] train loss: 0.003921, tar: 0.000048 
l0: 0.000080, l1: 0.000083, l2: 0.000127, l3: 0.000193, l4: 0.000761, l5: 0.001389, l6: 0.001696
[epoch: 803/1000, batch:   696/ 1052, ite: 211100] train loss: 0.003932, tar: 0.000048 
l0: 0.000070, l1: 0.000072, l2: 0.000121, l3: 0.000170, l4: 0.000426, l5: 0.001062, l6: 0.002176
[epoch: 803/1000, batch:   776/ 1052, ite: 211120] train loss: 0.003921, tar: 0.000048 
l0: 0.000011, l1: 0.000010, l2: 0.000038, l3: 0.000099, l4: 0.000452, l5: 0.000946, l6: 0.001159
[epoch: 803/1000, batch:   856/ 1052, ite: 211140] train loss: 0.003920, tar: 0.000049 
l0: 0.000041, l1: 0.000042, l2: 0.000066, l3: 0.000131, l4: 0.000353, l5: 0.000447, l6: 0.001520
[epoch: 803/1000, batch:   936/ 1052, ite: 211160] train loss: 0.003919, tar: 0.000048 
l0: 0.000007, l1: 0.000007, l2: 0.000022, l3: 0.000133, l4: 0.000369, l5: 0.001053, l6: 0.001707
[epoch: 803/1000, batch:  1016/ 1052, ite: 211180] train loss: 0.003917, tar: 0.000048 
[Epoch 803/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000024, l1: 0.000025, l2: 0.000047, l3: 0.000134, l4: 0.000310, l5: 0.000912, l6: 0.001988
[epoch: 804/1000, batch:    44/ 1052, ite: 211200] train loss: 0.003916, tar: 0.000048 
l0: 0.000040, l1: 0.000042, l2: 0.000046, l3: 0.000121, l4: 0.000446, l5: 0.001057, l6: 0.001856
[epoch: 804/1000, batch:   124/ 1052, ite: 211220] train loss: 0.003904, tar: 0.000048 
l0: 0.000135, l1: 0.000137, l2: 0.000216, l3: 0.000419, l4: 0.001024, l5: 0.001954, l6: 0.002866
[epoch: 804/1000, batch:   204/ 1052, ite: 211240] train loss: 0.003911, tar: 0.000048 
l0: 0.000006, l1: 0.000007, l2: 0.000044, l3: 0.000323, l4: 0.000607, l5: 0.001482, l6: 0.001452
[epoch: 804/1000, batch:   284/ 1052, ite: 211260] train loss: 0.003918, tar: 0.000048 
l0: 0.000021, l1: 0.000019, l2: 0.000058, l3: 0.000129, l4: 0.000313, l5: 0.000806, l6: 0.001949
[epoch: 804/1000, batch:   364/ 1052, ite: 211280] train loss: 0.003916, tar: 0.000048 
l0: 0.000006, l1: 0.000006, l2: 0.000016, l3: 0.000074, l4: 0.000205, l5: 0.000800, l6: 0.001120
[epoch: 804/1000, batch:   444/ 1052, ite: 211300] train loss: 0.003919, tar: 0.000047 
l0: 0.000021, l1: 0.000022, l2: 0.000057, l3: 0.000166, l4: 0.000561, l5: 0.001114, l6: 0.002910
[epoch: 804/1000, batch:   524/ 1052, ite: 211320] train loss: 0.003910, tar: 0.000047 
l0: 0.000215, l1: 0.000222, l2: 0.000231, l3: 0.000427, l4: 0.000656, l5: 0.001374, l6: 0.002928
[epoch: 804/1000, batch:   604/ 1052, ite: 211340] train loss: 0.003916, tar: 0.000047 
l0: 0.000012, l1: 0.000011, l2: 0.000055, l3: 0.000122, l4: 0.000421, l5: 0.001621, l6: 0.001897
[epoch: 804/1000, batch:   684/ 1052, ite: 211360] train loss: 0.003928, tar: 0.000048 
l0: 0.000049, l1: 0.000050, l2: 0.000064, l3: 0.000088, l4: 0.000250, l5: 0.000605, l6: 0.000975
[epoch: 804/1000, batch:   764/ 1052, ite: 211380] train loss: 0.003922, tar: 0.000048 
l0: 0.000017, l1: 0.000017, l2: 0.000055, l3: 0.000134, l4: 0.000331, l5: 0.001025, l6: 0.001549
[epoch: 804/1000, batch:   844/ 1052, ite: 211400] train loss: 0.003913, tar: 0.000047 
l0: 0.000066, l1: 0.000066, l2: 0.000084, l3: 0.000251, l4: 0.000597, l5: 0.001213, l6: 0.003157
[epoch: 804/1000, batch:   924/ 1052, ite: 211420] train loss: 0.003911, tar: 0.000048 
l0: 0.000005, l1: 0.000005, l2: 0.000024, l3: 0.000132, l4: 0.000448, l5: 0.001236, l6: 0.001709
[epoch: 804/1000, batch:  1004/ 1052, ite: 211440] train loss: 0.003902, tar: 0.000048 
[Epoch 804/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000068, l1: 0.000069, l2: 0.000110, l3: 0.000236, l4: 0.000545, l5: 0.002052, l6: 0.003037
[epoch: 805/1000, batch:    32/ 1052, ite: 211460] train loss: 0.003902, tar: 0.000048 
l0: 0.000025, l1: 0.000025, l2: 0.000042, l3: 0.000151, l4: 0.000426, l5: 0.000866, l6: 0.001789
[epoch: 805/1000, batch:   112/ 1052, ite: 211480] train loss: 0.003902, tar: 0.000048 
l0: 0.000101, l1: 0.000104, l2: 0.000121, l3: 0.000255, l4: 0.000636, l5: 0.000940, l6: 0.002172
[epoch: 805/1000, batch:   192/ 1052, ite: 211500] train loss: 0.003903, tar: 0.000048 
l0: 0.000028, l1: 0.000028, l2: 0.000052, l3: 0.000115, l4: 0.000214, l5: 0.000620, l6: 0.000997
[epoch: 805/1000, batch:   272/ 1052, ite: 211520] train loss: 0.003903, tar: 0.000048 
l0: 0.000004, l1: 0.000003, l2: 0.000018, l3: 0.000172, l4: 0.000360, l5: 0.001124, l6: 0.001733
[epoch: 805/1000, batch:   352/ 1052, ite: 211540] train loss: 0.003903, tar: 0.000047 
l0: 0.000069, l1: 0.000070, l2: 0.000117, l3: 0.000306, l4: 0.000618, l5: 0.001118, l6: 0.001767
[epoch: 805/1000, batch:   432/ 1052, ite: 211560] train loss: 0.003904, tar: 0.000047 
l0: 0.000007, l1: 0.000008, l2: 0.000024, l3: 0.000099, l4: 0.000264, l5: 0.001035, l6: 0.001158
[epoch: 805/1000, batch:   512/ 1052, ite: 211580] train loss: 0.003903, tar: 0.000047 
l0: 0.000030, l1: 0.000032, l2: 0.000051, l3: 0.000151, l4: 0.000578, l5: 0.001133, l6: 0.002164
[epoch: 805/1000, batch:   592/ 1052, ite: 211600] train loss: 0.003899, tar: 0.000047 
l0: 0.000059, l1: 0.000059, l2: 0.000070, l3: 0.000160, l4: 0.000249, l5: 0.000804, l6: 0.001289
[epoch: 805/1000, batch:   672/ 1052, ite: 211620] train loss: 0.003896, tar: 0.000047 
l0: 0.000136, l1: 0.000137, l2: 0.000155, l3: 0.000190, l4: 0.000305, l5: 0.000709, l6: 0.001453
[epoch: 805/1000, batch:   752/ 1052, ite: 211640] train loss: 0.003897, tar: 0.000047 
l0: 0.000097, l1: 0.000098, l2: 0.000126, l3: 0.000221, l4: 0.000556, l5: 0.000822, l6: 0.002537
[epoch: 805/1000, batch:   832/ 1052, ite: 211660] train loss: 0.003907, tar: 0.000048 
l0: 0.000039, l1: 0.000040, l2: 0.000080, l3: 0.000157, l4: 0.000448, l5: 0.000780, l6: 0.002025
[epoch: 805/1000, batch:   912/ 1052, ite: 211680] train loss: 0.003906, tar: 0.000048 
l0: 0.000105, l1: 0.000106, l2: 0.000145, l3: 0.000320, l4: 0.000576, l5: 0.001252, l6: 0.001802
[epoch: 805/1000, batch:   992/ 1052, ite: 211700] train loss: 0.003906, tar: 0.000048 
[Epoch 805/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000015, l1: 0.000014, l2: 0.000050, l3: 0.000149, l4: 0.000195, l5: 0.000966, l6: 0.001742
[epoch: 806/1000, batch:    20/ 1052, ite: 211720] train loss: 0.003907, tar: 0.000048 
l0: 0.000016, l1: 0.000015, l2: 0.000069, l3: 0.000226, l4: 0.000584, l5: 0.001897, l6: 0.002846
[epoch: 806/1000, batch:   100/ 1052, ite: 211740] train loss: 0.003908, tar: 0.000047 
l0: 0.000073, l1: 0.000074, l2: 0.000110, l3: 0.000224, l4: 0.000600, l5: 0.001392, l6: 0.002863
[epoch: 806/1000, batch:   180/ 1052, ite: 211760] train loss: 0.003902, tar: 0.000047 
l0: 0.000085, l1: 0.000086, l2: 0.000109, l3: 0.000271, l4: 0.000528, l5: 0.000965, l6: 0.001075
[epoch: 806/1000, batch:   260/ 1052, ite: 211780] train loss: 0.003907, tar: 0.000047 
l0: 0.000084, l1: 0.000084, l2: 0.000127, l3: 0.000278, l4: 0.000888, l5: 0.001499, l6: 0.002399
[epoch: 806/1000, batch:   340/ 1052, ite: 211800] train loss: 0.003902, tar: 0.000047 
l0: 0.000064, l1: 0.000067, l2: 0.000117, l3: 0.000298, l4: 0.000606, l5: 0.001545, l6: 0.003921
[epoch: 806/1000, batch:   420/ 1052, ite: 211820] train loss: 0.003903, tar: 0.000047 
l0: 0.000030, l1: 0.000030, l2: 0.000067, l3: 0.000102, l4: 0.000448, l5: 0.000826, l6: 0.001461
[epoch: 806/1000, batch:   500/ 1052, ite: 211840] train loss: 0.003903, tar: 0.000047 
l0: 0.000013, l1: 0.000015, l2: 0.000008, l3: 0.000072, l4: 0.000547, l5: 0.000686, l6: 0.001635
[epoch: 806/1000, batch:   580/ 1052, ite: 211860] train loss: 0.003909, tar: 0.000048 
l0: 0.000118, l1: 0.000118, l2: 0.000168, l3: 0.000281, l4: 0.000521, l5: 0.001290, l6: 0.002660
[epoch: 806/1000, batch:   660/ 1052, ite: 211880] train loss: 0.003911, tar: 0.000048 
l0: 0.000109, l1: 0.000116, l2: 0.000139, l3: 0.000330, l4: 0.001116, l5: 0.001576, l6: 0.002778
[epoch: 806/1000, batch:   740/ 1052, ite: 211900] train loss: 0.003917, tar: 0.000048 
l0: 0.000123, l1: 0.000122, l2: 0.000179, l3: 0.000226, l4: 0.000537, l5: 0.001394, l6: 0.002350
[epoch: 806/1000, batch:   820/ 1052, ite: 211920] train loss: 0.003920, tar: 0.000048 
l0: 0.000197, l1: 0.000197, l2: 0.000249, l3: 0.000442, l4: 0.000747, l5: 0.001465, l6: 0.003034
[epoch: 806/1000, batch:   900/ 1052, ite: 211940] train loss: 0.003921, tar: 0.000049 
l0: 0.000051, l1: 0.000052, l2: 0.000079, l3: 0.000200, l4: 0.000324, l5: 0.000745, l6: 0.001209
[epoch: 806/1000, batch:   980/ 1052, ite: 211960] train loss: 0.003921, tar: 0.000049 
[Epoch 806/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000029, l1: 0.000028, l2: 0.000123, l3: 0.000279, l4: 0.000502, l5: 0.001595, l6: 0.003651
[epoch: 807/1000, batch:     8/ 1052, ite: 211980] train loss: 0.003923, tar: 0.000049 
l0: 0.000032, l1: 0.000034, l2: 0.000060, l3: 0.000105, l4: 0.000373, l5: 0.001004, l6: 0.001524
[epoch: 807/1000, batch:    88/ 1052, ite: 212000] train loss: 0.003923, tar: 0.000049 
l0: 0.000031, l1: 0.000030, l2: 0.000057, l3: 0.000142, l4: 0.000299, l5: 0.000698, l6: 0.002467
[epoch: 807/1000, batch:   168/ 1052, ite: 212020] train loss: 0.003922, tar: 0.000049 
l0: 0.000026, l1: 0.000025, l2: 0.000056, l3: 0.000179, l4: 0.000486, l5: 0.001178, l6: 0.001440
[epoch: 807/1000, batch:   248/ 1052, ite: 212040] train loss: 0.003924, tar: 0.000049 
l0: 0.000030, l1: 0.000031, l2: 0.000055, l3: 0.000119, l4: 0.000503, l5: 0.001051, l6: 0.001254
[epoch: 807/1000, batch:   328/ 1052, ite: 212060] train loss: 0.003927, tar: 0.000049 
l0: 0.000050, l1: 0.000049, l2: 0.000079, l3: 0.000198, l4: 0.000554, l5: 0.001222, l6: 0.002361
[epoch: 807/1000, batch:   408/ 1052, ite: 212080] train loss: 0.003924, tar: 0.000049 
l0: 0.000061, l1: 0.000063, l2: 0.000056, l3: 0.000136, l4: 0.000315, l5: 0.000551, l6: 0.001298
[epoch: 807/1000, batch:   488/ 1052, ite: 212100] train loss: 0.003924, tar: 0.000050 
l0: 0.000034, l1: 0.000033, l2: 0.000058, l3: 0.000193, l4: 0.000458, l5: 0.001270, l6: 0.001960
[epoch: 807/1000, batch:   568/ 1052, ite: 212120] train loss: 0.003931, tar: 0.000050 
l0: 0.000008, l1: 0.000008, l2: 0.000030, l3: 0.000117, l4: 0.000376, l5: 0.001238, l6: 0.002341
[epoch: 807/1000, batch:   648/ 1052, ite: 212140] train loss: 0.003926, tar: 0.000050 
l0: 0.000047, l1: 0.000045, l2: 0.000078, l3: 0.000161, l4: 0.000405, l5: 0.000822, l6: 0.001471
[epoch: 807/1000, batch:   728/ 1052, ite: 212160] train loss: 0.003921, tar: 0.000049 
l0: 0.000065, l1: 0.000066, l2: 0.000072, l3: 0.000132, l4: 0.000295, l5: 0.001108, l6: 0.002938
[epoch: 807/1000, batch:   808/ 1052, ite: 212180] train loss: 0.003929, tar: 0.000049 
l0: 0.000008, l1: 0.000007, l2: 0.000027, l3: 0.000118, l4: 0.000581, l5: 0.000822, l6: 0.002432
[epoch: 807/1000, batch:   888/ 1052, ite: 212200] train loss: 0.003928, tar: 0.000049 
l0: 0.000030, l1: 0.000031, l2: 0.000043, l3: 0.000199, l4: 0.000410, l5: 0.000661, l6: 0.001267
[epoch: 807/1000, batch:   968/ 1052, ite: 212220] train loss: 0.003928, tar: 0.000049 
l0: 0.000023, l1: 0.000025, l2: 0.000047, l3: 0.000089, l4: 0.000349, l5: 0.001039, l6: 0.001887
[epoch: 807/1000, batch:  1048/ 1052, ite: 212240] train loss: 0.003927, tar: 0.000049 
[Epoch 807/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000016, l1: 0.000017, l2: 0.000021, l3: 0.000089, l4: 0.000240, l5: 0.000868, l6: 0.001509
[epoch: 808/1000, batch:    76/ 1052, ite: 212260] train loss: 0.003924, tar: 0.000049 
l0: 0.000081, l1: 0.000081, l2: 0.000108, l3: 0.000207, l4: 0.000416, l5: 0.000924, l6: 0.001337
[epoch: 808/1000, batch:   156/ 1052, ite: 212280] train loss: 0.003925, tar: 0.000049 
l0: 0.000108, l1: 0.000108, l2: 0.000206, l3: 0.000301, l4: 0.000577, l5: 0.001100, l6: 0.003184
[epoch: 808/1000, batch:   236/ 1052, ite: 212300] train loss: 0.003926, tar: 0.000049 
l0: 0.000014, l1: 0.000014, l2: 0.000021, l3: 0.000121, l4: 0.000384, l5: 0.001171, l6: 0.001298
[epoch: 808/1000, batch:   316/ 1052, ite: 212320] train loss: 0.003924, tar: 0.000049 
l0: 0.000025, l1: 0.000026, l2: 0.000024, l3: 0.000106, l4: 0.000414, l5: 0.000437, l6: 0.001229
[epoch: 808/1000, batch:   396/ 1052, ite: 212340] train loss: 0.003927, tar: 0.000049 
l0: 0.000075, l1: 0.000078, l2: 0.000107, l3: 0.000235, l4: 0.000566, l5: 0.001196, l6: 0.002475
[epoch: 808/1000, batch:   476/ 1052, ite: 212360] train loss: 0.003922, tar: 0.000049 
l0: 0.000096, l1: 0.000102, l2: 0.000110, l3: 0.000188, l4: 0.000476, l5: 0.001231, l6: 0.001696
[epoch: 808/1000, batch:   556/ 1052, ite: 212380] train loss: 0.003923, tar: 0.000049 
l0: 0.000001, l1: 0.000001, l2: 0.000009, l3: 0.000042, l4: 0.000202, l5: 0.000563, l6: 0.001315
[epoch: 808/1000, batch:   636/ 1052, ite: 212400] train loss: 0.003929, tar: 0.000050 
l0: 0.000258, l1: 0.000271, l2: 0.000335, l3: 0.000351, l4: 0.000661, l5: 0.001046, l6: 0.002204
[epoch: 808/1000, batch:   716/ 1052, ite: 212420] train loss: 0.003941, tar: 0.000052 
l0: 0.000076, l1: 0.000080, l2: 0.000100, l3: 0.000235, l4: 0.000325, l5: 0.001101, l6: 0.002792
[epoch: 808/1000, batch:   796/ 1052, ite: 212440] train loss: 0.003950, tar: 0.000053 
l0: 0.000029, l1: 0.000030, l2: 0.000060, l3: 0.000137, l4: 0.000439, l5: 0.001085, l6: 0.000792
[epoch: 808/1000, batch:   876/ 1052, ite: 212460] train loss: 0.003949, tar: 0.000054 
l0: 0.000015, l1: 0.000014, l2: 0.000049, l3: 0.000121, l4: 0.000412, l5: 0.000543, l6: 0.001561
[epoch: 808/1000, batch:   956/ 1052, ite: 212480] train loss: 0.003956, tar: 0.000055 
l0: 0.000113, l1: 0.000112, l2: 0.000125, l3: 0.000183, l4: 0.000480, l5: 0.001253, l6: 0.002162
[epoch: 808/1000, batch:  1036/ 1052, ite: 212500] train loss: 0.003966, tar: 0.000055 
[Epoch 808/1000] Test loss: 0.017, Test target loss: 0.002
l0: 0.000040, l1: 0.000038, l2: 0.000069, l3: 0.000156, l4: 0.000274, l5: 0.001090, l6: 0.001320
[epoch: 809/1000, batch:    64/ 1052, ite: 212520] train loss: 0.003968, tar: 0.000056 
l0: 0.000037, l1: 0.000038, l2: 0.000046, l3: 0.000106, l4: 0.000396, l5: 0.000775, l6: 0.001676
[epoch: 809/1000, batch:   144/ 1052, ite: 212540] train loss: 0.003969, tar: 0.000056 
l0: 0.000022, l1: 0.000020, l2: 0.000043, l3: 0.000154, l4: 0.000213, l5: 0.001007, l6: 0.001804
[epoch: 809/1000, batch:   224/ 1052, ite: 212560] train loss: 0.003971, tar: 0.000056 
l0: 0.000018, l1: 0.000017, l2: 0.000045, l3: 0.000153, l4: 0.000301, l5: 0.000531, l6: 0.001480
[epoch: 809/1000, batch:   304/ 1052, ite: 212580] train loss: 0.003982, tar: 0.000056 
l0: 0.000143, l1: 0.000157, l2: 0.000290, l3: 0.000498, l4: 0.001124, l5: 0.002866, l6: 0.004363
[epoch: 809/1000, batch:   384/ 1052, ite: 212600] train loss: 0.003984, tar: 0.000057 
l0: 0.000035, l1: 0.000030, l2: 0.000062, l3: 0.000193, l4: 0.000391, l5: 0.000797, l6: 0.001191
[epoch: 809/1000, batch:   464/ 1052, ite: 212620] train loss: 0.003984, tar: 0.000057 
l0: 0.000083, l1: 0.000086, l2: 0.000117, l3: 0.000221, l4: 0.000744, l5: 0.001587, l6: 0.002612
[epoch: 809/1000, batch:   544/ 1052, ite: 212640] train loss: 0.003982, tar: 0.000057 
l0: 0.000046, l1: 0.000048, l2: 0.000082, l3: 0.000149, l4: 0.000394, l5: 0.001145, l6: 0.001939
[epoch: 809/1000, batch:   624/ 1052, ite: 212660] train loss: 0.003982, tar: 0.000057 
l0: 0.000025, l1: 0.000023, l2: 0.000045, l3: 0.000099, l4: 0.000400, l5: 0.001107, l6: 0.001782
[epoch: 809/1000, batch:   704/ 1052, ite: 212680] train loss: 0.003978, tar: 0.000057 
l0: 0.000046, l1: 0.000046, l2: 0.000071, l3: 0.000136, l4: 0.000433, l5: 0.000819, l6: 0.001824
[epoch: 809/1000, batch:   784/ 1052, ite: 212700] train loss: 0.003976, tar: 0.000057 
l0: 0.000205, l1: 0.000207, l2: 0.000291, l3: 0.000589, l4: 0.001152, l5: 0.002309, l6: 0.004243
[epoch: 809/1000, batch:   864/ 1052, ite: 212720] train loss: 0.003975, tar: 0.000057 
l0: 0.000078, l1: 0.000080, l2: 0.000153, l3: 0.000340, l4: 0.000777, l5: 0.001618, l6: 0.002035
[epoch: 809/1000, batch:   944/ 1052, ite: 212740] train loss: 0.003974, tar: 0.000057 
l0: 0.000069, l1: 0.000069, l2: 0.000081, l3: 0.000205, l4: 0.000678, l5: 0.001160, l6: 0.001869
[epoch: 809/1000, batch:  1024/ 1052, ite: 212760] train loss: 0.003974, tar: 0.000057 
[Epoch 809/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000044, l1: 0.000042, l2: 0.000049, l3: 0.000142, l4: 0.000380, l5: 0.000918, l6: 0.001477
[epoch: 810/1000, batch:    52/ 1052, ite: 212780] train loss: 0.003972, tar: 0.000057 
l0: 0.000040, l1: 0.000041, l2: 0.000067, l3: 0.000256, l4: 0.000942, l5: 0.001387, l6: 0.002517
[epoch: 810/1000, batch:   132/ 1052, ite: 212800] train loss: 0.003975, tar: 0.000057 
l0: 0.000042, l1: 0.000042, l2: 0.000056, l3: 0.000084, l4: 0.000196, l5: 0.000706, l6: 0.000918
[epoch: 810/1000, batch:   212/ 1052, ite: 212820] train loss: 0.003978, tar: 0.000057 
l0: 0.000151, l1: 0.000155, l2: 0.000176, l3: 0.000215, l4: 0.000313, l5: 0.001018, l6: 0.001551
[epoch: 810/1000, batch:   292/ 1052, ite: 212840] train loss: 0.003972, tar: 0.000057 
l0: 0.000036, l1: 0.000028, l2: 0.000145, l3: 0.000337, l4: 0.000786, l5: 0.001524, l6: 0.002365
[epoch: 810/1000, batch:   372/ 1052, ite: 212860] train loss: 0.003974, tar: 0.000057 
l0: 0.000014, l1: 0.000016, l2: 0.000019, l3: 0.000059, l4: 0.000290, l5: 0.000430, l6: 0.000863
[epoch: 810/1000, batch:   452/ 1052, ite: 212880] train loss: 0.003972, tar: 0.000057 
l0: 0.000067, l1: 0.000067, l2: 0.000108, l3: 0.000173, l4: 0.000524, l5: 0.001438, l6: 0.002292
[epoch: 810/1000, batch:   532/ 1052, ite: 212900] train loss: 0.003972, tar: 0.000057 
l0: 0.000003, l1: 0.000003, l2: 0.000031, l3: 0.000142, l4: 0.000330, l5: 0.000699, l6: 0.001336
[epoch: 810/1000, batch:   612/ 1052, ite: 212920] train loss: 0.003972, tar: 0.000057 
l0: 0.000032, l1: 0.000030, l2: 0.000119, l3: 0.000334, l4: 0.001010, l5: 0.002256, l6: 0.002627
[epoch: 810/1000, batch:   692/ 1052, ite: 212940] train loss: 0.003971, tar: 0.000057 
l0: 0.000107, l1: 0.000106, l2: 0.000145, l3: 0.000288, l4: 0.000576, l5: 0.001257, l6: 0.001542
[epoch: 810/1000, batch:   772/ 1052, ite: 212960] train loss: 0.003973, tar: 0.000057 
l0: 0.000042, l1: 0.000041, l2: 0.000096, l3: 0.000285, l4: 0.000539, l5: 0.001143, l6: 0.003047
[epoch: 810/1000, batch:   852/ 1052, ite: 212980] train loss: 0.003979, tar: 0.000057 
l0: 0.000027, l1: 0.000025, l2: 0.000040, l3: 0.000103, l4: 0.000308, l5: 0.000715, l6: 0.001479
[epoch: 810/1000, batch:   932/ 1052, ite: 213000] train loss: 0.003981, tar: 0.000057 
l0: 0.000067, l1: 0.000067, l2: 0.000149, l3: 0.000314, l4: 0.000714, l5: 0.001331, l6: 0.003834
[epoch: 810/1000, batch:  1012/ 1052, ite: 213020] train loss: 0.003978, tar: 0.000057 
[Epoch 810/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000001, l1: 0.000001, l2: 0.000006, l3: 0.000060, l4: 0.000370, l5: 0.000677, l6: 0.000875
[epoch: 811/1000, batch:    40/ 1052, ite: 213040] train loss: 0.003977, tar: 0.000056 
l0: 0.000002, l1: 0.000002, l2: 0.000020, l3: 0.000050, l4: 0.000457, l5: 0.000670, l6: 0.001070
[epoch: 811/1000, batch:   120/ 1052, ite: 213060] train loss: 0.003977, tar: 0.000056 
l0: 0.000151, l1: 0.000150, l2: 0.000213, l3: 0.000395, l4: 0.001222, l5: 0.002102, l6: 0.002984
[epoch: 811/1000, batch:   200/ 1052, ite: 213080] train loss: 0.003977, tar: 0.000056 
l0: 0.000008, l1: 0.000007, l2: 0.000029, l3: 0.000087, l4: 0.000377, l5: 0.001092, l6: 0.001440
[epoch: 811/1000, batch:   280/ 1052, ite: 213100] train loss: 0.003979, tar: 0.000056 
l0: 0.000052, l1: 0.000053, l2: 0.000066, l3: 0.000186, l4: 0.000400, l5: 0.001560, l6: 0.001805
[epoch: 811/1000, batch:   360/ 1052, ite: 213120] train loss: 0.003975, tar: 0.000056 
l0: 0.000085, l1: 0.000087, l2: 0.000110, l3: 0.000153, l4: 0.000368, l5: 0.001086, l6: 0.000726
[epoch: 811/1000, batch:   440/ 1052, ite: 213140] train loss: 0.003977, tar: 0.000057 
l0: 0.000073, l1: 0.000078, l2: 0.000111, l3: 0.000149, l4: 0.000475, l5: 0.001219, l6: 0.001772
[epoch: 811/1000, batch:   520/ 1052, ite: 213160] train loss: 0.003981, tar: 0.000057 
l0: 0.000031, l1: 0.000030, l2: 0.000057, l3: 0.000163, l4: 0.000433, l5: 0.000714, l6: 0.001252
[epoch: 811/1000, batch:   600/ 1052, ite: 213180] train loss: 0.003982, tar: 0.000057 
l0: 0.000057, l1: 0.000063, l2: 0.000072, l3: 0.000204, l4: 0.000432, l5: 0.000825, l6: 0.001861
[epoch: 811/1000, batch:   680/ 1052, ite: 213200] train loss: 0.003981, tar: 0.000057 
l0: 0.000131, l1: 0.000144, l2: 0.000136, l3: 0.000198, l4: 0.000325, l5: 0.000902, l6: 0.002404
[epoch: 811/1000, batch:   760/ 1052, ite: 213220] train loss: 0.003980, tar: 0.000058 
l0: 0.000041, l1: 0.000042, l2: 0.000074, l3: 0.000151, l4: 0.000408, l5: 0.000958, l6: 0.001600
[epoch: 811/1000, batch:   840/ 1052, ite: 213240] train loss: 0.003981, tar: 0.000058 
l0: 0.000031, l1: 0.000032, l2: 0.000043, l3: 0.000075, l4: 0.000288, l5: 0.000780, l6: 0.001356
[epoch: 811/1000, batch:   920/ 1052, ite: 213260] train loss: 0.003983, tar: 0.000058 
l0: 0.000025, l1: 0.000021, l2: 0.000076, l3: 0.000210, l4: 0.000612, l5: 0.001643, l6: 0.002111
[epoch: 811/1000, batch:  1000/ 1052, ite: 213280] train loss: 0.003986, tar: 0.000058 
[Epoch 811/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000048, l1: 0.000045, l2: 0.000087, l3: 0.000238, l4: 0.000460, l5: 0.000790, l6: 0.001557
[epoch: 812/1000, batch:    28/ 1052, ite: 213300] train loss: 0.003988, tar: 0.000058 
l0: 0.000014, l1: 0.000013, l2: 0.000045, l3: 0.000158, l4: 0.000518, l5: 0.001032, l6: 0.002347
[epoch: 812/1000, batch:   108/ 1052, ite: 213320] train loss: 0.003985, tar: 0.000058 
l0: 0.000058, l1: 0.000059, l2: 0.000084, l3: 0.000130, l4: 0.000343, l5: 0.000926, l6: 0.001169
[epoch: 812/1000, batch:   188/ 1052, ite: 213340] train loss: 0.003985, tar: 0.000058 
l0: 0.000022, l1: 0.000022, l2: 0.000059, l3: 0.000168, l4: 0.000273, l5: 0.001098, l6: 0.002531
[epoch: 812/1000, batch:   268/ 1052, ite: 213360] train loss: 0.003985, tar: 0.000058 
l0: 0.000224, l1: 0.000233, l2: 0.000328, l3: 0.000657, l4: 0.001500, l5: 0.003130, l6: 0.005697
[epoch: 812/1000, batch:   348/ 1052, ite: 213380] train loss: 0.003984, tar: 0.000058 
l0: 0.000031, l1: 0.000030, l2: 0.000096, l3: 0.000303, l4: 0.000518, l5: 0.000955, l6: 0.001640
[epoch: 812/1000, batch:   428/ 1052, ite: 213400] train loss: 0.003986, tar: 0.000058 
l0: 0.000020, l1: 0.000020, l2: 0.000035, l3: 0.000073, l4: 0.000222, l5: 0.000275, l6: 0.000895
[epoch: 812/1000, batch:   508/ 1052, ite: 213420] train loss: 0.003987, tar: 0.000058 
l0: 0.000078, l1: 0.000078, l2: 0.000105, l3: 0.000188, l4: 0.000387, l5: 0.000769, l6: 0.001580
[epoch: 812/1000, batch:   588/ 1052, ite: 213440] train loss: 0.003987, tar: 0.000058 
l0: 0.000110, l1: 0.000105, l2: 0.000153, l3: 0.000257, l4: 0.000585, l5: 0.000868, l6: 0.001895
[epoch: 812/1000, batch:   668/ 1052, ite: 213460] train loss: 0.003988, tar: 0.000058 
l0: 0.000047, l1: 0.000047, l2: 0.000073, l3: 0.000207, l4: 0.000685, l5: 0.001209, l6: 0.002115
[epoch: 812/1000, batch:   748/ 1052, ite: 213480] train loss: 0.003988, tar: 0.000058 
l0: 0.000065, l1: 0.000068, l2: 0.000074, l3: 0.000246, l4: 0.000404, l5: 0.000950, l6: 0.001915
[epoch: 812/1000, batch:   828/ 1052, ite: 213500] train loss: 0.003991, tar: 0.000058 
l0: 0.000061, l1: 0.000060, l2: 0.000097, l3: 0.000240, l4: 0.000532, l5: 0.001071, l6: 0.001875
[epoch: 812/1000, batch:   908/ 1052, ite: 213520] train loss: 0.003990, tar: 0.000058 
l0: 0.000025, l1: 0.000028, l2: 0.000032, l3: 0.000086, l4: 0.000340, l5: 0.000449, l6: 0.002277
[epoch: 812/1000, batch:   988/ 1052, ite: 213540] train loss: 0.003989, tar: 0.000057 
[Epoch 812/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000013, l1: 0.000012, l2: 0.000046, l3: 0.000161, l4: 0.000589, l5: 0.001168, l6: 0.001681
[epoch: 813/1000, batch:    16/ 1052, ite: 213560] train loss: 0.003987, tar: 0.000057 
l0: 0.000012, l1: 0.000012, l2: 0.000017, l3: 0.000083, l4: 0.000376, l5: 0.000816, l6: 0.001346
[epoch: 813/1000, batch:    96/ 1052, ite: 213580] train loss: 0.003986, tar: 0.000057 
l0: 0.000023, l1: 0.000023, l2: 0.000047, l3: 0.000156, l4: 0.000363, l5: 0.001105, l6: 0.001321
[epoch: 813/1000, batch:   176/ 1052, ite: 213600] train loss: 0.003988, tar: 0.000057 
l0: 0.000016, l1: 0.000017, l2: 0.000031, l3: 0.000069, l4: 0.000314, l5: 0.000944, l6: 0.000770
[epoch: 813/1000, batch:   256/ 1052, ite: 213620] train loss: 0.003984, tar: 0.000057 
l0: 0.000056, l1: 0.000058, l2: 0.000097, l3: 0.000190, l4: 0.000425, l5: 0.001021, l6: 0.002279
[epoch: 813/1000, batch:   336/ 1052, ite: 213640] train loss: 0.003983, tar: 0.000057 
l0: 0.000058, l1: 0.000060, l2: 0.000107, l3: 0.000277, l4: 0.000665, l5: 0.001639, l6: 0.003547
[epoch: 813/1000, batch:   416/ 1052, ite: 213660] train loss: 0.003983, tar: 0.000057 
l0: 0.000026, l1: 0.000027, l2: 0.000058, l3: 0.000157, l4: 0.000320, l5: 0.001100, l6: 0.001487
[epoch: 813/1000, batch:   496/ 1052, ite: 213680] train loss: 0.003985, tar: 0.000057 
l0: 0.000060, l1: 0.000061, l2: 0.000078, l3: 0.000142, l4: 0.000278, l5: 0.001708, l6: 0.001792
[epoch: 813/1000, batch:   576/ 1052, ite: 213700] train loss: 0.003982, tar: 0.000057 
l0: 0.000040, l1: 0.000040, l2: 0.000045, l3: 0.000106, l4: 0.000347, l5: 0.000713, l6: 0.001156
[epoch: 813/1000, batch:   656/ 1052, ite: 213720] train loss: 0.003982, tar: 0.000057 
l0: 0.000005, l1: 0.000004, l2: 0.000024, l3: 0.000069, l4: 0.000302, l5: 0.000426, l6: 0.001278
[epoch: 813/1000, batch:   736/ 1052, ite: 213740] train loss: 0.003982, tar: 0.000057 
l0: 0.000032, l1: 0.000033, l2: 0.000056, l3: 0.000135, l4: 0.000258, l5: 0.000634, l6: 0.002012
[epoch: 813/1000, batch:   816/ 1052, ite: 213760] train loss: 0.003981, tar: 0.000057 
l0: 0.000099, l1: 0.000100, l2: 0.000119, l3: 0.000321, l4: 0.000649, l5: 0.001037, l6: 0.001407
[epoch: 813/1000, batch:   896/ 1052, ite: 213780] train loss: 0.003981, tar: 0.000057 
l0: 0.000026, l1: 0.000028, l2: 0.000042, l3: 0.000087, l4: 0.000395, l5: 0.000846, l6: 0.001446
[epoch: 813/1000, batch:   976/ 1052, ite: 213800] train loss: 0.003985, tar: 0.000057 
[Epoch 813/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000054, l1: 0.000054, l2: 0.000092, l3: 0.000237, l4: 0.000456, l5: 0.000895, l6: 0.001758
[epoch: 814/1000, batch:     4/ 1052, ite: 213820] train loss: 0.003985, tar: 0.000057 
l0: 0.000062, l1: 0.000062, l2: 0.000139, l3: 0.000313, l4: 0.000544, l5: 0.001320, l6: 0.002660
[epoch: 814/1000, batch:    84/ 1052, ite: 213840] train loss: 0.003986, tar: 0.000057 
l0: 0.000110, l1: 0.000115, l2: 0.000158, l3: 0.000198, l4: 0.000443, l5: 0.000795, l6: 0.001478
[epoch: 814/1000, batch:   164/ 1052, ite: 213860] train loss: 0.003985, tar: 0.000056 
l0: 0.000041, l1: 0.000043, l2: 0.000062, l3: 0.000177, l4: 0.000320, l5: 0.000737, l6: 0.001262
[epoch: 814/1000, batch:   244/ 1052, ite: 213880] train loss: 0.003986, tar: 0.000056 
l0: 0.000020, l1: 0.000023, l2: 0.000044, l3: 0.000107, l4: 0.000288, l5: 0.000575, l6: 0.001567
[epoch: 814/1000, batch:   324/ 1052, ite: 213900] train loss: 0.003986, tar: 0.000056 
l0: 0.000023, l1: 0.000023, l2: 0.000060, l3: 0.000264, l4: 0.000532, l5: 0.000696, l6: 0.001885
[epoch: 814/1000, batch:   404/ 1052, ite: 213920] train loss: 0.003985, tar: 0.000056 
l0: 0.000066, l1: 0.000067, l2: 0.000107, l3: 0.000217, l4: 0.000563, l5: 0.001321, l6: 0.002684
[epoch: 814/1000, batch:   484/ 1052, ite: 213940] train loss: 0.003984, tar: 0.000056 
l0: 0.000049, l1: 0.000049, l2: 0.000140, l3: 0.000318, l4: 0.000626, l5: 0.001845, l6: 0.004795
[epoch: 814/1000, batch:   564/ 1052, ite: 213960] train loss: 0.003982, tar: 0.000056 
l0: 0.000003, l1: 0.000003, l2: 0.000020, l3: 0.000132, l4: 0.000439, l5: 0.000928, l6: 0.001435
[epoch: 814/1000, batch:   644/ 1052, ite: 213980] train loss: 0.003978, tar: 0.000056 
l0: 0.000005, l1: 0.000005, l2: 0.000039, l3: 0.000213, l4: 0.000940, l5: 0.001549, l6: 0.002111
[epoch: 814/1000, batch:   724/ 1052, ite: 214000] train loss: 0.003975, tar: 0.000056 
l0: 0.000048, l1: 0.000047, l2: 0.000083, l3: 0.000149, l4: 0.000644, l5: 0.001397, l6: 0.001919
[epoch: 814/1000, batch:   804/ 1052, ite: 214020] train loss: 0.003980, tar: 0.000056 
l0: 0.000078, l1: 0.000078, l2: 0.000105, l3: 0.000206, l4: 0.000307, l5: 0.001312, l6: 0.002426
[epoch: 814/1000, batch:   884/ 1052, ite: 214040] train loss: 0.003980, tar: 0.000056 
l0: 0.000003, l1: 0.000004, l2: 0.000022, l3: 0.000070, l4: 0.000170, l5: 0.000641, l6: 0.001100
[epoch: 814/1000, batch:   964/ 1052, ite: 214060] train loss: 0.003980, tar: 0.000056 
l0: 0.000030, l1: 0.000031, l2: 0.000057, l3: 0.000169, l4: 0.000574, l5: 0.001533, l6: 0.002171
[epoch: 814/1000, batch:  1044/ 1052, ite: 214080] train loss: 0.003980, tar: 0.000056 
[Epoch 814/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000031, l1: 0.000034, l2: 0.000043, l3: 0.000186, l4: 0.000411, l5: 0.001153, l6: 0.002990
[epoch: 815/1000, batch:    72/ 1052, ite: 214100] train loss: 0.003980, tar: 0.000056 
l0: 0.000137, l1: 0.000140, l2: 0.000169, l3: 0.000355, l4: 0.000605, l5: 0.001603, l6: 0.001762
[epoch: 815/1000, batch:   152/ 1052, ite: 214120] train loss: 0.003979, tar: 0.000056 
l0: 0.000007, l1: 0.000007, l2: 0.000049, l3: 0.000150, l4: 0.000439, l5: 0.000918, l6: 0.002631
[epoch: 815/1000, batch:   232/ 1052, ite: 214140] train loss: 0.003979, tar: 0.000056 
l0: 0.000087, l1: 0.000085, l2: 0.000103, l3: 0.000183, l4: 0.000508, l5: 0.000784, l6: 0.001950
[epoch: 815/1000, batch:   312/ 1052, ite: 214160] train loss: 0.003978, tar: 0.000055 
l0: 0.000018, l1: 0.000017, l2: 0.000070, l3: 0.000155, l4: 0.000464, l5: 0.001030, l6: 0.001467
[epoch: 815/1000, batch:   392/ 1052, ite: 214180] train loss: 0.003977, tar: 0.000055 
l0: 0.000023, l1: 0.000023, l2: 0.000063, l3: 0.000211, l4: 0.000688, l5: 0.002406, l6: 0.003534
[epoch: 815/1000, batch:   472/ 1052, ite: 214200] train loss: 0.003979, tar: 0.000055 
l0: 0.000008, l1: 0.000008, l2: 0.000024, l3: 0.000088, l4: 0.000258, l5: 0.000478, l6: 0.000878
[epoch: 815/1000, batch:   552/ 1052, ite: 214220] train loss: 0.003975, tar: 0.000055 
l0: 0.000050, l1: 0.000050, l2: 0.000066, l3: 0.000187, l4: 0.000396, l5: 0.001054, l6: 0.002688
[epoch: 815/1000, batch:   632/ 1052, ite: 214240] train loss: 0.003975, tar: 0.000055 
l0: 0.000108, l1: 0.000107, l2: 0.000159, l3: 0.000233, l4: 0.000617, l5: 0.001189, l6: 0.001902
[epoch: 815/1000, batch:   712/ 1052, ite: 214260] train loss: 0.003976, tar: 0.000055 
l0: 0.000017, l1: 0.000018, l2: 0.000059, l3: 0.000169, l4: 0.000350, l5: 0.001037, l6: 0.001812
[epoch: 815/1000, batch:   792/ 1052, ite: 214280] train loss: 0.003975, tar: 0.000055 
l0: 0.000015, l1: 0.000015, l2: 0.000022, l3: 0.000133, l4: 0.000421, l5: 0.000847, l6: 0.001382
[epoch: 815/1000, batch:   872/ 1052, ite: 214300] train loss: 0.003973, tar: 0.000055 
l0: 0.000037, l1: 0.000038, l2: 0.000088, l3: 0.000221, l4: 0.000505, l5: 0.001071, l6: 0.001986
[epoch: 815/1000, batch:   952/ 1052, ite: 214320] train loss: 0.003972, tar: 0.000055 
l0: 0.000190, l1: 0.000195, l2: 0.000216, l3: 0.000389, l4: 0.000548, l5: 0.001204, l6: 0.002709
[epoch: 815/1000, batch:  1032/ 1052, ite: 214340] train loss: 0.003975, tar: 0.000055 
[Epoch 815/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000082, l1: 0.000083, l2: 0.000107, l3: 0.000183, l4: 0.000431, l5: 0.001249, l6: 0.001852
[epoch: 816/1000, batch:    60/ 1052, ite: 214360] train loss: 0.003974, tar: 0.000055 
l0: 0.000024, l1: 0.000024, l2: 0.000055, l3: 0.000109, l4: 0.000218, l5: 0.000762, l6: 0.002106
[epoch: 816/1000, batch:   140/ 1052, ite: 214380] train loss: 0.003974, tar: 0.000055 
l0: 0.000067, l1: 0.000068, l2: 0.000119, l3: 0.000205, l4: 0.000537, l5: 0.001100, l6: 0.001528
[epoch: 816/1000, batch:   220/ 1052, ite: 214400] train loss: 0.003974, tar: 0.000055 
l0: 0.000043, l1: 0.000043, l2: 0.000053, l3: 0.000121, l4: 0.000416, l5: 0.000636, l6: 0.001880
[epoch: 816/1000, batch:   300/ 1052, ite: 214420] train loss: 0.003973, tar: 0.000055 
l0: 0.000078, l1: 0.000079, l2: 0.000116, l3: 0.000268, l4: 0.000624, l5: 0.001263, l6: 0.002585
[epoch: 816/1000, batch:   380/ 1052, ite: 214440] train loss: 0.003973, tar: 0.000055 
l0: 0.000020, l1: 0.000019, l2: 0.000028, l3: 0.000066, l4: 0.000184, l5: 0.001257, l6: 0.001761
[epoch: 816/1000, batch:   460/ 1052, ite: 214460] train loss: 0.003971, tar: 0.000055 
l0: 0.000080, l1: 0.000082, l2: 0.000114, l3: 0.000283, l4: 0.000527, l5: 0.001391, l6: 0.001949
[epoch: 816/1000, batch:   540/ 1052, ite: 214480] train loss: 0.003967, tar: 0.000054 
l0: 0.000027, l1: 0.000027, l2: 0.000042, l3: 0.000154, l4: 0.000510, l5: 0.001247, l6: 0.002493
[epoch: 816/1000, batch:   620/ 1052, ite: 214500] train loss: 0.003967, tar: 0.000054 
l0: 0.000080, l1: 0.000078, l2: 0.000199, l3: 0.000626, l4: 0.000993, l5: 0.002226, l6: 0.002903
[epoch: 816/1000, batch:   700/ 1052, ite: 214520] train loss: 0.003969, tar: 0.000054 
l0: 0.000036, l1: 0.000035, l2: 0.000068, l3: 0.000284, l4: 0.000609, l5: 0.001449, l6: 0.003321
[epoch: 816/1000, batch:   780/ 1052, ite: 214540] train loss: 0.003969, tar: 0.000054 
l0: 0.000002, l1: 0.000002, l2: 0.000015, l3: 0.000149, l4: 0.000382, l5: 0.001022, l6: 0.001868
[epoch: 816/1000, batch:   860/ 1052, ite: 214560] train loss: 0.003970, tar: 0.000054 
l0: 0.000020, l1: 0.000018, l2: 0.000050, l3: 0.000165, l4: 0.000263, l5: 0.000975, l6: 0.001399
[epoch: 816/1000, batch:   940/ 1052, ite: 214580] train loss: 0.003968, tar: 0.000054 
l0: 0.000011, l1: 0.000012, l2: 0.000057, l3: 0.000148, l4: 0.000312, l5: 0.000502, l6: 0.001426
[epoch: 816/1000, batch:  1020/ 1052, ite: 214600] train loss: 0.003967, tar: 0.000054 
[Epoch 816/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000086, l1: 0.000087, l2: 0.000130, l3: 0.000272, l4: 0.000489, l5: 0.001054, l6: 0.003064
[epoch: 817/1000, batch:    48/ 1052, ite: 214620] train loss: 0.003966, tar: 0.000054 
l0: 0.000008, l1: 0.000009, l2: 0.000036, l3: 0.000095, l4: 0.000420, l5: 0.000578, l6: 0.001088
[epoch: 817/1000, batch:   128/ 1052, ite: 214640] train loss: 0.003963, tar: 0.000054 
l0: 0.000062, l1: 0.000060, l2: 0.000105, l3: 0.000218, l4: 0.000420, l5: 0.001489, l6: 0.003595
[epoch: 817/1000, batch:   208/ 1052, ite: 214660] train loss: 0.003963, tar: 0.000054 
l0: 0.000020, l1: 0.000018, l2: 0.000080, l3: 0.000215, l4: 0.000507, l5: 0.001648, l6: 0.002128
[epoch: 817/1000, batch:   288/ 1052, ite: 214680] train loss: 0.003965, tar: 0.000054 
l0: 0.000150, l1: 0.000149, l2: 0.000270, l3: 0.000337, l4: 0.000745, l5: 0.002184, l6: 0.005481
[epoch: 817/1000, batch:   368/ 1052, ite: 214700] train loss: 0.003966, tar: 0.000054 
l0: 0.000042, l1: 0.000041, l2: 0.000089, l3: 0.000111, l4: 0.000428, l5: 0.001319, l6: 0.002370
[epoch: 817/1000, batch:   448/ 1052, ite: 214720] train loss: 0.003964, tar: 0.000054 
l0: 0.000006, l1: 0.000006, l2: 0.000032, l3: 0.000148, l4: 0.000369, l5: 0.000578, l6: 0.001196
[epoch: 817/1000, batch:   528/ 1052, ite: 214740] train loss: 0.003965, tar: 0.000054 
l0: 0.000008, l1: 0.000007, l2: 0.000019, l3: 0.000161, l4: 0.000478, l5: 0.000874, l6: 0.001517
[epoch: 817/1000, batch:   608/ 1052, ite: 214760] train loss: 0.003965, tar: 0.000054 
l0: 0.000008, l1: 0.000008, l2: 0.000044, l3: 0.000132, l4: 0.000552, l5: 0.000720, l6: 0.002128
[epoch: 817/1000, batch:   688/ 1052, ite: 214780] train loss: 0.003967, tar: 0.000054 
l0: 0.000033, l1: 0.000035, l2: 0.000046, l3: 0.000207, l4: 0.000295, l5: 0.000946, l6: 0.001276
[epoch: 817/1000, batch:   768/ 1052, ite: 214800] train loss: 0.003964, tar: 0.000054 
l0: 0.000045, l1: 0.000044, l2: 0.000074, l3: 0.000161, l4: 0.000353, l5: 0.000679, l6: 0.000999
[epoch: 817/1000, batch:   848/ 1052, ite: 214820] train loss: 0.003964, tar: 0.000054 
l0: 0.000164, l1: 0.000164, l2: 0.000188, l3: 0.000216, l4: 0.000449, l5: 0.000782, l6: 0.001295
[epoch: 817/1000, batch:   928/ 1052, ite: 214840] train loss: 0.003964, tar: 0.000054 
l0: 0.000198, l1: 0.000200, l2: 0.000268, l3: 0.000375, l4: 0.000701, l5: 0.001324, l6: 0.001952
[epoch: 817/1000, batch:  1008/ 1052, ite: 214860] train loss: 0.003965, tar: 0.000054 
[Epoch 817/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000033, l1: 0.000034, l2: 0.000053, l3: 0.000135, l4: 0.000413, l5: 0.000947, l6: 0.001502
[epoch: 818/1000, batch:    36/ 1052, ite: 214880] train loss: 0.003966, tar: 0.000054 
l0: 0.000111, l1: 0.000114, l2: 0.000139, l3: 0.000231, l4: 0.000428, l5: 0.000897, l6: 0.001570
[epoch: 818/1000, batch:   116/ 1052, ite: 214900] train loss: 0.003964, tar: 0.000054 
l0: 0.000017, l1: 0.000015, l2: 0.000048, l3: 0.000151, l4: 0.000387, l5: 0.000960, l6: 0.001157
[epoch: 818/1000, batch:   196/ 1052, ite: 214920] train loss: 0.003963, tar: 0.000053 
l0: 0.000014, l1: 0.000015, l2: 0.000033, l3: 0.000062, l4: 0.000252, l5: 0.000831, l6: 0.001636
[epoch: 818/1000, batch:   276/ 1052, ite: 214940] train loss: 0.003962, tar: 0.000053 
l0: 0.000005, l1: 0.000004, l2: 0.000013, l3: 0.000080, l4: 0.000250, l5: 0.000772, l6: 0.001458
[epoch: 818/1000, batch:   356/ 1052, ite: 214960] train loss: 0.003962, tar: 0.000053 
l0: 0.000017, l1: 0.000018, l2: 0.000033, l3: 0.000131, l4: 0.000412, l5: 0.001059, l6: 0.001719
[epoch: 818/1000, batch:   436/ 1052, ite: 214980] train loss: 0.003962, tar: 0.000053 
l0: 0.000036, l1: 0.000035, l2: 0.000066, l3: 0.000169, l4: 0.000314, l5: 0.001000, l6: 0.001966
[epoch: 818/1000, batch:   516/ 1052, ite: 215000] train loss: 0.003962, tar: 0.000053 
l0: 0.000027, l1: 0.000027, l2: 0.000029, l3: 0.000157, l4: 0.000321, l5: 0.001236, l6: 0.002318
[epoch: 818/1000, batch:   596/ 1052, ite: 215020] train loss: 0.003962, tar: 0.000053 
l0: 0.000180, l1: 0.000183, l2: 0.000207, l3: 0.000261, l4: 0.000513, l5: 0.001732, l6: 0.002677
[epoch: 818/1000, batch:   676/ 1052, ite: 215040] train loss: 0.003961, tar: 0.000053 
l0: 0.000023, l1: 0.000024, l2: 0.000046, l3: 0.000136, l4: 0.000332, l5: 0.001008, l6: 0.001007
[epoch: 818/1000, batch:   756/ 1052, ite: 215060] train loss: 0.003958, tar: 0.000053 
l0: 0.000005, l1: 0.000005, l2: 0.000022, l3: 0.000078, l4: 0.000260, l5: 0.000670, l6: 0.001958
[epoch: 818/1000, batch:   836/ 1052, ite: 215080] train loss: 0.003958, tar: 0.000053 
l0: 0.000107, l1: 0.000108, l2: 0.000134, l3: 0.000249, l4: 0.000496, l5: 0.000772, l6: 0.002988
[epoch: 818/1000, batch:   916/ 1052, ite: 215100] train loss: 0.003957, tar: 0.000053 
l0: 0.000005, l1: 0.000004, l2: 0.000048, l3: 0.000329, l4: 0.001057, l5: 0.001766, l6: 0.003371
[epoch: 818/1000, batch:   996/ 1052, ite: 215120] train loss: 0.003960, tar: 0.000053 
[Epoch 818/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000076, l1: 0.000073, l2: 0.000090, l3: 0.000174, l4: 0.000458, l5: 0.001066, l6: 0.001664
[epoch: 819/1000, batch:    24/ 1052, ite: 215140] train loss: 0.003960, tar: 0.000053 
l0: 0.000018, l1: 0.000017, l2: 0.000113, l3: 0.000313, l4: 0.000852, l5: 0.001641, l6: 0.002860
[epoch: 819/1000, batch:   104/ 1052, ite: 215160] train loss: 0.003960, tar: 0.000053 
l0: 0.000126, l1: 0.000126, l2: 0.000159, l3: 0.000233, l4: 0.000428, l5: 0.000968, l6: 0.002573
[epoch: 819/1000, batch:   184/ 1052, ite: 215180] train loss: 0.003959, tar: 0.000053 
l0: 0.000026, l1: 0.000026, l2: 0.000060, l3: 0.000129, l4: 0.000264, l5: 0.001130, l6: 0.001768
[epoch: 819/1000, batch:   264/ 1052, ite: 215200] train loss: 0.003957, tar: 0.000053 
l0: 0.000037, l1: 0.000037, l2: 0.000096, l3: 0.000285, l4: 0.000695, l5: 0.001973, l6: 0.002708
[epoch: 819/1000, batch:   344/ 1052, ite: 215220] train loss: 0.003958, tar: 0.000053 
l0: 0.000075, l1: 0.000077, l2: 0.000114, l3: 0.000176, l4: 0.000456, l5: 0.001056, l6: 0.002463
[epoch: 819/1000, batch:   424/ 1052, ite: 215240] train loss: 0.003956, tar: 0.000053 
l0: 0.000010, l1: 0.000008, l2: 0.000055, l3: 0.000121, l4: 0.000410, l5: 0.001052, l6: 0.001647
[epoch: 819/1000, batch:   504/ 1052, ite: 215260] train loss: 0.003956, tar: 0.000053 
l0: 0.000033, l1: 0.000032, l2: 0.000082, l3: 0.000172, l4: 0.000458, l5: 0.000943, l6: 0.002044
[epoch: 819/1000, batch:   584/ 1052, ite: 215280] train loss: 0.003956, tar: 0.000053 
l0: 0.000136, l1: 0.000138, l2: 0.000186, l3: 0.000346, l4: 0.000886, l5: 0.001313, l6: 0.001849
[epoch: 819/1000, batch:   664/ 1052, ite: 215300] train loss: 0.003958, tar: 0.000053 
l0: 0.000066, l1: 0.000067, l2: 0.000143, l3: 0.000409, l4: 0.000783, l5: 0.001063, l6: 0.004163
[epoch: 819/1000, batch:   744/ 1052, ite: 215320] train loss: 0.003959, tar: 0.000053 
l0: 0.000093, l1: 0.000093, l2: 0.000109, l3: 0.000224, l4: 0.000600, l5: 0.001486, l6: 0.002158
[epoch: 819/1000, batch:   824/ 1052, ite: 215340] train loss: 0.003958, tar: 0.000053 
l0: 0.000016, l1: 0.000018, l2: 0.000055, l3: 0.000145, l4: 0.000282, l5: 0.000909, l6: 0.001768
[epoch: 819/1000, batch:   904/ 1052, ite: 215360] train loss: 0.003959, tar: 0.000053 
l0: 0.000040, l1: 0.000039, l2: 0.000062, l3: 0.000069, l4: 0.000316, l5: 0.000552, l6: 0.001118
[epoch: 819/1000, batch:   984/ 1052, ite: 215380] train loss: 0.003956, tar: 0.000053 
[Epoch 819/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000019, l1: 0.000020, l2: 0.000046, l3: 0.000139, l4: 0.000423, l5: 0.001258, l6: 0.002476
[epoch: 820/1000, batch:    12/ 1052, ite: 215400] train loss: 0.003955, tar: 0.000053 
l0: 0.000016, l1: 0.000018, l2: 0.000075, l3: 0.000162, l4: 0.000377, l5: 0.000956, l6: 0.001871
[epoch: 820/1000, batch:    92/ 1052, ite: 215420] train loss: 0.003955, tar: 0.000053 
l0: 0.000019, l1: 0.000019, l2: 0.000070, l3: 0.000287, l4: 0.000479, l5: 0.000819, l6: 0.002211
[epoch: 820/1000, batch:   172/ 1052, ite: 215440] train loss: 0.003953, tar: 0.000053 
l0: 0.000167, l1: 0.000170, l2: 0.000261, l3: 0.000430, l4: 0.000953, l5: 0.001596, l6: 0.004971
[epoch: 820/1000, batch:   252/ 1052, ite: 215460] train loss: 0.003954, tar: 0.000053 
l0: 0.000004, l1: 0.000004, l2: 0.000016, l3: 0.000103, l4: 0.000245, l5: 0.000849, l6: 0.001234
[epoch: 820/1000, batch:   332/ 1052, ite: 215480] train loss: 0.003954, tar: 0.000052 
l0: 0.000049, l1: 0.000051, l2: 0.000070, l3: 0.000174, l4: 0.000624, l5: 0.001375, l6: 0.002574
[epoch: 820/1000, batch:   412/ 1052, ite: 215500] train loss: 0.003954, tar: 0.000053 
l0: 0.000012, l1: 0.000013, l2: 0.000050, l3: 0.000107, l4: 0.000344, l5: 0.000491, l6: 0.001108
[epoch: 820/1000, batch:   492/ 1052, ite: 215520] train loss: 0.003954, tar: 0.000052 
l0: 0.000051, l1: 0.000051, l2: 0.000067, l3: 0.000161, l4: 0.000347, l5: 0.001130, l6: 0.002442
[epoch: 820/1000, batch:   572/ 1052, ite: 215540] train loss: 0.003956, tar: 0.000052 
l0: 0.000108, l1: 0.000113, l2: 0.000135, l3: 0.000160, l4: 0.000588, l5: 0.001927, l6: 0.002650
[epoch: 820/1000, batch:   652/ 1052, ite: 215560] train loss: 0.003956, tar: 0.000052 
l0: 0.000108, l1: 0.000107, l2: 0.000136, l3: 0.000319, l4: 0.000525, l5: 0.001032, l6: 0.002704
[epoch: 820/1000, batch:   732/ 1052, ite: 215580] train loss: 0.003954, tar: 0.000052 
l0: 0.000042, l1: 0.000042, l2: 0.000062, l3: 0.000110, l4: 0.000286, l5: 0.001009, l6: 0.001848
[epoch: 820/1000, batch:   812/ 1052, ite: 215600] train loss: 0.003954, tar: 0.000052 
l0: 0.000023, l1: 0.000024, l2: 0.000026, l3: 0.000066, l4: 0.000355, l5: 0.000701, l6: 0.001339
[epoch: 820/1000, batch:   892/ 1052, ite: 215620] train loss: 0.003953, tar: 0.000052 
l0: 0.000007, l1: 0.000007, l2: 0.000029, l3: 0.000136, l4: 0.000531, l5: 0.001686, l6: 0.002855
[epoch: 820/1000, batch:   972/ 1052, ite: 215640] train loss: 0.003951, tar: 0.000052 
l0: 0.000019, l1: 0.000020, l2: 0.000032, l3: 0.000080, l4: 0.000215, l5: 0.001161, l6: 0.001558
[epoch: 820/1000, batch:  1052/ 1052, ite: 215660] train loss: 0.003954, tar: 0.000052 
[Epoch 820/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000077, l1: 0.000080, l2: 0.000108, l3: 0.000177, l4: 0.000388, l5: 0.001033, l6: 0.001627
[epoch: 821/1000, batch:    80/ 1052, ite: 215680] train loss: 0.003953, tar: 0.000052 
l0: 0.000070, l1: 0.000071, l2: 0.000129, l3: 0.000321, l4: 0.000746, l5: 0.000829, l6: 0.002782
[epoch: 821/1000, batch:   160/ 1052, ite: 215700] train loss: 0.003954, tar: 0.000052 
l0: 0.000044, l1: 0.000041, l2: 0.000100, l3: 0.000194, l4: 0.000400, l5: 0.000810, l6: 0.000798
[epoch: 821/1000, batch:   240/ 1052, ite: 215720] train loss: 0.003954, tar: 0.000052 
l0: 0.000002, l1: 0.000002, l2: 0.000013, l3: 0.000082, l4: 0.000395, l5: 0.000924, l6: 0.001366
[epoch: 821/1000, batch:   320/ 1052, ite: 215740] train loss: 0.003954, tar: 0.000052 
l0: 0.000019, l1: 0.000019, l2: 0.000029, l3: 0.000087, l4: 0.000320, l5: 0.000698, l6: 0.001543
[epoch: 821/1000, batch:   400/ 1052, ite: 215760] train loss: 0.003954, tar: 0.000052 
l0: 0.000010, l1: 0.000009, l2: 0.000049, l3: 0.000201, l4: 0.000347, l5: 0.001040, l6: 0.001375
[epoch: 821/1000, batch:   480/ 1052, ite: 215780] train loss: 0.003954, tar: 0.000052 
l0: 0.000039, l1: 0.000040, l2: 0.000079, l3: 0.000276, l4: 0.000613, l5: 0.001651, l6: 0.002060
[epoch: 821/1000, batch:   560/ 1052, ite: 215800] train loss: 0.003954, tar: 0.000052 
l0: 0.000109, l1: 0.000109, l2: 0.000158, l3: 0.000278, l4: 0.000531, l5: 0.001405, l6: 0.002869
[epoch: 821/1000, batch:   640/ 1052, ite: 215820] train loss: 0.003954, tar: 0.000052 
l0: 0.000032, l1: 0.000034, l2: 0.000080, l3: 0.000198, l4: 0.000640, l5: 0.001832, l6: 0.004243
[epoch: 821/1000, batch:   720/ 1052, ite: 215840] train loss: 0.003954, tar: 0.000052 
l0: 0.000001, l1: 0.000001, l2: 0.000016, l3: 0.000075, l4: 0.000179, l5: 0.000863, l6: 0.001546
[epoch: 821/1000, batch:   800/ 1052, ite: 215860] train loss: 0.003953, tar: 0.000052 
l0: 0.000047, l1: 0.000047, l2: 0.000069, l3: 0.000165, l4: 0.000606, l5: 0.001126, l6: 0.001530
[epoch: 821/1000, batch:   880/ 1052, ite: 215880] train loss: 0.003953, tar: 0.000052 
l0: 0.000086, l1: 0.000088, l2: 0.000108, l3: 0.000183, l4: 0.000481, l5: 0.001662, l6: 0.003467
[epoch: 821/1000, batch:   960/ 1052, ite: 215900] train loss: 0.003953, tar: 0.000052 
l0: 0.000007, l1: 0.000007, l2: 0.000038, l3: 0.000129, l4: 0.000427, l5: 0.001128, l6: 0.002122
[epoch: 821/1000, batch:  1040/ 1052, ite: 215920] train loss: 0.003952, tar: 0.000052 
[Epoch 821/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000023, l1: 0.000023, l2: 0.000081, l3: 0.000151, l4: 0.000597, l5: 0.001072, l6: 0.002392
[epoch: 822/1000, batch:    68/ 1052, ite: 215940] train loss: 0.003952, tar: 0.000052 
l0: 0.000002, l1: 0.000002, l2: 0.000017, l3: 0.000095, l4: 0.000183, l5: 0.001375, l6: 0.001461
[epoch: 822/1000, batch:   148/ 1052, ite: 215960] train loss: 0.003951, tar: 0.000052 
l0: 0.000105, l1: 0.000103, l2: 0.000138, l3: 0.000282, l4: 0.000487, l5: 0.001189, l6: 0.002628
[epoch: 822/1000, batch:   228/ 1052, ite: 215980] train loss: 0.003951, tar: 0.000052 
l0: 0.000004, l1: 0.000004, l2: 0.000018, l3: 0.000140, l4: 0.000367, l5: 0.001328, l6: 0.001900
[epoch: 822/1000, batch:   308/ 1052, ite: 216000] train loss: 0.003951, tar: 0.000051 
l0: 0.000000, l1: 0.000000, l2: 0.000009, l3: 0.000080, l4: 0.000351, l5: 0.000589, l6: 0.000491
[epoch: 822/1000, batch:   388/ 1052, ite: 216020] train loss: 0.003950, tar: 0.000051 
l0: 0.000049, l1: 0.000052, l2: 0.000047, l3: 0.000109, l4: 0.000280, l5: 0.000842, l6: 0.001234
[epoch: 822/1000, batch:   468/ 1052, ite: 216040] train loss: 0.003950, tar: 0.000051 
l0: 0.000060, l1: 0.000060, l2: 0.000069, l3: 0.000142, l4: 0.000394, l5: 0.000990, l6: 0.001070
[epoch: 822/1000, batch:   548/ 1052, ite: 216060] train loss: 0.003950, tar: 0.000051 
l0: 0.000045, l1: 0.000047, l2: 0.000071, l3: 0.000172, l4: 0.000329, l5: 0.000884, l6: 0.002397
[epoch: 822/1000, batch:   628/ 1052, ite: 216080] train loss: 0.003951, tar: 0.000051 
l0: 0.000037, l1: 0.000038, l2: 0.000061, l3: 0.000138, l4: 0.000298, l5: 0.000910, l6: 0.002196
[epoch: 822/1000, batch:   708/ 1052, ite: 216100] train loss: 0.003949, tar: 0.000051 
l0: 0.000022, l1: 0.000023, l2: 0.000075, l3: 0.000244, l4: 0.000699, l5: 0.002299, l6: 0.003459
[epoch: 822/1000, batch:   788/ 1052, ite: 216120] train loss: 0.003950, tar: 0.000051 
l0: 0.000155, l1: 0.000157, l2: 0.000176, l3: 0.000235, l4: 0.000419, l5: 0.000982, l6: 0.001585
[epoch: 822/1000, batch:   868/ 1052, ite: 216140] train loss: 0.003950, tar: 0.000051 
l0: 0.000049, l1: 0.000048, l2: 0.000086, l3: 0.000215, l4: 0.000405, l5: 0.001174, l6: 0.002586
[epoch: 822/1000, batch:   948/ 1052, ite: 216160] train loss: 0.003951, tar: 0.000051 
l0: 0.000001, l1: 0.000001, l2: 0.000008, l3: 0.000052, l4: 0.000225, l5: 0.000624, l6: 0.001261
[epoch: 822/1000, batch:  1028/ 1052, ite: 216180] train loss: 0.003950, tar: 0.000051 
[Epoch 822/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000008, l1: 0.000008, l2: 0.000039, l3: 0.000169, l4: 0.000599, l5: 0.001260, l6: 0.002315
[epoch: 823/1000, batch:    56/ 1052, ite: 216200] train loss: 0.003950, tar: 0.000051 
l0: 0.000015, l1: 0.000015, l2: 0.000025, l3: 0.000115, l4: 0.000488, l5: 0.001138, l6: 0.001389
[epoch: 823/1000, batch:   136/ 1052, ite: 216220] train loss: 0.003949, tar: 0.000051 
l0: 0.000013, l1: 0.000014, l2: 0.000049, l3: 0.000135, l4: 0.000556, l5: 0.001353, l6: 0.001970
[epoch: 823/1000, batch:   216/ 1052, ite: 216240] train loss: 0.003950, tar: 0.000051 
l0: 0.000009, l1: 0.000008, l2: 0.000049, l3: 0.000149, l4: 0.000558, l5: 0.001130, l6: 0.001459
[epoch: 823/1000, batch:   296/ 1052, ite: 216260] train loss: 0.003949, tar: 0.000051 
l0: 0.000016, l1: 0.000015, l2: 0.000038, l3: 0.000091, l4: 0.000346, l5: 0.000767, l6: 0.001599
[epoch: 823/1000, batch:   376/ 1052, ite: 216280] train loss: 0.003947, tar: 0.000051 
l0: 0.000005, l1: 0.000005, l2: 0.000017, l3: 0.000046, l4: 0.000227, l5: 0.000746, l6: 0.000777
[epoch: 823/1000, batch:   456/ 1052, ite: 216300] train loss: 0.003947, tar: 0.000051 
l0: 0.000088, l1: 0.000087, l2: 0.000146, l3: 0.000317, l4: 0.000567, l5: 0.001452, l6: 0.004076
[epoch: 823/1000, batch:   536/ 1052, ite: 216320] train loss: 0.003948, tar: 0.000051 
l0: 0.000082, l1: 0.000080, l2: 0.000122, l3: 0.000298, l4: 0.000647, l5: 0.001056, l6: 0.003268
[epoch: 823/1000, batch:   616/ 1052, ite: 216340] train loss: 0.003949, tar: 0.000051 
l0: 0.000005, l1: 0.000004, l2: 0.000034, l3: 0.000177, l4: 0.000406, l5: 0.000717, l6: 0.001878
[epoch: 823/1000, batch:   696/ 1052, ite: 216360] train loss: 0.003947, tar: 0.000051 
l0: 0.000049, l1: 0.000050, l2: 0.000095, l3: 0.000215, l4: 0.000474, l5: 0.000915, l6: 0.002974
[epoch: 823/1000, batch:   776/ 1052, ite: 216380] train loss: 0.003948, tar: 0.000051 
l0: 0.000024, l1: 0.000024, l2: 0.000045, l3: 0.000187, l4: 0.000570, l5: 0.000670, l6: 0.001302
[epoch: 823/1000, batch:   856/ 1052, ite: 216400] train loss: 0.003948, tar: 0.000051 
l0: 0.000038, l1: 0.000038, l2: 0.000058, l3: 0.000266, l4: 0.000660, l5: 0.001024, l6: 0.002294
[epoch: 823/1000, batch:   936/ 1052, ite: 216420] train loss: 0.003948, tar: 0.000051 
l0: 0.000000, l1: 0.000000, l2: 0.000004, l3: 0.000030, l4: 0.000205, l5: 0.000430, l6: 0.000857
[epoch: 823/1000, batch:  1016/ 1052, ite: 216440] train loss: 0.003948, tar: 0.000051 
[Epoch 823/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000008, l1: 0.000008, l2: 0.000020, l3: 0.000029, l4: 0.000222, l5: 0.000341, l6: 0.000512
[epoch: 824/1000, batch:    44/ 1052, ite: 216460] train loss: 0.003949, tar: 0.000051 
l0: 0.000030, l1: 0.000030, l2: 0.000068, l3: 0.000179, l4: 0.000436, l5: 0.001384, l6: 0.002052
[epoch: 824/1000, batch:   124/ 1052, ite: 216480] train loss: 0.003948, tar: 0.000051 
l0: 0.000015, l1: 0.000015, l2: 0.000037, l3: 0.000088, l4: 0.000323, l5: 0.000593, l6: 0.001055
[epoch: 824/1000, batch:   204/ 1052, ite: 216500] train loss: 0.003948, tar: 0.000051 
l0: 0.000071, l1: 0.000070, l2: 0.000095, l3: 0.000196, l4: 0.000430, l5: 0.001034, l6: 0.001888
[epoch: 824/1000, batch:   284/ 1052, ite: 216520] train loss: 0.003948, tar: 0.000051 
l0: 0.000050, l1: 0.000051, l2: 0.000068, l3: 0.000145, l4: 0.000340, l5: 0.000901, l6: 0.000905
[epoch: 824/1000, batch:   364/ 1052, ite: 216540] train loss: 0.003948, tar: 0.000051 
l0: 0.000141, l1: 0.000142, l2: 0.000168, l3: 0.000265, l4: 0.000484, l5: 0.000974, l6: 0.002922
[epoch: 824/1000, batch:   444/ 1052, ite: 216560] train loss: 0.003948, tar: 0.000051 
l0: 0.000037, l1: 0.000038, l2: 0.000050, l3: 0.000135, l4: 0.000578, l5: 0.001333, l6: 0.002618
[epoch: 824/1000, batch:   524/ 1052, ite: 216580] train loss: 0.003948, tar: 0.000051 
l0: 0.000041, l1: 0.000042, l2: 0.000088, l3: 0.000222, l4: 0.000448, l5: 0.000740, l6: 0.001784
[epoch: 824/1000, batch:   604/ 1052, ite: 216600] train loss: 0.003946, tar: 0.000051 
l0: 0.000019, l1: 0.000019, l2: 0.000072, l3: 0.000259, l4: 0.000642, l5: 0.001346, l6: 0.002545
[epoch: 824/1000, batch:   684/ 1052, ite: 216620] train loss: 0.003945, tar: 0.000051 
l0: 0.000007, l1: 0.000007, l2: 0.000018, l3: 0.000110, l4: 0.000346, l5: 0.000946, l6: 0.001190
[epoch: 824/1000, batch:   764/ 1052, ite: 216640] train loss: 0.003946, tar: 0.000051 
l0: 0.000055, l1: 0.000055, l2: 0.000084, l3: 0.000166, l4: 0.000324, l5: 0.000848, l6: 0.001244
[epoch: 824/1000, batch:   844/ 1052, ite: 216660] train loss: 0.003945, tar: 0.000051 
l0: 0.000097, l1: 0.000105, l2: 0.000116, l3: 0.000255, l4: 0.000793, l5: 0.001458, l6: 0.002659
[epoch: 824/1000, batch:   924/ 1052, ite: 216680] train loss: 0.003944, tar: 0.000051 
l0: 0.000016, l1: 0.000017, l2: 0.000063, l3: 0.000242, l4: 0.000440, l5: 0.001206, l6: 0.002280
[epoch: 824/1000, batch:  1004/ 1052, ite: 216700] train loss: 0.003945, tar: 0.000051 
[Epoch 824/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000042, l1: 0.000044, l2: 0.000040, l3: 0.000099, l4: 0.000294, l5: 0.001019, l6: 0.001517
[epoch: 825/1000, batch:    32/ 1052, ite: 216720] train loss: 0.003944, tar: 0.000051 
l0: 0.000001, l1: 0.000001, l2: 0.000009, l3: 0.000075, l4: 0.000216, l5: 0.000724, l6: 0.001241
[epoch: 825/1000, batch:   112/ 1052, ite: 216740] train loss: 0.003944, tar: 0.000051 
l0: 0.000001, l1: 0.000001, l2: 0.000023, l3: 0.000105, l4: 0.000413, l5: 0.000923, l6: 0.002360
[epoch: 825/1000, batch:   192/ 1052, ite: 216760] train loss: 0.003945, tar: 0.000051 
l0: 0.000081, l1: 0.000087, l2: 0.000081, l3: 0.000182, l4: 0.000363, l5: 0.000916, l6: 0.001670
[epoch: 825/1000, batch:   272/ 1052, ite: 216780] train loss: 0.003944, tar: 0.000050 
l0: 0.000019, l1: 0.000017, l2: 0.000050, l3: 0.000089, l4: 0.000312, l5: 0.000755, l6: 0.000838
[epoch: 825/1000, batch:   352/ 1052, ite: 216800] train loss: 0.003942, tar: 0.000050 
l0: 0.000016, l1: 0.000016, l2: 0.000052, l3: 0.000184, l4: 0.000398, l5: 0.001366, l6: 0.002787
[epoch: 825/1000, batch:   432/ 1052, ite: 216820] train loss: 0.003943, tar: 0.000051 
l0: 0.000014, l1: 0.000016, l2: 0.000041, l3: 0.000106, l4: 0.000353, l5: 0.000698, l6: 0.001335
[epoch: 825/1000, batch:   512/ 1052, ite: 216840] train loss: 0.003944, tar: 0.000051 
l0: 0.000154, l1: 0.000158, l2: 0.000184, l3: 0.000255, l4: 0.000618, l5: 0.001320, l6: 0.002391
[epoch: 825/1000, batch:   592/ 1052, ite: 216860] train loss: 0.003944, tar: 0.000051 
l0: 0.000012, l1: 0.000013, l2: 0.000054, l3: 0.000295, l4: 0.001201, l5: 0.001311, l6: 0.003826
[epoch: 825/1000, batch:   672/ 1052, ite: 216880] train loss: 0.003944, tar: 0.000050 
l0: 0.000049, l1: 0.000050, l2: 0.000078, l3: 0.000250, l4: 0.000744, l5: 0.001622, l6: 0.002738
[epoch: 825/1000, batch:   752/ 1052, ite: 216900] train loss: 0.003943, tar: 0.000050 
l0: 0.000048, l1: 0.000048, l2: 0.000064, l3: 0.000275, l4: 0.000787, l5: 0.001359, l6: 0.002186
[epoch: 825/1000, batch:   832/ 1052, ite: 216920] train loss: 0.003944, tar: 0.000050 
l0: 0.000018, l1: 0.000018, l2: 0.000029, l3: 0.000049, l4: 0.000249, l5: 0.000622, l6: 0.000839
[epoch: 825/1000, batch:   912/ 1052, ite: 216940] train loss: 0.003942, tar: 0.000050 
l0: 0.000007, l1: 0.000006, l2: 0.000032, l3: 0.000155, l4: 0.000379, l5: 0.000776, l6: 0.001711
[epoch: 825/1000, batch:   992/ 1052, ite: 216960] train loss: 0.003942, tar: 0.000050 
[Epoch 825/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000030, l1: 0.000029, l2: 0.000097, l3: 0.000188, l4: 0.000449, l5: 0.000623, l6: 0.001490
[epoch: 826/1000, batch:    20/ 1052, ite: 216980] train loss: 0.003941, tar: 0.000050 
l0: 0.000005, l1: 0.000006, l2: 0.000015, l3: 0.000110, l4: 0.000429, l5: 0.000647, l6: 0.000988
[epoch: 826/1000, batch:   100/ 1052, ite: 217000] train loss: 0.003941, tar: 0.000050 
l0: 0.000060, l1: 0.000060, l2: 0.000098, l3: 0.000234, l4: 0.000478, l5: 0.000887, l6: 0.001873
[epoch: 826/1000, batch:   180/ 1052, ite: 217020] train loss: 0.003940, tar: 0.000050 
l0: 0.000017, l1: 0.000017, l2: 0.000043, l3: 0.000095, l4: 0.000369, l5: 0.000760, l6: 0.002346
[epoch: 826/1000, batch:   260/ 1052, ite: 217040] train loss: 0.003941, tar: 0.000050 
l0: 0.000087, l1: 0.000087, l2: 0.000108, l3: 0.000200, l4: 0.000429, l5: 0.000776, l6: 0.003565
[epoch: 826/1000, batch:   340/ 1052, ite: 217060] train loss: 0.003940, tar: 0.000050 
l0: 0.000004, l1: 0.000004, l2: 0.000038, l3: 0.000112, l4: 0.000384, l5: 0.001374, l6: 0.002153
[epoch: 826/1000, batch:   420/ 1052, ite: 217080] train loss: 0.003940, tar: 0.000050 
l0: 0.000004, l1: 0.000003, l2: 0.000009, l3: 0.000113, l4: 0.000561, l5: 0.000513, l6: 0.001383
[epoch: 826/1000, batch:   500/ 1052, ite: 217100] train loss: 0.003940, tar: 0.000050 
l0: 0.000111, l1: 0.000113, l2: 0.000168, l3: 0.000436, l4: 0.001149, l5: 0.002291, l6: 0.004471
[epoch: 826/1000, batch:   580/ 1052, ite: 217120] train loss: 0.003940, tar: 0.000050 
l0: 0.000013, l1: 0.000012, l2: 0.000043, l3: 0.000067, l4: 0.000353, l5: 0.000916, l6: 0.001485
[epoch: 826/1000, batch:   660/ 1052, ite: 217140] train loss: 0.003940, tar: 0.000050 
l0: 0.000063, l1: 0.000064, l2: 0.000097, l3: 0.000250, l4: 0.000610, l5: 0.000765, l6: 0.002240
[epoch: 826/1000, batch:   740/ 1052, ite: 217160] train loss: 0.003939, tar: 0.000050 
l0: 0.000044, l1: 0.000043, l2: 0.000086, l3: 0.000109, l4: 0.000275, l5: 0.000664, l6: 0.001417
[epoch: 826/1000, batch:   820/ 1052, ite: 217180] train loss: 0.003938, tar: 0.000050 
l0: 0.000016, l1: 0.000016, l2: 0.000037, l3: 0.000098, l4: 0.000208, l5: 0.000596, l6: 0.000995
[epoch: 826/1000, batch:   900/ 1052, ite: 217200] train loss: 0.003939, tar: 0.000050 
l0: 0.000040, l1: 0.000040, l2: 0.000058, l3: 0.000176, l4: 0.000520, l5: 0.001158, l6: 0.002937
[epoch: 826/1000, batch:   980/ 1052, ite: 217220] train loss: 0.003939, tar: 0.000050 
[Epoch 826/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000008, l1: 0.000009, l2: 0.000056, l3: 0.000138, l4: 0.000783, l5: 0.000829, l6: 0.002217
[epoch: 827/1000, batch:     8/ 1052, ite: 217240] train loss: 0.003939, tar: 0.000050 
l0: 0.000048, l1: 0.000048, l2: 0.000092, l3: 0.000271, l4: 0.000540, l5: 0.001313, l6: 0.002382
[epoch: 827/1000, batch:    88/ 1052, ite: 217260] train loss: 0.003939, tar: 0.000050 
l0: 0.000166, l1: 0.000169, l2: 0.000175, l3: 0.000271, l4: 0.000358, l5: 0.000832, l6: 0.001282
[epoch: 827/1000, batch:   168/ 1052, ite: 217280] train loss: 0.003940, tar: 0.000050 
l0: 0.000028, l1: 0.000026, l2: 0.000066, l3: 0.000208, l4: 0.000605, l5: 0.000689, l6: 0.002406
[epoch: 827/1000, batch:   248/ 1052, ite: 217300] train loss: 0.003939, tar: 0.000050 
l0: 0.000022, l1: 0.000022, l2: 0.000046, l3: 0.000120, l4: 0.000359, l5: 0.001059, l6: 0.001712
[epoch: 827/1000, batch:   328/ 1052, ite: 217320] train loss: 0.003939, tar: 0.000050 
l0: 0.000084, l1: 0.000086, l2: 0.000109, l3: 0.000155, l4: 0.000378, l5: 0.000725, l6: 0.002193
[epoch: 827/1000, batch:   408/ 1052, ite: 217340] train loss: 0.003940, tar: 0.000050 
l0: 0.000018, l1: 0.000018, l2: 0.000070, l3: 0.000285, l4: 0.000711, l5: 0.002119, l6: 0.002609
[epoch: 827/1000, batch:   488/ 1052, ite: 217360] train loss: 0.003941, tar: 0.000050 
l0: 0.000005, l1: 0.000005, l2: 0.000030, l3: 0.000135, l4: 0.000470, l5: 0.000590, l6: 0.001681
[epoch: 827/1000, batch:   568/ 1052, ite: 217380] train loss: 0.003943, tar: 0.000050 
l0: 0.000044, l1: 0.000046, l2: 0.000063, l3: 0.000119, l4: 0.000384, l5: 0.001017, l6: 0.001336
[epoch: 827/1000, batch:   648/ 1052, ite: 217400] train loss: 0.003943, tar: 0.000050 
l0: 0.000104, l1: 0.000106, l2: 0.000160, l3: 0.000288, l4: 0.000751, l5: 0.001176, l6: 0.002955
[epoch: 827/1000, batch:   728/ 1052, ite: 217420] train loss: 0.003942, tar: 0.000050 
l0: 0.000009, l1: 0.000009, l2: 0.000021, l3: 0.000157, l4: 0.000301, l5: 0.001163, l6: 0.001767
[epoch: 827/1000, batch:   808/ 1052, ite: 217440] train loss: 0.003943, tar: 0.000050 
l0: 0.000006, l1: 0.000005, l2: 0.000029, l3: 0.000112, l4: 0.000444, l5: 0.000963, l6: 0.002053
[epoch: 827/1000, batch:   888/ 1052, ite: 217460] train loss: 0.003941, tar: 0.000050 
l0: 0.000025, l1: 0.000025, l2: 0.000033, l3: 0.000102, l4: 0.000510, l5: 0.001705, l6: 0.002457
[epoch: 827/1000, batch:   968/ 1052, ite: 217480] train loss: 0.003939, tar: 0.000050 
l0: 0.000110, l1: 0.000109, l2: 0.000146, l3: 0.000171, l4: 0.000390, l5: 0.001238, l6: 0.002160
[epoch: 827/1000, batch:  1048/ 1052, ite: 217500] train loss: 0.003938, tar: 0.000050 
[Epoch 827/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000080, l1: 0.000081, l2: 0.000088, l3: 0.000305, l4: 0.000454, l5: 0.001701, l6: 0.001847
[epoch: 828/1000, batch:    76/ 1052, ite: 217520] train loss: 0.003939, tar: 0.000050 
l0: 0.000056, l1: 0.000055, l2: 0.000078, l3: 0.000133, l4: 0.000537, l5: 0.001268, l6: 0.002065
[epoch: 828/1000, batch:   156/ 1052, ite: 217540] train loss: 0.003939, tar: 0.000050 
l0: 0.000009, l1: 0.000010, l2: 0.000038, l3: 0.000140, l4: 0.000522, l5: 0.001573, l6: 0.002566
[epoch: 828/1000, batch:   236/ 1052, ite: 217560] train loss: 0.003940, tar: 0.000050 
l0: 0.000087, l1: 0.000089, l2: 0.000097, l3: 0.000176, l4: 0.000584, l5: 0.001231, l6: 0.001931
[epoch: 828/1000, batch:   316/ 1052, ite: 217580] train loss: 0.003940, tar: 0.000050 
l0: 0.000111, l1: 0.000111, l2: 0.000127, l3: 0.000202, l4: 0.000534, l5: 0.001336, l6: 0.001663
[epoch: 828/1000, batch:   396/ 1052, ite: 217600] train loss: 0.003940, tar: 0.000050 
l0: 0.000014, l1: 0.000013, l2: 0.000048, l3: 0.000276, l4: 0.000975, l5: 0.001645, l6: 0.001812
[epoch: 828/1000, batch:   476/ 1052, ite: 217620] train loss: 0.003939, tar: 0.000050 
l0: 0.000016, l1: 0.000017, l2: 0.000055, l3: 0.000186, l4: 0.000594, l5: 0.001422, l6: 0.002335
[epoch: 828/1000, batch:   556/ 1052, ite: 217640] train loss: 0.003938, tar: 0.000050 
l0: 0.000018, l1: 0.000019, l2: 0.000027, l3: 0.000087, l4: 0.000221, l5: 0.000682, l6: 0.001522
[epoch: 828/1000, batch:   636/ 1052, ite: 217660] train loss: 0.003939, tar: 0.000050 
l0: 0.000024, l1: 0.000024, l2: 0.000059, l3: 0.000098, l4: 0.000375, l5: 0.000937, l6: 0.001295
[epoch: 828/1000, batch:   716/ 1052, ite: 217680] train loss: 0.003938, tar: 0.000050 
l0: 0.000013, l1: 0.000013, l2: 0.000054, l3: 0.000159, l4: 0.000324, l5: 0.001118, l6: 0.001711
[epoch: 828/1000, batch:   796/ 1052, ite: 217700] train loss: 0.003939, tar: 0.000050 
l0: 0.000077, l1: 0.000080, l2: 0.000130, l3: 0.000174, l4: 0.000404, l5: 0.000891, l6: 0.001207
[epoch: 828/1000, batch:   876/ 1052, ite: 217720] train loss: 0.003938, tar: 0.000050 
l0: 0.000006, l1: 0.000006, l2: 0.000025, l3: 0.000162, l4: 0.000639, l5: 0.001099, l6: 0.001772
[epoch: 828/1000, batch:   956/ 1052, ite: 217740] train loss: 0.003937, tar: 0.000050 
l0: 0.000033, l1: 0.000034, l2: 0.000095, l3: 0.000226, l4: 0.000758, l5: 0.002012, l6: 0.003505
[epoch: 828/1000, batch:  1036/ 1052, ite: 217760] train loss: 0.003937, tar: 0.000050 
[Epoch 828/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000027, l1: 0.000028, l2: 0.000063, l3: 0.000175, l4: 0.000445, l5: 0.001013, l6: 0.001466
[epoch: 829/1000, batch:    64/ 1052, ite: 217780] train loss: 0.003936, tar: 0.000050 
l0: 0.000042, l1: 0.000045, l2: 0.000075, l3: 0.000132, l4: 0.000191, l5: 0.000722, l6: 0.001077
[epoch: 829/1000, batch:   144/ 1052, ite: 217800] train loss: 0.003936, tar: 0.000050 
l0: 0.000015, l1: 0.000014, l2: 0.000046, l3: 0.000142, l4: 0.000517, l5: 0.001436, l6: 0.002382
[epoch: 829/1000, batch:   224/ 1052, ite: 217820] train loss: 0.003936, tar: 0.000050 
l0: 0.000048, l1: 0.000051, l2: 0.000072, l3: 0.000240, l4: 0.000447, l5: 0.001235, l6: 0.002305
[epoch: 829/1000, batch:   304/ 1052, ite: 217840] train loss: 0.003936, tar: 0.000050 
l0: 0.000063, l1: 0.000064, l2: 0.000062, l3: 0.000174, l4: 0.000446, l5: 0.001042, l6: 0.001843
[epoch: 829/1000, batch:   384/ 1052, ite: 217860] train loss: 0.003935, tar: 0.000050 
l0: 0.000074, l1: 0.000072, l2: 0.000140, l3: 0.000369, l4: 0.000955, l5: 0.003215, l6: 0.004981
[epoch: 829/1000, batch:   464/ 1052, ite: 217880] train loss: 0.003936, tar: 0.000050 
l0: 0.000251, l1: 0.000247, l2: 0.000323, l3: 0.000491, l4: 0.000491, l5: 0.001555, l6: 0.003390
[epoch: 829/1000, batch:   544/ 1052, ite: 217900] train loss: 0.003936, tar: 0.000050 
l0: 0.000053, l1: 0.000054, l2: 0.000081, l3: 0.000173, l4: 0.000408, l5: 0.000843, l6: 0.001939
[epoch: 829/1000, batch:   624/ 1052, ite: 217920] train loss: 0.003935, tar: 0.000050 
l0: 0.000154, l1: 0.000162, l2: 0.000166, l3: 0.000216, l4: 0.000427, l5: 0.001525, l6: 0.002687
[epoch: 829/1000, batch:   704/ 1052, ite: 217940] train loss: 0.003935, tar: 0.000050 
l0: 0.000029, l1: 0.000029, l2: 0.000053, l3: 0.000221, l4: 0.000558, l5: 0.000913, l6: 0.001397
[epoch: 829/1000, batch:   784/ 1052, ite: 217960] train loss: 0.003935, tar: 0.000050 
l0: 0.000063, l1: 0.000064, l2: 0.000097, l3: 0.000210, l4: 0.000387, l5: 0.001228, l6: 0.002055
[epoch: 829/1000, batch:   864/ 1052, ite: 217980] train loss: 0.003938, tar: 0.000050 
l0: 0.000007, l1: 0.000006, l2: 0.000021, l3: 0.000074, l4: 0.000317, l5: 0.001045, l6: 0.001167
[epoch: 829/1000, batch:   944/ 1052, ite: 218000] train loss: 0.003936, tar: 0.000050 
l0: 0.000030, l1: 0.000029, l2: 0.000056, l3: 0.000092, l4: 0.000247, l5: 0.000615, l6: 0.001274
[epoch: 829/1000, batch:  1024/ 1052, ite: 218020] train loss: 0.003936, tar: 0.000050 
[Epoch 829/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000013, l1: 0.000012, l2: 0.000066, l3: 0.000195, l4: 0.000372, l5: 0.001541, l6: 0.002175
[epoch: 830/1000, batch:    52/ 1052, ite: 218040] train loss: 0.003937, tar: 0.000050 
l0: 0.000048, l1: 0.000050, l2: 0.000060, l3: 0.000098, l4: 0.000438, l5: 0.000695, l6: 0.001540
[epoch: 830/1000, batch:   132/ 1052, ite: 218060] train loss: 0.003936, tar: 0.000050 
l0: 0.000012, l1: 0.000011, l2: 0.000048, l3: 0.000133, l4: 0.000387, l5: 0.000968, l6: 0.001906
[epoch: 830/1000, batch:   212/ 1052, ite: 218080] train loss: 0.003936, tar: 0.000050 
l0: 0.000004, l1: 0.000004, l2: 0.000031, l3: 0.000169, l4: 0.000589, l5: 0.001244, l6: 0.002628
[epoch: 830/1000, batch:   292/ 1052, ite: 218100] train loss: 0.003937, tar: 0.000050 
l0: 0.000063, l1: 0.000063, l2: 0.000096, l3: 0.000133, l4: 0.000498, l5: 0.001433, l6: 0.002076
[epoch: 830/1000, batch:   372/ 1052, ite: 218120] train loss: 0.003937, tar: 0.000050 
l0: 0.000014, l1: 0.000016, l2: 0.000056, l3: 0.000135, l4: 0.000469, l5: 0.000902, l6: 0.002641
[epoch: 830/1000, batch:   452/ 1052, ite: 218140] train loss: 0.003936, tar: 0.000050 
l0: 0.000029, l1: 0.000030, l2: 0.000063, l3: 0.000125, l4: 0.000400, l5: 0.000610, l6: 0.001125
[epoch: 830/1000, batch:   532/ 1052, ite: 218160] train loss: 0.003936, tar: 0.000050 
l0: 0.000007, l1: 0.000006, l2: 0.000022, l3: 0.000142, l4: 0.000348, l5: 0.001146, l6: 0.001855
[epoch: 830/1000, batch:   612/ 1052, ite: 218180] train loss: 0.003935, tar: 0.000050 
l0: 0.000002, l1: 0.000002, l2: 0.000015, l3: 0.000045, l4: 0.000421, l5: 0.000555, l6: 0.001508
[epoch: 830/1000, batch:   692/ 1052, ite: 218200] train loss: 0.003934, tar: 0.000050 
l0: 0.000010, l1: 0.000010, l2: 0.000047, l3: 0.000109, l4: 0.000370, l5: 0.000744, l6: 0.000959
[epoch: 830/1000, batch:   772/ 1052, ite: 218220] train loss: 0.003936, tar: 0.000050 
l0: 0.000011, l1: 0.000011, l2: 0.000025, l3: 0.000067, l4: 0.000413, l5: 0.000591, l6: 0.001499
[epoch: 830/1000, batch:   852/ 1052, ite: 218240] train loss: 0.003937, tar: 0.000050 
l0: 0.000228, l1: 0.000231, l2: 0.000317, l3: 0.000547, l4: 0.001032, l5: 0.002436, l6: 0.004299
[epoch: 830/1000, batch:   932/ 1052, ite: 218260] train loss: 0.003937, tar: 0.000050 
l0: 0.000112, l1: 0.000114, l2: 0.000136, l3: 0.000272, l4: 0.000468, l5: 0.000983, l6: 0.001639
[epoch: 830/1000, batch:  1012/ 1052, ite: 218280] train loss: 0.003937, tar: 0.000050 
[Epoch 830/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000070, l1: 0.000070, l2: 0.000106, l3: 0.000212, l4: 0.000448, l5: 0.001115, l6: 0.001860
[epoch: 831/1000, batch:    40/ 1052, ite: 218300] train loss: 0.003937, tar: 0.000050 
l0: 0.000103, l1: 0.000102, l2: 0.000135, l3: 0.000280, l4: 0.000411, l5: 0.001156, l6: 0.001562
[epoch: 831/1000, batch:   120/ 1052, ite: 218320] train loss: 0.003937, tar: 0.000050 
l0: 0.000025, l1: 0.000025, l2: 0.000066, l3: 0.000176, l4: 0.000592, l5: 0.001159, l6: 0.002090
[epoch: 831/1000, batch:   200/ 1052, ite: 218340] train loss: 0.003936, tar: 0.000050 
l0: 0.000031, l1: 0.000032, l2: 0.000043, l3: 0.000097, l4: 0.000488, l5: 0.000561, l6: 0.001531
[epoch: 831/1000, batch:   280/ 1052, ite: 218360] train loss: 0.003936, tar: 0.000050 
l0: 0.000094, l1: 0.000093, l2: 0.000150, l3: 0.000152, l4: 0.000623, l5: 0.001201, l6: 0.001837
[epoch: 831/1000, batch:   360/ 1052, ite: 218380] train loss: 0.003936, tar: 0.000050 
l0: 0.000059, l1: 0.000061, l2: 0.000085, l3: 0.000230, l4: 0.000611, l5: 0.001073, l6: 0.002664
[epoch: 831/1000, batch:   440/ 1052, ite: 218400] train loss: 0.003937, tar: 0.000050 
l0: 0.000004, l1: 0.000004, l2: 0.000026, l3: 0.000081, l4: 0.000310, l5: 0.000670, l6: 0.001370
[epoch: 831/1000, batch:   520/ 1052, ite: 218420] train loss: 0.003936, tar: 0.000050 
l0: 0.000039, l1: 0.000037, l2: 0.000111, l3: 0.000407, l4: 0.000721, l5: 0.001431, l6: 0.002477
[epoch: 831/1000, batch:   600/ 1052, ite: 218440] train loss: 0.003938, tar: 0.000050 
l0: 0.000012, l1: 0.000011, l2: 0.000030, l3: 0.000074, l4: 0.000170, l5: 0.000715, l6: 0.000931
[epoch: 831/1000, batch:   680/ 1052, ite: 218460] train loss: 0.003936, tar: 0.000050 
l0: 0.000038, l1: 0.000039, l2: 0.000061, l3: 0.000147, l4: 0.000300, l5: 0.001098, l6: 0.000975
[epoch: 831/1000, batch:   760/ 1052, ite: 218480] train loss: 0.003936, tar: 0.000050 
l0: 0.000025, l1: 0.000025, l2: 0.000094, l3: 0.000236, l4: 0.000609, l5: 0.001569, l6: 0.001936
[epoch: 831/1000, batch:   840/ 1052, ite: 218500] train loss: 0.003935, tar: 0.000049 
l0: 0.000033, l1: 0.000033, l2: 0.000086, l3: 0.000187, l4: 0.000350, l5: 0.001583, l6: 0.002766
[epoch: 831/1000, batch:   920/ 1052, ite: 218520] train loss: 0.003936, tar: 0.000049 
l0: 0.000019, l1: 0.000018, l2: 0.000064, l3: 0.000148, l4: 0.000428, l5: 0.000975, l6: 0.002076
[epoch: 831/1000, batch:  1000/ 1052, ite: 218540] train loss: 0.003936, tar: 0.000049 
[Epoch 831/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000080, l1: 0.000077, l2: 0.000158, l3: 0.000339, l4: 0.000567, l5: 0.001634, l6: 0.003943
[epoch: 832/1000, batch:    28/ 1052, ite: 218560] train loss: 0.003936, tar: 0.000050 
l0: 0.000024, l1: 0.000025, l2: 0.000052, l3: 0.000125, l4: 0.000414, l5: 0.000674, l6: 0.001366
[epoch: 832/1000, batch:   108/ 1052, ite: 218580] train loss: 0.003936, tar: 0.000050 
l0: 0.000049, l1: 0.000048, l2: 0.000061, l3: 0.000216, l4: 0.000393, l5: 0.000741, l6: 0.001720
[epoch: 832/1000, batch:   188/ 1052, ite: 218600] train loss: 0.003936, tar: 0.000050 
l0: 0.000005, l1: 0.000004, l2: 0.000022, l3: 0.000199, l4: 0.000424, l5: 0.000792, l6: 0.001823
[epoch: 832/1000, batch:   268/ 1052, ite: 218620] train loss: 0.003936, tar: 0.000049 
l0: 0.000012, l1: 0.000012, l2: 0.000050, l3: 0.000218, l4: 0.000589, l5: 0.001223, l6: 0.002347
[epoch: 832/1000, batch:   348/ 1052, ite: 218640] train loss: 0.003936, tar: 0.000049 
l0: 0.000017, l1: 0.000018, l2: 0.000018, l3: 0.000079, l4: 0.000372, l5: 0.000909, l6: 0.000984
[epoch: 832/1000, batch:   428/ 1052, ite: 218660] train loss: 0.003937, tar: 0.000049 
l0: 0.000035, l1: 0.000036, l2: 0.000044, l3: 0.000200, l4: 0.000471, l5: 0.001521, l6: 0.001777
[epoch: 832/1000, batch:   508/ 1052, ite: 218680] train loss: 0.003938, tar: 0.000049 
l0: 0.000052, l1: 0.000051, l2: 0.000107, l3: 0.000290, l4: 0.000572, l5: 0.001382, l6: 0.002838
[epoch: 832/1000, batch:   588/ 1052, ite: 218700] train loss: 0.003938, tar: 0.000049 
l0: 0.000007, l1: 0.000006, l2: 0.000025, l3: 0.000114, l4: 0.000406, l5: 0.001376, l6: 0.001722
[epoch: 832/1000, batch:   668/ 1052, ite: 218720] train loss: 0.003938, tar: 0.000049 
l0: 0.000020, l1: 0.000020, l2: 0.000034, l3: 0.000099, l4: 0.000433, l5: 0.000957, l6: 0.000930
[epoch: 832/1000, batch:   748/ 1052, ite: 218740] train loss: 0.003939, tar: 0.000049 
l0: 0.000001, l1: 0.000001, l2: 0.000013, l3: 0.000078, l4: 0.000154, l5: 0.000571, l6: 0.001229
[epoch: 832/1000, batch:   828/ 1052, ite: 218760] train loss: 0.003938, tar: 0.000049 
l0: 0.000034, l1: 0.000035, l2: 0.000039, l3: 0.000058, l4: 0.000196, l5: 0.000880, l6: 0.000853
[epoch: 832/1000, batch:   908/ 1052, ite: 218780] train loss: 0.003936, tar: 0.000049 
l0: 0.000013, l1: 0.000013, l2: 0.000021, l3: 0.000043, l4: 0.000103, l5: 0.000505, l6: 0.000938
[epoch: 832/1000, batch:   988/ 1052, ite: 218800] train loss: 0.003936, tar: 0.000049 
[Epoch 832/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000043, l1: 0.000041, l2: 0.000075, l3: 0.000173, l4: 0.000439, l5: 0.001323, l6: 0.001997
[epoch: 833/1000, batch:    16/ 1052, ite: 218820] train loss: 0.003936, tar: 0.000049 
l0: 0.000010, l1: 0.000010, l2: 0.000020, l3: 0.000051, l4: 0.000135, l5: 0.000730, l6: 0.001181
[epoch: 833/1000, batch:    96/ 1052, ite: 218840] train loss: 0.003936, tar: 0.000049 
l0: 0.000025, l1: 0.000026, l2: 0.000031, l3: 0.000090, l4: 0.000312, l5: 0.000632, l6: 0.001254
[epoch: 833/1000, batch:   176/ 1052, ite: 218860] train loss: 0.003936, tar: 0.000049 
l0: 0.000065, l1: 0.000066, l2: 0.000114, l3: 0.000201, l4: 0.000530, l5: 0.000983, l6: 0.001359
[epoch: 833/1000, batch:   256/ 1052, ite: 218880] train loss: 0.003935, tar: 0.000049 
l0: 0.000030, l1: 0.000031, l2: 0.000061, l3: 0.000147, l4: 0.000364, l5: 0.001051, l6: 0.001605
[epoch: 833/1000, batch:   336/ 1052, ite: 218900] train loss: 0.003934, tar: 0.000049 
l0: 0.000072, l1: 0.000071, l2: 0.000128, l3: 0.000236, l4: 0.000608, l5: 0.001228, l6: 0.001933
[epoch: 833/1000, batch:   416/ 1052, ite: 218920] train loss: 0.003933, tar: 0.000049 
l0: 0.000041, l1: 0.000041, l2: 0.000071, l3: 0.000291, l4: 0.000454, l5: 0.001282, l6: 0.002918
[epoch: 833/1000, batch:   496/ 1052, ite: 218940] train loss: 0.003933, tar: 0.000049 
l0: 0.000036, l1: 0.000037, l2: 0.000063, l3: 0.000119, l4: 0.000357, l5: 0.000915, l6: 0.001046
[epoch: 833/1000, batch:   576/ 1052, ite: 218960] train loss: 0.003934, tar: 0.000049 
l0: 0.000043, l1: 0.000043, l2: 0.000070, l3: 0.000119, l4: 0.000272, l5: 0.000967, l6: 0.001445
[epoch: 833/1000, batch:   656/ 1052, ite: 218980] train loss: 0.003934, tar: 0.000049 
l0: 0.000082, l1: 0.000081, l2: 0.000130, l3: 0.000273, l4: 0.000845, l5: 0.002311, l6: 0.003507
[epoch: 833/1000, batch:   736/ 1052, ite: 219000] train loss: 0.003934, tar: 0.000049 
l0: 0.000046, l1: 0.000048, l2: 0.000081, l3: 0.000253, l4: 0.000671, l5: 0.001522, l6: 0.002268
[epoch: 833/1000, batch:   816/ 1052, ite: 219020] train loss: 0.003934, tar: 0.000049 
l0: 0.000016, l1: 0.000015, l2: 0.000042, l3: 0.000087, l4: 0.000268, l5: 0.000795, l6: 0.002184
[epoch: 833/1000, batch:   896/ 1052, ite: 219040] train loss: 0.003935, tar: 0.000049 
l0: 0.000005, l1: 0.000005, l2: 0.000019, l3: 0.000178, l4: 0.000581, l5: 0.001234, l6: 0.001955
[epoch: 833/1000, batch:   976/ 1052, ite: 219060] train loss: 0.003935, tar: 0.000049 
[Epoch 833/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000136, l1: 0.000139, l2: 0.000185, l3: 0.000311, l4: 0.000687, l5: 0.001982, l6: 0.004271
[epoch: 834/1000, batch:     4/ 1052, ite: 219080] train loss: 0.003935, tar: 0.000049 
l0: 0.000091, l1: 0.000089, l2: 0.000184, l3: 0.000358, l4: 0.000642, l5: 0.001578, l6: 0.003845
[epoch: 834/1000, batch:    84/ 1052, ite: 219100] train loss: 0.003936, tar: 0.000049 
l0: 0.000040, l1: 0.000041, l2: 0.000063, l3: 0.000167, l4: 0.000340, l5: 0.001068, l6: 0.003103
[epoch: 834/1000, batch:   164/ 1052, ite: 219120] train loss: 0.003936, tar: 0.000049 
l0: 0.000001, l1: 0.000000, l2: 0.000010, l3: 0.000077, l4: 0.000368, l5: 0.000872, l6: 0.001467
[epoch: 834/1000, batch:   244/ 1052, ite: 219140] train loss: 0.003935, tar: 0.000049 
l0: 0.000008, l1: 0.000008, l2: 0.000042, l3: 0.000127, l4: 0.000324, l5: 0.001272, l6: 0.003608
[epoch: 834/1000, batch:   324/ 1052, ite: 219160] train loss: 0.003934, tar: 0.000049 
l0: 0.000089, l1: 0.000090, l2: 0.000108, l3: 0.000154, l4: 0.000496, l5: 0.001520, l6: 0.002525
[epoch: 834/1000, batch:   404/ 1052, ite: 219180] train loss: 0.003934, tar: 0.000049 
l0: 0.000025, l1: 0.000024, l2: 0.000038, l3: 0.000096, l4: 0.000204, l5: 0.000613, l6: 0.000769
[epoch: 834/1000, batch:   484/ 1052, ite: 219200] train loss: 0.003934, tar: 0.000049 
l0: 0.000091, l1: 0.000097, l2: 0.000134, l3: 0.000307, l4: 0.000857, l5: 0.002500, l6: 0.002963
[epoch: 834/1000, batch:   564/ 1052, ite: 219220] train loss: 0.003934, tar: 0.000049 
l0: 0.000092, l1: 0.000091, l2: 0.000129, l3: 0.000294, l4: 0.000586, l5: 0.001256, l6: 0.002007
[epoch: 834/1000, batch:   644/ 1052, ite: 219240] train loss: 0.003933, tar: 0.000049 
l0: 0.000059, l1: 0.000060, l2: 0.000069, l3: 0.000092, l4: 0.000388, l5: 0.000871, l6: 0.001346
[epoch: 834/1000, batch:   724/ 1052, ite: 219260] train loss: 0.003933, tar: 0.000049 
l0: 0.000046, l1: 0.000047, l2: 0.000065, l3: 0.000127, l4: 0.000395, l5: 0.000733, l6: 0.002600
[epoch: 834/1000, batch:   804/ 1052, ite: 219280] train loss: 0.003934, tar: 0.000049 
l0: 0.000005, l1: 0.000004, l2: 0.000015, l3: 0.000044, l4: 0.000087, l5: 0.000339, l6: 0.000189
[epoch: 834/1000, batch:   884/ 1052, ite: 219300] train loss: 0.003933, tar: 0.000049 
l0: 0.000077, l1: 0.000078, l2: 0.000119, l3: 0.000247, l4: 0.000595, l5: 0.001475, l6: 0.002849
[epoch: 834/1000, batch:   964/ 1052, ite: 219320] train loss: 0.003933, tar: 0.000049 
l0: 0.000087, l1: 0.000087, l2: 0.000127, l3: 0.000390, l4: 0.000835, l5: 0.002016, l6: 0.002908
[epoch: 834/1000, batch:  1044/ 1052, ite: 219340] train loss: 0.003933, tar: 0.000049 
[Epoch 834/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000003, l1: 0.000003, l2: 0.000024, l3: 0.000133, l4: 0.000445, l5: 0.000707, l6: 0.002158
[epoch: 835/1000, batch:    72/ 1052, ite: 219360] train loss: 0.003933, tar: 0.000049 
l0: 0.000037, l1: 0.000037, l2: 0.000079, l3: 0.000225, l4: 0.000696, l5: 0.001431, l6: 0.002254
[epoch: 835/1000, batch:   152/ 1052, ite: 219380] train loss: 0.003934, tar: 0.000049 
l0: 0.000008, l1: 0.000009, l2: 0.000051, l3: 0.000114, l4: 0.000531, l5: 0.001328, l6: 0.001849
[epoch: 835/1000, batch:   232/ 1052, ite: 219400] train loss: 0.003933, tar: 0.000049 
l0: 0.000066, l1: 0.000070, l2: 0.000085, l3: 0.000181, l4: 0.000641, l5: 0.001331, l6: 0.001959
[epoch: 835/1000, batch:   312/ 1052, ite: 219420] train loss: 0.003934, tar: 0.000049 
l0: 0.000041, l1: 0.000041, l2: 0.000084, l3: 0.000349, l4: 0.000560, l5: 0.001583, l6: 0.001875
[epoch: 835/1000, batch:   392/ 1052, ite: 219440] train loss: 0.003934, tar: 0.000049 
l0: 0.000033, l1: 0.000034, l2: 0.000056, l3: 0.000158, l4: 0.000366, l5: 0.000889, l6: 0.001289
[epoch: 835/1000, batch:   472/ 1052, ite: 219460] train loss: 0.003934, tar: 0.000049 
l0: 0.000028, l1: 0.000028, l2: 0.000038, l3: 0.000090, l4: 0.000239, l5: 0.000802, l6: 0.000909
[epoch: 835/1000, batch:   552/ 1052, ite: 219480] train loss: 0.003934, tar: 0.000049 
l0: 0.000005, l1: 0.000005, l2: 0.000037, l3: 0.000263, l4: 0.001023, l5: 0.001800, l6: 0.001799
[epoch: 835/1000, batch:   632/ 1052, ite: 219500] train loss: 0.003934, tar: 0.000049 
l0: 0.000131, l1: 0.000133, l2: 0.000158, l3: 0.000197, l4: 0.000670, l5: 0.001208, l6: 0.002067
[epoch: 835/1000, batch:   712/ 1052, ite: 219520] train loss: 0.003933, tar: 0.000049 
l0: 0.000031, l1: 0.000031, l2: 0.000047, l3: 0.000125, l4: 0.000347, l5: 0.001335, l6: 0.001870
[epoch: 835/1000, batch:   792/ 1052, ite: 219540] train loss: 0.003933, tar: 0.000049 
l0: 0.000060, l1: 0.000060, l2: 0.000079, l3: 0.000228, l4: 0.000482, l5: 0.001372, l6: 0.003302
[epoch: 835/1000, batch:   872/ 1052, ite: 219560] train loss: 0.003933, tar: 0.000049 
l0: 0.000051, l1: 0.000051, l2: 0.000079, l3: 0.000235, l4: 0.000431, l5: 0.000922, l6: 0.001739
[epoch: 835/1000, batch:   952/ 1052, ite: 219580] train loss: 0.003933, tar: 0.000049 
l0: 0.000055, l1: 0.000054, l2: 0.000128, l3: 0.000324, l4: 0.000637, l5: 0.002974, l6: 0.003319
[epoch: 835/1000, batch:  1032/ 1052, ite: 219600] train loss: 0.003933, tar: 0.000049 
[Epoch 835/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000021, l1: 0.000021, l2: 0.000067, l3: 0.000215, l4: 0.000635, l5: 0.001616, l6: 0.002957
[epoch: 836/1000, batch:    60/ 1052, ite: 219620] train loss: 0.003935, tar: 0.000049 
l0: 0.000000, l1: 0.000000, l2: 0.000001, l3: 0.000032, l4: 0.000380, l5: 0.000374, l6: 0.001057
[epoch: 836/1000, batch:   140/ 1052, ite: 219640] train loss: 0.003935, tar: 0.000049 
l0: 0.000008, l1: 0.000009, l2: 0.000029, l3: 0.000109, l4: 0.000376, l5: 0.001106, l6: 0.002762
[epoch: 836/1000, batch:   220/ 1052, ite: 219660] train loss: 0.003934, tar: 0.000049 
l0: 0.000060, l1: 0.000061, l2: 0.000077, l3: 0.000165, l4: 0.000463, l5: 0.001387, l6: 0.001978
[epoch: 836/1000, batch:   300/ 1052, ite: 219680] train loss: 0.003935, tar: 0.000049 
l0: 0.000012, l1: 0.000013, l2: 0.000024, l3: 0.000113, l4: 0.000234, l5: 0.000890, l6: 0.001976
[epoch: 836/1000, batch:   380/ 1052, ite: 219700] train loss: 0.003935, tar: 0.000049 
l0: 0.000006, l1: 0.000006, l2: 0.000063, l3: 0.000235, l4: 0.000530, l5: 0.001332, l6: 0.002646
[epoch: 836/1000, batch:   460/ 1052, ite: 219720] train loss: 0.003936, tar: 0.000049 
l0: 0.000099, l1: 0.000102, l2: 0.000124, l3: 0.000195, l4: 0.000457, l5: 0.000974, l6: 0.003252
[epoch: 836/1000, batch:   540/ 1052, ite: 219740] train loss: 0.003936, tar: 0.000049 
l0: 0.000016, l1: 0.000015, l2: 0.000034, l3: 0.000147, l4: 0.000281, l5: 0.000504, l6: 0.001286
[epoch: 836/1000, batch:   620/ 1052, ite: 219760] train loss: 0.003935, tar: 0.000049 
l0: 0.000004, l1: 0.000004, l2: 0.000013, l3: 0.000081, l4: 0.000295, l5: 0.000671, l6: 0.001125
[epoch: 836/1000, batch:   700/ 1052, ite: 219780] train loss: 0.003936, tar: 0.000049 
l0: 0.000022, l1: 0.000023, l2: 0.000036, l3: 0.000118, l4: 0.000310, l5: 0.000942, l6: 0.001228
[epoch: 836/1000, batch:   780/ 1052, ite: 219800] train loss: 0.003935, tar: 0.000049 
l0: 0.000219, l1: 0.000224, l2: 0.000262, l3: 0.000262, l4: 0.000557, l5: 0.000999, l6: 0.002295
[epoch: 836/1000, batch:   860/ 1052, ite: 219820] train loss: 0.003934, tar: 0.000049 
l0: 0.000038, l1: 0.000038, l2: 0.000078, l3: 0.000181, l4: 0.000389, l5: 0.001294, l6: 0.002269
[epoch: 836/1000, batch:   940/ 1052, ite: 219840] train loss: 0.003933, tar: 0.000049 
l0: 0.000068, l1: 0.000071, l2: 0.000086, l3: 0.000160, l4: 0.000663, l5: 0.001614, l6: 0.002568
[epoch: 836/1000, batch:  1020/ 1052, ite: 219860] train loss: 0.003933, tar: 0.000049 
[Epoch 836/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000065, l1: 0.000065, l2: 0.000111, l3: 0.000210, l4: 0.000535, l5: 0.000980, l6: 0.001653
[epoch: 837/1000, batch:    48/ 1052, ite: 219880] train loss: 0.003933, tar: 0.000049 
l0: 0.000032, l1: 0.000032, l2: 0.000072, l3: 0.000207, l4: 0.000271, l5: 0.000548, l6: 0.000955
[epoch: 837/1000, batch:   128/ 1052, ite: 219900] train loss: 0.003933, tar: 0.000049 
l0: 0.000011, l1: 0.000012, l2: 0.000042, l3: 0.000235, l4: 0.000540, l5: 0.000638, l6: 0.001758
[epoch: 837/1000, batch:   208/ 1052, ite: 219920] train loss: 0.003932, tar: 0.000049 
l0: 0.000052, l1: 0.000051, l2: 0.000066, l3: 0.000124, l4: 0.000392, l5: 0.000881, l6: 0.002337
[epoch: 837/1000, batch:   288/ 1052, ite: 219940] train loss: 0.003932, tar: 0.000049 
l0: 0.000032, l1: 0.000034, l2: 0.000065, l3: 0.000171, l4: 0.000230, l5: 0.000853, l6: 0.001314
[epoch: 837/1000, batch:   368/ 1052, ite: 219960] train loss: 0.003932, tar: 0.000049 
l0: 0.000039, l1: 0.000037, l2: 0.000069, l3: 0.000279, l4: 0.000700, l5: 0.002415, l6: 0.002667
[epoch: 837/1000, batch:   448/ 1052, ite: 219980] train loss: 0.003933, tar: 0.000049 
l0: 0.000017, l1: 0.000017, l2: 0.000047, l3: 0.000098, l4: 0.000246, l5: 0.000885, l6: 0.001361
[epoch: 837/1000, batch:   528/ 1052, ite: 220000] train loss: 0.003932, tar: 0.000049 
l0: 0.000003, l1: 0.000004, l2: 0.000006, l3: 0.000048, l4: 0.000362, l5: 0.000400, l6: 0.000977
[epoch: 837/1000, batch:   608/ 1052, ite: 220020] train loss: 0.003932, tar: 0.000049 
l0: 0.000045, l1: 0.000047, l2: 0.000078, l3: 0.000156, l4: 0.000389, l5: 0.000792, l6: 0.002206
[epoch: 837/1000, batch:   688/ 1052, ite: 220040] train loss: 0.003932, tar: 0.000049 
l0: 0.000037, l1: 0.000036, l2: 0.000063, l3: 0.000153, l4: 0.000442, l5: 0.000919, l6: 0.001965
[epoch: 837/1000, batch:   768/ 1052, ite: 220060] train loss: 0.003932, tar: 0.000049 
l0: 0.000069, l1: 0.000069, l2: 0.000111, l3: 0.000234, l4: 0.000512, l5: 0.001101, l6: 0.001768
[epoch: 837/1000, batch:   848/ 1052, ite: 220080] train loss: 0.003931, tar: 0.000049 
l0: 0.000045, l1: 0.000047, l2: 0.000057, l3: 0.000103, l4: 0.000251, l5: 0.000660, l6: 0.000829
[epoch: 837/1000, batch:   928/ 1052, ite: 220100] train loss: 0.003930, tar: 0.000049 
l0: 0.000022, l1: 0.000023, l2: 0.000026, l3: 0.000058, l4: 0.000252, l5: 0.000707, l6: 0.001639
[epoch: 837/1000, batch:  1008/ 1052, ite: 220120] train loss: 0.003930, tar: 0.000049 
[Epoch 837/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000025, l1: 0.000028, l2: 0.000048, l3: 0.000126, l4: 0.000413, l5: 0.001450, l6: 0.002320
[epoch: 838/1000, batch:    36/ 1052, ite: 220140] train loss: 0.003930, tar: 0.000049 
l0: 0.000006, l1: 0.000006, l2: 0.000012, l3: 0.000056, l4: 0.000298, l5: 0.000691, l6: 0.001327
[epoch: 838/1000, batch:   116/ 1052, ite: 220160] train loss: 0.003930, tar: 0.000049 
l0: 0.000001, l1: 0.000001, l2: 0.000010, l3: 0.000070, l4: 0.000252, l5: 0.000652, l6: 0.000963
[epoch: 838/1000, batch:   196/ 1052, ite: 220180] train loss: 0.003930, tar: 0.000049 
l0: 0.000072, l1: 0.000073, l2: 0.000098, l3: 0.000276, l4: 0.000787, l5: 0.001914, l6: 0.002569
[epoch: 838/1000, batch:   276/ 1052, ite: 220200] train loss: 0.003930, tar: 0.000049 
l0: 0.000040, l1: 0.000042, l2: 0.000051, l3: 0.000110, l4: 0.000254, l5: 0.000618, l6: 0.001606
[epoch: 838/1000, batch:   356/ 1052, ite: 220220] train loss: 0.003929, tar: 0.000049 
l0: 0.000016, l1: 0.000016, l2: 0.000023, l3: 0.000055, l4: 0.000152, l5: 0.000338, l6: 0.001165
[epoch: 838/1000, batch:   436/ 1052, ite: 220240] train loss: 0.003928, tar: 0.000049 
l0: 0.000033, l1: 0.000030, l2: 0.000152, l3: 0.000369, l4: 0.000494, l5: 0.002022, l6: 0.003336
[epoch: 838/1000, batch:   516/ 1052, ite: 220260] train loss: 0.003929, tar: 0.000049 
l0: 0.000027, l1: 0.000027, l2: 0.000054, l3: 0.000173, l4: 0.000343, l5: 0.001117, l6: 0.002252
[epoch: 838/1000, batch:   596/ 1052, ite: 220280] train loss: 0.003930, tar: 0.000049 
l0: 0.000003, l1: 0.000004, l2: 0.000012, l3: 0.000101, l4: 0.000371, l5: 0.000693, l6: 0.001651
[epoch: 838/1000, batch:   676/ 1052, ite: 220300] train loss: 0.003930, tar: 0.000049 
l0: 0.000008, l1: 0.000008, l2: 0.000042, l3: 0.000129, l4: 0.000401, l5: 0.000806, l6: 0.002130
[epoch: 838/1000, batch:   756/ 1052, ite: 220320] train loss: 0.003930, tar: 0.000049 
l0: 0.000056, l1: 0.000055, l2: 0.000159, l3: 0.000336, l4: 0.000540, l5: 0.001696, l6: 0.003328
[epoch: 838/1000, batch:   836/ 1052, ite: 220340] train loss: 0.003930, tar: 0.000049 
l0: 0.000017, l1: 0.000017, l2: 0.000044, l3: 0.000123, l4: 0.000251, l5: 0.000893, l6: 0.001674
[epoch: 838/1000, batch:   916/ 1052, ite: 220360] train loss: 0.003929, tar: 0.000049 
l0: 0.000237, l1: 0.000229, l2: 0.000258, l3: 0.000336, l4: 0.000460, l5: 0.000866, l6: 0.000980
[epoch: 838/1000, batch:   996/ 1052, ite: 220380] train loss: 0.003929, tar: 0.000049 
[Epoch 838/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000007, l1: 0.000006, l2: 0.000009, l3: 0.000114, l4: 0.000420, l5: 0.000747, l6: 0.001386
[epoch: 839/1000, batch:    24/ 1052, ite: 220400] train loss: 0.003929, tar: 0.000049 
l0: 0.000054, l1: 0.000055, l2: 0.000088, l3: 0.000347, l4: 0.000661, l5: 0.001129, l6: 0.002785
[epoch: 839/1000, batch:   104/ 1052, ite: 220420] train loss: 0.003930, tar: 0.000049 
l0: 0.000083, l1: 0.000089, l2: 0.000083, l3: 0.000292, l4: 0.000728, l5: 0.001235, l6: 0.001610
[epoch: 839/1000, batch:   184/ 1052, ite: 220440] train loss: 0.003929, tar: 0.000049 
l0: 0.000104, l1: 0.000106, l2: 0.000120, l3: 0.000245, l4: 0.000437, l5: 0.001309, l6: 0.002956
[epoch: 839/1000, batch:   264/ 1052, ite: 220460] train loss: 0.003930, tar: 0.000049 
l0: 0.000050, l1: 0.000050, l2: 0.000073, l3: 0.000183, l4: 0.000527, l5: 0.000708, l6: 0.002186
[epoch: 839/1000, batch:   344/ 1052, ite: 220480] train loss: 0.003930, tar: 0.000049 
l0: 0.000059, l1: 0.000059, l2: 0.000084, l3: 0.000208, l4: 0.000477, l5: 0.001071, l6: 0.001802
[epoch: 839/1000, batch:   424/ 1052, ite: 220500] train loss: 0.003931, tar: 0.000049 
l0: 0.000019, l1: 0.000019, l2: 0.000039, l3: 0.000142, l4: 0.000307, l5: 0.000986, l6: 0.001708
[epoch: 839/1000, batch:   504/ 1052, ite: 220520] train loss: 0.003930, tar: 0.000049 
l0: 0.000025, l1: 0.000025, l2: 0.000064, l3: 0.000187, l4: 0.000341, l5: 0.000740, l6: 0.000859
[epoch: 839/1000, batch:   584/ 1052, ite: 220540] train loss: 0.003929, tar: 0.000049 
l0: 0.000096, l1: 0.000096, l2: 0.000174, l3: 0.000340, l4: 0.000488, l5: 0.001070, l6: 0.002794
[epoch: 839/1000, batch:   664/ 1052, ite: 220560] train loss: 0.003929, tar: 0.000049 
l0: 0.000035, l1: 0.000035, l2: 0.000080, l3: 0.000241, l4: 0.000755, l5: 0.001082, l6: 0.002676
[epoch: 839/1000, batch:   744/ 1052, ite: 220580] train loss: 0.003928, tar: 0.000049 
l0: 0.000134, l1: 0.000136, l2: 0.000163, l3: 0.000200, l4: 0.000545, l5: 0.001692, l6: 0.003737
[epoch: 839/1000, batch:   824/ 1052, ite: 220600] train loss: 0.003928, tar: 0.000049 
l0: 0.000109, l1: 0.000108, l2: 0.000147, l3: 0.000288, l4: 0.000524, l5: 0.001058, l6: 0.001816
[epoch: 839/1000, batch:   904/ 1052, ite: 220620] train loss: 0.003929, tar: 0.000049 
l0: 0.000144, l1: 0.000146, l2: 0.000179, l3: 0.000232, l4: 0.000594, l5: 0.001620, l6: 0.003031
[epoch: 839/1000, batch:   984/ 1052, ite: 220640] train loss: 0.003929, tar: 0.000049 
[Epoch 839/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000084, l1: 0.000087, l2: 0.000125, l3: 0.000245, l4: 0.000685, l5: 0.001250, l6: 0.002895
[epoch: 840/1000, batch:    12/ 1052, ite: 220660] train loss: 0.003928, tar: 0.000049 
l0: 0.000035, l1: 0.000035, l2: 0.000072, l3: 0.000183, l4: 0.000483, l5: 0.001270, l6: 0.002796
[epoch: 840/1000, batch:    92/ 1052, ite: 220680] train loss: 0.003929, tar: 0.000049 
l0: 0.000019, l1: 0.000020, l2: 0.000037, l3: 0.000125, l4: 0.000462, l5: 0.000834, l6: 0.002024
[epoch: 840/1000, batch:   172/ 1052, ite: 220700] train loss: 0.003929, tar: 0.000049 
l0: 0.000087, l1: 0.000090, l2: 0.000108, l3: 0.000205, l4: 0.000451, l5: 0.001646, l6: 0.002687
[epoch: 840/1000, batch:   252/ 1052, ite: 220720] train loss: 0.003929, tar: 0.000049 
l0: 0.000092, l1: 0.000095, l2: 0.000130, l3: 0.000268, l4: 0.000498, l5: 0.001238, l6: 0.003528
[epoch: 840/1000, batch:   332/ 1052, ite: 220740] train loss: 0.003928, tar: 0.000049 
l0: 0.000014, l1: 0.000012, l2: 0.000080, l3: 0.000218, l4: 0.000579, l5: 0.001277, l6: 0.002163
[epoch: 840/1000, batch:   412/ 1052, ite: 220760] train loss: 0.003928, tar: 0.000049 
l0: 0.000038, l1: 0.000039, l2: 0.000060, l3: 0.000187, l4: 0.000485, l5: 0.001475, l6: 0.002360
[epoch: 840/1000, batch:   492/ 1052, ite: 220780] train loss: 0.003929, tar: 0.000049 
l0: 0.000058, l1: 0.000057, l2: 0.000108, l3: 0.000228, l4: 0.000469, l5: 0.001663, l6: 0.001851
[epoch: 840/1000, batch:   572/ 1052, ite: 220800] train loss: 0.003928, tar: 0.000049 
l0: 0.000013, l1: 0.000012, l2: 0.000047, l3: 0.000119, l4: 0.000332, l5: 0.001478, l6: 0.002365
[epoch: 840/1000, batch:   652/ 1052, ite: 220820] train loss: 0.003929, tar: 0.000049 
l0: 0.000108, l1: 0.000121, l2: 0.000100, l3: 0.000258, l4: 0.000609, l5: 0.001346, l6: 0.003329
[epoch: 840/1000, batch:   732/ 1052, ite: 220840] train loss: 0.003929, tar: 0.000049 
l0: 0.000002, l1: 0.000002, l2: 0.000014, l3: 0.000094, l4: 0.000264, l5: 0.000712, l6: 0.001689
[epoch: 840/1000, batch:   812/ 1052, ite: 220860] train loss: 0.003928, tar: 0.000049 
l0: 0.000043, l1: 0.000043, l2: 0.000107, l3: 0.000241, l4: 0.000520, l5: 0.001133, l6: 0.003284
[epoch: 840/1000, batch:   892/ 1052, ite: 220880] train loss: 0.003929, tar: 0.000049 
l0: 0.000088, l1: 0.000090, l2: 0.000141, l3: 0.000291, l4: 0.000585, l5: 0.001957, l6: 0.002586
[epoch: 840/1000, batch:   972/ 1052, ite: 220900] train loss: 0.003929, tar: 0.000049 
l0: 0.000024, l1: 0.000025, l2: 0.000056, l3: 0.000120, l4: 0.000290, l5: 0.000933, l6: 0.001226
[epoch: 840/1000, batch:  1052/ 1052, ite: 220920] train loss: 0.003928, tar: 0.000048 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_220920.pkl
[Epoch 840/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000014, l1: 0.000013, l2: 0.000031, l3: 0.000119, l4: 0.000342, l5: 0.000915, l6: 0.001285
[epoch: 841/1000, batch:    80/ 1052, ite: 220940] train loss: 0.003925, tar: 0.000048 
l0: 0.000064, l1: 0.000066, l2: 0.000109, l3: 0.000199, l4: 0.000387, l5: 0.000649, l6: 0.001514
[epoch: 841/1000, batch:   160/ 1052, ite: 220960] train loss: 0.003867, tar: 0.000054 
l0: 0.000120, l1: 0.000123, l2: 0.000175, l3: 0.000342, l4: 0.000698, l5: 0.002242, l6: 0.004011
[epoch: 841/1000, batch:   240/ 1052, ite: 220980] train loss: 0.004066, tar: 0.000055 
l0: 0.000092, l1: 0.000098, l2: 0.000111, l3: 0.000205, l4: 0.000405, l5: 0.001320, l6: 0.001857
[epoch: 841/1000, batch:   320/ 1052, ite: 221000] train loss: 0.003926, tar: 0.000053 
l0: 0.000006, l1: 0.000006, l2: 0.000042, l3: 0.000121, l4: 0.000355, l5: 0.000609, l6: 0.002257
[epoch: 841/1000, batch:   400/ 1052, ite: 221020] train loss: 0.003879, tar: 0.000050 
l0: 0.000173, l1: 0.000183, l2: 0.000269, l3: 0.000497, l4: 0.000871, l5: 0.002303, l6: 0.004199
[epoch: 841/1000, batch:   480/ 1052, ite: 221040] train loss: 0.003855, tar: 0.000053 
l0: 0.000019, l1: 0.000019, l2: 0.000030, l3: 0.000106, l4: 0.000348, l5: 0.001106, l6: 0.002295
[epoch: 841/1000, batch:   560/ 1052, ite: 221060] train loss: 0.003917, tar: 0.000054 
l0: 0.000022, l1: 0.000021, l2: 0.000026, l3: 0.000073, l4: 0.000255, l5: 0.000786, l6: 0.001615
[epoch: 841/1000, batch:   640/ 1052, ite: 221080] train loss: 0.003954, tar: 0.000055 
l0: 0.000076, l1: 0.000077, l2: 0.000107, l3: 0.000215, l4: 0.000713, l5: 0.001488, l6: 0.003008
[epoch: 841/1000, batch:   720/ 1052, ite: 221100] train loss: 0.003945, tar: 0.000054 
l0: 0.000010, l1: 0.000010, l2: 0.000030, l3: 0.000130, l4: 0.000162, l5: 0.000616, l6: 0.001106
[epoch: 841/1000, batch:   800/ 1052, ite: 221120] train loss: 0.003936, tar: 0.000052 
l0: 0.000002, l1: 0.000002, l2: 0.000021, l3: 0.000155, l4: 0.000325, l5: 0.000945, l6: 0.001512
[epoch: 841/1000, batch:   880/ 1052, ite: 221140] train loss: 0.003946, tar: 0.000051 
l0: 0.000005, l1: 0.000004, l2: 0.000027, l3: 0.000143, l4: 0.000299, l5: 0.000734, l6: 0.001754
[epoch: 841/1000, batch:   960/ 1052, ite: 221160] train loss: 0.003899, tar: 0.000049 
l0: 0.000018, l1: 0.000016, l2: 0.000052, l3: 0.000154, l4: 0.000255, l5: 0.000711, l6: 0.001316
[epoch: 841/1000, batch:  1040/ 1052, ite: 221180] train loss: 0.003902, tar: 0.000049 
[Epoch 841/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000137, l1: 0.000143, l2: 0.000164, l3: 0.000195, l4: 0.000398, l5: 0.001300, l6: 0.001602
[epoch: 842/1000, batch:    68/ 1052, ite: 221200] train loss: 0.003928, tar: 0.000050 
l0: 0.000032, l1: 0.000032, l2: 0.000053, l3: 0.000105, l4: 0.000364, l5: 0.000815, l6: 0.001316
[epoch: 842/1000, batch:   148/ 1052, ite: 221220] train loss: 0.003911, tar: 0.000049 
l0: 0.000031, l1: 0.000031, l2: 0.000055, l3: 0.000117, l4: 0.000381, l5: 0.001152, l6: 0.001844
[epoch: 842/1000, batch:   228/ 1052, ite: 221240] train loss: 0.003907, tar: 0.000049 
l0: 0.000041, l1: 0.000040, l2: 0.000056, l3: 0.000094, l4: 0.000359, l5: 0.000444, l6: 0.001384
[epoch: 842/1000, batch:   308/ 1052, ite: 221260] train loss: 0.003888, tar: 0.000048 
l0: 0.000013, l1: 0.000011, l2: 0.000047, l3: 0.000127, l4: 0.000429, l5: 0.001004, l6: 0.002249
[epoch: 842/1000, batch:   388/ 1052, ite: 221280] train loss: 0.003888, tar: 0.000048 
l0: 0.000029, l1: 0.000028, l2: 0.000059, l3: 0.000156, l4: 0.000448, l5: 0.001043, l6: 0.001645
[epoch: 842/1000, batch:   468/ 1052, ite: 221300] train loss: 0.003893, tar: 0.000047 
l0: 0.000076, l1: 0.000077, l2: 0.000105, l3: 0.000238, l4: 0.000737, l5: 0.000858, l6: 0.001837
[epoch: 842/1000, batch:   548/ 1052, ite: 221320] train loss: 0.003907, tar: 0.000048 
l0: 0.000007, l1: 0.000007, l2: 0.000035, l3: 0.000082, l4: 0.000263, l5: 0.001004, l6: 0.001317
[epoch: 842/1000, batch:   628/ 1052, ite: 221340] train loss: 0.003914, tar: 0.000047 
l0: 0.000003, l1: 0.000004, l2: 0.000020, l3: 0.000109, l4: 0.000303, l5: 0.000788, l6: 0.001240
[epoch: 842/1000, batch:   708/ 1052, ite: 221360] train loss: 0.003895, tar: 0.000047 
l0: 0.000025, l1: 0.000023, l2: 0.000067, l3: 0.000242, l4: 0.000719, l5: 0.001542, l6: 0.002341
[epoch: 842/1000, batch:   788/ 1052, ite: 221380] train loss: 0.003899, tar: 0.000047 
l0: 0.000032, l1: 0.000031, l2: 0.000055, l3: 0.000120, l4: 0.000410, l5: 0.001332, l6: 0.002548
[epoch: 842/1000, batch:   868/ 1052, ite: 221400] train loss: 0.003903, tar: 0.000046 
l0: 0.000040, l1: 0.000039, l2: 0.000067, l3: 0.000152, l4: 0.000358, l5: 0.000647, l6: 0.001315
[epoch: 842/1000, batch:   948/ 1052, ite: 221420] train loss: 0.003887, tar: 0.000046 
l0: 0.000133, l1: 0.000139, l2: 0.000159, l3: 0.000238, l4: 0.000429, l5: 0.000877, l6: 0.001987
[epoch: 842/1000, batch:  1028/ 1052, ite: 221440] train loss: 0.003892, tar: 0.000047 
[Epoch 842/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000116, l1: 0.000118, l2: 0.000196, l3: 0.000301, l4: 0.000425, l5: 0.001255, l6: 0.002760
[epoch: 843/1000, batch:    56/ 1052, ite: 221460] train loss: 0.003876, tar: 0.000047 
l0: 0.000029, l1: 0.000030, l2: 0.000044, l3: 0.000109, l4: 0.000249, l5: 0.001057, l6: 0.001461
[epoch: 843/1000, batch:   136/ 1052, ite: 221480] train loss: 0.003876, tar: 0.000046 
l0: 0.000067, l1: 0.000068, l2: 0.000092, l3: 0.000286, l4: 0.000573, l5: 0.001505, l6: 0.002238
[epoch: 843/1000, batch:   216/ 1052, ite: 221500] train loss: 0.003867, tar: 0.000046 
l0: 0.000027, l1: 0.000027, l2: 0.000039, l3: 0.000103, l4: 0.000181, l5: 0.000673, l6: 0.001278
[epoch: 843/1000, batch:   296/ 1052, ite: 221520] train loss: 0.003870, tar: 0.000046 
l0: 0.000017, l1: 0.000017, l2: 0.000040, l3: 0.000129, l4: 0.000413, l5: 0.001175, l6: 0.002900
[epoch: 843/1000, batch:   376/ 1052, ite: 221540] train loss: 0.003876, tar: 0.000047 
l0: 0.000057, l1: 0.000062, l2: 0.000135, l3: 0.000199, l4: 0.000528, l5: 0.001148, l6: 0.003092
[epoch: 843/1000, batch:   456/ 1052, ite: 221560] train loss: 0.003881, tar: 0.000047 
l0: 0.000027, l1: 0.000025, l2: 0.000143, l3: 0.000303, l4: 0.000598, l5: 0.001126, l6: 0.003482
[epoch: 843/1000, batch:   536/ 1052, ite: 221580] train loss: 0.003891, tar: 0.000047 
l0: 0.000022, l1: 0.000022, l2: 0.000040, l3: 0.000099, l4: 0.000283, l5: 0.000656, l6: 0.001442
[epoch: 843/1000, batch:   616/ 1052, ite: 221600] train loss: 0.003876, tar: 0.000046 
l0: 0.000146, l1: 0.000148, l2: 0.000175, l3: 0.000368, l4: 0.000566, l5: 0.001575, l6: 0.002958
[epoch: 843/1000, batch:   696/ 1052, ite: 221620] train loss: 0.003884, tar: 0.000046 
l0: 0.000036, l1: 0.000036, l2: 0.000080, l3: 0.000177, l4: 0.000291, l5: 0.000974, l6: 0.000938
[epoch: 843/1000, batch:   776/ 1052, ite: 221640] train loss: 0.003866, tar: 0.000046 
l0: 0.000049, l1: 0.000048, l2: 0.000083, l3: 0.000135, l4: 0.000652, l5: 0.001023, l6: 0.001712
[epoch: 843/1000, batch:   856/ 1052, ite: 221660] train loss: 0.003887, tar: 0.000046 
l0: 0.000328, l1: 0.000330, l2: 0.000340, l3: 0.000431, l4: 0.000577, l5: 0.001254, l6: 0.002748
[epoch: 843/1000, batch:   936/ 1052, ite: 221680] train loss: 0.003892, tar: 0.000047 
l0: 0.000057, l1: 0.000057, l2: 0.000075, l3: 0.000183, l4: 0.000482, l5: 0.001387, l6: 0.002559
[epoch: 843/1000, batch:  1016/ 1052, ite: 221700] train loss: 0.003898, tar: 0.000047 
[Epoch 843/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000014, l1: 0.000015, l2: 0.000051, l3: 0.000161, l4: 0.000433, l5: 0.000947, l6: 0.002062
[epoch: 844/1000, batch:    44/ 1052, ite: 221720] train loss: 0.003893, tar: 0.000047 
l0: 0.000017, l1: 0.000016, l2: 0.000028, l3: 0.000081, l4: 0.000279, l5: 0.000656, l6: 0.001058
[epoch: 844/1000, batch:   124/ 1052, ite: 221740] train loss: 0.003896, tar: 0.000046 
l0: 0.000007, l1: 0.000007, l2: 0.000031, l3: 0.000135, l4: 0.000373, l5: 0.000752, l6: 0.001175
[epoch: 844/1000, batch:   204/ 1052, ite: 221760] train loss: 0.003891, tar: 0.000046 
l0: 0.000087, l1: 0.000088, l2: 0.000107, l3: 0.000224, l4: 0.000755, l5: 0.001245, l6: 0.002285
[epoch: 844/1000, batch:   284/ 1052, ite: 221780] train loss: 0.003891, tar: 0.000046 
l0: 0.000009, l1: 0.000009, l2: 0.000041, l3: 0.000231, l4: 0.000411, l5: 0.000990, l6: 0.001717
[epoch: 844/1000, batch:   364/ 1052, ite: 221800] train loss: 0.003903, tar: 0.000046 
l0: 0.000024, l1: 0.000025, l2: 0.000036, l3: 0.000086, l4: 0.000347, l5: 0.000404, l6: 0.001152
[epoch: 844/1000, batch:   444/ 1052, ite: 221820] train loss: 0.003905, tar: 0.000046 
l0: 0.000009, l1: 0.000011, l2: 0.000035, l3: 0.000070, l4: 0.000188, l5: 0.000525, l6: 0.001766
[epoch: 844/1000, batch:   524/ 1052, ite: 221840] train loss: 0.003905, tar: 0.000046 
l0: 0.000326, l1: 0.000310, l2: 0.000348, l3: 0.000433, l4: 0.000544, l5: 0.000833, l6: 0.002237
[epoch: 844/1000, batch:   604/ 1052, ite: 221860] train loss: 0.003902, tar: 0.000046 
l0: 0.000008, l1: 0.000008, l2: 0.000044, l3: 0.000152, l4: 0.000395, l5: 0.000954, l6: 0.001972
[epoch: 844/1000, batch:   684/ 1052, ite: 221880] train loss: 0.003904, tar: 0.000046 
l0: 0.000003, l1: 0.000003, l2: 0.000010, l3: 0.000041, l4: 0.000138, l5: 0.000791, l6: 0.001267
[epoch: 844/1000, batch:   764/ 1052, ite: 221900] train loss: 0.003904, tar: 0.000047 
l0: 0.000074, l1: 0.000074, l2: 0.000111, l3: 0.000242, l4: 0.000618, l5: 0.001259, l6: 0.002679
[epoch: 844/1000, batch:   844/ 1052, ite: 221920] train loss: 0.003907, tar: 0.000047 
l0: 0.000070, l1: 0.000070, l2: 0.000095, l3: 0.000159, l4: 0.000428, l5: 0.001013, l6: 0.001949
[epoch: 844/1000, batch:   924/ 1052, ite: 221940] train loss: 0.003904, tar: 0.000047 
l0: 0.000006, l1: 0.000006, l2: 0.000020, l3: 0.000071, l4: 0.000276, l5: 0.000658, l6: 0.001175
[epoch: 844/1000, batch:  1004/ 1052, ite: 221960] train loss: 0.003899, tar: 0.000046 
[Epoch 844/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000028, l1: 0.000028, l2: 0.000049, l3: 0.000109, l4: 0.000195, l5: 0.000606, l6: 0.001548
[epoch: 845/1000, batch:    32/ 1052, ite: 221980] train loss: 0.003900, tar: 0.000046 
l0: 0.000061, l1: 0.000060, l2: 0.000073, l3: 0.000138, l4: 0.000380, l5: 0.001091, l6: 0.001181
[epoch: 845/1000, batch:   112/ 1052, ite: 222000] train loss: 0.003914, tar: 0.000046 
l0: 0.000006, l1: 0.000006, l2: 0.000002, l3: 0.000064, l4: 0.000238, l5: 0.000510, l6: 0.000940
[epoch: 845/1000, batch:   192/ 1052, ite: 222020] train loss: 0.003909, tar: 0.000046 
l0: 0.000067, l1: 0.000070, l2: 0.000121, l3: 0.000333, l4: 0.000756, l5: 0.002492, l6: 0.004866
[epoch: 845/1000, batch:   272/ 1052, ite: 222040] train loss: 0.003901, tar: 0.000046 
l0: 0.000023, l1: 0.000021, l2: 0.000075, l3: 0.000206, l4: 0.000273, l5: 0.001057, l6: 0.001647
[epoch: 845/1000, batch:   352/ 1052, ite: 222060] train loss: 0.003902, tar: 0.000046 
l0: 0.000036, l1: 0.000037, l2: 0.000067, l3: 0.000222, l4: 0.000409, l5: 0.002213, l6: 0.003263
[epoch: 845/1000, batch:   432/ 1052, ite: 222080] train loss: 0.003904, tar: 0.000046 
l0: 0.000032, l1: 0.000033, l2: 0.000055, l3: 0.000158, l4: 0.000561, l5: 0.001185, l6: 0.001790
[epoch: 845/1000, batch:   512/ 1052, ite: 222100] train loss: 0.003911, tar: 0.000046 
l0: 0.000180, l1: 0.000181, l2: 0.000234, l3: 0.000441, l4: 0.000886, l5: 0.001770, l6: 0.003038
[epoch: 845/1000, batch:   592/ 1052, ite: 222120] train loss: 0.003907, tar: 0.000046 
l0: 0.000077, l1: 0.000078, l2: 0.000107, l3: 0.000188, l4: 0.000399, l5: 0.000830, l6: 0.001633
[epoch: 845/1000, batch:   672/ 1052, ite: 222140] train loss: 0.003904, tar: 0.000046 
l0: 0.000021, l1: 0.000022, l2: 0.000034, l3: 0.000148, l4: 0.000429, l5: 0.001363, l6: 0.001952
[epoch: 845/1000, batch:   752/ 1052, ite: 222160] train loss: 0.003901, tar: 0.000046 
l0: 0.000030, l1: 0.000030, l2: 0.000085, l3: 0.000204, l4: 0.000373, l5: 0.000696, l6: 0.001464
[epoch: 845/1000, batch:   832/ 1052, ite: 222180] train loss: 0.003901, tar: 0.000046 
l0: 0.000034, l1: 0.000033, l2: 0.000069, l3: 0.000144, l4: 0.000470, l5: 0.000708, l6: 0.001230
[epoch: 845/1000, batch:   912/ 1052, ite: 222200] train loss: 0.003902, tar: 0.000046 
l0: 0.000014, l1: 0.000012, l2: 0.000117, l3: 0.000424, l4: 0.001143, l5: 0.001872, l6: 0.003341
[epoch: 845/1000, batch:   992/ 1052, ite: 222220] train loss: 0.003910, tar: 0.000046 
[Epoch 845/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000010, l1: 0.000010, l2: 0.000028, l3: 0.000110, l4: 0.000354, l5: 0.000961, l6: 0.002265
[epoch: 846/1000, batch:    20/ 1052, ite: 222240] train loss: 0.003908, tar: 0.000046 
l0: 0.000033, l1: 0.000033, l2: 0.000048, l3: 0.000145, l4: 0.000407, l5: 0.000735, l6: 0.001352
[epoch: 846/1000, batch:   100/ 1052, ite: 222260] train loss: 0.003902, tar: 0.000046 
l0: 0.000007, l1: 0.000006, l2: 0.000046, l3: 0.000137, l4: 0.000260, l5: 0.001196, l6: 0.001646
[epoch: 846/1000, batch:   180/ 1052, ite: 222280] train loss: 0.003899, tar: 0.000045 
l0: 0.000047, l1: 0.000043, l2: 0.000171, l3: 0.000276, l4: 0.000630, l5: 0.001641, l6: 0.004719
[epoch: 846/1000, batch:   260/ 1052, ite: 222300] train loss: 0.003905, tar: 0.000046 
l0: 0.000021, l1: 0.000022, l2: 0.000046, l3: 0.000195, l4: 0.000373, l5: 0.000987, l6: 0.001599
[epoch: 846/1000, batch:   340/ 1052, ite: 222320] train loss: 0.003908, tar: 0.000046 
l0: 0.000025, l1: 0.000025, l2: 0.000061, l3: 0.000140, l4: 0.000460, l5: 0.001155, l6: 0.001810
[epoch: 846/1000, batch:   420/ 1052, ite: 222340] train loss: 0.003915, tar: 0.000046 
l0: 0.000079, l1: 0.000081, l2: 0.000103, l3: 0.000186, l4: 0.000586, l5: 0.001393, l6: 0.002499
[epoch: 846/1000, batch:   500/ 1052, ite: 222360] train loss: 0.003923, tar: 0.000046 
l0: 0.000032, l1: 0.000032, l2: 0.000043, l3: 0.000081, l4: 0.000197, l5: 0.000890, l6: 0.001523
[epoch: 846/1000, batch:   580/ 1052, ite: 222380] train loss: 0.003923, tar: 0.000045 
l0: 0.000049, l1: 0.000051, l2: 0.000066, l3: 0.000110, l4: 0.000302, l5: 0.000604, l6: 0.001218
[epoch: 846/1000, batch:   660/ 1052, ite: 222400] train loss: 0.003913, tar: 0.000045 
l0: 0.000001, l1: 0.000001, l2: 0.000010, l3: 0.000086, l4: 0.000281, l5: 0.000466, l6: 0.001033
[epoch: 846/1000, batch:   740/ 1052, ite: 222420] train loss: 0.003918, tar: 0.000046 
l0: 0.000052, l1: 0.000049, l2: 0.000083, l3: 0.000184, l4: 0.000311, l5: 0.000898, l6: 0.001705
[epoch: 846/1000, batch:   820/ 1052, ite: 222440] train loss: 0.003918, tar: 0.000045 
l0: 0.000035, l1: 0.000036, l2: 0.000065, l3: 0.000311, l4: 0.000864, l5: 0.001464, l6: 0.002521
[epoch: 846/1000, batch:   900/ 1052, ite: 222460] train loss: 0.003919, tar: 0.000045 
l0: 0.000019, l1: 0.000019, l2: 0.000051, l3: 0.000084, l4: 0.000336, l5: 0.000670, l6: 0.001949
[epoch: 846/1000, batch:   980/ 1052, ite: 222480] train loss: 0.003918, tar: 0.000045 
[Epoch 846/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000085, l1: 0.000085, l2: 0.000136, l3: 0.000269, l4: 0.000661, l5: 0.001030, l6: 0.001915
[epoch: 847/1000, batch:     8/ 1052, ite: 222500] train loss: 0.003918, tar: 0.000045 
l0: 0.000064, l1: 0.000062, l2: 0.000130, l3: 0.000336, l4: 0.000535, l5: 0.000795, l6: 0.001455
[epoch: 847/1000, batch:    88/ 1052, ite: 222520] train loss: 0.003917, tar: 0.000045 
l0: 0.000097, l1: 0.000098, l2: 0.000146, l3: 0.000303, l4: 0.000745, l5: 0.001646, l6: 0.002049
[epoch: 847/1000, batch:   168/ 1052, ite: 222540] train loss: 0.003920, tar: 0.000046 
l0: 0.000006, l1: 0.000007, l2: 0.000012, l3: 0.000075, l4: 0.000316, l5: 0.000704, l6: 0.001645
[epoch: 847/1000, batch:   248/ 1052, ite: 222560] train loss: 0.003922, tar: 0.000046 
l0: 0.000039, l1: 0.000040, l2: 0.000070, l3: 0.000161, l4: 0.000340, l5: 0.000852, l6: 0.002267
[epoch: 847/1000, batch:   328/ 1052, ite: 222580] train loss: 0.003917, tar: 0.000046 
l0: 0.000045, l1: 0.000046, l2: 0.000070, l3: 0.000170, l4: 0.000459, l5: 0.000799, l6: 0.001585
[epoch: 847/1000, batch:   408/ 1052, ite: 222600] train loss: 0.003917, tar: 0.000046 
l0: 0.000011, l1: 0.000010, l2: 0.000055, l3: 0.000152, l4: 0.000516, l5: 0.001279, l6: 0.001494
[epoch: 847/1000, batch:   488/ 1052, ite: 222620] train loss: 0.003923, tar: 0.000046 
l0: 0.000006, l1: 0.000006, l2: 0.000023, l3: 0.000116, l4: 0.000328, l5: 0.000830, l6: 0.001662
[epoch: 847/1000, batch:   568/ 1052, ite: 222640] train loss: 0.003923, tar: 0.000046 
l0: 0.000072, l1: 0.000070, l2: 0.000118, l3: 0.000218, l4: 0.000807, l5: 0.001385, l6: 0.002218
[epoch: 847/1000, batch:   648/ 1052, ite: 222660] train loss: 0.003921, tar: 0.000045 
l0: 0.000019, l1: 0.000018, l2: 0.000058, l3: 0.000262, l4: 0.001105, l5: 0.001626, l6: 0.002311
[epoch: 847/1000, batch:   728/ 1052, ite: 222680] train loss: 0.003921, tar: 0.000045 
l0: 0.000005, l1: 0.000004, l2: 0.000031, l3: 0.000108, l4: 0.000286, l5: 0.000846, l6: 0.002030
[epoch: 847/1000, batch:   808/ 1052, ite: 222700] train loss: 0.003921, tar: 0.000045 
l0: 0.000087, l1: 0.000090, l2: 0.000094, l3: 0.000177, l4: 0.000352, l5: 0.000880, l6: 0.001294
[epoch: 847/1000, batch:   888/ 1052, ite: 222720] train loss: 0.003916, tar: 0.000045 
l0: 0.000017, l1: 0.000018, l2: 0.000041, l3: 0.000207, l4: 0.000286, l5: 0.001374, l6: 0.002619
[epoch: 847/1000, batch:   968/ 1052, ite: 222740] train loss: 0.003915, tar: 0.000045 
l0: 0.000055, l1: 0.000055, l2: 0.000128, l3: 0.000206, l4: 0.000625, l5: 0.001318, l6: 0.002184
[epoch: 847/1000, batch:  1048/ 1052, ite: 222760] train loss: 0.003917, tar: 0.000045 
[Epoch 847/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000008, l1: 0.000007, l2: 0.000036, l3: 0.000080, l4: 0.000334, l5: 0.000979, l6: 0.000886
[epoch: 848/1000, batch:    76/ 1052, ite: 222780] train loss: 0.003908, tar: 0.000045 
l0: 0.000005, l1: 0.000005, l2: 0.000020, l3: 0.000070, l4: 0.000493, l5: 0.001030, l6: 0.001567
[epoch: 848/1000, batch:   156/ 1052, ite: 222800] train loss: 0.003907, tar: 0.000045 
l0: 0.000092, l1: 0.000092, l2: 0.000112, l3: 0.000162, l4: 0.000395, l5: 0.000652, l6: 0.001758
[epoch: 848/1000, batch:   236/ 1052, ite: 222820] train loss: 0.003907, tar: 0.000045 
l0: 0.000029, l1: 0.000030, l2: 0.000071, l3: 0.000341, l4: 0.000699, l5: 0.001821, l6: 0.003341
[epoch: 848/1000, batch:   316/ 1052, ite: 222840] train loss: 0.003911, tar: 0.000045 
l0: 0.000007, l1: 0.000007, l2: 0.000024, l3: 0.000070, l4: 0.000258, l5: 0.001062, l6: 0.001266
[epoch: 848/1000, batch:   396/ 1052, ite: 222860] train loss: 0.003913, tar: 0.000045 
l0: 0.000049, l1: 0.000051, l2: 0.000126, l3: 0.000250, l4: 0.000466, l5: 0.001365, l6: 0.003692
[epoch: 848/1000, batch:   476/ 1052, ite: 222880] train loss: 0.003911, tar: 0.000044 
l0: 0.000228, l1: 0.000209, l2: 0.000279, l3: 0.000418, l4: 0.000474, l5: 0.000653, l6: 0.001623
[epoch: 848/1000, batch:   556/ 1052, ite: 222900] train loss: 0.003914, tar: 0.000045 
l0: 0.000023, l1: 0.000022, l2: 0.000091, l3: 0.000325, l4: 0.000755, l5: 0.002044, l6: 0.002618
[epoch: 848/1000, batch:   636/ 1052, ite: 222920] train loss: 0.003915, tar: 0.000045 
l0: 0.000123, l1: 0.000123, l2: 0.000126, l3: 0.000230, l4: 0.000577, l5: 0.001081, l6: 0.002180
[epoch: 848/1000, batch:   716/ 1052, ite: 222940] train loss: 0.003921, tar: 0.000045 
l0: 0.000033, l1: 0.000036, l2: 0.000034, l3: 0.000107, l4: 0.000302, l5: 0.000459, l6: 0.001232
[epoch: 848/1000, batch:   796/ 1052, ite: 222960] train loss: 0.003917, tar: 0.000045 
l0: 0.000007, l1: 0.000008, l2: 0.000011, l3: 0.000034, l4: 0.000292, l5: 0.000805, l6: 0.001150
[epoch: 848/1000, batch:   876/ 1052, ite: 222980] train loss: 0.003910, tar: 0.000045 
l0: 0.000021, l1: 0.000022, l2: 0.000047, l3: 0.000214, l4: 0.000518, l5: 0.001213, l6: 0.002550
[epoch: 848/1000, batch:   956/ 1052, ite: 223000] train loss: 0.003909, tar: 0.000045 
l0: 0.000171, l1: 0.000176, l2: 0.000210, l3: 0.000355, l4: 0.000801, l5: 0.002547, l6: 0.004525
[epoch: 848/1000, batch:  1036/ 1052, ite: 223020] train loss: 0.003915, tar: 0.000045 
[Epoch 848/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000008, l1: 0.000006, l2: 0.000046, l3: 0.000239, l4: 0.000637, l5: 0.001142, l6: 0.002119
[epoch: 849/1000, batch:    64/ 1052, ite: 223040] train loss: 0.003911, tar: 0.000045 
l0: 0.000010, l1: 0.000009, l2: 0.000020, l3: 0.000070, l4: 0.000192, l5: 0.000767, l6: 0.001728
[epoch: 849/1000, batch:   144/ 1052, ite: 223060] train loss: 0.003910, tar: 0.000045 
l0: 0.000027, l1: 0.000024, l2: 0.000067, l3: 0.000167, l4: 0.000485, l5: 0.001052, l6: 0.002603
[epoch: 849/1000, batch:   224/ 1052, ite: 223080] train loss: 0.003915, tar: 0.000045 
l0: 0.000028, l1: 0.000027, l2: 0.000053, l3: 0.000098, l4: 0.000507, l5: 0.000704, l6: 0.001402
[epoch: 849/1000, batch:   304/ 1052, ite: 223100] train loss: 0.003913, tar: 0.000045 
l0: 0.000056, l1: 0.000055, l2: 0.000105, l3: 0.000207, l4: 0.000506, l5: 0.001032, l6: 0.001918
[epoch: 849/1000, batch:   384/ 1052, ite: 223120] train loss: 0.003918, tar: 0.000045 
l0: 0.000043, l1: 0.000045, l2: 0.000076, l3: 0.000141, l4: 0.000404, l5: 0.000891, l6: 0.001614
[epoch: 849/1000, batch:   464/ 1052, ite: 223140] train loss: 0.003919, tar: 0.000045 
l0: 0.000015, l1: 0.000015, l2: 0.000039, l3: 0.000121, l4: 0.000250, l5: 0.000739, l6: 0.001783
[epoch: 849/1000, batch:   544/ 1052, ite: 223160] train loss: 0.003914, tar: 0.000045 
l0: 0.000010, l1: 0.000011, l2: 0.000018, l3: 0.000109, l4: 0.000373, l5: 0.000920, l6: 0.001519
[epoch: 849/1000, batch:   624/ 1052, ite: 223180] train loss: 0.003913, tar: 0.000045 
l0: 0.000012, l1: 0.000012, l2: 0.000015, l3: 0.000038, l4: 0.000234, l5: 0.000379, l6: 0.000479
[epoch: 849/1000, batch:   704/ 1052, ite: 223200] train loss: 0.003910, tar: 0.000045 
l0: 0.000003, l1: 0.000003, l2: 0.000011, l3: 0.000073, l4: 0.000204, l5: 0.000699, l6: 0.001323
[epoch: 849/1000, batch:   784/ 1052, ite: 223220] train loss: 0.003909, tar: 0.000045 
l0: 0.000136, l1: 0.000136, l2: 0.000182, l3: 0.000257, l4: 0.000581, l5: 0.001200, l6: 0.003342
[epoch: 849/1000, batch:   864/ 1052, ite: 223240] train loss: 0.003909, tar: 0.000045 
l0: 0.000004, l1: 0.000004, l2: 0.000011, l3: 0.000152, l4: 0.000451, l5: 0.000882, l6: 0.001308
[epoch: 849/1000, batch:   944/ 1052, ite: 223260] train loss: 0.003909, tar: 0.000045 
l0: 0.000033, l1: 0.000038, l2: 0.000041, l3: 0.000087, l4: 0.000316, l5: 0.000464, l6: 0.001389
[epoch: 849/1000, batch:  1024/ 1052, ite: 223280] train loss: 0.003911, tar: 0.000045 
[Epoch 849/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000054, l1: 0.000054, l2: 0.000086, l3: 0.000242, l4: 0.000475, l5: 0.000769, l6: 0.001392
[epoch: 850/1000, batch:    52/ 1052, ite: 223300] train loss: 0.003909, tar: 0.000045 
l0: 0.000055, l1: 0.000058, l2: 0.000072, l3: 0.000173, l4: 0.000463, l5: 0.001110, l6: 0.001806
[epoch: 850/1000, batch:   132/ 1052, ite: 223320] train loss: 0.003910, tar: 0.000045 
l0: 0.000135, l1: 0.000131, l2: 0.000194, l3: 0.000294, l4: 0.000547, l5: 0.001212, l6: 0.002300
[epoch: 850/1000, batch:   212/ 1052, ite: 223340] train loss: 0.003911, tar: 0.000045 
l0: 0.000044, l1: 0.000041, l2: 0.000077, l3: 0.000149, l4: 0.000441, l5: 0.000857, l6: 0.001874
[epoch: 850/1000, batch:   292/ 1052, ite: 223360] train loss: 0.003911, tar: 0.000045 
l0: 0.000009, l1: 0.000009, l2: 0.000033, l3: 0.000209, l4: 0.000608, l5: 0.001281, l6: 0.001562
[epoch: 850/1000, batch:   372/ 1052, ite: 223380] train loss: 0.003912, tar: 0.000045 
l0: 0.000003, l1: 0.000003, l2: 0.000014, l3: 0.000082, l4: 0.000424, l5: 0.001137, l6: 0.002302
[epoch: 850/1000, batch:   452/ 1052, ite: 223400] train loss: 0.003913, tar: 0.000045 
l0: 0.000078, l1: 0.000079, l2: 0.000096, l3: 0.000186, l4: 0.000619, l5: 0.001041, l6: 0.002282
[epoch: 850/1000, batch:   532/ 1052, ite: 223420] train loss: 0.003917, tar: 0.000045 
l0: 0.000048, l1: 0.000048, l2: 0.000083, l3: 0.000190, l4: 0.000290, l5: 0.000490, l6: 0.001058
[epoch: 850/1000, batch:   612/ 1052, ite: 223440] train loss: 0.003917, tar: 0.000045 
l0: 0.000032, l1: 0.000032, l2: 0.000058, l3: 0.000082, l4: 0.000270, l5: 0.000831, l6: 0.001782
[epoch: 850/1000, batch:   692/ 1052, ite: 223460] train loss: 0.003919, tar: 0.000045 
l0: 0.000035, l1: 0.000036, l2: 0.000066, l3: 0.000166, l4: 0.000422, l5: 0.001112, l6: 0.001928
[epoch: 850/1000, batch:   772/ 1052, ite: 223480] train loss: 0.003910, tar: 0.000045 
l0: 0.000012, l1: 0.000014, l2: 0.000006, l3: 0.000057, l4: 0.000241, l5: 0.000447, l6: 0.001457
[epoch: 850/1000, batch:   852/ 1052, ite: 223500] train loss: 0.003911, tar: 0.000045 
l0: 0.000047, l1: 0.000047, l2: 0.000097, l3: 0.000180, l4: 0.000474, l5: 0.001471, l6: 0.003032
[epoch: 850/1000, batch:   932/ 1052, ite: 223520] train loss: 0.003915, tar: 0.000045 
l0: 0.000022, l1: 0.000021, l2: 0.000089, l3: 0.000446, l4: 0.000976, l5: 0.002812, l6: 0.004061
[epoch: 850/1000, batch:  1012/ 1052, ite: 223540] train loss: 0.003917, tar: 0.000045 
[Epoch 850/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000005, l1: 0.000005, l2: 0.000016, l3: 0.000077, l4: 0.000288, l5: 0.000750, l6: 0.001219
[epoch: 851/1000, batch:    40/ 1052, ite: 223560] train loss: 0.003915, tar: 0.000045 
l0: 0.000016, l1: 0.000017, l2: 0.000041, l3: 0.000153, l4: 0.000440, l5: 0.001563, l6: 0.002603
[epoch: 851/1000, batch:   120/ 1052, ite: 223580] train loss: 0.003919, tar: 0.000045 
l0: 0.000021, l1: 0.000023, l2: 0.000027, l3: 0.000033, l4: 0.000189, l5: 0.000382, l6: 0.000649
[epoch: 851/1000, batch:   200/ 1052, ite: 223600] train loss: 0.003918, tar: 0.000045 
l0: 0.000058, l1: 0.000058, l2: 0.000088, l3: 0.000150, l4: 0.000340, l5: 0.000987, l6: 0.001889
[epoch: 851/1000, batch:   280/ 1052, ite: 223620] train loss: 0.003916, tar: 0.000045 
l0: 0.000015, l1: 0.000015, l2: 0.000031, l3: 0.000116, l4: 0.000333, l5: 0.000841, l6: 0.001178
[epoch: 851/1000, batch:   360/ 1052, ite: 223640] train loss: 0.003917, tar: 0.000045 
l0: 0.000115, l1: 0.000117, l2: 0.000158, l3: 0.000300, l4: 0.000598, l5: 0.001503, l6: 0.002599
[epoch: 851/1000, batch:   440/ 1052, ite: 223660] train loss: 0.003917, tar: 0.000045 
l0: 0.000022, l1: 0.000022, l2: 0.000041, l3: 0.000148, l4: 0.000479, l5: 0.000617, l6: 0.001842
[epoch: 851/1000, batch:   520/ 1052, ite: 223680] train loss: 0.003920, tar: 0.000045 
l0: 0.000017, l1: 0.000016, l2: 0.000063, l3: 0.000225, l4: 0.000893, l5: 0.001309, l6: 0.002561
[epoch: 851/1000, batch:   600/ 1052, ite: 223700] train loss: 0.003920, tar: 0.000045 
l0: 0.000072, l1: 0.000073, l2: 0.000089, l3: 0.000165, l4: 0.000498, l5: 0.001119, l6: 0.001307
[epoch: 851/1000, batch:   680/ 1052, ite: 223720] train loss: 0.003925, tar: 0.000045 
l0: 0.000023, l1: 0.000023, l2: 0.000022, l3: 0.000105, l4: 0.000222, l5: 0.000521, l6: 0.000876
[epoch: 851/1000, batch:   760/ 1052, ite: 223740] train loss: 0.003922, tar: 0.000045 
l0: 0.000012, l1: 0.000012, l2: 0.000038, l3: 0.000114, l4: 0.000227, l5: 0.001109, l6: 0.001671
[epoch: 851/1000, batch:   840/ 1052, ite: 223760] train loss: 0.003918, tar: 0.000045 
l0: 0.000076, l1: 0.000078, l2: 0.000074, l3: 0.000154, l4: 0.000412, l5: 0.001203, l6: 0.002337
[epoch: 851/1000, batch:   920/ 1052, ite: 223780] train loss: 0.003919, tar: 0.000045 
l0: 0.000072, l1: 0.000073, l2: 0.000106, l3: 0.000211, l4: 0.000385, l5: 0.000761, l6: 0.002173
[epoch: 851/1000, batch:  1000/ 1052, ite: 223800] train loss: 0.003918, tar: 0.000045 
[Epoch 851/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000006, l1: 0.000006, l2: 0.000018, l3: 0.000093, l4: 0.000287, l5: 0.000691, l6: 0.001465
[epoch: 852/1000, batch:    28/ 1052, ite: 223820] train loss: 0.003918, tar: 0.000045 
l0: 0.000006, l1: 0.000006, l2: 0.000036, l3: 0.000137, l4: 0.000263, l5: 0.001116, l6: 0.001464
[epoch: 852/1000, batch:   108/ 1052, ite: 223840] train loss: 0.003921, tar: 0.000045 
l0: 0.000074, l1: 0.000077, l2: 0.000090, l3: 0.000140, l4: 0.000462, l5: 0.000789, l6: 0.002033
[epoch: 852/1000, batch:   188/ 1052, ite: 223860] train loss: 0.003922, tar: 0.000045 
l0: 0.000013, l1: 0.000013, l2: 0.000028, l3: 0.000076, l4: 0.000243, l5: 0.000676, l6: 0.001093
[epoch: 852/1000, batch:   268/ 1052, ite: 223880] train loss: 0.003919, tar: 0.000045 
l0: 0.000074, l1: 0.000074, l2: 0.000112, l3: 0.000192, l4: 0.000484, l5: 0.001024, l6: 0.002212
[epoch: 852/1000, batch:   348/ 1052, ite: 223900] train loss: 0.003920, tar: 0.000045 
l0: 0.000013, l1: 0.000013, l2: 0.000031, l3: 0.000112, l4: 0.000517, l5: 0.001222, l6: 0.002629
[epoch: 852/1000, batch:   428/ 1052, ite: 223920] train loss: 0.003920, tar: 0.000045 
l0: 0.000094, l1: 0.000098, l2: 0.000127, l3: 0.000302, l4: 0.000594, l5: 0.001602, l6: 0.003101
[epoch: 852/1000, batch:   508/ 1052, ite: 223940] train loss: 0.003918, tar: 0.000045 
l0: 0.000003, l1: 0.000003, l2: 0.000025, l3: 0.000104, l4: 0.000287, l5: 0.000578, l6: 0.001333
[epoch: 852/1000, batch:   588/ 1052, ite: 223960] train loss: 0.003916, tar: 0.000045 
l0: 0.000063, l1: 0.000064, l2: 0.000092, l3: 0.000138, l4: 0.000414, l5: 0.001076, l6: 0.002218
[epoch: 852/1000, batch:   668/ 1052, ite: 223980] train loss: 0.003916, tar: 0.000045 
l0: 0.000047, l1: 0.000046, l2: 0.000066, l3: 0.000151, l4: 0.000339, l5: 0.001012, l6: 0.001283
[epoch: 852/1000, batch:   748/ 1052, ite: 224000] train loss: 0.003914, tar: 0.000045 
l0: 0.000023, l1: 0.000024, l2: 0.000034, l3: 0.000099, l4: 0.000277, l5: 0.000822, l6: 0.001947
[epoch: 852/1000, batch:   828/ 1052, ite: 224020] train loss: 0.003914, tar: 0.000045 
l0: 0.000003, l1: 0.000003, l2: 0.000017, l3: 0.000120, l4: 0.000440, l5: 0.001166, l6: 0.001751
[epoch: 852/1000, batch:   908/ 1052, ite: 224040] train loss: 0.003915, tar: 0.000045 
l0: 0.000027, l1: 0.000028, l2: 0.000035, l3: 0.000085, l4: 0.000289, l5: 0.000603, l6: 0.001122
[epoch: 852/1000, batch:   988/ 1052, ite: 224060] train loss: 0.003915, tar: 0.000045 
[Epoch 852/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000112, l1: 0.000114, l2: 0.000162, l3: 0.000475, l4: 0.000948, l5: 0.002113, l6: 0.004724
[epoch: 853/1000, batch:    16/ 1052, ite: 224080] train loss: 0.003918, tar: 0.000045 
l0: 0.000031, l1: 0.000032, l2: 0.000061, l3: 0.000204, l4: 0.000400, l5: 0.001049, l6: 0.001873
[epoch: 853/1000, batch:    96/ 1052, ite: 224100] train loss: 0.003914, tar: 0.000045 
l0: 0.000033, l1: 0.000033, l2: 0.000046, l3: 0.000151, l4: 0.000282, l5: 0.000810, l6: 0.001329
[epoch: 853/1000, batch:   176/ 1052, ite: 224120] train loss: 0.003913, tar: 0.000045 
l0: 0.000006, l1: 0.000006, l2: 0.000022, l3: 0.000131, l4: 0.000569, l5: 0.000890, l6: 0.001732
[epoch: 853/1000, batch:   256/ 1052, ite: 224140] train loss: 0.003914, tar: 0.000045 
l0: 0.000010, l1: 0.000008, l2: 0.000023, l3: 0.000090, l4: 0.000312, l5: 0.000672, l6: 0.000852
[epoch: 853/1000, batch:   336/ 1052, ite: 224160] train loss: 0.003913, tar: 0.000045 
l0: 0.000042, l1: 0.000044, l2: 0.000132, l3: 0.000326, l4: 0.000790, l5: 0.001792, l6: 0.004454
[epoch: 853/1000, batch:   416/ 1052, ite: 224180] train loss: 0.003915, tar: 0.000045 
l0: 0.000012, l1: 0.000012, l2: 0.000046, l3: 0.000264, l4: 0.000696, l5: 0.001746, l6: 0.002648
[epoch: 853/1000, batch:   496/ 1052, ite: 224200] train loss: 0.003919, tar: 0.000045 
l0: 0.000043, l1: 0.000043, l2: 0.000077, l3: 0.000165, l4: 0.000503, l5: 0.001424, l6: 0.002434
[epoch: 853/1000, batch:   576/ 1052, ite: 224220] train loss: 0.003917, tar: 0.000045 
l0: 0.000044, l1: 0.000044, l2: 0.000083, l3: 0.000237, l4: 0.000547, l5: 0.001089, l6: 0.001849
[epoch: 853/1000, batch:   656/ 1052, ite: 224240] train loss: 0.003916, tar: 0.000045 
l0: 0.000054, l1: 0.000057, l2: 0.000066, l3: 0.000157, l4: 0.000231, l5: 0.000933, l6: 0.000886
[epoch: 853/1000, batch:   736/ 1052, ite: 224260] train loss: 0.003914, tar: 0.000045 
l0: 0.000070, l1: 0.000069, l2: 0.000083, l3: 0.000227, l4: 0.000550, l5: 0.001681, l6: 0.002407
[epoch: 853/1000, batch:   816/ 1052, ite: 224280] train loss: 0.003916, tar: 0.000045 
l0: 0.000105, l1: 0.000107, l2: 0.000122, l3: 0.000245, l4: 0.000527, l5: 0.001591, l6: 0.004215
[epoch: 853/1000, batch:   896/ 1052, ite: 224300] train loss: 0.003915, tar: 0.000045 
l0: 0.000101, l1: 0.000102, l2: 0.000148, l3: 0.000257, l4: 0.000406, l5: 0.001252, l6: 0.002118
[epoch: 853/1000, batch:   976/ 1052, ite: 224320] train loss: 0.003918, tar: 0.000045 
[Epoch 853/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000009, l1: 0.000009, l2: 0.000033, l3: 0.000141, l4: 0.000479, l5: 0.000961, l6: 0.001873
[epoch: 854/1000, batch:     4/ 1052, ite: 224340] train loss: 0.003916, tar: 0.000045 
l0: 0.000030, l1: 0.000030, l2: 0.000033, l3: 0.000094, l4: 0.000374, l5: 0.000622, l6: 0.000608
[epoch: 854/1000, batch:    84/ 1052, ite: 224360] train loss: 0.003915, tar: 0.000045 
l0: 0.000032, l1: 0.000032, l2: 0.000057, l3: 0.000197, l4: 0.000461, l5: 0.000866, l6: 0.001485
[epoch: 854/1000, batch:   164/ 1052, ite: 224380] train loss: 0.003916, tar: 0.000045 
l0: 0.000023, l1: 0.000022, l2: 0.000030, l3: 0.000126, l4: 0.000219, l5: 0.000808, l6: 0.000827
[epoch: 854/1000, batch:   244/ 1052, ite: 224400] train loss: 0.003916, tar: 0.000045 
l0: 0.000023, l1: 0.000025, l2: 0.000043, l3: 0.000112, l4: 0.000357, l5: 0.001032, l6: 0.001472
[epoch: 854/1000, batch:   324/ 1052, ite: 224420] train loss: 0.003915, tar: 0.000045 
l0: 0.000005, l1: 0.000006, l2: 0.000020, l3: 0.000077, l4: 0.000158, l5: 0.000509, l6: 0.000717
[epoch: 854/1000, batch:   404/ 1052, ite: 224440] train loss: 0.003914, tar: 0.000045 
l0: 0.000010, l1: 0.000009, l2: 0.000028, l3: 0.000096, l4: 0.000281, l5: 0.000824, l6: 0.001790
[epoch: 854/1000, batch:   484/ 1052, ite: 224460] train loss: 0.003912, tar: 0.000045 
l0: 0.000024, l1: 0.000024, l2: 0.000043, l3: 0.000093, l4: 0.000501, l5: 0.001124, l6: 0.001623
[epoch: 854/1000, batch:   564/ 1052, ite: 224480] train loss: 0.003913, tar: 0.000045 
l0: 0.000059, l1: 0.000061, l2: 0.000099, l3: 0.000175, l4: 0.000347, l5: 0.000949, l6: 0.001669
[epoch: 854/1000, batch:   644/ 1052, ite: 224500] train loss: 0.003913, tar: 0.000045 
l0: 0.000048, l1: 0.000047, l2: 0.000128, l3: 0.000341, l4: 0.000689, l5: 0.001639, l6: 0.004750
[epoch: 854/1000, batch:   724/ 1052, ite: 224520] train loss: 0.003915, tar: 0.000045 
l0: 0.000075, l1: 0.000076, l2: 0.000135, l3: 0.000196, l4: 0.000482, l5: 0.001643, l6: 0.002653
[epoch: 854/1000, batch:   804/ 1052, ite: 224540] train loss: 0.003915, tar: 0.000045 
l0: 0.000036, l1: 0.000035, l2: 0.000091, l3: 0.000307, l4: 0.000919, l5: 0.001062, l6: 0.002456
[epoch: 854/1000, batch:   884/ 1052, ite: 224560] train loss: 0.003917, tar: 0.000045 
l0: 0.000012, l1: 0.000015, l2: 0.000022, l3: 0.000173, l4: 0.000336, l5: 0.001568, l6: 0.002579
[epoch: 854/1000, batch:   964/ 1052, ite: 224580] train loss: 0.003915, tar: 0.000045 
l0: 0.000088, l1: 0.000085, l2: 0.000121, l3: 0.000213, l4: 0.000587, l5: 0.000664, l6: 0.001365
[epoch: 854/1000, batch:  1044/ 1052, ite: 224600] train loss: 0.003916, tar: 0.000045 
[Epoch 854/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000072, l1: 0.000073, l2: 0.000102, l3: 0.000203, l4: 0.000318, l5: 0.001418, l6: 0.002484
[epoch: 855/1000, batch:    72/ 1052, ite: 224620] train loss: 0.003914, tar: 0.000045 
l0: 0.000083, l1: 0.000083, l2: 0.000111, l3: 0.000219, l4: 0.000431, l5: 0.001062, l6: 0.002672
[epoch: 855/1000, batch:   152/ 1052, ite: 224640] train loss: 0.003912, tar: 0.000045 
l0: 0.000014, l1: 0.000014, l2: 0.000061, l3: 0.000160, l4: 0.000428, l5: 0.001361, l6: 0.002872
[epoch: 855/1000, batch:   232/ 1052, ite: 224660] train loss: 0.003912, tar: 0.000045 
l0: 0.000025, l1: 0.000025, l2: 0.000061, l3: 0.000155, l4: 0.000786, l5: 0.001006, l6: 0.002733
[epoch: 855/1000, batch:   312/ 1052, ite: 224680] train loss: 0.003912, tar: 0.000045 
l0: 0.000009, l1: 0.000008, l2: 0.000035, l3: 0.000160, l4: 0.000519, l5: 0.001524, l6: 0.001654
[epoch: 855/1000, batch:   392/ 1052, ite: 224700] train loss: 0.003912, tar: 0.000045 
l0: 0.000014, l1: 0.000013, l2: 0.000048, l3: 0.000147, l4: 0.000408, l5: 0.001046, l6: 0.003020
[epoch: 855/1000, batch:   472/ 1052, ite: 224720] train loss: 0.003916, tar: 0.000045 
l0: 0.000015, l1: 0.000015, l2: 0.000037, l3: 0.000093, l4: 0.000303, l5: 0.001128, l6: 0.001742
[epoch: 855/1000, batch:   552/ 1052, ite: 224740] train loss: 0.003916, tar: 0.000045 
l0: 0.000032, l1: 0.000032, l2: 0.000039, l3: 0.000056, l4: 0.000163, l5: 0.000393, l6: 0.000700
[epoch: 855/1000, batch:   632/ 1052, ite: 224760] train loss: 0.003913, tar: 0.000045 
l0: 0.000047, l1: 0.000047, l2: 0.000098, l3: 0.000166, l4: 0.000471, l5: 0.000945, l6: 0.001857
[epoch: 855/1000, batch:   712/ 1052, ite: 224780] train loss: 0.003912, tar: 0.000045 
l0: 0.000025, l1: 0.000026, l2: 0.000046, l3: 0.000079, l4: 0.000208, l5: 0.001171, l6: 0.001699
[epoch: 855/1000, batch:   792/ 1052, ite: 224800] train loss: 0.003912, tar: 0.000045 
l0: 0.000089, l1: 0.000096, l2: 0.000083, l3: 0.000205, l4: 0.000453, l5: 0.000940, l6: 0.001842
[epoch: 855/1000, batch:   872/ 1052, ite: 224820] train loss: 0.003912, tar: 0.000045 
l0: 0.000090, l1: 0.000091, l2: 0.000111, l3: 0.000186, l4: 0.000470, l5: 0.001036, l6: 0.002924
[epoch: 855/1000, batch:   952/ 1052, ite: 224840] train loss: 0.003915, tar: 0.000045 
l0: 0.000105, l1: 0.000105, l2: 0.000141, l3: 0.000326, l4: 0.000635, l5: 0.001445, l6: 0.001911
[epoch: 855/1000, batch:  1032/ 1052, ite: 224860] train loss: 0.003917, tar: 0.000045 
[Epoch 855/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000017, l1: 0.000016, l2: 0.000030, l3: 0.000070, l4: 0.000295, l5: 0.000905, l6: 0.001910
[epoch: 856/1000, batch:    60/ 1052, ite: 224880] train loss: 0.003915, tar: 0.000045 
l0: 0.000050, l1: 0.000051, l2: 0.000055, l3: 0.000112, l4: 0.000327, l5: 0.000594, l6: 0.001065
[epoch: 856/1000, batch:   140/ 1052, ite: 224900] train loss: 0.003913, tar: 0.000045 
l0: 0.000022, l1: 0.000022, l2: 0.000034, l3: 0.000105, l4: 0.000110, l5: 0.000540, l6: 0.000574
[epoch: 856/1000, batch:   220/ 1052, ite: 224920] train loss: 0.003912, tar: 0.000045 
l0: 0.000071, l1: 0.000073, l2: 0.000100, l3: 0.000217, l4: 0.000590, l5: 0.001388, l6: 0.001744
[epoch: 856/1000, batch:   300/ 1052, ite: 224940] train loss: 0.003913, tar: 0.000045 
l0: 0.000053, l1: 0.000055, l2: 0.000078, l3: 0.000118, l4: 0.000297, l5: 0.000802, l6: 0.001056
[epoch: 856/1000, batch:   380/ 1052, ite: 224960] train loss: 0.003911, tar: 0.000045 
l0: 0.000143, l1: 0.000146, l2: 0.000172, l3: 0.000310, l4: 0.000662, l5: 0.001721, l6: 0.003943
[epoch: 856/1000, batch:   460/ 1052, ite: 224980] train loss: 0.003912, tar: 0.000045 
l0: 0.000004, l1: 0.000004, l2: 0.000008, l3: 0.000050, l4: 0.000296, l5: 0.000557, l6: 0.000995
[epoch: 856/1000, batch:   540/ 1052, ite: 225000] train loss: 0.003914, tar: 0.000045 
l0: 0.000064, l1: 0.000065, l2: 0.000079, l3: 0.000280, l4: 0.000718, l5: 0.001473, l6: 0.002619
[epoch: 856/1000, batch:   620/ 1052, ite: 225020] train loss: 0.003914, tar: 0.000045 
l0: 0.000080, l1: 0.000082, l2: 0.000133, l3: 0.000375, l4: 0.001104, l5: 0.001783, l6: 0.003451
[epoch: 856/1000, batch:   700/ 1052, ite: 225040] train loss: 0.003914, tar: 0.000045 
l0: 0.000025, l1: 0.000026, l2: 0.000041, l3: 0.000121, l4: 0.000370, l5: 0.000857, l6: 0.002328
[epoch: 856/1000, batch:   780/ 1052, ite: 225060] train loss: 0.003916, tar: 0.000045 
l0: 0.000034, l1: 0.000033, l2: 0.000081, l3: 0.000175, l4: 0.000584, l5: 0.000893, l6: 0.001432
[epoch: 856/1000, batch:   860/ 1052, ite: 225080] train loss: 0.003917, tar: 0.000045 
l0: 0.000029, l1: 0.000026, l2: 0.000088, l3: 0.000287, l4: 0.000541, l5: 0.001329, l6: 0.002278
[epoch: 856/1000, batch:   940/ 1052, ite: 225100] train loss: 0.003916, tar: 0.000045 
l0: 0.000038, l1: 0.000038, l2: 0.000068, l3: 0.000176, l4: 0.000461, l5: 0.001002, l6: 0.002473
[epoch: 856/1000, batch:  1020/ 1052, ite: 225120] train loss: 0.003917, tar: 0.000045 
[Epoch 856/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000034, l1: 0.000033, l2: 0.000039, l3: 0.000128, l4: 0.000267, l5: 0.000669, l6: 0.001052
[epoch: 857/1000, batch:    48/ 1052, ite: 225140] train loss: 0.003916, tar: 0.000045 
l0: 0.000006, l1: 0.000006, l2: 0.000020, l3: 0.000098, l4: 0.000327, l5: 0.000687, l6: 0.001008
[epoch: 857/1000, batch:   128/ 1052, ite: 225160] train loss: 0.003917, tar: 0.000045 
l0: 0.000014, l1: 0.000014, l2: 0.000037, l3: 0.000117, l4: 0.000233, l5: 0.000856, l6: 0.001525
[epoch: 857/1000, batch:   208/ 1052, ite: 225180] train loss: 0.003917, tar: 0.000045 
l0: 0.000004, l1: 0.000005, l2: 0.000012, l3: 0.000023, l4: 0.000125, l5: 0.000421, l6: 0.000534
[epoch: 857/1000, batch:   288/ 1052, ite: 225200] train loss: 0.003916, tar: 0.000045 
l0: 0.000109, l1: 0.000112, l2: 0.000146, l3: 0.000193, l4: 0.000575, l5: 0.000858, l6: 0.002189
[epoch: 857/1000, batch:   368/ 1052, ite: 225220] train loss: 0.003922, tar: 0.000046 
l0: 0.000059, l1: 0.000066, l2: 0.000112, l3: 0.000098, l4: 0.000238, l5: 0.000841, l6: 0.001245
[epoch: 857/1000, batch:   448/ 1052, ite: 225240] train loss: 0.003923, tar: 0.000047 
l0: 0.000113, l1: 0.000108, l2: 0.000165, l3: 0.000217, l4: 0.000270, l5: 0.000835, l6: 0.001179
[epoch: 857/1000, batch:   528/ 1052, ite: 225260] train loss: 0.003934, tar: 0.000049 
l0: 0.000254, l1: 0.000236, l2: 0.000237, l3: 0.000410, l4: 0.000715, l5: 0.001388, l6: 0.001643
[epoch: 857/1000, batch:   608/ 1052, ite: 225280] train loss: 0.003938, tar: 0.000050 
l0: 0.000103, l1: 0.000159, l2: 0.000135, l3: 0.000253, l4: 0.000519, l5: 0.000853, l6: 0.001138
[epoch: 857/1000, batch:   688/ 1052, ite: 225300] train loss: 0.003944, tar: 0.000051 
l0: 0.000123, l1: 0.000116, l2: 0.000123, l3: 0.000273, l4: 0.000344, l5: 0.001138, l6: 0.001918
[epoch: 857/1000, batch:   768/ 1052, ite: 225320] train loss: 0.003951, tar: 0.000052 
l0: 0.000155, l1: 0.000177, l2: 0.000192, l3: 0.000270, l4: 0.000472, l5: 0.001317, l6: 0.001662
[epoch: 857/1000, batch:   848/ 1052, ite: 225340] train loss: 0.003960, tar: 0.000054 
l0: 0.000090, l1: 0.000095, l2: 0.000093, l3: 0.000115, l4: 0.000167, l5: 0.000590, l6: 0.000948
[epoch: 857/1000, batch:   928/ 1052, ite: 225360] train loss: 0.003965, tar: 0.000054 
l0: 0.000068, l1: 0.000063, l2: 0.000085, l3: 0.000166, l4: 0.000503, l5: 0.001003, l6: 0.001642
[epoch: 857/1000, batch:  1008/ 1052, ite: 225380] train loss: 0.003967, tar: 0.000055 
[Epoch 857/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000137, l1: 0.000148, l2: 0.000138, l3: 0.000218, l4: 0.000366, l5: 0.000795, l6: 0.001090
[epoch: 858/1000, batch:    36/ 1052, ite: 225400] train loss: 0.003967, tar: 0.000055 
l0: 0.000062, l1: 0.000058, l2: 0.000095, l3: 0.000139, l4: 0.000417, l5: 0.000788, l6: 0.001593
[epoch: 858/1000, batch:   116/ 1052, ite: 225420] train loss: 0.003967, tar: 0.000055 
l0: 0.000043, l1: 0.000043, l2: 0.000050, l3: 0.000116, l4: 0.000182, l5: 0.000958, l6: 0.001175
[epoch: 858/1000, batch:   196/ 1052, ite: 225440] train loss: 0.003968, tar: 0.000055 
l0: 0.000056, l1: 0.000063, l2: 0.000082, l3: 0.000101, l4: 0.000282, l5: 0.000954, l6: 0.001475
[epoch: 858/1000, batch:   276/ 1052, ite: 225460] train loss: 0.003968, tar: 0.000055 
l0: 0.000224, l1: 0.000243, l2: 0.000233, l3: 0.000283, l4: 0.000635, l5: 0.001372, l6: 0.002983
[epoch: 858/1000, batch:   356/ 1052, ite: 225480] train loss: 0.003970, tar: 0.000056 
l0: 0.000015, l1: 0.000015, l2: 0.000018, l3: 0.000089, l4: 0.000218, l5: 0.000578, l6: 0.001363
[epoch: 858/1000, batch:   436/ 1052, ite: 225500] train loss: 0.003969, tar: 0.000056 
l0: 0.000101, l1: 0.000091, l2: 0.000153, l3: 0.000452, l4: 0.000917, l5: 0.001477, l6: 0.002769
[epoch: 858/1000, batch:   516/ 1052, ite: 225520] train loss: 0.003972, tar: 0.000056 
l0: 0.000022, l1: 0.000021, l2: 0.000038, l3: 0.000153, l4: 0.000264, l5: 0.000703, l6: 0.001256
[epoch: 858/1000, batch:   596/ 1052, ite: 225540] train loss: 0.003974, tar: 0.000056 
l0: 0.000106, l1: 0.000097, l2: 0.000127, l3: 0.000201, l4: 0.000380, l5: 0.000776, l6: 0.000886
[epoch: 858/1000, batch:   676/ 1052, ite: 225560] train loss: 0.003974, tar: 0.000056 
l0: 0.000030, l1: 0.000030, l2: 0.000063, l3: 0.000287, l4: 0.000838, l5: 0.001504, l6: 0.002968
[epoch: 858/1000, batch:   756/ 1052, ite: 225580] train loss: 0.003976, tar: 0.000056 
l0: 0.000015, l1: 0.000013, l2: 0.000052, l3: 0.000094, l4: 0.000394, l5: 0.000787, l6: 0.001838
[epoch: 858/1000, batch:   836/ 1052, ite: 225600] train loss: 0.003976, tar: 0.000056 
l0: 0.000073, l1: 0.000074, l2: 0.000095, l3: 0.000172, l4: 0.000357, l5: 0.000914, l6: 0.001604
[epoch: 858/1000, batch:   916/ 1052, ite: 225620] train loss: 0.003976, tar: 0.000056 
l0: 0.000046, l1: 0.000046, l2: 0.000065, l3: 0.000163, l4: 0.000482, l5: 0.001299, l6: 0.001298
[epoch: 858/1000, batch:   996/ 1052, ite: 225640] train loss: 0.003977, tar: 0.000056 
[Epoch 858/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000050, l1: 0.000050, l2: 0.000062, l3: 0.000124, l4: 0.000343, l5: 0.000726, l6: 0.001343
[epoch: 859/1000, batch:    24/ 1052, ite: 225660] train loss: 0.003975, tar: 0.000056 
l0: 0.000178, l1: 0.000179, l2: 0.000179, l3: 0.000258, l4: 0.000648, l5: 0.001466, l6: 0.002238
[epoch: 859/1000, batch:   104/ 1052, ite: 225680] train loss: 0.003975, tar: 0.000056 
l0: 0.000022, l1: 0.000022, l2: 0.000064, l3: 0.000130, l4: 0.000399, l5: 0.001051, l6: 0.001666
[epoch: 859/1000, batch:   184/ 1052, ite: 225700] train loss: 0.003980, tar: 0.000057 
l0: 0.000136, l1: 0.000134, l2: 0.000160, l3: 0.000241, l4: 0.000524, l5: 0.001202, l6: 0.003036
[epoch: 859/1000, batch:   264/ 1052, ite: 225720] train loss: 0.003983, tar: 0.000057 
l0: 0.000091, l1: 0.000101, l2: 0.000101, l3: 0.000161, l4: 0.000294, l5: 0.001074, l6: 0.001544
[epoch: 859/1000, batch:   344/ 1052, ite: 225740] train loss: 0.003982, tar: 0.000057 
l0: 0.000021, l1: 0.000020, l2: 0.000042, l3: 0.000353, l4: 0.000813, l5: 0.001513, l6: 0.002834
[epoch: 859/1000, batch:   424/ 1052, ite: 225760] train loss: 0.003981, tar: 0.000057 
l0: 0.000102, l1: 0.000102, l2: 0.000134, l3: 0.000253, l4: 0.000662, l5: 0.001418, l6: 0.002496
[epoch: 859/1000, batch:   504/ 1052, ite: 225780] train loss: 0.003981, tar: 0.000057 
l0: 0.000017, l1: 0.000019, l2: 0.000040, l3: 0.000165, l4: 0.000410, l5: 0.001355, l6: 0.002181
[epoch: 859/1000, batch:   584/ 1052, ite: 225800] train loss: 0.003982, tar: 0.000057 
l0: 0.000035, l1: 0.000034, l2: 0.000062, l3: 0.000131, l4: 0.000448, l5: 0.000953, l6: 0.001618
[epoch: 859/1000, batch:   664/ 1052, ite: 225820] train loss: 0.003981, tar: 0.000057 
l0: 0.000014, l1: 0.000014, l2: 0.000037, l3: 0.000115, l4: 0.000367, l5: 0.000953, l6: 0.001732
[epoch: 859/1000, batch:   744/ 1052, ite: 225840] train loss: 0.003977, tar: 0.000057 
l0: 0.000034, l1: 0.000033, l2: 0.000065, l3: 0.000135, l4: 0.000396, l5: 0.001167, l6: 0.001745
[epoch: 859/1000, batch:   824/ 1052, ite: 225860] train loss: 0.003977, tar: 0.000057 
l0: 0.000049, l1: 0.000053, l2: 0.000081, l3: 0.000239, l4: 0.000483, l5: 0.000825, l6: 0.001381
[epoch: 859/1000, batch:   904/ 1052, ite: 225880] train loss: 0.003981, tar: 0.000057 
l0: 0.000095, l1: 0.000099, l2: 0.000101, l3: 0.000188, l4: 0.000302, l5: 0.001305, l6: 0.002256
[epoch: 859/1000, batch:   984/ 1052, ite: 225900] train loss: 0.003980, tar: 0.000057 
[Epoch 859/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000024, l1: 0.000024, l2: 0.000044, l3: 0.000128, l4: 0.000332, l5: 0.000927, l6: 0.002113
[epoch: 860/1000, batch:    12/ 1052, ite: 225920] train loss: 0.003979, tar: 0.000057 
l0: 0.000020, l1: 0.000019, l2: 0.000046, l3: 0.000095, l4: 0.000464, l5: 0.000794, l6: 0.000913
[epoch: 860/1000, batch:    92/ 1052, ite: 225940] train loss: 0.003980, tar: 0.000057 
l0: 0.000013, l1: 0.000010, l2: 0.000022, l3: 0.000132, l4: 0.000333, l5: 0.000870, l6: 0.001768
[epoch: 860/1000, batch:   172/ 1052, ite: 225960] train loss: 0.003981, tar: 0.000057 
l0: 0.000015, l1: 0.000015, l2: 0.000015, l3: 0.000066, l4: 0.000246, l5: 0.000475, l6: 0.001065
[epoch: 860/1000, batch:   252/ 1052, ite: 225980] train loss: 0.003980, tar: 0.000057 
l0: 0.000057, l1: 0.000055, l2: 0.000122, l3: 0.000291, l4: 0.000613, l5: 0.001398, l6: 0.002183
[epoch: 860/1000, batch:   332/ 1052, ite: 226000] train loss: 0.003980, tar: 0.000057 
l0: 0.000042, l1: 0.000045, l2: 0.000046, l3: 0.000096, l4: 0.000264, l5: 0.000652, l6: 0.001432
[epoch: 860/1000, batch:   412/ 1052, ite: 226020] train loss: 0.003981, tar: 0.000057 
l0: 0.000011, l1: 0.000010, l2: 0.000041, l3: 0.000118, l4: 0.000439, l5: 0.001088, l6: 0.000996
[epoch: 860/1000, batch:   492/ 1052, ite: 226040] train loss: 0.003979, tar: 0.000057 
l0: 0.000091, l1: 0.000088, l2: 0.000152, l3: 0.000317, l4: 0.000741, l5: 0.001590, l6: 0.002531
[epoch: 860/1000, batch:   572/ 1052, ite: 226060] train loss: 0.003978, tar: 0.000057 
l0: 0.000024, l1: 0.000021, l2: 0.000064, l3: 0.000270, l4: 0.000570, l5: 0.001037, l6: 0.001631
[epoch: 860/1000, batch:   652/ 1052, ite: 226080] train loss: 0.003978, tar: 0.000057 
l0: 0.000013, l1: 0.000014, l2: 0.000036, l3: 0.000188, l4: 0.000594, l5: 0.001372, l6: 0.002295
[epoch: 860/1000, batch:   732/ 1052, ite: 226100] train loss: 0.003980, tar: 0.000057 
l0: 0.000156, l1: 0.000161, l2: 0.000198, l3: 0.000384, l4: 0.000882, l5: 0.001362, l6: 0.001826
[epoch: 860/1000, batch:   812/ 1052, ite: 226120] train loss: 0.003980, tar: 0.000057 
l0: 0.000013, l1: 0.000012, l2: 0.000037, l3: 0.000092, l4: 0.000354, l5: 0.000856, l6: 0.001178
[epoch: 860/1000, batch:   892/ 1052, ite: 226140] train loss: 0.003980, tar: 0.000057 
l0: 0.000013, l1: 0.000013, l2: 0.000029, l3: 0.000127, l4: 0.000275, l5: 0.001274, l6: 0.002398
[epoch: 860/1000, batch:   972/ 1052, ite: 226160] train loss: 0.003979, tar: 0.000057 
l0: 0.000097, l1: 0.000100, l2: 0.000135, l3: 0.000192, l4: 0.000508, l5: 0.001337, l6: 0.002270
[epoch: 860/1000, batch:  1052/ 1052, ite: 226180] train loss: 0.003979, tar: 0.000057 
[Epoch 860/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000023, l1: 0.000026, l2: 0.000022, l3: 0.000048, l4: 0.000327, l5: 0.000620, l6: 0.001168
[epoch: 861/1000, batch:    80/ 1052, ite: 226200] train loss: 0.003978, tar: 0.000057 
l0: 0.000012, l1: 0.000012, l2: 0.000024, l3: 0.000114, l4: 0.000317, l5: 0.000694, l6: 0.001064
[epoch: 861/1000, batch:   160/ 1052, ite: 226220] train loss: 0.003978, tar: 0.000057 
l0: 0.000078, l1: 0.000078, l2: 0.000137, l3: 0.000310, l4: 0.001264, l5: 0.001553, l6: 0.004656
[epoch: 861/1000, batch:   240/ 1052, ite: 226240] train loss: 0.003977, tar: 0.000057 
l0: 0.000008, l1: 0.000065, l2: 0.000021, l3: 0.000114, l4: 0.000448, l5: 0.001083, l6: 0.001347
[epoch: 861/1000, batch:   320/ 1052, ite: 226260] train loss: 0.003979, tar: 0.000057 
l0: 0.000113, l1: 0.000118, l2: 0.000130, l3: 0.000157, l4: 0.000299, l5: 0.000986, l6: 0.002682
[epoch: 861/1000, batch:   400/ 1052, ite: 226280] train loss: 0.003980, tar: 0.000057 
l0: 0.000009, l1: 0.000010, l2: 0.000013, l3: 0.000048, l4: 0.000230, l5: 0.000569, l6: 0.000954
[epoch: 861/1000, batch:   480/ 1052, ite: 226300] train loss: 0.003980, tar: 0.000057 
l0: 0.000180, l1: 0.000181, l2: 0.000226, l3: 0.000385, l4: 0.000678, l5: 0.001782, l6: 0.002991
[epoch: 861/1000, batch:   560/ 1052, ite: 226320] train loss: 0.003981, tar: 0.000057 
l0: 0.000116, l1: 0.000127, l2: 0.000140, l3: 0.000297, l4: 0.000479, l5: 0.001532, l6: 0.002922
[epoch: 861/1000, batch:   640/ 1052, ite: 226340] train loss: 0.003981, tar: 0.000057 
l0: 0.000013, l1: 0.000017, l2: 0.000042, l3: 0.000118, l4: 0.000239, l5: 0.000918, l6: 0.002149
[epoch: 861/1000, batch:   720/ 1052, ite: 226360] train loss: 0.003982, tar: 0.000057 
l0: 0.000016, l1: 0.000022, l2: 0.000034, l3: 0.000122, l4: 0.000216, l5: 0.000899, l6: 0.001690
[epoch: 861/1000, batch:   800/ 1052, ite: 226380] train loss: 0.003980, tar: 0.000057 
l0: 0.000071, l1: 0.000065, l2: 0.000176, l3: 0.000377, l4: 0.000893, l5: 0.001564, l6: 0.002264
[epoch: 861/1000, batch:   880/ 1052, ite: 226400] train loss: 0.003979, tar: 0.000057 
l0: 0.000090, l1: 0.000089, l2: 0.000112, l3: 0.000222, l4: 0.000430, l5: 0.000746, l6: 0.002094
[epoch: 861/1000, batch:   960/ 1052, ite: 226420] train loss: 0.003980, tar: 0.000057 
l0: 0.000027, l1: 0.000029, l2: 0.000058, l3: 0.000143, l4: 0.000462, l5: 0.000955, l6: 0.002641
[epoch: 861/1000, batch:  1040/ 1052, ite: 226440] train loss: 0.003980, tar: 0.000057 
[Epoch 861/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000043, l1: 0.000043, l2: 0.000083, l3: 0.000366, l4: 0.000753, l5: 0.001581, l6: 0.003433
[epoch: 862/1000, batch:    68/ 1052, ite: 226460] train loss: 0.003980, tar: 0.000057 
l0: 0.000124, l1: 0.000129, l2: 0.000210, l3: 0.000265, l4: 0.000453, l5: 0.000723, l6: 0.001136
[epoch: 862/1000, batch:   148/ 1052, ite: 226480] train loss: 0.003982, tar: 0.000057 
l0: 0.000073, l1: 0.000071, l2: 0.000108, l3: 0.000287, l4: 0.000522, l5: 0.001351, l6: 0.001660
[epoch: 862/1000, batch:   228/ 1052, ite: 226500] train loss: 0.003981, tar: 0.000057 
l0: 0.000101, l1: 0.000102, l2: 0.000109, l3: 0.000218, l4: 0.000486, l5: 0.001586, l6: 0.001909
[epoch: 862/1000, batch:   308/ 1052, ite: 226520] train loss: 0.003982, tar: 0.000058 
l0: 0.000009, l1: 0.000007, l2: 0.000029, l3: 0.000178, l4: 0.000417, l5: 0.001068, l6: 0.001688
[epoch: 862/1000, batch:   388/ 1052, ite: 226540] train loss: 0.003982, tar: 0.000058 
l0: 0.000031, l1: 0.000029, l2: 0.000048, l3: 0.000093, l4: 0.000315, l5: 0.000487, l6: 0.001215
[epoch: 862/1000, batch:   468/ 1052, ite: 226560] train loss: 0.003982, tar: 0.000058 
l0: 0.000017, l1: 0.000016, l2: 0.000057, l3: 0.000162, l4: 0.000401, l5: 0.001113, l6: 0.002326
[epoch: 862/1000, batch:   548/ 1052, ite: 226580] train loss: 0.003983, tar: 0.000058 
l0: 0.000100, l1: 0.000099, l2: 0.000173, l3: 0.000390, l4: 0.000706, l5: 0.000765, l6: 0.001545
[epoch: 862/1000, batch:   628/ 1052, ite: 226600] train loss: 0.003983, tar: 0.000058 
l0: 0.000029, l1: 0.000030, l2: 0.000056, l3: 0.000133, l4: 0.000503, l5: 0.000885, l6: 0.002243
[epoch: 862/1000, batch:   708/ 1052, ite: 226620] train loss: 0.003983, tar: 0.000058 
l0: 0.000046, l1: 0.000041, l2: 0.000128, l3: 0.000359, l4: 0.000643, l5: 0.001398, l6: 0.004115
[epoch: 862/1000, batch:   788/ 1052, ite: 226640] train loss: 0.003983, tar: 0.000058 
l0: 0.000072, l1: 0.000070, l2: 0.000190, l3: 0.000368, l4: 0.000689, l5: 0.001859, l6: 0.003904
[epoch: 862/1000, batch:   868/ 1052, ite: 226660] train loss: 0.003984, tar: 0.000058 
l0: 0.000071, l1: 0.000069, l2: 0.000134, l3: 0.000335, l4: 0.000766, l5: 0.001174, l6: 0.001862
[epoch: 862/1000, batch:   948/ 1052, ite: 226680] train loss: 0.003983, tar: 0.000058 
l0: 0.000317, l1: 0.000348, l2: 0.000364, l3: 0.000387, l4: 0.000726, l5: 0.001171, l6: 0.001683
[epoch: 862/1000, batch:  1028/ 1052, ite: 226700] train loss: 0.003995, tar: 0.000060 
[Epoch 862/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000179, l1: 0.000187, l2: 0.000225, l3: 0.000333, l4: 0.000557, l5: 0.000965, l6: 0.001367
[epoch: 863/1000, batch:    56/ 1052, ite: 226720] train loss: 0.003998, tar: 0.000060 
l0: 0.000111, l1: 0.000133, l2: 0.000145, l3: 0.000209, l4: 0.000482, l5: 0.001258, l6: 0.001848
[epoch: 863/1000, batch:   136/ 1052, ite: 226740] train loss: 0.004004, tar: 0.000061 
l0: 0.000155, l1: 0.000169, l2: 0.000194, l3: 0.000212, l4: 0.000247, l5: 0.000610, l6: 0.000721
[epoch: 863/1000, batch:   216/ 1052, ite: 226760] train loss: 0.004004, tar: 0.000062 
l0: 0.000080, l1: 0.000086, l2: 0.000132, l3: 0.000143, l4: 0.000354, l5: 0.000507, l6: 0.000944
[epoch: 863/1000, batch:   296/ 1052, ite: 226780] train loss: 0.004005, tar: 0.000062 
l0: 0.000029, l1: 0.000030, l2: 0.000053, l3: 0.000099, l4: 0.000285, l5: 0.000528, l6: 0.000644
[epoch: 863/1000, batch:   376/ 1052, ite: 226800] train loss: 0.004008, tar: 0.000062 
l0: 0.000053, l1: 0.000054, l2: 0.000076, l3: 0.000224, l4: 0.000518, l5: 0.001451, l6: 0.002012
[epoch: 863/1000, batch:   456/ 1052, ite: 226820] train loss: 0.004010, tar: 0.000062 
l0: 0.000066, l1: 0.000066, l2: 0.000065, l3: 0.000160, l4: 0.000423, l5: 0.000990, l6: 0.001673
[epoch: 863/1000, batch:   536/ 1052, ite: 226840] train loss: 0.004015, tar: 0.000062 
l0: 0.000069, l1: 0.000074, l2: 0.000078, l3: 0.000148, l4: 0.000248, l5: 0.000591, l6: 0.001619
[epoch: 863/1000, batch:   616/ 1052, ite: 226860] train loss: 0.004014, tar: 0.000063 
l0: 0.000085, l1: 0.000084, l2: 0.000101, l3: 0.000175, l4: 0.000369, l5: 0.000658, l6: 0.001286
[epoch: 863/1000, batch:   696/ 1052, ite: 226880] train loss: 0.004016, tar: 0.000063 
l0: 0.000070, l1: 0.000073, l2: 0.000128, l3: 0.000280, l4: 0.000821, l5: 0.001265, l6: 0.002744
[epoch: 863/1000, batch:   776/ 1052, ite: 226900] train loss: 0.004018, tar: 0.000063 
l0: 0.000101, l1: 0.000153, l2: 0.000136, l3: 0.000229, l4: 0.000303, l5: 0.001494, l6: 0.001882
[epoch: 863/1000, batch:   856/ 1052, ite: 226920] train loss: 0.004019, tar: 0.000063 
l0: 0.000048, l1: 0.000052, l2: 0.000103, l3: 0.000201, l4: 0.000510, l5: 0.001085, l6: 0.001744
[epoch: 863/1000, batch:   936/ 1052, ite: 226940] train loss: 0.004020, tar: 0.000063 
l0: 0.000061, l1: 0.000059, l2: 0.000087, l3: 0.000162, l4: 0.000457, l5: 0.001246, l6: 0.001794
[epoch: 863/1000, batch:  1016/ 1052, ite: 226960] train loss: 0.004021, tar: 0.000063 
[Epoch 863/1000] Test loss: 0.011, Test target loss: 0.001
l0: 0.000055, l1: 0.000053, l2: 0.000066, l3: 0.000098, l4: 0.000353, l5: 0.000706, l6: 0.000826
[epoch: 864/1000, batch:    44/ 1052, ite: 226980] train loss: 0.004022, tar: 0.000063 
l0: 0.000081, l1: 0.000086, l2: 0.000116, l3: 0.000207, l4: 0.000601, l5: 0.001330, l6: 0.002213
[epoch: 864/1000, batch:   124/ 1052, ite: 227000] train loss: 0.004023, tar: 0.000063 
l0: 0.000068, l1: 0.000067, l2: 0.000106, l3: 0.000252, l4: 0.000450, l5: 0.000663, l6: 0.000946
[epoch: 864/1000, batch:   204/ 1052, ite: 227020] train loss: 0.004022, tar: 0.000064 
l0: 0.000064, l1: 0.000062, l2: 0.000079, l3: 0.000111, l4: 0.000277, l5: 0.000729, l6: 0.001092
[epoch: 864/1000, batch:   284/ 1052, ite: 227040] train loss: 0.004023, tar: 0.000064 
l0: 0.000043, l1: 0.000042, l2: 0.000066, l3: 0.000113, l4: 0.000341, l5: 0.000529, l6: 0.001148
[epoch: 864/1000, batch:   364/ 1052, ite: 227060] train loss: 0.004023, tar: 0.000064 
l0: 0.000027, l1: 0.000026, l2: 0.000052, l3: 0.000194, l4: 0.000701, l5: 0.001213, l6: 0.002350
[epoch: 864/1000, batch:   444/ 1052, ite: 227080] train loss: 0.004023, tar: 0.000064 
l0: 0.000091, l1: 0.000103, l2: 0.000083, l3: 0.000197, l4: 0.000477, l5: 0.001263, l6: 0.002128
[epoch: 864/1000, batch:   524/ 1052, ite: 227100] train loss: 0.004024, tar: 0.000064 
l0: 0.000032, l1: 0.000029, l2: 0.000076, l3: 0.000232, l4: 0.000623, l5: 0.001304, l6: 0.002414
[epoch: 864/1000, batch:   604/ 1052, ite: 227120] train loss: 0.004025, tar: 0.000064 
l0: 0.000129, l1: 0.000137, l2: 0.000149, l3: 0.000270, l4: 0.000477, l5: 0.000969, l6: 0.001588
[epoch: 864/1000, batch:   684/ 1052, ite: 227140] train loss: 0.004023, tar: 0.000064 
l0: 0.000083, l1: 0.000085, l2: 0.000122, l3: 0.000172, l4: 0.000457, l5: 0.000865, l6: 0.001507
[epoch: 864/1000, batch:   764/ 1052, ite: 227160] train loss: 0.004025, tar: 0.000064 
l0: 0.000010, l1: 0.000009, l2: 0.000014, l3: 0.000047, l4: 0.000344, l5: 0.000751, l6: 0.001147
[epoch: 864/1000, batch:   844/ 1052, ite: 227180] train loss: 0.004025, tar: 0.000064 
l0: 0.000111, l1: 0.000118, l2: 0.000128, l3: 0.000229, l4: 0.000484, l5: 0.001359, l6: 0.002404
[epoch: 864/1000, batch:   924/ 1052, ite: 227200] train loss: 0.004024, tar: 0.000064 
l0: 0.000028, l1: 0.000029, l2: 0.000045, l3: 0.000099, l4: 0.000171, l5: 0.000721, l6: 0.000680
[epoch: 864/1000, batch:  1004/ 1052, ite: 227220] train loss: 0.004023, tar: 0.000064 
[Epoch 864/1000] Test loss: 0.012, Test target loss: 0.001
l0: 0.000026, l1: 0.000026, l2: 0.000042, l3: 0.000076, l4: 0.000335, l5: 0.000752, l6: 0.000970
[epoch: 865/1000, batch:    32/ 1052, ite: 227240] train loss: 0.004025, tar: 0.000064 
l0: 0.000050, l1: 0.000051, l2: 0.000060, l3: 0.000126, l4: 0.000294, l5: 0.000619, l6: 0.001288
[epoch: 865/1000, batch:   112/ 1052, ite: 227260] train loss: 0.004025, tar: 0.000064 
l0: 0.000026, l1: 0.000030, l2: 0.000032, l3: 0.000116, l4: 0.000487, l5: 0.001117, l6: 0.001757
[epoch: 865/1000, batch:   192/ 1052, ite: 227280] train loss: 0.004026, tar: 0.000064 
l0: 0.000071, l1: 0.000070, l2: 0.000126, l3: 0.000341, l4: 0.000740, l5: 0.001946, l6: 0.002963
[epoch: 865/1000, batch:   272/ 1052, ite: 227300] train loss: 0.004028, tar: 0.000064 
l0: 0.000078, l1: 0.000081, l2: 0.000114, l3: 0.000192, l4: 0.000449, l5: 0.001694, l6: 0.002160
[epoch: 865/1000, batch:   352/ 1052, ite: 227320] train loss: 0.004027, tar: 0.000064 
l0: 0.000019, l1: 0.000019, l2: 0.000060, l3: 0.000195, l4: 0.000572, l5: 0.001137, l6: 0.002533
[epoch: 865/1000, batch:   432/ 1052, ite: 227340] train loss: 0.004025, tar: 0.000063 
l0: 0.000050, l1: 0.000053, l2: 0.000090, l3: 0.000215, l4: 0.000574, l5: 0.000993, l6: 0.002427
[epoch: 865/1000, batch:   512/ 1052, ite: 227360] train loss: 0.004025, tar: 0.000063 
l0: 0.000040, l1: 0.000036, l2: 0.000088, l3: 0.000275, l4: 0.000486, l5: 0.001419, l6: 0.002809
[epoch: 865/1000, batch:   592/ 1052, ite: 227380] train loss: 0.004024, tar: 0.000063 
l0: 0.000042, l1: 0.000042, l2: 0.000060, l3: 0.000142, l4: 0.000295, l5: 0.000657, l6: 0.001131
[epoch: 865/1000, batch:   672/ 1052, ite: 227400] train loss: 0.004024, tar: 0.000063 
l0: 0.000106, l1: 0.000105, l2: 0.000121, l3: 0.000269, l4: 0.000810, l5: 0.001473, l6: 0.002256
[epoch: 865/1000, batch:   752/ 1052, ite: 227420] train loss: 0.004025, tar: 0.000063 
l0: 0.000010, l1: 0.000008, l2: 0.000027, l3: 0.000116, l4: 0.000165, l5: 0.000793, l6: 0.001344
[epoch: 865/1000, batch:   832/ 1052, ite: 227440] train loss: 0.004023, tar: 0.000063 
l0: 0.000058, l1: 0.000056, l2: 0.000138, l3: 0.000324, l4: 0.001130, l5: 0.002616, l6: 0.004814
[epoch: 865/1000, batch:   912/ 1052, ite: 227460] train loss: 0.004024, tar: 0.000063 
l0: 0.000077, l1: 0.000074, l2: 0.000115, l3: 0.000264, l4: 0.000572, l5: 0.000768, l6: 0.002434
[epoch: 865/1000, batch:   992/ 1052, ite: 227480] train loss: 0.004024, tar: 0.000063 
[Epoch 865/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000010, l1: 0.000011, l2: 0.000034, l3: 0.000146, l4: 0.000615, l5: 0.000888, l6: 0.001683
[epoch: 866/1000, batch:    20/ 1052, ite: 227500] train loss: 0.004024, tar: 0.000063 
l0: 0.000005, l1: 0.000005, l2: 0.000023, l3: 0.000059, l4: 0.000234, l5: 0.000748, l6: 0.001182
[epoch: 866/1000, batch:   100/ 1052, ite: 227520] train loss: 0.004025, tar: 0.000063 
l0: 0.000167, l1: 0.000167, l2: 0.000239, l3: 0.000442, l4: 0.000998, l5: 0.002168, l6: 0.004146
[epoch: 866/1000, batch:   180/ 1052, ite: 227540] train loss: 0.004024, tar: 0.000063 
l0: 0.000021, l1: 0.000021, l2: 0.000047, l3: 0.000372, l4: 0.000658, l5: 0.001297, l6: 0.002080
[epoch: 866/1000, batch:   260/ 1052, ite: 227560] train loss: 0.004025, tar: 0.000063 
l0: 0.000038, l1: 0.000040, l2: 0.000053, l3: 0.000144, l4: 0.000461, l5: 0.001067, l6: 0.001948
[epoch: 866/1000, batch:   340/ 1052, ite: 227580] train loss: 0.004025, tar: 0.000063 
l0: 0.000077, l1: 0.000074, l2: 0.000155, l3: 0.000579, l4: 0.001092, l5: 0.002419, l6: 0.003910
[epoch: 866/1000, batch:   420/ 1052, ite: 227600] train loss: 0.004027, tar: 0.000063 
l0: 0.000088, l1: 0.000087, l2: 0.000143, l3: 0.000287, l4: 0.000597, l5: 0.001413, l6: 0.003068
[epoch: 866/1000, batch:   500/ 1052, ite: 227620] train loss: 0.004025, tar: 0.000063 
l0: 0.000056, l1: 0.000056, l2: 0.000088, l3: 0.000235, l4: 0.000572, l5: 0.001375, l6: 0.002089
[epoch: 866/1000, batch:   580/ 1052, ite: 227640] train loss: 0.004026, tar: 0.000063 
l0: 0.000039, l1: 0.000039, l2: 0.000061, l3: 0.000127, l4: 0.000327, l5: 0.001402, l6: 0.002880
[epoch: 866/1000, batch:   660/ 1052, ite: 227660] train loss: 0.004026, tar: 0.000063 
l0: 0.000010, l1: 0.000010, l2: 0.000020, l3: 0.000136, l4: 0.000305, l5: 0.000968, l6: 0.001433
[epoch: 866/1000, batch:   740/ 1052, ite: 227680] train loss: 0.004026, tar: 0.000063 
l0: 0.000014, l1: 0.000016, l2: 0.000040, l3: 0.000106, l4: 0.000411, l5: 0.001167, l6: 0.002640
[epoch: 866/1000, batch:   820/ 1052, ite: 227700] train loss: 0.004024, tar: 0.000063 
l0: 0.000045, l1: 0.000047, l2: 0.000072, l3: 0.000138, l4: 0.000362, l5: 0.000953, l6: 0.001288
[epoch: 866/1000, batch:   900/ 1052, ite: 227720] train loss: 0.004023, tar: 0.000063 
l0: 0.000038, l1: 0.000038, l2: 0.000069, l3: 0.000190, l4: 0.000515, l5: 0.001450, l6: 0.001779
[epoch: 866/1000, batch:   980/ 1052, ite: 227740] train loss: 0.004024, tar: 0.000063 
[Epoch 866/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000029, l1: 0.000028, l2: 0.000038, l3: 0.000116, l4: 0.000321, l5: 0.000650, l6: 0.000845
[epoch: 867/1000, batch:     8/ 1052, ite: 227760] train loss: 0.004024, tar: 0.000063 
l0: 0.000044, l1: 0.000043, l2: 0.000070, l3: 0.000170, l4: 0.000448, l5: 0.000632, l6: 0.001585
[epoch: 867/1000, batch:    88/ 1052, ite: 227780] train loss: 0.004023, tar: 0.000063 
l0: 0.000005, l1: 0.000005, l2: 0.000007, l3: 0.000050, l4: 0.000227, l5: 0.000227, l6: 0.001263
[epoch: 867/1000, batch:   168/ 1052, ite: 227800] train loss: 0.004024, tar: 0.000063 
l0: 0.000046, l1: 0.000046, l2: 0.000106, l3: 0.000209, l4: 0.000547, l5: 0.000931, l6: 0.002628
[epoch: 867/1000, batch:   248/ 1052, ite: 227820] train loss: 0.004022, tar: 0.000063 
l0: 0.000011, l1: 0.000009, l2: 0.000024, l3: 0.000075, l4: 0.000332, l5: 0.000615, l6: 0.001302
[epoch: 867/1000, batch:   328/ 1052, ite: 227840] train loss: 0.004022, tar: 0.000063 
l0: 0.000132, l1: 0.000145, l2: 0.000179, l3: 0.000311, l4: 0.000508, l5: 0.001179, l6: 0.001564
[epoch: 867/1000, batch:   408/ 1052, ite: 227860] train loss: 0.004022, tar: 0.000063 
l0: 0.000061, l1: 0.000062, l2: 0.000108, l3: 0.000217, l4: 0.000499, l5: 0.001313, l6: 0.003092
[epoch: 867/1000, batch:   488/ 1052, ite: 227880] train loss: 0.004022, tar: 0.000063 
l0: 0.000006, l1: 0.000004, l2: 0.000022, l3: 0.000120, l4: 0.000294, l5: 0.000715, l6: 0.001973
[epoch: 867/1000, batch:   568/ 1052, ite: 227900] train loss: 0.004021, tar: 0.000063 
l0: 0.000044, l1: 0.000046, l2: 0.000060, l3: 0.000117, l4: 0.000339, l5: 0.000933, l6: 0.001231
[epoch: 867/1000, batch:   648/ 1052, ite: 227920] train loss: 0.004020, tar: 0.000063 
l0: 0.000022, l1: 0.000026, l2: 0.000060, l3: 0.000201, l4: 0.000478, l5: 0.001361, l6: 0.002733
[epoch: 867/1000, batch:   728/ 1052, ite: 227940] train loss: 0.004019, tar: 0.000062 
l0: 0.000067, l1: 0.000069, l2: 0.000083, l3: 0.000136, l4: 0.000510, l5: 0.000767, l6: 0.001509
[epoch: 867/1000, batch:   808/ 1052, ite: 227960] train loss: 0.004018, tar: 0.000062 
l0: 0.000080, l1: 0.000080, l2: 0.000102, l3: 0.000258, l4: 0.000713, l5: 0.001349, l6: 0.001368
[epoch: 867/1000, batch:   888/ 1052, ite: 227980] train loss: 0.004019, tar: 0.000062 
l0: 0.000004, l1: 0.000003, l2: 0.000021, l3: 0.000087, l4: 0.000385, l5: 0.001455, l6: 0.002396
[epoch: 867/1000, batch:   968/ 1052, ite: 228000] train loss: 0.004019, tar: 0.000062 
l0: 0.000005, l1: 0.000005, l2: 0.000032, l3: 0.000126, l4: 0.000316, l5: 0.000652, l6: 0.001295
[epoch: 867/1000, batch:  1048/ 1052, ite: 228020] train loss: 0.004020, tar: 0.000062 
[Epoch 867/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000033, l1: 0.000032, l2: 0.000053, l3: 0.000141, l4: 0.000299, l5: 0.000780, l6: 0.000739
[epoch: 868/1000, batch:    76/ 1052, ite: 228040] train loss: 0.004020, tar: 0.000062 
l0: 0.000005, l1: 0.000005, l2: 0.000048, l3: 0.000194, l4: 0.000478, l5: 0.001066, l6: 0.002261
[epoch: 868/1000, batch:   156/ 1052, ite: 228060] train loss: 0.004019, tar: 0.000062 
l0: 0.000082, l1: 0.000084, l2: 0.000096, l3: 0.000153, l4: 0.000304, l5: 0.000676, l6: 0.002193
[epoch: 868/1000, batch:   236/ 1052, ite: 228080] train loss: 0.004019, tar: 0.000062 
l0: 0.000024, l1: 0.000023, l2: 0.000041, l3: 0.000117, l4: 0.000317, l5: 0.000853, l6: 0.001381
[epoch: 868/1000, batch:   316/ 1052, ite: 228100] train loss: 0.004018, tar: 0.000062 
l0: 0.000019, l1: 0.000019, l2: 0.000060, l3: 0.000179, l4: 0.000372, l5: 0.000781, l6: 0.001437
[epoch: 868/1000, batch:   396/ 1052, ite: 228120] train loss: 0.004017, tar: 0.000062 
l0: 0.000039, l1: 0.000041, l2: 0.000059, l3: 0.000139, l4: 0.000494, l5: 0.001066, l6: 0.001713
[epoch: 868/1000, batch:   476/ 1052, ite: 228140] train loss: 0.004018, tar: 0.000062 
l0: 0.000065, l1: 0.000062, l2: 0.000119, l3: 0.000236, l4: 0.000553, l5: 0.001046, l6: 0.001327
[epoch: 868/1000, batch:   556/ 1052, ite: 228160] train loss: 0.004018, tar: 0.000062 
l0: 0.000045, l1: 0.000044, l2: 0.000071, l3: 0.000203, l4: 0.000721, l5: 0.001410, l6: 0.002695
[epoch: 868/1000, batch:   636/ 1052, ite: 228180] train loss: 0.004019, tar: 0.000062 
l0: 0.000027, l1: 0.000026, l2: 0.000041, l3: 0.000109, l4: 0.000467, l5: 0.000623, l6: 0.002321
[epoch: 868/1000, batch:   716/ 1052, ite: 228200] train loss: 0.004018, tar: 0.000062 
l0: 0.000062, l1: 0.000057, l2: 0.000137, l3: 0.000311, l4: 0.000756, l5: 0.001511, l6: 0.002674
[epoch: 868/1000, batch:   796/ 1052, ite: 228220] train loss: 0.004019, tar: 0.000062 
l0: 0.000018, l1: 0.000016, l2: 0.000038, l3: 0.000136, l4: 0.000494, l5: 0.000973, l6: 0.001415
[epoch: 868/1000, batch:   876/ 1052, ite: 228240] train loss: 0.004017, tar: 0.000062 
l0: 0.000065, l1: 0.000064, l2: 0.000096, l3: 0.000259, l4: 0.000629, l5: 0.001164, l6: 0.002223
[epoch: 868/1000, batch:   956/ 1052, ite: 228260] train loss: 0.004017, tar: 0.000062 
l0: 0.000073, l1: 0.000076, l2: 0.000093, l3: 0.000177, l4: 0.000371, l5: 0.000953, l6: 0.002122
[epoch: 868/1000, batch:  1036/ 1052, ite: 228280] train loss: 0.004017, tar: 0.000062 
[Epoch 868/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000014, l1: 0.000015, l2: 0.000025, l3: 0.000127, l4: 0.000424, l5: 0.001144, l6: 0.002198
[epoch: 869/1000, batch:    64/ 1052, ite: 228300] train loss: 0.004017, tar: 0.000062 
l0: 0.000001, l1: 0.000001, l2: 0.000014, l3: 0.000119, l4: 0.000381, l5: 0.001201, l6: 0.001442
[epoch: 869/1000, batch:   144/ 1052, ite: 228320] train loss: 0.004017, tar: 0.000062 
l0: 0.000060, l1: 0.000059, l2: 0.000099, l3: 0.000192, l4: 0.000272, l5: 0.001147, l6: 0.002342
[epoch: 869/1000, batch:   224/ 1052, ite: 228340] train loss: 0.004017, tar: 0.000062 
l0: 0.000024, l1: 0.000021, l2: 0.000082, l3: 0.000279, l4: 0.000699, l5: 0.001642, l6: 0.002946
[epoch: 869/1000, batch:   304/ 1052, ite: 228360] train loss: 0.004017, tar: 0.000062 
l0: 0.000054, l1: 0.000053, l2: 0.000095, l3: 0.000187, l4: 0.000407, l5: 0.001346, l6: 0.002065
[epoch: 869/1000, batch:   384/ 1052, ite: 228380] train loss: 0.004017, tar: 0.000061 
l0: 0.000035, l1: 0.000035, l2: 0.000073, l3: 0.000389, l4: 0.000941, l5: 0.001661, l6: 0.002850
[epoch: 869/1000, batch:   464/ 1052, ite: 228400] train loss: 0.004018, tar: 0.000061 
l0: 0.000053, l1: 0.000054, l2: 0.000077, l3: 0.000166, l4: 0.000301, l5: 0.000350, l6: 0.000888
[epoch: 869/1000, batch:   544/ 1052, ite: 228420] train loss: 0.004016, tar: 0.000061 
l0: 0.000004, l1: 0.000003, l2: 0.000022, l3: 0.000097, l4: 0.000336, l5: 0.000648, l6: 0.001037
[epoch: 869/1000, batch:   624/ 1052, ite: 228440] train loss: 0.004016, tar: 0.000061 
l0: 0.000031, l1: 0.000030, l2: 0.000069, l3: 0.000264, l4: 0.001061, l5: 0.000824, l6: 0.002248
[epoch: 869/1000, batch:   704/ 1052, ite: 228460] train loss: 0.004013, tar: 0.000061 
l0: 0.000008, l1: 0.000008, l2: 0.000025, l3: 0.000105, l4: 0.000357, l5: 0.001086, l6: 0.002314
[epoch: 869/1000, batch:   784/ 1052, ite: 228480] train loss: 0.004014, tar: 0.000061 
l0: 0.000019, l1: 0.000021, l2: 0.000051, l3: 0.000203, l4: 0.000531, l5: 0.001361, l6: 0.001958
[epoch: 869/1000, batch:   864/ 1052, ite: 228500] train loss: 0.004013, tar: 0.000061 
l0: 0.000195, l1: 0.000195, l2: 0.000254, l3: 0.000504, l4: 0.000985, l5: 0.002205, l6: 0.003556
[epoch: 869/1000, batch:   944/ 1052, ite: 228520] train loss: 0.004013, tar: 0.000061 
l0: 0.000005, l1: 0.000005, l2: 0.000044, l3: 0.000154, l4: 0.000439, l5: 0.001054, l6: 0.001442
[epoch: 869/1000, batch:  1024/ 1052, ite: 228540] train loss: 0.004012, tar: 0.000061 
[Epoch 869/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000067, l1: 0.000071, l2: 0.000081, l3: 0.000107, l4: 0.000478, l5: 0.000823, l6: 0.001473
[epoch: 870/1000, batch:    52/ 1052, ite: 228560] train loss: 0.004012, tar: 0.000061 
l0: 0.000027, l1: 0.000027, l2: 0.000041, l3: 0.000129, l4: 0.000227, l5: 0.000537, l6: 0.001254
[epoch: 870/1000, batch:   132/ 1052, ite: 228580] train loss: 0.004012, tar: 0.000061 
l0: 0.000049, l1: 0.000051, l2: 0.000117, l3: 0.000237, l4: 0.000661, l5: 0.001384, l6: 0.001834
[epoch: 870/1000, batch:   212/ 1052, ite: 228600] train loss: 0.004011, tar: 0.000061 
l0: 0.000034, l1: 0.000035, l2: 0.000062, l3: 0.000161, l4: 0.000414, l5: 0.000826, l6: 0.001774
[epoch: 870/1000, batch:   292/ 1052, ite: 228620] train loss: 0.004011, tar: 0.000061 
l0: 0.000006, l1: 0.000006, l2: 0.000028, l3: 0.000144, l4: 0.000539, l5: 0.001348, l6: 0.001996
[epoch: 870/1000, batch:   372/ 1052, ite: 228640] train loss: 0.004011, tar: 0.000061 
l0: 0.000020, l1: 0.000020, l2: 0.000045, l3: 0.000164, l4: 0.000667, l5: 0.001473, l6: 0.002250
[epoch: 870/1000, batch:   452/ 1052, ite: 228660] train loss: 0.004011, tar: 0.000061 
l0: 0.000025, l1: 0.000024, l2: 0.000040, l3: 0.000088, l4: 0.000216, l5: 0.000913, l6: 0.001053
[epoch: 870/1000, batch:   532/ 1052, ite: 228680] train loss: 0.004013, tar: 0.000061 
l0: 0.000029, l1: 0.000028, l2: 0.000072, l3: 0.000136, l4: 0.000265, l5: 0.000579, l6: 0.001413
[epoch: 870/1000, batch:   612/ 1052, ite: 228700] train loss: 0.004011, tar: 0.000061 
l0: 0.000048, l1: 0.000054, l2: 0.000058, l3: 0.000200, l4: 0.000465, l5: 0.001090, l6: 0.001428
[epoch: 870/1000, batch:   692/ 1052, ite: 228720] train loss: 0.004011, tar: 0.000061 
l0: 0.000010, l1: 0.000010, l2: 0.000029, l3: 0.000108, l4: 0.000437, l5: 0.001037, l6: 0.002159
[epoch: 870/1000, batch:   772/ 1052, ite: 228740] train loss: 0.004011, tar: 0.000061 
l0: 0.000059, l1: 0.000057, l2: 0.000082, l3: 0.000167, l4: 0.000374, l5: 0.001109, l6: 0.001542
[epoch: 870/1000, batch:   852/ 1052, ite: 228760] train loss: 0.004010, tar: 0.000060 
l0: 0.000025, l1: 0.000025, l2: 0.000056, l3: 0.000184, l4: 0.000231, l5: 0.000833, l6: 0.001391
[epoch: 870/1000, batch:   932/ 1052, ite: 228780] train loss: 0.004010, tar: 0.000060 
l0: 0.000072, l1: 0.000072, l2: 0.000109, l3: 0.000188, l4: 0.000357, l5: 0.001003, l6: 0.001718
[epoch: 870/1000, batch:  1012/ 1052, ite: 228800] train loss: 0.004009, tar: 0.000060 
[Epoch 870/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000037, l1: 0.000038, l2: 0.000051, l3: 0.000155, l4: 0.000352, l5: 0.000532, l6: 0.000865
[epoch: 871/1000, batch:    40/ 1052, ite: 228820] train loss: 0.004009, tar: 0.000060 
l0: 0.000087, l1: 0.000087, l2: 0.000104, l3: 0.000207, l4: 0.000456, l5: 0.001134, l6: 0.002301
[epoch: 871/1000, batch:   120/ 1052, ite: 228840] train loss: 0.004009, tar: 0.000060 
l0: 0.000067, l1: 0.000067, l2: 0.000105, l3: 0.000270, l4: 0.000653, l5: 0.001381, l6: 0.002789
[epoch: 871/1000, batch:   200/ 1052, ite: 228860] train loss: 0.004009, tar: 0.000060 
l0: 0.000032, l1: 0.000037, l2: 0.000069, l3: 0.000179, l4: 0.000447, l5: 0.001062, l6: 0.001759
[epoch: 871/1000, batch:   280/ 1052, ite: 228880] train loss: 0.004009, tar: 0.000060 
l0: 0.000007, l1: 0.000006, l2: 0.000022, l3: 0.000084, l4: 0.000472, l5: 0.001163, l6: 0.001482
[epoch: 871/1000, batch:   360/ 1052, ite: 228900] train loss: 0.004008, tar: 0.000060 
l0: 0.000007, l1: 0.000006, l2: 0.000040, l3: 0.000208, l4: 0.000934, l5: 0.001574, l6: 0.002320
[epoch: 871/1000, batch:   440/ 1052, ite: 228920] train loss: 0.004010, tar: 0.000060 
l0: 0.000002, l1: 0.000003, l2: 0.000011, l3: 0.000105, l4: 0.000407, l5: 0.001121, l6: 0.001548
[epoch: 871/1000, batch:   520/ 1052, ite: 228940] train loss: 0.004009, tar: 0.000060 
l0: 0.000026, l1: 0.000027, l2: 0.000040, l3: 0.000081, l4: 0.000235, l5: 0.000849, l6: 0.001449
[epoch: 871/1000, batch:   600/ 1052, ite: 228960] train loss: 0.004008, tar: 0.000060 
l0: 0.000003, l1: 0.000003, l2: 0.000052, l3: 0.000223, l4: 0.000601, l5: 0.001748, l6: 0.003200
[epoch: 871/1000, batch:   680/ 1052, ite: 228980] train loss: 0.004008, tar: 0.000060 
l0: 0.000018, l1: 0.000018, l2: 0.000031, l3: 0.000080, l4: 0.000518, l5: 0.000916, l6: 0.001537
[epoch: 871/1000, batch:   760/ 1052, ite: 229000] train loss: 0.004007, tar: 0.000060 
l0: 0.000016, l1: 0.000016, l2: 0.000029, l3: 0.000117, l4: 0.000316, l5: 0.001339, l6: 0.001495
[epoch: 871/1000, batch:   840/ 1052, ite: 229020] train loss: 0.004007, tar: 0.000060 
l0: 0.000030, l1: 0.000030, l2: 0.000062, l3: 0.000232, l4: 0.000577, l5: 0.001268, l6: 0.002377
[epoch: 871/1000, batch:   920/ 1052, ite: 229040] train loss: 0.004007, tar: 0.000060 
l0: 0.000019, l1: 0.000021, l2: 0.000025, l3: 0.000051, l4: 0.000135, l5: 0.000701, l6: 0.000690
[epoch: 871/1000, batch:  1000/ 1052, ite: 229060] train loss: 0.004007, tar: 0.000060 
[Epoch 871/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000051, l1: 0.000053, l2: 0.000091, l3: 0.000265, l4: 0.000603, l5: 0.001404, l6: 0.002193
[epoch: 872/1000, batch:    28/ 1052, ite: 229080] train loss: 0.004006, tar: 0.000060 
l0: 0.000012, l1: 0.000010, l2: 0.000053, l3: 0.000171, l4: 0.000287, l5: 0.001131, l6: 0.001889
[epoch: 872/1000, batch:   108/ 1052, ite: 229100] train loss: 0.004006, tar: 0.000060 
l0: 0.000030, l1: 0.000030, l2: 0.000061, l3: 0.000166, l4: 0.000540, l5: 0.001214, l6: 0.001775
[epoch: 872/1000, batch:   188/ 1052, ite: 229120] train loss: 0.004006, tar: 0.000060 
l0: 0.000031, l1: 0.000031, l2: 0.000106, l3: 0.000329, l4: 0.000771, l5: 0.001358, l6: 0.001537
[epoch: 872/1000, batch:   268/ 1052, ite: 229140] train loss: 0.004005, tar: 0.000060 
l0: 0.000029, l1: 0.000029, l2: 0.000049, l3: 0.000149, l4: 0.000208, l5: 0.000720, l6: 0.001655
[epoch: 872/1000, batch:   348/ 1052, ite: 229160] train loss: 0.004004, tar: 0.000059 
l0: 0.000091, l1: 0.000093, l2: 0.000115, l3: 0.000189, l4: 0.000630, l5: 0.001392, l6: 0.002454
[epoch: 872/1000, batch:   428/ 1052, ite: 229180] train loss: 0.004004, tar: 0.000059 
l0: 0.000005, l1: 0.000007, l2: 0.000029, l3: 0.000107, l4: 0.000214, l5: 0.000753, l6: 0.002421
[epoch: 872/1000, batch:   508/ 1052, ite: 229200] train loss: 0.004006, tar: 0.000059 
l0: 0.000001, l1: 0.000001, l2: 0.000026, l3: 0.000168, l4: 0.000484, l5: 0.001043, l6: 0.002085
[epoch: 872/1000, batch:   588/ 1052, ite: 229220] train loss: 0.004005, tar: 0.000059 
l0: 0.000003, l1: 0.000002, l2: 0.000019, l3: 0.000094, l4: 0.000525, l5: 0.000971, l6: 0.001367
[epoch: 872/1000, batch:   668/ 1052, ite: 229240] train loss: 0.004004, tar: 0.000059 
l0: 0.000097, l1: 0.000097, l2: 0.000135, l3: 0.000313, l4: 0.000670, l5: 0.000934, l6: 0.001376
[epoch: 872/1000, batch:   748/ 1052, ite: 229260] train loss: 0.004003, tar: 0.000059 
l0: 0.000032, l1: 0.000030, l2: 0.000070, l3: 0.000135, l4: 0.000370, l5: 0.001086, l6: 0.001495
[epoch: 872/1000, batch:   828/ 1052, ite: 229280] train loss: 0.004003, tar: 0.000059 
l0: 0.000061, l1: 0.000060, l2: 0.000091, l3: 0.000196, l4: 0.000353, l5: 0.000674, l6: 0.001885
[epoch: 872/1000, batch:   908/ 1052, ite: 229300] train loss: 0.004003, tar: 0.000059 
l0: 0.000014, l1: 0.000015, l2: 0.000048, l3: 0.000171, l4: 0.000283, l5: 0.000861, l6: 0.001237
[epoch: 872/1000, batch:   988/ 1052, ite: 229320] train loss: 0.004002, tar: 0.000059 
[Epoch 872/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000075, l1: 0.000076, l2: 0.000109, l3: 0.000245, l4: 0.000554, l5: 0.001506, l6: 0.002859
[epoch: 873/1000, batch:    16/ 1052, ite: 229340] train loss: 0.004002, tar: 0.000059 
l0: 0.000083, l1: 0.000084, l2: 0.000093, l3: 0.000183, l4: 0.000373, l5: 0.001190, l6: 0.003025
[epoch: 873/1000, batch:    96/ 1052, ite: 229360] train loss: 0.004002, tar: 0.000059 
l0: 0.000054, l1: 0.000057, l2: 0.000088, l3: 0.000254, l4: 0.000371, l5: 0.001122, l6: 0.001751
[epoch: 873/1000, batch:   176/ 1052, ite: 229380] train loss: 0.004001, tar: 0.000059 
l0: 0.000033, l1: 0.000033, l2: 0.000047, l3: 0.000075, l4: 0.000387, l5: 0.001013, l6: 0.000747
[epoch: 873/1000, batch:   256/ 1052, ite: 229400] train loss: 0.004000, tar: 0.000059 
l0: 0.000002, l1: 0.000002, l2: 0.000010, l3: 0.000072, l4: 0.000268, l5: 0.000547, l6: 0.001432
[epoch: 873/1000, batch:   336/ 1052, ite: 229420] train loss: 0.004000, tar: 0.000059 
l0: 0.000037, l1: 0.000034, l2: 0.000063, l3: 0.000231, l4: 0.000776, l5: 0.001526, l6: 0.002572
[epoch: 873/1000, batch:   416/ 1052, ite: 229440] train loss: 0.004000, tar: 0.000059 
l0: 0.000010, l1: 0.000007, l2: 0.000056, l3: 0.000255, l4: 0.000713, l5: 0.001580, l6: 0.001639
[epoch: 873/1000, batch:   496/ 1052, ite: 229460] train loss: 0.003999, tar: 0.000059 
l0: 0.000054, l1: 0.000054, l2: 0.000096, l3: 0.000142, l4: 0.000581, l5: 0.000990, l6: 0.002378
[epoch: 873/1000, batch:   576/ 1052, ite: 229480] train loss: 0.003998, tar: 0.000059 
l0: 0.000058, l1: 0.000060, l2: 0.000073, l3: 0.000235, l4: 0.000491, l5: 0.000884, l6: 0.003243
[epoch: 873/1000, batch:   656/ 1052, ite: 229500] train loss: 0.003998, tar: 0.000059 
l0: 0.000008, l1: 0.000007, l2: 0.000019, l3: 0.000148, l4: 0.000275, l5: 0.000671, l6: 0.001123
[epoch: 873/1000, batch:   736/ 1052, ite: 229520] train loss: 0.003999, tar: 0.000059 
l0: 0.000003, l1: 0.000003, l2: 0.000016, l3: 0.000084, l4: 0.000301, l5: 0.000942, l6: 0.001267
[epoch: 873/1000, batch:   816/ 1052, ite: 229540] train loss: 0.004000, tar: 0.000059 
l0: 0.000033, l1: 0.000034, l2: 0.000049, l3: 0.000176, l4: 0.000777, l5: 0.001675, l6: 0.002546
[epoch: 873/1000, batch:   896/ 1052, ite: 229560] train loss: 0.004000, tar: 0.000059 
l0: 0.000006, l1: 0.000006, l2: 0.000038, l3: 0.000101, l4: 0.000367, l5: 0.000716, l6: 0.001543
[epoch: 873/1000, batch:   976/ 1052, ite: 229580] train loss: 0.004000, tar: 0.000059 
[Epoch 873/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000005, l1: 0.000005, l2: 0.000029, l3: 0.000084, l4: 0.000263, l5: 0.000875, l6: 0.001516
[epoch: 874/1000, batch:     4/ 1052, ite: 229600] train loss: 0.003999, tar: 0.000058 
l0: 0.000043, l1: 0.000045, l2: 0.000071, l3: 0.000172, l4: 0.000467, l5: 0.001093, l6: 0.001977
[epoch: 874/1000, batch:    84/ 1052, ite: 229620] train loss: 0.003999, tar: 0.000058 
l0: 0.000042, l1: 0.000041, l2: 0.000131, l3: 0.000348, l4: 0.000678, l5: 0.001272, l6: 0.002475
[epoch: 874/1000, batch:   164/ 1052, ite: 229640] train loss: 0.003999, tar: 0.000058 
l0: 0.000034, l1: 0.000033, l2: 0.000075, l3: 0.000179, l4: 0.000387, l5: 0.001373, l6: 0.001853
[epoch: 874/1000, batch:   244/ 1052, ite: 229660] train loss: 0.003998, tar: 0.000058 
l0: 0.000091, l1: 0.000095, l2: 0.000108, l3: 0.000195, l4: 0.000744, l5: 0.001208, l6: 0.002313
[epoch: 874/1000, batch:   324/ 1052, ite: 229680] train loss: 0.003999, tar: 0.000058 
l0: 0.000050, l1: 0.000051, l2: 0.000080, l3: 0.000189, l4: 0.000323, l5: 0.000997, l6: 0.001149
[epoch: 874/1000, batch:   404/ 1052, ite: 229700] train loss: 0.003998, tar: 0.000058 
l0: 0.000047, l1: 0.000045, l2: 0.000077, l3: 0.000231, l4: 0.000391, l5: 0.000876, l6: 0.001320
[epoch: 874/1000, batch:   484/ 1052, ite: 229720] train loss: 0.003998, tar: 0.000058 
l0: 0.000117, l1: 0.000118, l2: 0.000170, l3: 0.000389, l4: 0.000895, l5: 0.001881, l6: 0.003613
[epoch: 874/1000, batch:   564/ 1052, ite: 229740] train loss: 0.003998, tar: 0.000058 
l0: 0.000031, l1: 0.000029, l2: 0.000072, l3: 0.000304, l4: 0.000708, l5: 0.001375, l6: 0.002973
[epoch: 874/1000, batch:   644/ 1052, ite: 229760] train loss: 0.003998, tar: 0.000058 
l0: 0.000009, l1: 0.000009, l2: 0.000024, l3: 0.000139, l4: 0.000290, l5: 0.001112, l6: 0.001321
[epoch: 874/1000, batch:   724/ 1052, ite: 229780] train loss: 0.003998, tar: 0.000058 
l0: 0.000036, l1: 0.000035, l2: 0.000126, l3: 0.000237, l4: 0.000735, l5: 0.001655, l6: 0.002672
[epoch: 874/1000, batch:   804/ 1052, ite: 229800] train loss: 0.003996, tar: 0.000058 
l0: 0.000014, l1: 0.000013, l2: 0.000034, l3: 0.000111, l4: 0.000337, l5: 0.000666, l6: 0.001198
[epoch: 874/1000, batch:   884/ 1052, ite: 229820] train loss: 0.003996, tar: 0.000058 
l0: 0.000003, l1: 0.000003, l2: 0.000011, l3: 0.000057, l4: 0.000316, l5: 0.001000, l6: 0.001146
[epoch: 874/1000, batch:   964/ 1052, ite: 229840] train loss: 0.003996, tar: 0.000058 
l0: 0.000145, l1: 0.000161, l2: 0.000152, l3: 0.000285, l4: 0.000648, l5: 0.001993, l6: 0.003968
[epoch: 874/1000, batch:  1044/ 1052, ite: 229860] train loss: 0.003996, tar: 0.000058 
[Epoch 874/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000045, l1: 0.000045, l2: 0.000075, l3: 0.000121, l4: 0.000426, l5: 0.000652, l6: 0.001724
[epoch: 875/1000, batch:    72/ 1052, ite: 229880] train loss: 0.003996, tar: 0.000058 
l0: 0.000001, l1: 0.000001, l2: 0.000014, l3: 0.000131, l4: 0.000460, l5: 0.001539, l6: 0.001432
[epoch: 875/1000, batch:   152/ 1052, ite: 229900] train loss: 0.003995, tar: 0.000058 
l0: 0.000008, l1: 0.000008, l2: 0.000022, l3: 0.000155, l4: 0.000359, l5: 0.001124, l6: 0.002701
[epoch: 875/1000, batch:   232/ 1052, ite: 229920] train loss: 0.003996, tar: 0.000058 
l0: 0.000039, l1: 0.000042, l2: 0.000048, l3: 0.000118, l4: 0.000519, l5: 0.000713, l6: 0.001203
[epoch: 875/1000, batch:   312/ 1052, ite: 229940] train loss: 0.003995, tar: 0.000058 
l0: 0.000023, l1: 0.000024, l2: 0.000018, l3: 0.000091, l4: 0.000361, l5: 0.001065, l6: 0.001172
[epoch: 875/1000, batch:   392/ 1052, ite: 229960] train loss: 0.003994, tar: 0.000058 
l0: 0.000026, l1: 0.000025, l2: 0.000046, l3: 0.000154, l4: 0.000360, l5: 0.001053, l6: 0.001002
[epoch: 875/1000, batch:   472/ 1052, ite: 229980] train loss: 0.003993, tar: 0.000058 
l0: 0.000012, l1: 0.000013, l2: 0.000042, l3: 0.000140, l4: 0.000377, l5: 0.001335, l6: 0.002075
[epoch: 875/1000, batch:   552/ 1052, ite: 230000] train loss: 0.003993, tar: 0.000058 
l0: 0.000056, l1: 0.000056, l2: 0.000100, l3: 0.000189, l4: 0.000416, l5: 0.001208, l6: 0.001662
[epoch: 875/1000, batch:   632/ 1052, ite: 230020] train loss: 0.003993, tar: 0.000058 
l0: 0.000069, l1: 0.000073, l2: 0.000117, l3: 0.000134, l4: 0.000487, l5: 0.000576, l6: 0.001404
[epoch: 875/1000, batch:   712/ 1052, ite: 230040] train loss: 0.003992, tar: 0.000058 
l0: 0.000032, l1: 0.000032, l2: 0.000053, l3: 0.000099, l4: 0.000378, l5: 0.000668, l6: 0.001516
[epoch: 875/1000, batch:   792/ 1052, ite: 230060] train loss: 0.003993, tar: 0.000058 
l0: 0.000011, l1: 0.000011, l2: 0.000040, l3: 0.000206, l4: 0.000323, l5: 0.001389, l6: 0.002210
[epoch: 875/1000, batch:   872/ 1052, ite: 230080] train loss: 0.003992, tar: 0.000057 
l0: 0.000010, l1: 0.000009, l2: 0.000022, l3: 0.000115, l4: 0.000479, l5: 0.001077, l6: 0.001653
[epoch: 875/1000, batch:   952/ 1052, ite: 230100] train loss: 0.003992, tar: 0.000057 
l0: 0.000003, l1: 0.000003, l2: 0.000027, l3: 0.000137, l4: 0.000399, l5: 0.000962, l6: 0.001005
[epoch: 875/1000, batch:  1032/ 1052, ite: 230120] train loss: 0.003993, tar: 0.000057 
[Epoch 875/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000018, l1: 0.000019, l2: 0.000070, l3: 0.000230, l4: 0.000742, l5: 0.001942, l6: 0.003252
[epoch: 876/1000, batch:    60/ 1052, ite: 230140] train loss: 0.003992, tar: 0.000057 
l0: 0.000014, l1: 0.000013, l2: 0.000027, l3: 0.000090, l4: 0.000189, l5: 0.000527, l6: 0.000983
[epoch: 876/1000, batch:   140/ 1052, ite: 230160] train loss: 0.003992, tar: 0.000057 
l0: 0.000013, l1: 0.000015, l2: 0.000027, l3: 0.000181, l4: 0.000350, l5: 0.001133, l6: 0.001911
[epoch: 876/1000, batch:   220/ 1052, ite: 230180] train loss: 0.003991, tar: 0.000057 
l0: 0.000018, l1: 0.000017, l2: 0.000022, l3: 0.000098, l4: 0.000262, l5: 0.001029, l6: 0.000785
[epoch: 876/1000, batch:   300/ 1052, ite: 230200] train loss: 0.003990, tar: 0.000057 
l0: 0.000016, l1: 0.000016, l2: 0.000026, l3: 0.000067, l4: 0.000304, l5: 0.000511, l6: 0.000728
[epoch: 876/1000, batch:   380/ 1052, ite: 230220] train loss: 0.003990, tar: 0.000057 
l0: 0.000007, l1: 0.000008, l2: 0.000018, l3: 0.000066, l4: 0.000207, l5: 0.000599, l6: 0.001173
[epoch: 876/1000, batch:   460/ 1052, ite: 230240] train loss: 0.003989, tar: 0.000057 
l0: 0.000097, l1: 0.000096, l2: 0.000151, l3: 0.000275, l4: 0.000726, l5: 0.001575, l6: 0.003704
[epoch: 876/1000, batch:   540/ 1052, ite: 230260] train loss: 0.003988, tar: 0.000057 
l0: 0.000128, l1: 0.000133, l2: 0.000136, l3: 0.000248, l4: 0.000643, l5: 0.001539, l6: 0.002870
[epoch: 876/1000, batch:   620/ 1052, ite: 230280] train loss: 0.003989, tar: 0.000057 
l0: 0.000130, l1: 0.000131, l2: 0.000180, l3: 0.000376, l4: 0.000779, l5: 0.001491, l6: 0.002684
[epoch: 876/1000, batch:   700/ 1052, ite: 230300] train loss: 0.003988, tar: 0.000057 
l0: 0.000056, l1: 0.000055, l2: 0.000074, l3: 0.000159, l4: 0.000550, l5: 0.000831, l6: 0.002172
[epoch: 876/1000, batch:   780/ 1052, ite: 230320] train loss: 0.003989, tar: 0.000057 
l0: 0.000011, l1: 0.000011, l2: 0.000020, l3: 0.000102, l4: 0.000371, l5: 0.000624, l6: 0.001515
[epoch: 876/1000, batch:   860/ 1052, ite: 230340] train loss: 0.003989, tar: 0.000057 
l0: 0.000009, l1: 0.000009, l2: 0.000035, l3: 0.000195, l4: 0.000632, l5: 0.001547, l6: 0.002560
[epoch: 876/1000, batch:   940/ 1052, ite: 230360] train loss: 0.003989, tar: 0.000057 
l0: 0.000052, l1: 0.000056, l2: 0.000082, l3: 0.000205, l4: 0.000562, l5: 0.001426, l6: 0.001421
[epoch: 876/1000, batch:  1020/ 1052, ite: 230380] train loss: 0.003990, tar: 0.000057 
[Epoch 876/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000000, l1: 0.000001, l2: 0.000006, l3: 0.000061, l4: 0.000529, l5: 0.001048, l6: 0.001802
[epoch: 877/1000, batch:    48/ 1052, ite: 230400] train loss: 0.003989, tar: 0.000057 
l0: 0.000041, l1: 0.000042, l2: 0.000083, l3: 0.000266, l4: 0.000515, l5: 0.001630, l6: 0.003389
[epoch: 877/1000, batch:   128/ 1052, ite: 230420] train loss: 0.003989, tar: 0.000057 
l0: 0.000012, l1: 0.000014, l2: 0.000019, l3: 0.000128, l4: 0.000420, l5: 0.001250, l6: 0.002157
[epoch: 877/1000, batch:   208/ 1052, ite: 230440] train loss: 0.003989, tar: 0.000057 
l0: 0.000058, l1: 0.000059, l2: 0.000095, l3: 0.000210, l4: 0.000513, l5: 0.000801, l6: 0.001793
[epoch: 877/1000, batch:   288/ 1052, ite: 230460] train loss: 0.003990, tar: 0.000057 
l0: 0.000027, l1: 0.000028, l2: 0.000049, l3: 0.000169, l4: 0.000289, l5: 0.001037, l6: 0.001773
[epoch: 877/1000, batch:   368/ 1052, ite: 230480] train loss: 0.003989, tar: 0.000057 
l0: 0.000014, l1: 0.000013, l2: 0.000041, l3: 0.000175, l4: 0.000696, l5: 0.000919, l6: 0.002059
[epoch: 877/1000, batch:   448/ 1052, ite: 230500] train loss: 0.003990, tar: 0.000057 
l0: 0.000019, l1: 0.000019, l2: 0.000035, l3: 0.000144, l4: 0.000251, l5: 0.000979, l6: 0.002182
[epoch: 877/1000, batch:   528/ 1052, ite: 230520] train loss: 0.003989, tar: 0.000057 
l0: 0.000062, l1: 0.000064, l2: 0.000084, l3: 0.000269, l4: 0.000569, l5: 0.001416, l6: 0.001557
[epoch: 877/1000, batch:   608/ 1052, ite: 230540] train loss: 0.003988, tar: 0.000057 
l0: 0.000005, l1: 0.000005, l2: 0.000013, l3: 0.000068, l4: 0.000380, l5: 0.000729, l6: 0.000617
[epoch: 877/1000, batch:   688/ 1052, ite: 230560] train loss: 0.003987, tar: 0.000057 
l0: 0.000025, l1: 0.000025, l2: 0.000079, l3: 0.000264, l4: 0.000557, l5: 0.001003, l6: 0.001470
[epoch: 877/1000, batch:   768/ 1052, ite: 230580] train loss: 0.003986, tar: 0.000057 
l0: 0.000112, l1: 0.000117, l2: 0.000151, l3: 0.000288, l4: 0.000443, l5: 0.001480, l6: 0.002240
[epoch: 877/1000, batch:   848/ 1052, ite: 230600] train loss: 0.003986, tar: 0.000057 
l0: 0.000071, l1: 0.000070, l2: 0.000114, l3: 0.000277, l4: 0.000616, l5: 0.001027, l6: 0.002870
[epoch: 877/1000, batch:   928/ 1052, ite: 230620] train loss: 0.003985, tar: 0.000057 
l0: 0.000006, l1: 0.000008, l2: 0.000028, l3: 0.000160, l4: 0.000407, l5: 0.000931, l6: 0.001792
[epoch: 877/1000, batch:  1008/ 1052, ite: 230640] train loss: 0.003985, tar: 0.000056 
[Epoch 877/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000064, l1: 0.000065, l2: 0.000107, l3: 0.000225, l4: 0.000581, l5: 0.001233, l6: 0.002396
[epoch: 878/1000, batch:    36/ 1052, ite: 230660] train loss: 0.003984, tar: 0.000056 
l0: 0.000024, l1: 0.000021, l2: 0.000089, l3: 0.000283, l4: 0.000534, l5: 0.001420, l6: 0.005081
[epoch: 878/1000, batch:   116/ 1052, ite: 230680] train loss: 0.003985, tar: 0.000056 
l0: 0.000008, l1: 0.000007, l2: 0.000023, l3: 0.000094, l4: 0.000307, l5: 0.001071, l6: 0.001602
[epoch: 878/1000, batch:   196/ 1052, ite: 230700] train loss: 0.003985, tar: 0.000056 
l0: 0.000023, l1: 0.000022, l2: 0.000054, l3: 0.000294, l4: 0.000700, l5: 0.000920, l6: 0.002198
[epoch: 878/1000, batch:   276/ 1052, ite: 230720] train loss: 0.003984, tar: 0.000056 
l0: 0.000025, l1: 0.000026, l2: 0.000084, l3: 0.000321, l4: 0.000878, l5: 0.001054, l6: 0.002340
[epoch: 878/1000, batch:   356/ 1052, ite: 230740] train loss: 0.003985, tar: 0.000056 
l0: 0.000029, l1: 0.000029, l2: 0.000113, l3: 0.000240, l4: 0.001141, l5: 0.002602, l6: 0.004043
[epoch: 878/1000, batch:   436/ 1052, ite: 230760] train loss: 0.003985, tar: 0.000056 
l0: 0.000002, l1: 0.000002, l2: 0.000033, l3: 0.000102, l4: 0.000378, l5: 0.000960, l6: 0.001380
[epoch: 878/1000, batch:   516/ 1052, ite: 230780] train loss: 0.003985, tar: 0.000056 
l0: 0.000079, l1: 0.000078, l2: 0.000091, l3: 0.000167, l4: 0.000413, l5: 0.000823, l6: 0.001331
[epoch: 878/1000, batch:   596/ 1052, ite: 230800] train loss: 0.003985, tar: 0.000056 
l0: 0.000070, l1: 0.000071, l2: 0.000100, l3: 0.000189, l4: 0.000559, l5: 0.001589, l6: 0.002092
[epoch: 878/1000, batch:   676/ 1052, ite: 230820] train loss: 0.003985, tar: 0.000056 
l0: 0.000017, l1: 0.000016, l2: 0.000046, l3: 0.000147, l4: 0.000443, l5: 0.000781, l6: 0.001816
[epoch: 878/1000, batch:   756/ 1052, ite: 230840] train loss: 0.003986, tar: 0.000056 
l0: 0.000035, l1: 0.000036, l2: 0.000047, l3: 0.000147, l4: 0.000429, l5: 0.000827, l6: 0.001306
[epoch: 878/1000, batch:   836/ 1052, ite: 230860] train loss: 0.003985, tar: 0.000056 
l0: 0.000012, l1: 0.000012, l2: 0.000020, l3: 0.000099, l4: 0.000272, l5: 0.000629, l6: 0.001052
[epoch: 878/1000, batch:   916/ 1052, ite: 230880] train loss: 0.003984, tar: 0.000056 
l0: 0.000029, l1: 0.000034, l2: 0.000050, l3: 0.000114, l4: 0.000447, l5: 0.000524, l6: 0.001430
[epoch: 878/1000, batch:   996/ 1052, ite: 230900] train loss: 0.003984, tar: 0.000056 
[Epoch 878/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000032, l1: 0.000033, l2: 0.000055, l3: 0.000172, l4: 0.000420, l5: 0.000731, l6: 0.001919
[epoch: 879/1000, batch:    24/ 1052, ite: 230920] train loss: 0.003982, tar: 0.000056 
l0: 0.000049, l1: 0.000050, l2: 0.000068, l3: 0.000171, l4: 0.000379, l5: 0.001060, l6: 0.001646
[epoch: 879/1000, batch:   104/ 1052, ite: 230940] train loss: 0.003982, tar: 0.000056 
l0: 0.000019, l1: 0.000018, l2: 0.000128, l3: 0.000459, l4: 0.001060, l5: 0.002744, l6: 0.005482
[epoch: 879/1000, batch:   184/ 1052, ite: 230960] train loss: 0.003982, tar: 0.000056 
l0: 0.000021, l1: 0.000021, l2: 0.000045, l3: 0.000069, l4: 0.000338, l5: 0.000741, l6: 0.001313
[epoch: 879/1000, batch:   264/ 1052, ite: 230980] train loss: 0.003982, tar: 0.000056 
l0: 0.000014, l1: 0.000014, l2: 0.000035, l3: 0.000237, l4: 0.000552, l5: 0.001264, l6: 0.002106
[epoch: 879/1000, batch:   344/ 1052, ite: 231000] train loss: 0.003982, tar: 0.000056 
l0: 0.000001, l1: 0.000002, l2: 0.000010, l3: 0.000061, l4: 0.000157, l5: 0.000725, l6: 0.001010
[epoch: 879/1000, batch:   424/ 1052, ite: 231020] train loss: 0.003982, tar: 0.000056 
l0: 0.000033, l1: 0.000033, l2: 0.000059, l3: 0.000163, l4: 0.000548, l5: 0.001186, l6: 0.001860
[epoch: 879/1000, batch:   504/ 1052, ite: 231040] train loss: 0.003981, tar: 0.000056 
l0: 0.000047, l1: 0.000047, l2: 0.000055, l3: 0.000163, l4: 0.000350, l5: 0.000807, l6: 0.001390
[epoch: 879/1000, batch:   584/ 1052, ite: 231060] train loss: 0.003980, tar: 0.000056 
l0: 0.000031, l1: 0.000031, l2: 0.000049, l3: 0.000184, l4: 0.000410, l5: 0.000642, l6: 0.001287
[epoch: 879/1000, batch:   664/ 1052, ite: 231080] train loss: 0.003980, tar: 0.000056 
l0: 0.000100, l1: 0.000102, l2: 0.000137, l3: 0.000233, l4: 0.000401, l5: 0.001190, l6: 0.003387
[epoch: 879/1000, batch:   744/ 1052, ite: 231100] train loss: 0.003980, tar: 0.000056 
l0: 0.000051, l1: 0.000053, l2: 0.000116, l3: 0.000169, l4: 0.000476, l5: 0.001615, l6: 0.002822
[epoch: 879/1000, batch:   824/ 1052, ite: 231120] train loss: 0.003980, tar: 0.000056 
l0: 0.000005, l1: 0.000004, l2: 0.000017, l3: 0.000095, l4: 0.000285, l5: 0.001316, l6: 0.001377
[epoch: 879/1000, batch:   904/ 1052, ite: 231140] train loss: 0.003980, tar: 0.000056 
l0: 0.000001, l1: 0.000001, l2: 0.000021, l3: 0.000118, l4: 0.000299, l5: 0.001082, l6: 0.001353
[epoch: 879/1000, batch:   984/ 1052, ite: 231160] train loss: 0.003980, tar: 0.000056 
[Epoch 879/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000013, l1: 0.000015, l2: 0.000036, l3: 0.000125, l4: 0.000244, l5: 0.000498, l6: 0.001196
[epoch: 880/1000, batch:    12/ 1052, ite: 231180] train loss: 0.003979, tar: 0.000056 
l0: 0.000072, l1: 0.000072, l2: 0.000096, l3: 0.000237, l4: 0.000656, l5: 0.001236, l6: 0.002560
[epoch: 880/1000, batch:    92/ 1052, ite: 231200] train loss: 0.003979, tar: 0.000056 
l0: 0.000015, l1: 0.000017, l2: 0.000044, l3: 0.000117, l4: 0.000272, l5: 0.001266, l6: 0.002632
[epoch: 880/1000, batch:   172/ 1052, ite: 231220] train loss: 0.003978, tar: 0.000056 
l0: 0.000015, l1: 0.000015, l2: 0.000041, l3: 0.000226, l4: 0.000684, l5: 0.001266, l6: 0.003065
[epoch: 880/1000, batch:   252/ 1052, ite: 231240] train loss: 0.003977, tar: 0.000056 
l0: 0.000006, l1: 0.000005, l2: 0.000068, l3: 0.000276, l4: 0.001049, l5: 0.001217, l6: 0.002982
[epoch: 880/1000, batch:   332/ 1052, ite: 231260] train loss: 0.003978, tar: 0.000055 
l0: 0.000021, l1: 0.000019, l2: 0.000038, l3: 0.000120, l4: 0.000319, l5: 0.000690, l6: 0.001199
[epoch: 880/1000, batch:   412/ 1052, ite: 231280] train loss: 0.003977, tar: 0.000055 
l0: 0.000097, l1: 0.000097, l2: 0.000116, l3: 0.000249, l4: 0.000520, l5: 0.001777, l6: 0.002703
[epoch: 880/1000, batch:   492/ 1052, ite: 231300] train loss: 0.003977, tar: 0.000055 
l0: 0.000018, l1: 0.000019, l2: 0.000081, l3: 0.000316, l4: 0.000636, l5: 0.001005, l6: 0.004162
[epoch: 880/1000, batch:   572/ 1052, ite: 231320] train loss: 0.003977, tar: 0.000055 
l0: 0.000024, l1: 0.000024, l2: 0.000072, l3: 0.000197, l4: 0.000466, l5: 0.001359, l6: 0.001707
[epoch: 880/1000, batch:   652/ 1052, ite: 231340] train loss: 0.003977, tar: 0.000055 
l0: 0.000006, l1: 0.000006, l2: 0.000031, l3: 0.000280, l4: 0.000636, l5: 0.001274, l6: 0.002459
[epoch: 880/1000, batch:   732/ 1052, ite: 231360] train loss: 0.003977, tar: 0.000055 
l0: 0.000056, l1: 0.000056, l2: 0.000115, l3: 0.000218, l4: 0.000685, l5: 0.002557, l6: 0.002851
[epoch: 880/1000, batch:   812/ 1052, ite: 231380] train loss: 0.003977, tar: 0.000055 
l0: 0.000009, l1: 0.000009, l2: 0.000048, l3: 0.000168, l4: 0.000564, l5: 0.001381, l6: 0.001698
[epoch: 880/1000, batch:   892/ 1052, ite: 231400] train loss: 0.003977, tar: 0.000055 
l0: 0.000048, l1: 0.000052, l2: 0.000060, l3: 0.000164, l4: 0.000414, l5: 0.000763, l6: 0.001649
[epoch: 880/1000, batch:   972/ 1052, ite: 231420] train loss: 0.003977, tar: 0.000055 
l0: 0.000062, l1: 0.000060, l2: 0.000076, l3: 0.000186, l4: 0.000730, l5: 0.001492, l6: 0.002374
[epoch: 880/1000, batch:  1052/ 1052, ite: 231440] train loss: 0.003977, tar: 0.000055 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_231440.pkl
[Epoch 880/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000055, l1: 0.000054, l2: 0.000097, l3: 0.000216, l4: 0.000461, l5: 0.000676, l6: 0.001624
[epoch: 881/1000, batch:    80/ 1052, ite: 231460] train loss: 0.003743, tar: 0.000039 
l0: 0.000005, l1: 0.000005, l2: 0.000015, l3: 0.000091, l4: 0.000243, l5: 0.000738, l6: 0.002003
[epoch: 881/1000, batch:   160/ 1052, ite: 231480] train loss: 0.003824, tar: 0.000037 
l0: 0.000016, l1: 0.000021, l2: 0.000042, l3: 0.000125, l4: 0.000278, l5: 0.000795, l6: 0.002113
[epoch: 881/1000, batch:   240/ 1052, ite: 231500] train loss: 0.003970, tar: 0.000044 
l0: 0.000068, l1: 0.000069, l2: 0.000107, l3: 0.000321, l4: 0.000940, l5: 0.001802, l6: 0.003835
[epoch: 881/1000, batch:   320/ 1052, ite: 231520] train loss: 0.004019, tar: 0.000044 
l0: 0.000002, l1: 0.000002, l2: 0.000016, l3: 0.000161, l4: 0.000541, l5: 0.001063, l6: 0.002533
[epoch: 881/1000, batch:   400/ 1052, ite: 231540] train loss: 0.004064, tar: 0.000044 
l0: 0.000012, l1: 0.000013, l2: 0.000083, l3: 0.000165, l4: 0.000326, l5: 0.000900, l6: 0.001235
[epoch: 881/1000, batch:   480/ 1052, ite: 231560] train loss: 0.004062, tar: 0.000042 
l0: 0.000001, l1: 0.000001, l2: 0.000003, l3: 0.000038, l4: 0.000287, l5: 0.000325, l6: 0.000877
[epoch: 881/1000, batch:   560/ 1052, ite: 231580] train loss: 0.004032, tar: 0.000044 
l0: 0.000112, l1: 0.000114, l2: 0.000141, l3: 0.000217, l4: 0.000588, l5: 0.001571, l6: 0.003591
[epoch: 881/1000, batch:   640/ 1052, ite: 231600] train loss: 0.004019, tar: 0.000044 
l0: 0.000106, l1: 0.000105, l2: 0.000141, l3: 0.000265, l4: 0.000440, l5: 0.001025, l6: 0.002616
[epoch: 881/1000, batch:   720/ 1052, ite: 231620] train loss: 0.003957, tar: 0.000044 
l0: 0.000019, l1: 0.000018, l2: 0.000054, l3: 0.000125, l4: 0.000221, l5: 0.000590, l6: 0.001169
[epoch: 881/1000, batch:   800/ 1052, ite: 231640] train loss: 0.003875, tar: 0.000042 
l0: 0.000008, l1: 0.000007, l2: 0.000021, l3: 0.000106, l4: 0.000441, l5: 0.000697, l6: 0.002178
[epoch: 881/1000, batch:   880/ 1052, ite: 231660] train loss: 0.003920, tar: 0.000042 
l0: 0.000008, l1: 0.000008, l2: 0.000016, l3: 0.000066, l4: 0.000355, l5: 0.000955, l6: 0.001431
[epoch: 881/1000, batch:   960/ 1052, ite: 231680] train loss: 0.003862, tar: 0.000041 
l0: 0.000015, l1: 0.000014, l2: 0.000046, l3: 0.000188, l4: 0.000489, l5: 0.001974, l6: 0.003869
[epoch: 881/1000, batch:  1040/ 1052, ite: 231700] train loss: 0.003871, tar: 0.000041 
[Epoch 881/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000033, l1: 0.000034, l2: 0.000068, l3: 0.000180, l4: 0.000431, l5: 0.001319, l6: 0.002230
[epoch: 882/1000, batch:    68/ 1052, ite: 231720] train loss: 0.003838, tar: 0.000040 
l0: 0.000010, l1: 0.000010, l2: 0.000028, l3: 0.000133, l4: 0.000313, l5: 0.000403, l6: 0.001232
[epoch: 882/1000, batch:   148/ 1052, ite: 231740] train loss: 0.003831, tar: 0.000039 
l0: 0.000150, l1: 0.000150, l2: 0.000207, l3: 0.000552, l4: 0.001077, l5: 0.001735, l6: 0.002474
[epoch: 882/1000, batch:   228/ 1052, ite: 231760] train loss: 0.003849, tar: 0.000040 
l0: 0.000034, l1: 0.000035, l2: 0.000060, l3: 0.000169, l4: 0.000598, l5: 0.001225, l6: 0.001174
[epoch: 882/1000, batch:   308/ 1052, ite: 231780] train loss: 0.003852, tar: 0.000039 
l0: 0.000033, l1: 0.000033, l2: 0.000057, l3: 0.000118, l4: 0.000328, l5: 0.000755, l6: 0.001637
[epoch: 882/1000, batch:   388/ 1052, ite: 231800] train loss: 0.003826, tar: 0.000039 
l0: 0.000186, l1: 0.000188, l2: 0.000200, l3: 0.000257, l4: 0.000593, l5: 0.001808, l6: 0.003367
[epoch: 882/1000, batch:   468/ 1052, ite: 231820] train loss: 0.003846, tar: 0.000039 
l0: 0.000016, l1: 0.000017, l2: 0.000033, l3: 0.000133, l4: 0.000342, l5: 0.000499, l6: 0.001236
[epoch: 882/1000, batch:   548/ 1052, ite: 231840] train loss: 0.003839, tar: 0.000039 
l0: 0.000025, l1: 0.000025, l2: 0.000065, l3: 0.000169, l4: 0.000532, l5: 0.001248, l6: 0.001771
[epoch: 882/1000, batch:   628/ 1052, ite: 231860] train loss: 0.003854, tar: 0.000039 
l0: 0.000019, l1: 0.000019, l2: 0.000040, l3: 0.000273, l4: 0.000592, l5: 0.001561, l6: 0.002619
[epoch: 882/1000, batch:   708/ 1052, ite: 231880] train loss: 0.003874, tar: 0.000040 
l0: 0.000074, l1: 0.000075, l2: 0.000107, l3: 0.000194, l4: 0.000457, l5: 0.000976, l6: 0.001112
[epoch: 882/1000, batch:   788/ 1052, ite: 231900] train loss: 0.003863, tar: 0.000039 
l0: 0.000084, l1: 0.000085, l2: 0.000118, l3: 0.000242, l4: 0.000697, l5: 0.001077, l6: 0.002440
[epoch: 882/1000, batch:   868/ 1052, ite: 231920] train loss: 0.003857, tar: 0.000040 
l0: 0.000003, l1: 0.000003, l2: 0.000014, l3: 0.000175, l4: 0.000348, l5: 0.000524, l6: 0.001403
[epoch: 882/1000, batch:   948/ 1052, ite: 231940] train loss: 0.003855, tar: 0.000040 
l0: 0.000005, l1: 0.000005, l2: 0.000011, l3: 0.000116, l4: 0.000311, l5: 0.000843, l6: 0.001287
[epoch: 882/1000, batch:  1028/ 1052, ite: 231960] train loss: 0.003859, tar: 0.000040 
[Epoch 882/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000056, l1: 0.000057, l2: 0.000074, l3: 0.000178, l4: 0.000497, l5: 0.001065, l6: 0.001620
[epoch: 883/1000, batch:    56/ 1052, ite: 231980] train loss: 0.003859, tar: 0.000040 
l0: 0.000020, l1: 0.000020, l2: 0.000044, l3: 0.000109, l4: 0.000328, l5: 0.001135, l6: 0.001488
[epoch: 883/1000, batch:   136/ 1052, ite: 232000] train loss: 0.003876, tar: 0.000040 
l0: 0.000019, l1: 0.000018, l2: 0.000053, l3: 0.000181, l4: 0.000343, l5: 0.001294, l6: 0.002254
[epoch: 883/1000, batch:   216/ 1052, ite: 232020] train loss: 0.003857, tar: 0.000040 
l0: 0.000021, l1: 0.000022, l2: 0.000026, l3: 0.000061, l4: 0.000187, l5: 0.000535, l6: 0.001695
[epoch: 883/1000, batch:   296/ 1052, ite: 232040] train loss: 0.003847, tar: 0.000040 
l0: 0.000047, l1: 0.000047, l2: 0.000080, l3: 0.000238, l4: 0.000374, l5: 0.001282, l6: 0.002163
[epoch: 883/1000, batch:   376/ 1052, ite: 232060] train loss: 0.003856, tar: 0.000040 
l0: 0.000016, l1: 0.000016, l2: 0.000056, l3: 0.000203, l4: 0.000483, l5: 0.000968, l6: 0.002632
[epoch: 883/1000, batch:   456/ 1052, ite: 232080] train loss: 0.003861, tar: 0.000040 
l0: 0.000116, l1: 0.000116, l2: 0.000177, l3: 0.000303, l4: 0.000458, l5: 0.000808, l6: 0.001411
[epoch: 883/1000, batch:   536/ 1052, ite: 232100] train loss: 0.003851, tar: 0.000040 
l0: 0.000048, l1: 0.000047, l2: 0.000077, l3: 0.000225, l4: 0.000433, l5: 0.000876, l6: 0.001955
[epoch: 883/1000, batch:   616/ 1052, ite: 232120] train loss: 0.003847, tar: 0.000040 
l0: 0.000064, l1: 0.000064, l2: 0.000076, l3: 0.000210, l4: 0.000402, l5: 0.000948, l6: 0.001629
[epoch: 883/1000, batch:   696/ 1052, ite: 232140] train loss: 0.003850, tar: 0.000041 
l0: 0.000004, l1: 0.000004, l2: 0.000019, l3: 0.000065, l4: 0.000124, l5: 0.000922, l6: 0.001429
[epoch: 883/1000, batch:   776/ 1052, ite: 232160] train loss: 0.003841, tar: 0.000041 
l0: 0.000031, l1: 0.000031, l2: 0.000075, l3: 0.000223, l4: 0.000557, l5: 0.000799, l6: 0.002019
[epoch: 883/1000, batch:   856/ 1052, ite: 232180] train loss: 0.003835, tar: 0.000040 
l0: 0.000117, l1: 0.000118, l2: 0.000152, l3: 0.000252, l4: 0.000532, l5: 0.001167, l6: 0.002187
[epoch: 883/1000, batch:   936/ 1052, ite: 232200] train loss: 0.003857, tar: 0.000040 
l0: 0.000006, l1: 0.000006, l2: 0.000030, l3: 0.000128, l4: 0.000314, l5: 0.000446, l6: 0.001096
[epoch: 883/1000, batch:  1016/ 1052, ite: 232220] train loss: 0.003847, tar: 0.000040 
[Epoch 883/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000125, l1: 0.000126, l2: 0.000157, l3: 0.000252, l4: 0.000401, l5: 0.001468, l6: 0.001930
[epoch: 884/1000, batch:    44/ 1052, ite: 232240] train loss: 0.003857, tar: 0.000040 
l0: 0.000063, l1: 0.000065, l2: 0.000086, l3: 0.000240, l4: 0.000667, l5: 0.002090, l6: 0.003732
[epoch: 884/1000, batch:   124/ 1052, ite: 232260] train loss: 0.003855, tar: 0.000040 
l0: 0.000003, l1: 0.000002, l2: 0.000006, l3: 0.000082, l4: 0.000351, l5: 0.000870, l6: 0.000837
[epoch: 884/1000, batch:   204/ 1052, ite: 232280] train loss: 0.003858, tar: 0.000040 
l0: 0.000042, l1: 0.000043, l2: 0.000050, l3: 0.000100, l4: 0.000455, l5: 0.000672, l6: 0.001192
[epoch: 884/1000, batch:   284/ 1052, ite: 232300] train loss: 0.003864, tar: 0.000040 
l0: 0.000032, l1: 0.000033, l2: 0.000093, l3: 0.000438, l4: 0.001163, l5: 0.002191, l6: 0.004241
[epoch: 884/1000, batch:   364/ 1052, ite: 232320] train loss: 0.003878, tar: 0.000041 
l0: 0.000032, l1: 0.000034, l2: 0.000084, l3: 0.000258, l4: 0.000769, l5: 0.001462, l6: 0.002037
[epoch: 884/1000, batch:   444/ 1052, ite: 232340] train loss: 0.003877, tar: 0.000041 
l0: 0.000024, l1: 0.000023, l2: 0.000040, l3: 0.000130, l4: 0.000278, l5: 0.000937, l6: 0.001320
[epoch: 884/1000, batch:   524/ 1052, ite: 232360] train loss: 0.003871, tar: 0.000041 
l0: 0.000006, l1: 0.000006, l2: 0.000038, l3: 0.000114, l4: 0.000304, l5: 0.000561, l6: 0.001711
[epoch: 884/1000, batch:   604/ 1052, ite: 232380] train loss: 0.003861, tar: 0.000041 
l0: 0.000001, l1: 0.000001, l2: 0.000015, l3: 0.000076, l4: 0.000381, l5: 0.001223, l6: 0.001838
[epoch: 884/1000, batch:   684/ 1052, ite: 232400] train loss: 0.003856, tar: 0.000041 
l0: 0.000047, l1: 0.000049, l2: 0.000082, l3: 0.000098, l4: 0.000411, l5: 0.000722, l6: 0.001895
[epoch: 884/1000, batch:   764/ 1052, ite: 232420] train loss: 0.003853, tar: 0.000041 
l0: 0.000010, l1: 0.000009, l2: 0.000035, l3: 0.000173, l4: 0.000352, l5: 0.000964, l6: 0.003237
[epoch: 884/1000, batch:   844/ 1052, ite: 232440] train loss: 0.003859, tar: 0.000041 
l0: 0.000031, l1: 0.000029, l2: 0.000066, l3: 0.000275, l4: 0.000613, l5: 0.001344, l6: 0.002541
[epoch: 884/1000, batch:   924/ 1052, ite: 232460] train loss: 0.003863, tar: 0.000041 
l0: 0.000011, l1: 0.000010, l2: 0.000037, l3: 0.000149, l4: 0.000421, l5: 0.001057, l6: 0.001859
[epoch: 884/1000, batch:  1004/ 1052, ite: 232480] train loss: 0.003866, tar: 0.000041 
[Epoch 884/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000016, l1: 0.000017, l2: 0.000018, l3: 0.000053, l4: 0.000137, l5: 0.000639, l6: 0.000859
[epoch: 885/1000, batch:    32/ 1052, ite: 232500] train loss: 0.003868, tar: 0.000041 
l0: 0.000016, l1: 0.000015, l2: 0.000030, l3: 0.000100, l4: 0.000232, l5: 0.000665, l6: 0.001235
[epoch: 885/1000, batch:   112/ 1052, ite: 232520] train loss: 0.003876, tar: 0.000041 
l0: 0.000006, l1: 0.000006, l2: 0.000012, l3: 0.000096, l4: 0.000290, l5: 0.000712, l6: 0.001642
[epoch: 885/1000, batch:   192/ 1052, ite: 232540] train loss: 0.003878, tar: 0.000041 
l0: 0.000045, l1: 0.000046, l2: 0.000075, l3: 0.000226, l4: 0.000588, l5: 0.001041, l6: 0.001940
[epoch: 885/1000, batch:   272/ 1052, ite: 232560] train loss: 0.003874, tar: 0.000041 
l0: 0.000039, l1: 0.000040, l2: 0.000052, l3: 0.000077, l4: 0.000246, l5: 0.000839, l6: 0.001194
[epoch: 885/1000, batch:   352/ 1052, ite: 232580] train loss: 0.003868, tar: 0.000041 
l0: 0.000008, l1: 0.000008, l2: 0.000020, l3: 0.000064, l4: 0.000402, l5: 0.000592, l6: 0.001160
[epoch: 885/1000, batch:   432/ 1052, ite: 232600] train loss: 0.003863, tar: 0.000041 
l0: 0.000020, l1: 0.000021, l2: 0.000031, l3: 0.000070, l4: 0.000395, l5: 0.001064, l6: 0.001931
[epoch: 885/1000, batch:   512/ 1052, ite: 232620] train loss: 0.003871, tar: 0.000041 
l0: 0.000059, l1: 0.000061, l2: 0.000099, l3: 0.000186, l4: 0.000470, l5: 0.001188, l6: 0.002443
[epoch: 885/1000, batch:   592/ 1052, ite: 232640] train loss: 0.003872, tar: 0.000041 
l0: 0.000055, l1: 0.000055, l2: 0.000087, l3: 0.000162, l4: 0.000512, l5: 0.001042, l6: 0.001606
[epoch: 885/1000, batch:   672/ 1052, ite: 232660] train loss: 0.003884, tar: 0.000041 
l0: 0.000063, l1: 0.000064, l2: 0.000111, l3: 0.000324, l4: 0.000594, l5: 0.001268, l6: 0.002440
[epoch: 885/1000, batch:   752/ 1052, ite: 232680] train loss: 0.003885, tar: 0.000041 
l0: 0.000031, l1: 0.000031, l2: 0.000060, l3: 0.000174, l4: 0.000450, l5: 0.001178, l6: 0.001624
[epoch: 885/1000, batch:   832/ 1052, ite: 232700] train loss: 0.003886, tar: 0.000041 
l0: 0.000026, l1: 0.000028, l2: 0.000035, l3: 0.000119, l4: 0.000398, l5: 0.000643, l6: 0.001520
[epoch: 885/1000, batch:   912/ 1052, ite: 232720] train loss: 0.003880, tar: 0.000041 
l0: 0.000107, l1: 0.000103, l2: 0.000201, l3: 0.000311, l4: 0.000487, l5: 0.001361, l6: 0.004371
[epoch: 885/1000, batch:   992/ 1052, ite: 232740] train loss: 0.003880, tar: 0.000041 
[Epoch 885/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000001, l1: 0.000001, l2: 0.000012, l3: 0.000088, l4: 0.000258, l5: 0.000516, l6: 0.001244
[epoch: 886/1000, batch:    20/ 1052, ite: 232760] train loss: 0.003869, tar: 0.000041 
l0: 0.000006, l1: 0.000006, l2: 0.000036, l3: 0.000084, l4: 0.000376, l5: 0.000808, l6: 0.001464
[epoch: 886/1000, batch:   100/ 1052, ite: 232780] train loss: 0.003872, tar: 0.000041 
l0: 0.000026, l1: 0.000026, l2: 0.000064, l3: 0.000104, l4: 0.000166, l5: 0.000765, l6: 0.001323
[epoch: 886/1000, batch:   180/ 1052, ite: 232800] train loss: 0.003871, tar: 0.000041 
l0: 0.000065, l1: 0.000067, l2: 0.000123, l3: 0.000344, l4: 0.000673, l5: 0.001745, l6: 0.002769
[epoch: 886/1000, batch:   260/ 1052, ite: 232820] train loss: 0.003878, tar: 0.000041 
l0: 0.000179, l1: 0.000189, l2: 0.000208, l3: 0.000270, l4: 0.000317, l5: 0.000837, l6: 0.001471
[epoch: 886/1000, batch:   340/ 1052, ite: 232840] train loss: 0.003875, tar: 0.000041 
l0: 0.000012, l1: 0.000012, l2: 0.000028, l3: 0.000111, l4: 0.000369, l5: 0.000948, l6: 0.001320
[epoch: 886/1000, batch:   420/ 1052, ite: 232860] train loss: 0.003874, tar: 0.000041 
l0: 0.000095, l1: 0.000091, l2: 0.000138, l3: 0.000292, l4: 0.000485, l5: 0.001041, l6: 0.002133
[epoch: 886/1000, batch:   500/ 1052, ite: 232880] train loss: 0.003865, tar: 0.000041 
l0: 0.000024, l1: 0.000024, l2: 0.000050, l3: 0.000124, l4: 0.000340, l5: 0.001315, l6: 0.003244
[epoch: 886/1000, batch:   580/ 1052, ite: 232900] train loss: 0.003867, tar: 0.000041 
l0: 0.000124, l1: 0.000126, l2: 0.000168, l3: 0.000231, l4: 0.000875, l5: 0.001717, l6: 0.003053
[epoch: 886/1000, batch:   660/ 1052, ite: 232920] train loss: 0.003868, tar: 0.000041 
l0: 0.000050, l1: 0.000049, l2: 0.000080, l3: 0.000134, l4: 0.000374, l5: 0.001054, l6: 0.002257
[epoch: 886/1000, batch:   740/ 1052, ite: 232940] train loss: 0.003866, tar: 0.000042 
l0: 0.000021, l1: 0.000020, l2: 0.000037, l3: 0.000090, l4: 0.000282, l5: 0.000686, l6: 0.001009
[epoch: 886/1000, batch:   820/ 1052, ite: 232960] train loss: 0.003860, tar: 0.000041 
l0: 0.000190, l1: 0.000192, l2: 0.000233, l3: 0.000386, l4: 0.000743, l5: 0.002329, l6: 0.004019
[epoch: 886/1000, batch:   900/ 1052, ite: 232980] train loss: 0.003863, tar: 0.000041 
l0: 0.000134, l1: 0.000144, l2: 0.000192, l3: 0.000284, l4: 0.000656, l5: 0.002092, l6: 0.003901
[epoch: 886/1000, batch:   980/ 1052, ite: 233000] train loss: 0.003864, tar: 0.000041 
[Epoch 886/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000062, l1: 0.000062, l2: 0.000102, l3: 0.000200, l4: 0.000784, l5: 0.001642, l6: 0.002684
[epoch: 887/1000, batch:     8/ 1052, ite: 233020] train loss: 0.003866, tar: 0.000041 
l0: 0.000017, l1: 0.000018, l2: 0.000050, l3: 0.000181, l4: 0.000496, l5: 0.001237, l6: 0.001678
[epoch: 887/1000, batch:    88/ 1052, ite: 233040] train loss: 0.003867, tar: 0.000041 
l0: 0.000009, l1: 0.000008, l2: 0.000019, l3: 0.000121, l4: 0.000162, l5: 0.000723, l6: 0.001468
[epoch: 887/1000, batch:   168/ 1052, ite: 233060] train loss: 0.003872, tar: 0.000041 
l0: 0.000004, l1: 0.000003, l2: 0.000039, l3: 0.000170, l4: 0.000514, l5: 0.000746, l6: 0.001919
[epoch: 887/1000, batch:   248/ 1052, ite: 233080] train loss: 0.003878, tar: 0.000041 
l0: 0.000004, l1: 0.000004, l2: 0.000020, l3: 0.000126, l4: 0.000205, l5: 0.000731, l6: 0.001135
[epoch: 887/1000, batch:   328/ 1052, ite: 233100] train loss: 0.003878, tar: 0.000041 
l0: 0.000106, l1: 0.000104, l2: 0.000160, l3: 0.000301, l4: 0.000657, l5: 0.001503, l6: 0.002644
[epoch: 887/1000, batch:   408/ 1052, ite: 233120] train loss: 0.003876, tar: 0.000041 
l0: 0.000063, l1: 0.000064, l2: 0.000082, l3: 0.000152, l4: 0.000329, l5: 0.000638, l6: 0.002647
[epoch: 887/1000, batch:   488/ 1052, ite: 233140] train loss: 0.003875, tar: 0.000041 
l0: 0.000045, l1: 0.000045, l2: 0.000070, l3: 0.000139, l4: 0.000468, l5: 0.000768, l6: 0.001235
[epoch: 887/1000, batch:   568/ 1052, ite: 233160] train loss: 0.003877, tar: 0.000041 
l0: 0.000009, l1: 0.000008, l2: 0.000015, l3: 0.000093, l4: 0.000366, l5: 0.000968, l6: 0.001865
[epoch: 887/1000, batch:   648/ 1052, ite: 233180] train loss: 0.003876, tar: 0.000041 
l0: 0.000004, l1: 0.000004, l2: 0.000026, l3: 0.000178, l4: 0.000556, l5: 0.001615, l6: 0.002710
[epoch: 887/1000, batch:   728/ 1052, ite: 233200] train loss: 0.003872, tar: 0.000041 
l0: 0.000040, l1: 0.000042, l2: 0.000069, l3: 0.000269, l4: 0.000549, l5: 0.001256, l6: 0.001389
[epoch: 887/1000, batch:   808/ 1052, ite: 233220] train loss: 0.003871, tar: 0.000041 
l0: 0.000089, l1: 0.000090, l2: 0.000103, l3: 0.000164, l4: 0.000378, l5: 0.000880, l6: 0.001181
[epoch: 887/1000, batch:   888/ 1052, ite: 233240] train loss: 0.003865, tar: 0.000041 
l0: 0.000091, l1: 0.000088, l2: 0.000114, l3: 0.000205, l4: 0.000739, l5: 0.001244, l6: 0.002544
[epoch: 887/1000, batch:   968/ 1052, ite: 233260] train loss: 0.003865, tar: 0.000041 
l0: 0.000001, l1: 0.000001, l2: 0.000025, l3: 0.000102, l4: 0.000471, l5: 0.001396, l6: 0.001768
[epoch: 887/1000, batch:  1048/ 1052, ite: 233280] train loss: 0.003870, tar: 0.000041 
[Epoch 887/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000049, l1: 0.000049, l2: 0.000066, l3: 0.000104, l4: 0.000314, l5: 0.001057, l6: 0.001862
[epoch: 888/1000, batch:    76/ 1052, ite: 233300] train loss: 0.003869, tar: 0.000041 
l0: 0.000044, l1: 0.000041, l2: 0.000105, l3: 0.000352, l4: 0.000591, l5: 0.001020, l6: 0.001791
[epoch: 888/1000, batch:   156/ 1052, ite: 233320] train loss: 0.003874, tar: 0.000041 
l0: 0.000024, l1: 0.000026, l2: 0.000035, l3: 0.000073, l4: 0.000265, l5: 0.001157, l6: 0.001460
[epoch: 888/1000, batch:   236/ 1052, ite: 233340] train loss: 0.003878, tar: 0.000041 
l0: 0.000008, l1: 0.000008, l2: 0.000041, l3: 0.000125, l4: 0.000413, l5: 0.001192, l6: 0.003359
[epoch: 888/1000, batch:   316/ 1052, ite: 233360] train loss: 0.003879, tar: 0.000041 
l0: 0.000033, l1: 0.000034, l2: 0.000066, l3: 0.000105, l4: 0.000518, l5: 0.002075, l6: 0.003070
[epoch: 888/1000, batch:   396/ 1052, ite: 233380] train loss: 0.003884, tar: 0.000041 
l0: 0.000028, l1: 0.000026, l2: 0.000091, l3: 0.000213, l4: 0.000520, l5: 0.000652, l6: 0.001554
[epoch: 888/1000, batch:   476/ 1052, ite: 233400] train loss: 0.003883, tar: 0.000041 
l0: 0.000003, l1: 0.000003, l2: 0.000011, l3: 0.000066, l4: 0.000186, l5: 0.000552, l6: 0.000674
[epoch: 888/1000, batch:   556/ 1052, ite: 233420] train loss: 0.003878, tar: 0.000041 
l0: 0.000005, l1: 0.000005, l2: 0.000021, l3: 0.000089, l4: 0.000362, l5: 0.000615, l6: 0.001916
[epoch: 888/1000, batch:   636/ 1052, ite: 233440] train loss: 0.003881, tar: 0.000041 
l0: 0.000033, l1: 0.000034, l2: 0.000049, l3: 0.000110, l4: 0.000272, l5: 0.000839, l6: 0.001616
[epoch: 888/1000, batch:   716/ 1052, ite: 233460] train loss: 0.003878, tar: 0.000041 
l0: 0.000054, l1: 0.000054, l2: 0.000113, l3: 0.000265, l4: 0.000476, l5: 0.000896, l6: 0.001141
[epoch: 888/1000, batch:   796/ 1052, ite: 233480] train loss: 0.003878, tar: 0.000041 
l0: 0.000121, l1: 0.000121, l2: 0.000198, l3: 0.000317, l4: 0.000583, l5: 0.001170, l6: 0.001870
[epoch: 888/1000, batch:   876/ 1052, ite: 233500] train loss: 0.003874, tar: 0.000041 
l0: 0.000018, l1: 0.000019, l2: 0.000045, l3: 0.000088, l4: 0.000215, l5: 0.000758, l6: 0.001454
[epoch: 888/1000, batch:   956/ 1052, ite: 233520] train loss: 0.003871, tar: 0.000041 
l0: 0.000141, l1: 0.000144, l2: 0.000211, l3: 0.000497, l4: 0.000948, l5: 0.001497, l6: 0.003147
[epoch: 888/1000, batch:  1036/ 1052, ite: 233540] train loss: 0.003870, tar: 0.000041 
[Epoch 888/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000050, l1: 0.000051, l2: 0.000061, l3: 0.000092, l4: 0.000419, l5: 0.001084, l6: 0.001626
[epoch: 889/1000, batch:    64/ 1052, ite: 233560] train loss: 0.003872, tar: 0.000041 
l0: 0.000040, l1: 0.000035, l2: 0.000072, l3: 0.000261, l4: 0.000493, l5: 0.001389, l6: 0.002687
[epoch: 889/1000, batch:   144/ 1052, ite: 233580] train loss: 0.003868, tar: 0.000041 
l0: 0.000006, l1: 0.000006, l2: 0.000034, l3: 0.000109, l4: 0.000381, l5: 0.000602, l6: 0.001775
[epoch: 889/1000, batch:   224/ 1052, ite: 233600] train loss: 0.003867, tar: 0.000041 
l0: 0.000027, l1: 0.000027, l2: 0.000055, l3: 0.000187, l4: 0.000323, l5: 0.000978, l6: 0.000968
[epoch: 889/1000, batch:   304/ 1052, ite: 233620] train loss: 0.003870, tar: 0.000041 
l0: 0.000065, l1: 0.000066, l2: 0.000064, l3: 0.000204, l4: 0.000615, l5: 0.001100, l6: 0.002732
[epoch: 889/1000, batch:   384/ 1052, ite: 233640] train loss: 0.003875, tar: 0.000041 
l0: 0.000041, l1: 0.000031, l2: 0.000198, l3: 0.000306, l4: 0.000600, l5: 0.001837, l6: 0.002901
[epoch: 889/1000, batch:   464/ 1052, ite: 233660] train loss: 0.003877, tar: 0.000041 
l0: 0.000015, l1: 0.000015, l2: 0.000023, l3: 0.000064, l4: 0.000453, l5: 0.000767, l6: 0.001610
[epoch: 889/1000, batch:   544/ 1052, ite: 233680] train loss: 0.003877, tar: 0.000041 
l0: 0.000010, l1: 0.000010, l2: 0.000034, l3: 0.000147, l4: 0.000338, l5: 0.001691, l6: 0.002826
[epoch: 889/1000, batch:   624/ 1052, ite: 233700] train loss: 0.003877, tar: 0.000041 
l0: 0.000066, l1: 0.000068, l2: 0.000113, l3: 0.000218, l4: 0.000379, l5: 0.000586, l6: 0.001624
[epoch: 889/1000, batch:   704/ 1052, ite: 233720] train loss: 0.003877, tar: 0.000041 
l0: 0.000001, l1: 0.000001, l2: 0.000006, l3: 0.000056, l4: 0.000505, l5: 0.000955, l6: 0.001260
[epoch: 889/1000, batch:   784/ 1052, ite: 233740] train loss: 0.003872, tar: 0.000041 
l0: 0.000055, l1: 0.000055, l2: 0.000147, l3: 0.000337, l4: 0.000763, l5: 0.001801, l6: 0.002786
[epoch: 889/1000, batch:   864/ 1052, ite: 233760] train loss: 0.003875, tar: 0.000041 
l0: 0.000073, l1: 0.000074, l2: 0.000102, l3: 0.000207, l4: 0.000482, l5: 0.000987, l6: 0.001334
[epoch: 889/1000, batch:   944/ 1052, ite: 233780] train loss: 0.003873, tar: 0.000041 
l0: 0.000004, l1: 0.000004, l2: 0.000011, l3: 0.000115, l4: 0.000596, l5: 0.001137, l6: 0.001967
[epoch: 889/1000, batch:  1024/ 1052, ite: 233800] train loss: 0.003868, tar: 0.000041 
[Epoch 889/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000005, l1: 0.000005, l2: 0.000027, l3: 0.000182, l4: 0.000684, l5: 0.001522, l6: 0.002881
[epoch: 890/1000, batch:    52/ 1052, ite: 233820] train loss: 0.003870, tar: 0.000041 
l0: 0.000173, l1: 0.000178, l2: 0.000206, l3: 0.000328, l4: 0.000703, l5: 0.002250, l6: 0.004714
[epoch: 890/1000, batch:   132/ 1052, ite: 233840] train loss: 0.003873, tar: 0.000041 
l0: 0.000090, l1: 0.000091, l2: 0.000112, l3: 0.000182, l4: 0.000439, l5: 0.001345, l6: 0.001972
[epoch: 890/1000, batch:   212/ 1052, ite: 233860] train loss: 0.003872, tar: 0.000041 
l0: 0.000225, l1: 0.000231, l2: 0.000254, l3: 0.000383, l4: 0.001000, l5: 0.002495, l6: 0.004553
[epoch: 890/1000, batch:   292/ 1052, ite: 233880] train loss: 0.003871, tar: 0.000042 
l0: 0.000016, l1: 0.000017, l2: 0.000022, l3: 0.000104, l4: 0.000304, l5: 0.000756, l6: 0.001874
[epoch: 890/1000, batch:   372/ 1052, ite: 233900] train loss: 0.003874, tar: 0.000042 
l0: 0.000020, l1: 0.000020, l2: 0.000040, l3: 0.000131, l4: 0.000500, l5: 0.000696, l6: 0.001533
[epoch: 890/1000, batch:   452/ 1052, ite: 233920] train loss: 0.003874, tar: 0.000042 
l0: 0.000009, l1: 0.000009, l2: 0.000025, l3: 0.000057, l4: 0.000285, l5: 0.000874, l6: 0.001592
[epoch: 890/1000, batch:   532/ 1052, ite: 233940] train loss: 0.003874, tar: 0.000042 
l0: 0.000053, l1: 0.000053, l2: 0.000080, l3: 0.000189, l4: 0.000528, l5: 0.000795, l6: 0.001295
[epoch: 890/1000, batch:   612/ 1052, ite: 233960] train loss: 0.003875, tar: 0.000042 
l0: 0.000162, l1: 0.000171, l2: 0.000194, l3: 0.000300, l4: 0.000530, l5: 0.001311, l6: 0.002234
[epoch: 890/1000, batch:   692/ 1052, ite: 233980] train loss: 0.003874, tar: 0.000042 
l0: 0.000006, l1: 0.000007, l2: 0.000023, l3: 0.000145, l4: 0.000409, l5: 0.000926, l6: 0.002417
[epoch: 890/1000, batch:   772/ 1052, ite: 234000] train loss: 0.003873, tar: 0.000042 
l0: 0.000030, l1: 0.000030, l2: 0.000060, l3: 0.000099, l4: 0.000327, l5: 0.000926, l6: 0.001590
[epoch: 890/1000, batch:   852/ 1052, ite: 234020] train loss: 0.003872, tar: 0.000042 
l0: 0.000008, l1: 0.000007, l2: 0.000034, l3: 0.000131, l4: 0.000649, l5: 0.001447, l6: 0.002171
[epoch: 890/1000, batch:   932/ 1052, ite: 234040] train loss: 0.003869, tar: 0.000041 
l0: 0.000015, l1: 0.000013, l2: 0.000057, l3: 0.000159, l4: 0.000496, l5: 0.001014, l6: 0.002122
[epoch: 890/1000, batch:  1012/ 1052, ite: 234060] train loss: 0.003871, tar: 0.000041 
[Epoch 890/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000009, l1: 0.000009, l2: 0.000032, l3: 0.000082, l4: 0.000443, l5: 0.001054, l6: 0.001386
[epoch: 891/1000, batch:    40/ 1052, ite: 234080] train loss: 0.003870, tar: 0.000041 
l0: 0.000106, l1: 0.000109, l2: 0.000122, l3: 0.000219, l4: 0.000361, l5: 0.001305, l6: 0.002668
[epoch: 891/1000, batch:   120/ 1052, ite: 234100] train loss: 0.003872, tar: 0.000041 
l0: 0.000003, l1: 0.000003, l2: 0.000007, l3: 0.000056, l4: 0.000316, l5: 0.000854, l6: 0.001078
[epoch: 891/1000, batch:   200/ 1052, ite: 234120] train loss: 0.003868, tar: 0.000041 
l0: 0.000089, l1: 0.000089, l2: 0.000123, l3: 0.000346, l4: 0.000582, l5: 0.001351, l6: 0.003436
[epoch: 891/1000, batch:   280/ 1052, ite: 234140] train loss: 0.003865, tar: 0.000041 
l0: 0.000038, l1: 0.000039, l2: 0.000069, l3: 0.000181, l4: 0.000382, l5: 0.001276, l6: 0.002976
[epoch: 891/1000, batch:   360/ 1052, ite: 234160] train loss: 0.003866, tar: 0.000041 
l0: 0.000021, l1: 0.000020, l2: 0.000059, l3: 0.000185, l4: 0.000356, l5: 0.000891, l6: 0.001582
[epoch: 891/1000, batch:   440/ 1052, ite: 234180] train loss: 0.003866, tar: 0.000042 
l0: 0.000040, l1: 0.000041, l2: 0.000066, l3: 0.000219, l4: 0.000497, l5: 0.001099, l6: 0.001767
[epoch: 891/1000, batch:   520/ 1052, ite: 234200] train loss: 0.003866, tar: 0.000041 
l0: 0.000039, l1: 0.000039, l2: 0.000073, l3: 0.000142, l4: 0.000295, l5: 0.000911, l6: 0.001796
[epoch: 891/1000, batch:   600/ 1052, ite: 234220] train loss: 0.003864, tar: 0.000041 
l0: 0.000010, l1: 0.000010, l2: 0.000010, l3: 0.000050, l4: 0.000128, l5: 0.000506, l6: 0.000575
[epoch: 891/1000, batch:   680/ 1052, ite: 234240] train loss: 0.003861, tar: 0.000041 
l0: 0.000017, l1: 0.000019, l2: 0.000045, l3: 0.000104, l4: 0.000304, l5: 0.000830, l6: 0.001876
[epoch: 891/1000, batch:   760/ 1052, ite: 234260] train loss: 0.003857, tar: 0.000041 
l0: 0.000056, l1: 0.000055, l2: 0.000058, l3: 0.000121, l4: 0.000388, l5: 0.000954, l6: 0.001379
[epoch: 891/1000, batch:   840/ 1052, ite: 234280] train loss: 0.003861, tar: 0.000041 
l0: 0.000148, l1: 0.000148, l2: 0.000169, l3: 0.000236, l4: 0.000375, l5: 0.001761, l6: 0.002443
[epoch: 891/1000, batch:   920/ 1052, ite: 234300] train loss: 0.003864, tar: 0.000042 
l0: 0.000023, l1: 0.000021, l2: 0.000078, l3: 0.000231, l4: 0.000580, l5: 0.001415, l6: 0.002575
[epoch: 891/1000, batch:  1000/ 1052, ite: 234320] train loss: 0.003863, tar: 0.000041 
[Epoch 891/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000028, l1: 0.000029, l2: 0.000044, l3: 0.000083, l4: 0.000293, l5: 0.000701, l6: 0.001104
[epoch: 892/1000, batch:    28/ 1052, ite: 234340] train loss: 0.003867, tar: 0.000042 
l0: 0.000107, l1: 0.000105, l2: 0.000203, l3: 0.000526, l4: 0.001043, l5: 0.002461, l6: 0.004922
[epoch: 892/1000, batch:   108/ 1052, ite: 234360] train loss: 0.003867, tar: 0.000042 
l0: 0.000049, l1: 0.000049, l2: 0.000075, l3: 0.000135, l4: 0.000278, l5: 0.000960, l6: 0.001926
[epoch: 892/1000, batch:   188/ 1052, ite: 234380] train loss: 0.003869, tar: 0.000042 
l0: 0.000034, l1: 0.000033, l2: 0.000055, l3: 0.000115, l4: 0.000258, l5: 0.001154, l6: 0.001347
[epoch: 892/1000, batch:   268/ 1052, ite: 234400] train loss: 0.003865, tar: 0.000042 
l0: 0.000023, l1: 0.000022, l2: 0.000064, l3: 0.000271, l4: 0.000779, l5: 0.001848, l6: 0.003822
[epoch: 892/1000, batch:   348/ 1052, ite: 234420] train loss: 0.003864, tar: 0.000042 
l0: 0.000040, l1: 0.000040, l2: 0.000063, l3: 0.000157, l4: 0.000466, l5: 0.001174, l6: 0.001559
[epoch: 892/1000, batch:   428/ 1052, ite: 234440] train loss: 0.003863, tar: 0.000042 
l0: 0.000161, l1: 0.000162, l2: 0.000181, l3: 0.000251, l4: 0.000305, l5: 0.001355, l6: 0.002848
[epoch: 892/1000, batch:   508/ 1052, ite: 234460] train loss: 0.003863, tar: 0.000042 
l0: 0.000085, l1: 0.000084, l2: 0.000117, l3: 0.000176, l4: 0.000326, l5: 0.001287, l6: 0.003227
[epoch: 892/1000, batch:   588/ 1052, ite: 234480] train loss: 0.003864, tar: 0.000042 
l0: 0.000174, l1: 0.000176, l2: 0.000208, l3: 0.000367, l4: 0.000945, l5: 0.001455, l6: 0.002962
[epoch: 892/1000, batch:   668/ 1052, ite: 234500] train loss: 0.003865, tar: 0.000042 
l0: 0.000009, l1: 0.000009, l2: 0.000011, l3: 0.000064, l4: 0.000269, l5: 0.001150, l6: 0.001716
[epoch: 892/1000, batch:   748/ 1052, ite: 234520] train loss: 0.003866, tar: 0.000042 
l0: 0.000019, l1: 0.000023, l2: 0.000039, l3: 0.000078, l4: 0.000129, l5: 0.000638, l6: 0.001350
[epoch: 892/1000, batch:   828/ 1052, ite: 234540] train loss: 0.003867, tar: 0.000042 
l0: 0.000018, l1: 0.000018, l2: 0.000024, l3: 0.000078, l4: 0.000127, l5: 0.000489, l6: 0.001074
[epoch: 892/1000, batch:   908/ 1052, ite: 234560] train loss: 0.003866, tar: 0.000042 
l0: 0.000011, l1: 0.000010, l2: 0.000079, l3: 0.000209, l4: 0.000611, l5: 0.001324, l6: 0.001832
[epoch: 892/1000, batch:   988/ 1052, ite: 234580] train loss: 0.003868, tar: 0.000042 
[Epoch 892/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000004, l1: 0.000003, l2: 0.000009, l3: 0.000069, l4: 0.000225, l5: 0.000625, l6: 0.000835
[epoch: 893/1000, batch:    16/ 1052, ite: 234600] train loss: 0.003868, tar: 0.000042 
l0: 0.000019, l1: 0.000021, l2: 0.000035, l3: 0.000171, l4: 0.000531, l5: 0.001325, l6: 0.001864
[epoch: 893/1000, batch:    96/ 1052, ite: 234620] train loss: 0.003868, tar: 0.000042 
l0: 0.000115, l1: 0.000117, l2: 0.000180, l3: 0.000292, l4: 0.000546, l5: 0.001163, l6: 0.002649
[epoch: 893/1000, batch:   176/ 1052, ite: 234640] train loss: 0.003868, tar: 0.000042 
l0: 0.000054, l1: 0.000052, l2: 0.000115, l3: 0.000493, l4: 0.001258, l5: 0.002231, l6: 0.003393
[epoch: 893/1000, batch:   256/ 1052, ite: 234660] train loss: 0.003868, tar: 0.000042 
l0: 0.000009, l1: 0.000010, l2: 0.000041, l3: 0.000092, l4: 0.000551, l5: 0.001086, l6: 0.002092
[epoch: 893/1000, batch:   336/ 1052, ite: 234680] train loss: 0.003870, tar: 0.000042 
l0: 0.000059, l1: 0.000059, l2: 0.000064, l3: 0.000132, l4: 0.000326, l5: 0.001229, l6: 0.001718
[epoch: 893/1000, batch:   416/ 1052, ite: 234700] train loss: 0.003867, tar: 0.000042 
l0: 0.000059, l1: 0.000054, l2: 0.000075, l3: 0.000212, l4: 0.000408, l5: 0.000955, l6: 0.001809
[epoch: 893/1000, batch:   496/ 1052, ite: 234720] train loss: 0.003864, tar: 0.000042 
l0: 0.000133, l1: 0.000139, l2: 0.000160, l3: 0.000374, l4: 0.000716, l5: 0.003341, l6: 0.004770
[epoch: 893/1000, batch:   576/ 1052, ite: 234740] train loss: 0.003865, tar: 0.000042 
l0: 0.000008, l1: 0.000009, l2: 0.000042, l3: 0.000130, l4: 0.000392, l5: 0.000834, l6: 0.001983
[epoch: 893/1000, batch:   656/ 1052, ite: 234760] train loss: 0.003867, tar: 0.000042 
l0: 0.000073, l1: 0.000075, l2: 0.000096, l3: 0.000196, l4: 0.000399, l5: 0.001264, l6: 0.002007
[epoch: 893/1000, batch:   736/ 1052, ite: 234780] train loss: 0.003870, tar: 0.000042 
l0: 0.000033, l1: 0.000032, l2: 0.000054, l3: 0.000103, l4: 0.000216, l5: 0.001282, l6: 0.002539
[epoch: 893/1000, batch:   816/ 1052, ite: 234800] train loss: 0.003867, tar: 0.000042 
l0: 0.000033, l1: 0.000032, l2: 0.000078, l3: 0.000154, l4: 0.000551, l5: 0.001374, l6: 0.001405
[epoch: 893/1000, batch:   896/ 1052, ite: 234820] train loss: 0.003868, tar: 0.000042 
l0: 0.000013, l1: 0.000011, l2: 0.000027, l3: 0.000073, l4: 0.000310, l5: 0.000495, l6: 0.001367
[epoch: 893/1000, batch:   976/ 1052, ite: 234840] train loss: 0.003868, tar: 0.000042 
[Epoch 893/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000069, l1: 0.000068, l2: 0.000091, l3: 0.000211, l4: 0.000518, l5: 0.000953, l6: 0.002373
[epoch: 894/1000, batch:     4/ 1052, ite: 234860] train loss: 0.003872, tar: 0.000042 
l0: 0.000007, l1: 0.000005, l2: 0.000040, l3: 0.000212, l4: 0.000708, l5: 0.001152, l6: 0.001735
[epoch: 894/1000, batch:    84/ 1052, ite: 234880] train loss: 0.003871, tar: 0.000042 
l0: 0.000043, l1: 0.000044, l2: 0.000070, l3: 0.000203, l4: 0.000491, l5: 0.002576, l6: 0.003303
[epoch: 894/1000, batch:   164/ 1052, ite: 234900] train loss: 0.003873, tar: 0.000042 
l0: 0.000002, l1: 0.000002, l2: 0.000003, l3: 0.000100, l4: 0.000217, l5: 0.000686, l6: 0.001274
[epoch: 894/1000, batch:   244/ 1052, ite: 234920] train loss: 0.003871, tar: 0.000042 
l0: 0.000062, l1: 0.000063, l2: 0.000135, l3: 0.000198, l4: 0.000447, l5: 0.000888, l6: 0.001571
[epoch: 894/1000, batch:   324/ 1052, ite: 234940] train loss: 0.003870, tar: 0.000042 
l0: 0.000019, l1: 0.000019, l2: 0.000063, l3: 0.000222, l4: 0.000627, l5: 0.000878, l6: 0.002175
[epoch: 894/1000, batch:   404/ 1052, ite: 234960] train loss: 0.003870, tar: 0.000042 
l0: 0.000015, l1: 0.000015, l2: 0.000061, l3: 0.000134, l4: 0.000351, l5: 0.001374, l6: 0.002081
[epoch: 894/1000, batch:   484/ 1052, ite: 234980] train loss: 0.003872, tar: 0.000042 
l0: 0.000013, l1: 0.000014, l2: 0.000030, l3: 0.000150, l4: 0.000378, l5: 0.000825, l6: 0.001760
[epoch: 894/1000, batch:   564/ 1052, ite: 235000] train loss: 0.003872, tar: 0.000042 
l0: 0.000008, l1: 0.000008, l2: 0.000042, l3: 0.000273, l4: 0.000943, l5: 0.001224, l6: 0.003214
[epoch: 894/1000, batch:   644/ 1052, ite: 235020] train loss: 0.003874, tar: 0.000042 
l0: 0.000050, l1: 0.000050, l2: 0.000063, l3: 0.000143, l4: 0.000360, l5: 0.000919, l6: 0.001494
[epoch: 894/1000, batch:   724/ 1052, ite: 235040] train loss: 0.003876, tar: 0.000042 
l0: 0.000006, l1: 0.000006, l2: 0.000027, l3: 0.000063, l4: 0.000261, l5: 0.000600, l6: 0.001403
[epoch: 894/1000, batch:   804/ 1052, ite: 235060] train loss: 0.003879, tar: 0.000042 
l0: 0.000039, l1: 0.000039, l2: 0.000065, l3: 0.000136, l4: 0.000364, l5: 0.000687, l6: 0.001309
[epoch: 894/1000, batch:   884/ 1052, ite: 235080] train loss: 0.003877, tar: 0.000042 
l0: 0.000027, l1: 0.000026, l2: 0.000040, l3: 0.000126, l4: 0.000349, l5: 0.000813, l6: 0.001648
[epoch: 894/1000, batch:   964/ 1052, ite: 235100] train loss: 0.003876, tar: 0.000042 
l0: 0.000108, l1: 0.000111, l2: 0.000151, l3: 0.000287, l4: 0.000722, l5: 0.001744, l6: 0.003682
[epoch: 894/1000, batch:  1044/ 1052, ite: 235120] train loss: 0.003878, tar: 0.000042 
[Epoch 894/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000020, l1: 0.000020, l2: 0.000043, l3: 0.000135, l4: 0.000371, l5: 0.000962, l6: 0.001293
[epoch: 895/1000, batch:    72/ 1052, ite: 235140] train loss: 0.003882, tar: 0.000042 
l0: 0.000257, l1: 0.000268, l2: 0.000293, l3: 0.000406, l4: 0.000855, l5: 0.002353, l6: 0.005374
[epoch: 895/1000, batch:   152/ 1052, ite: 235160] train loss: 0.003885, tar: 0.000042 
l0: 0.000100, l1: 0.000100, l2: 0.000113, l3: 0.000180, l4: 0.000320, l5: 0.000840, l6: 0.001156
[epoch: 895/1000, batch:   232/ 1052, ite: 235180] train loss: 0.003883, tar: 0.000043 
l0: 0.000023, l1: 0.000022, l2: 0.000042, l3: 0.000102, l4: 0.000332, l5: 0.001035, l6: 0.001255
[epoch: 895/1000, batch:   312/ 1052, ite: 235200] train loss: 0.003882, tar: 0.000043 
l0: 0.000010, l1: 0.000010, l2: 0.000026, l3: 0.000092, l4: 0.000491, l5: 0.001040, l6: 0.001771
[epoch: 895/1000, batch:   392/ 1052, ite: 235220] train loss: 0.003883, tar: 0.000043 
l0: 0.000025, l1: 0.000025, l2: 0.000028, l3: 0.000136, l4: 0.000347, l5: 0.001235, l6: 0.001632
[epoch: 895/1000, batch:   472/ 1052, ite: 235240] train loss: 0.003882, tar: 0.000043 
l0: 0.000063, l1: 0.000061, l2: 0.000092, l3: 0.000238, l4: 0.000310, l5: 0.000878, l6: 0.001319
[epoch: 895/1000, batch:   552/ 1052, ite: 235260] train loss: 0.003884, tar: 0.000043 
l0: 0.000043, l1: 0.000039, l2: 0.000079, l3: 0.000143, l4: 0.000357, l5: 0.001236, l6: 0.001825
[epoch: 895/1000, batch:   632/ 1052, ite: 235280] train loss: 0.003881, tar: 0.000043 
l0: 0.000001, l1: 0.000002, l2: 0.000013, l3: 0.000107, l4: 0.000281, l5: 0.000866, l6: 0.001483
[epoch: 895/1000, batch:   712/ 1052, ite: 235300] train loss: 0.003881, tar: 0.000043 
l0: 0.000069, l1: 0.000068, l2: 0.000108, l3: 0.000245, l4: 0.000557, l5: 0.000976, l6: 0.002362
[epoch: 895/1000, batch:   792/ 1052, ite: 235320] train loss: 0.003882, tar: 0.000042 
l0: 0.000010, l1: 0.000012, l2: 0.000046, l3: 0.000158, l4: 0.000490, l5: 0.000910, l6: 0.001930
[epoch: 895/1000, batch:   872/ 1052, ite: 235340] train loss: 0.003884, tar: 0.000043 
l0: 0.000046, l1: 0.000045, l2: 0.000084, l3: 0.000263, l4: 0.000381, l5: 0.000628, l6: 0.001487
[epoch: 895/1000, batch:   952/ 1052, ite: 235360] train loss: 0.003883, tar: 0.000042 
l0: 0.000004, l1: 0.000003, l2: 0.000047, l3: 0.000158, l4: 0.000442, l5: 0.001020, l6: 0.001820
[epoch: 895/1000, batch:  1032/ 1052, ite: 235380] train loss: 0.003883, tar: 0.000042 
[Epoch 895/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000018, l1: 0.000018, l2: 0.000061, l3: 0.000204, l4: 0.000459, l5: 0.000799, l6: 0.002189
[epoch: 896/1000, batch:    60/ 1052, ite: 235400] train loss: 0.003881, tar: 0.000042 
l0: 0.000119, l1: 0.000119, l2: 0.000148, l3: 0.000240, l4: 0.000475, l5: 0.001132, l6: 0.002490
[epoch: 896/1000, batch:   140/ 1052, ite: 235420] train loss: 0.003882, tar: 0.000042 
l0: 0.000045, l1: 0.000047, l2: 0.000045, l3: 0.000033, l4: 0.000226, l5: 0.000823, l6: 0.000809
[epoch: 896/1000, batch:   220/ 1052, ite: 235440] train loss: 0.003881, tar: 0.000042 
l0: 0.000016, l1: 0.000016, l2: 0.000025, l3: 0.000096, l4: 0.000254, l5: 0.000604, l6: 0.001360
[epoch: 896/1000, batch:   300/ 1052, ite: 235460] train loss: 0.003880, tar: 0.000042 
l0: 0.000029, l1: 0.000029, l2: 0.000046, l3: 0.000108, l4: 0.000221, l5: 0.000539, l6: 0.000680
[epoch: 896/1000, batch:   380/ 1052, ite: 235480] train loss: 0.003879, tar: 0.000042 
l0: 0.000089, l1: 0.000096, l2: 0.000100, l3: 0.000313, l4: 0.000700, l5: 0.001266, l6: 0.003213
[epoch: 896/1000, batch:   460/ 1052, ite: 235500] train loss: 0.003881, tar: 0.000042 
l0: 0.000031, l1: 0.000032, l2: 0.000074, l3: 0.000216, l4: 0.000486, l5: 0.001183, l6: 0.002443
[epoch: 896/1000, batch:   540/ 1052, ite: 235520] train loss: 0.003878, tar: 0.000042 
l0: 0.000069, l1: 0.000071, l2: 0.000087, l3: 0.000155, l4: 0.000427, l5: 0.001211, l6: 0.002640
[epoch: 896/1000, batch:   620/ 1052, ite: 235540] train loss: 0.003878, tar: 0.000042 
l0: 0.000030, l1: 0.000028, l2: 0.000074, l3: 0.000117, l4: 0.000302, l5: 0.000847, l6: 0.001224
[epoch: 896/1000, batch:   700/ 1052, ite: 235560] train loss: 0.003877, tar: 0.000042 
l0: 0.000001, l1: 0.000001, l2: 0.000016, l3: 0.000069, l4: 0.000168, l5: 0.000594, l6: 0.001376
[epoch: 896/1000, batch:   780/ 1052, ite: 235580] train loss: 0.003879, tar: 0.000042 
l0: 0.000081, l1: 0.000079, l2: 0.000136, l3: 0.000249, l4: 0.000550, l5: 0.000848, l6: 0.001944
[epoch: 896/1000, batch:   860/ 1052, ite: 235600] train loss: 0.003879, tar: 0.000042 
l0: 0.000013, l1: 0.000012, l2: 0.000052, l3: 0.000195, l4: 0.000468, l5: 0.001083, l6: 0.001501
[epoch: 896/1000, batch:   940/ 1052, ite: 235620] train loss: 0.003878, tar: 0.000042 
l0: 0.000089, l1: 0.000088, l2: 0.000114, l3: 0.000227, l4: 0.000671, l5: 0.001824, l6: 0.003182
[epoch: 896/1000, batch:  1020/ 1052, ite: 235640] train loss: 0.003879, tar: 0.000043 
[Epoch 896/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000061, l1: 0.000059, l2: 0.000099, l3: 0.000164, l4: 0.000331, l5: 0.001199, l6: 0.002790
[epoch: 897/1000, batch:    48/ 1052, ite: 235660] train loss: 0.003877, tar: 0.000042 
l0: 0.000018, l1: 0.000020, l2: 0.000027, l3: 0.000057, l4: 0.000214, l5: 0.000610, l6: 0.001716
[epoch: 897/1000, batch:   128/ 1052, ite: 235680] train loss: 0.003875, tar: 0.000042 
l0: 0.000019, l1: 0.000019, l2: 0.000066, l3: 0.000327, l4: 0.000733, l5: 0.001521, l6: 0.002512
[epoch: 897/1000, batch:   208/ 1052, ite: 235700] train loss: 0.003877, tar: 0.000042 
l0: 0.000007, l1: 0.000007, l2: 0.000033, l3: 0.000142, l4: 0.000536, l5: 0.001175, l6: 0.002029
[epoch: 897/1000, batch:   288/ 1052, ite: 235720] train loss: 0.003878, tar: 0.000042 
l0: 0.000001, l1: 0.000001, l2: 0.000010, l3: 0.000038, l4: 0.000206, l5: 0.000657, l6: 0.001112
[epoch: 897/1000, batch:   368/ 1052, ite: 235740] train loss: 0.003879, tar: 0.000042 
l0: 0.000067, l1: 0.000068, l2: 0.000109, l3: 0.000271, l4: 0.000435, l5: 0.000804, l6: 0.001334
[epoch: 897/1000, batch:   448/ 1052, ite: 235760] train loss: 0.003880, tar: 0.000042 
l0: 0.000015, l1: 0.000016, l2: 0.000025, l3: 0.000096, l4: 0.000283, l5: 0.000736, l6: 0.001557
[epoch: 897/1000, batch:   528/ 1052, ite: 235780] train loss: 0.003882, tar: 0.000042 
l0: 0.000195, l1: 0.000196, l2: 0.000233, l3: 0.000391, l4: 0.000734, l5: 0.001613, l6: 0.003118
[epoch: 897/1000, batch:   608/ 1052, ite: 235800] train loss: 0.003881, tar: 0.000042 
l0: 0.000059, l1: 0.000059, l2: 0.000067, l3: 0.000122, l4: 0.000259, l5: 0.000609, l6: 0.001598
[epoch: 897/1000, batch:   688/ 1052, ite: 235820] train loss: 0.003881, tar: 0.000042 
l0: 0.000032, l1: 0.000032, l2: 0.000055, l3: 0.000100, l4: 0.000460, l5: 0.000769, l6: 0.001245
[epoch: 897/1000, batch:   768/ 1052, ite: 235840] train loss: 0.003882, tar: 0.000042 
l0: 0.000091, l1: 0.000093, l2: 0.000128, l3: 0.000480, l4: 0.001101, l5: 0.002048, l6: 0.004564
[epoch: 897/1000, batch:   848/ 1052, ite: 235860] train loss: 0.003881, tar: 0.000042 
l0: 0.000032, l1: 0.000033, l2: 0.000053, l3: 0.000155, l4: 0.000391, l5: 0.001544, l6: 0.002818
[epoch: 897/1000, batch:   928/ 1052, ite: 235880] train loss: 0.003881, tar: 0.000042 
l0: 0.000128, l1: 0.000131, l2: 0.000149, l3: 0.000226, l4: 0.000705, l5: 0.001449, l6: 0.002757
[epoch: 897/1000, batch:  1008/ 1052, ite: 235900] train loss: 0.003882, tar: 0.000042 
[Epoch 897/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000000, l1: 0.000000, l2: 0.000010, l3: 0.000099, l4: 0.000231, l5: 0.000629, l6: 0.001160
[epoch: 898/1000, batch:    36/ 1052, ite: 235920] train loss: 0.003881, tar: 0.000042 
l0: 0.000043, l1: 0.000040, l2: 0.000092, l3: 0.000284, l4: 0.000783, l5: 0.001352, l6: 0.003160
[epoch: 898/1000, batch:   116/ 1052, ite: 235940] train loss: 0.003880, tar: 0.000042 
l0: 0.000003, l1: 0.000003, l2: 0.000037, l3: 0.000121, l4: 0.000235, l5: 0.001250, l6: 0.002223
[epoch: 898/1000, batch:   196/ 1052, ite: 235960] train loss: 0.003882, tar: 0.000042 
l0: 0.000017, l1: 0.000017, l2: 0.000027, l3: 0.000141, l4: 0.000244, l5: 0.000547, l6: 0.001475
[epoch: 898/1000, batch:   276/ 1052, ite: 235980] train loss: 0.003880, tar: 0.000042 
l0: 0.000002, l1: 0.000002, l2: 0.000010, l3: 0.000058, l4: 0.000397, l5: 0.000732, l6: 0.001060
[epoch: 898/1000, batch:   356/ 1052, ite: 236000] train loss: 0.003881, tar: 0.000042 
l0: 0.000184, l1: 0.000182, l2: 0.000267, l3: 0.000436, l4: 0.000679, l5: 0.001088, l6: 0.002435
[epoch: 898/1000, batch:   436/ 1052, ite: 236020] train loss: 0.003884, tar: 0.000042 
l0: 0.000043, l1: 0.000045, l2: 0.000049, l3: 0.000119, l4: 0.000340, l5: 0.001068, l6: 0.001228
[epoch: 898/1000, batch:   516/ 1052, ite: 236040] train loss: 0.003885, tar: 0.000042 
l0: 0.000025, l1: 0.000023, l2: 0.000084, l3: 0.000154, l4: 0.000503, l5: 0.001760, l6: 0.002353
[epoch: 898/1000, batch:   596/ 1052, ite: 236060] train loss: 0.003886, tar: 0.000042 
l0: 0.000016, l1: 0.000016, l2: 0.000035, l3: 0.000088, l4: 0.000456, l5: 0.001795, l6: 0.002203
[epoch: 898/1000, batch:   676/ 1052, ite: 236080] train loss: 0.003888, tar: 0.000042 
l0: 0.000008, l1: 0.000008, l2: 0.000028, l3: 0.000070, l4: 0.000291, l5: 0.000803, l6: 0.001608
[epoch: 898/1000, batch:   756/ 1052, ite: 236100] train loss: 0.003889, tar: 0.000042 
l0: 0.000054, l1: 0.000055, l2: 0.000077, l3: 0.000177, l4: 0.000423, l5: 0.001164, l6: 0.001171
[epoch: 898/1000, batch:   836/ 1052, ite: 236120] train loss: 0.003889, tar: 0.000042 
l0: 0.000022, l1: 0.000023, l2: 0.000039, l3: 0.000104, l4: 0.000221, l5: 0.000606, l6: 0.001383
[epoch: 898/1000, batch:   916/ 1052, ite: 236140] train loss: 0.003888, tar: 0.000042 
l0: 0.000054, l1: 0.000054, l2: 0.000070, l3: 0.000115, l4: 0.000322, l5: 0.000863, l6: 0.001440
[epoch: 898/1000, batch:   996/ 1052, ite: 236160] train loss: 0.003885, tar: 0.000042 
[Epoch 898/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000005, l1: 0.000005, l2: 0.000032, l3: 0.000065, l4: 0.000366, l5: 0.001302, l6: 0.001189
[epoch: 899/1000, batch:    24/ 1052, ite: 236180] train loss: 0.003884, tar: 0.000042 
l0: 0.000009, l1: 0.000009, l2: 0.000029, l3: 0.000108, l4: 0.000391, l5: 0.001071, l6: 0.001449
[epoch: 899/1000, batch:   104/ 1052, ite: 236200] train loss: 0.003884, tar: 0.000042 
l0: 0.000037, l1: 0.000038, l2: 0.000089, l3: 0.000101, l4: 0.000313, l5: 0.000826, l6: 0.002471
[epoch: 899/1000, batch:   184/ 1052, ite: 236220] train loss: 0.003883, tar: 0.000042 
l0: 0.000011, l1: 0.000010, l2: 0.000049, l3: 0.000102, l4: 0.000242, l5: 0.000826, l6: 0.001670
[epoch: 899/1000, batch:   264/ 1052, ite: 236240] train loss: 0.003884, tar: 0.000042 
l0: 0.000013, l1: 0.000012, l2: 0.000034, l3: 0.000197, l4: 0.000663, l5: 0.001075, l6: 0.002333
[epoch: 899/1000, batch:   344/ 1052, ite: 236260] train loss: 0.003885, tar: 0.000042 
l0: 0.000032, l1: 0.000032, l2: 0.000043, l3: 0.000126, l4: 0.000409, l5: 0.000696, l6: 0.001701
[epoch: 899/1000, batch:   424/ 1052, ite: 236280] train loss: 0.003884, tar: 0.000042 
l0: 0.000078, l1: 0.000080, l2: 0.000095, l3: 0.000190, l4: 0.000433, l5: 0.001094, l6: 0.002422
[epoch: 899/1000, batch:   504/ 1052, ite: 236300] train loss: 0.003884, tar: 0.000042 
l0: 0.000008, l1: 0.000008, l2: 0.000024, l3: 0.000093, l4: 0.000224, l5: 0.000735, l6: 0.001324
[epoch: 899/1000, batch:   584/ 1052, ite: 236320] train loss: 0.003884, tar: 0.000042 
l0: 0.000001, l1: 0.000001, l2: 0.000004, l3: 0.000073, l4: 0.000370, l5: 0.000601, l6: 0.001367
[epoch: 899/1000, batch:   664/ 1052, ite: 236340] train loss: 0.003886, tar: 0.000042 
l0: 0.000054, l1: 0.000058, l2: 0.000123, l3: 0.000383, l4: 0.001233, l5: 0.002695, l6: 0.004554
[epoch: 899/1000, batch:   744/ 1052, ite: 236360] train loss: 0.003886, tar: 0.000042 
l0: 0.000026, l1: 0.000026, l2: 0.000080, l3: 0.000320, l4: 0.001011, l5: 0.002137, l6: 0.002656
[epoch: 899/1000, batch:   824/ 1052, ite: 236380] train loss: 0.003886, tar: 0.000042 
l0: 0.000025, l1: 0.000025, l2: 0.000041, l3: 0.000133, l4: 0.000489, l5: 0.001392, l6: 0.002377
[epoch: 899/1000, batch:   904/ 1052, ite: 236400] train loss: 0.003885, tar: 0.000042 
l0: 0.000084, l1: 0.000084, l2: 0.000122, l3: 0.000216, l4: 0.000403, l5: 0.000758, l6: 0.002563
[epoch: 899/1000, batch:   984/ 1052, ite: 236420] train loss: 0.003886, tar: 0.000043 
[Epoch 899/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000037, l1: 0.000038, l2: 0.000059, l3: 0.000172, l4: 0.000330, l5: 0.001349, l6: 0.002130
[epoch: 900/1000, batch:    12/ 1052, ite: 236440] train loss: 0.003885, tar: 0.000043 
l0: 0.000012, l1: 0.000013, l2: 0.000018, l3: 0.000047, l4: 0.000344, l5: 0.000789, l6: 0.001367
[epoch: 900/1000, batch:    92/ 1052, ite: 236460] train loss: 0.003884, tar: 0.000043 
l0: 0.000031, l1: 0.000031, l2: 0.000046, l3: 0.000087, l4: 0.000286, l5: 0.000929, l6: 0.001918
[epoch: 900/1000, batch:   172/ 1052, ite: 236480] train loss: 0.003885, tar: 0.000043 
l0: 0.000008, l1: 0.000008, l2: 0.000044, l3: 0.000185, l4: 0.000596, l5: 0.001159, l6: 0.002868
[epoch: 900/1000, batch:   252/ 1052, ite: 236500] train loss: 0.003884, tar: 0.000043 
l0: 0.000111, l1: 0.000113, l2: 0.000109, l3: 0.000188, l4: 0.000464, l5: 0.001207, l6: 0.002177
[epoch: 900/1000, batch:   332/ 1052, ite: 236520] train loss: 0.003886, tar: 0.000043 
l0: 0.000005, l1: 0.000006, l2: 0.000030, l3: 0.000118, l4: 0.000281, l5: 0.000921, l6: 0.001157
[epoch: 900/1000, batch:   412/ 1052, ite: 236540] train loss: 0.003887, tar: 0.000043 
l0: 0.000005, l1: 0.000004, l2: 0.000021, l3: 0.000056, l4: 0.000149, l5: 0.000968, l6: 0.001559
[epoch: 900/1000, batch:   492/ 1052, ite: 236560] train loss: 0.003887, tar: 0.000043 
l0: 0.000010, l1: 0.000011, l2: 0.000012, l3: 0.000087, l4: 0.000305, l5: 0.000601, l6: 0.000940
[epoch: 900/1000, batch:   572/ 1052, ite: 236580] train loss: 0.003886, tar: 0.000043 
l0: 0.000199, l1: 0.000199, l2: 0.000253, l3: 0.000362, l4: 0.000616, l5: 0.001090, l6: 0.001421
[epoch: 900/1000, batch:   652/ 1052, ite: 236600] train loss: 0.003885, tar: 0.000043 
l0: 0.000014, l1: 0.000016, l2: 0.000029, l3: 0.000071, l4: 0.000405, l5: 0.001067, l6: 0.001487
[epoch: 900/1000, batch:   732/ 1052, ite: 236620] train loss: 0.003886, tar: 0.000043 
l0: 0.000005, l1: 0.000004, l2: 0.000021, l3: 0.000176, l4: 0.000362, l5: 0.000850, l6: 0.001855
[epoch: 900/1000, batch:   812/ 1052, ite: 236640] train loss: 0.003884, tar: 0.000043 
l0: 0.000045, l1: 0.000044, l2: 0.000111, l3: 0.000241, l4: 0.000649, l5: 0.001662, l6: 0.003331
[epoch: 900/1000, batch:   892/ 1052, ite: 236660] train loss: 0.003884, tar: 0.000043 
l0: 0.000046, l1: 0.000042, l2: 0.000147, l3: 0.000248, l4: 0.000436, l5: 0.001635, l6: 0.002294
[epoch: 900/1000, batch:   972/ 1052, ite: 236680] train loss: 0.003886, tar: 0.000043 
l0: 0.000036, l1: 0.000034, l2: 0.000052, l3: 0.000120, l4: 0.000215, l5: 0.000249, l6: 0.000690
[epoch: 900/1000, batch:  1052/ 1052, ite: 236700] train loss: 0.003885, tar: 0.000043 
[Epoch 900/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000002, l1: 0.000002, l2: 0.000026, l3: 0.000159, l4: 0.000394, l5: 0.000920, l6: 0.001318
[epoch: 901/1000, batch:    80/ 1052, ite: 236720] train loss: 0.003886, tar: 0.000043 
l0: 0.000043, l1: 0.000044, l2: 0.000064, l3: 0.000126, l4: 0.000453, l5: 0.000938, l6: 0.001084
[epoch: 901/1000, batch:   160/ 1052, ite: 236740] train loss: 0.003885, tar: 0.000043 
l0: 0.000038, l1: 0.000038, l2: 0.000046, l3: 0.000134, l4: 0.000379, l5: 0.000517, l6: 0.001868
[epoch: 901/1000, batch:   240/ 1052, ite: 236760] train loss: 0.003882, tar: 0.000042 
l0: 0.000003, l1: 0.000003, l2: 0.000022, l3: 0.000132, l4: 0.000464, l5: 0.000671, l6: 0.001706
[epoch: 901/1000, batch:   320/ 1052, ite: 236780] train loss: 0.003881, tar: 0.000043 
l0: 0.000087, l1: 0.000088, l2: 0.000110, l3: 0.000192, l4: 0.000526, l5: 0.001028, l6: 0.002600
[epoch: 901/1000, batch:   400/ 1052, ite: 236800] train loss: 0.003883, tar: 0.000043 
l0: 0.000008, l1: 0.000007, l2: 0.000019, l3: 0.000104, l4: 0.000298, l5: 0.000797, l6: 0.001203
[epoch: 901/1000, batch:   480/ 1052, ite: 236820] train loss: 0.003882, tar: 0.000043 
l0: 0.000082, l1: 0.000083, l2: 0.000122, l3: 0.000213, l4: 0.000431, l5: 0.001638, l6: 0.003178
[epoch: 901/1000, batch:   560/ 1052, ite: 236840] train loss: 0.003885, tar: 0.000043 
l0: 0.000068, l1: 0.000068, l2: 0.000087, l3: 0.000187, l4: 0.000509, l5: 0.000884, l6: 0.001576
[epoch: 901/1000, batch:   640/ 1052, ite: 236860] train loss: 0.003884, tar: 0.000043 
l0: 0.000041, l1: 0.000039, l2: 0.000068, l3: 0.000154, l4: 0.000443, l5: 0.000838, l6: 0.001657
[epoch: 901/1000, batch:   720/ 1052, ite: 236880] train loss: 0.003885, tar: 0.000043 
l0: 0.000005, l1: 0.000005, l2: 0.000009, l3: 0.000056, l4: 0.000114, l5: 0.000555, l6: 0.000941
[epoch: 901/1000, batch:   800/ 1052, ite: 236900] train loss: 0.003885, tar: 0.000043 
l0: 0.000045, l1: 0.000046, l2: 0.000063, l3: 0.000164, l4: 0.000468, l5: 0.000832, l6: 0.001763
[epoch: 901/1000, batch:   880/ 1052, ite: 236920] train loss: 0.003885, tar: 0.000043 
l0: 0.000003, l1: 0.000002, l2: 0.000031, l3: 0.000141, l4: 0.000456, l5: 0.001008, l6: 0.001381
[epoch: 901/1000, batch:   960/ 1052, ite: 236940] train loss: 0.003885, tar: 0.000043 
l0: 0.000010, l1: 0.000010, l2: 0.000021, l3: 0.000135, l4: 0.000365, l5: 0.000777, l6: 0.001582
[epoch: 901/1000, batch:  1040/ 1052, ite: 236960] train loss: 0.003884, tar: 0.000043 
[Epoch 901/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000000, l1: 0.000000, l2: 0.000024, l3: 0.000086, l4: 0.000496, l5: 0.000805, l6: 0.001674
[epoch: 902/1000, batch:    68/ 1052, ite: 236980] train loss: 0.003885, tar: 0.000043 
l0: 0.000003, l1: 0.000004, l2: 0.000031, l3: 0.000115, l4: 0.000496, l5: 0.000718, l6: 0.001739
[epoch: 902/1000, batch:   148/ 1052, ite: 237000] train loss: 0.003883, tar: 0.000043 
l0: 0.000045, l1: 0.000044, l2: 0.000092, l3: 0.000232, l4: 0.000638, l5: 0.001903, l6: 0.003248
[epoch: 902/1000, batch:   228/ 1052, ite: 237020] train loss: 0.003883, tar: 0.000043 
l0: 0.000035, l1: 0.000036, l2: 0.000054, l3: 0.000121, l4: 0.000226, l5: 0.000748, l6: 0.001363
[epoch: 902/1000, batch:   308/ 1052, ite: 237040] train loss: 0.003883, tar: 0.000043 
l0: 0.000016, l1: 0.000016, l2: 0.000060, l3: 0.000100, l4: 0.000321, l5: 0.000796, l6: 0.002071
[epoch: 902/1000, batch:   388/ 1052, ite: 237060] train loss: 0.003882, tar: 0.000043 
l0: 0.000011, l1: 0.000011, l2: 0.000078, l3: 0.000189, l4: 0.000391, l5: 0.001189, l6: 0.002475
[epoch: 902/1000, batch:   468/ 1052, ite: 237080] train loss: 0.003885, tar: 0.000043 
l0: 0.000067, l1: 0.000066, l2: 0.000119, l3: 0.000222, l4: 0.000405, l5: 0.000855, l6: 0.001750
[epoch: 902/1000, batch:   548/ 1052, ite: 237100] train loss: 0.003885, tar: 0.000043 
l0: 0.000046, l1: 0.000046, l2: 0.000077, l3: 0.000230, l4: 0.000498, l5: 0.001258, l6: 0.002294
[epoch: 902/1000, batch:   628/ 1052, ite: 237120] train loss: 0.003884, tar: 0.000043 
l0: 0.000044, l1: 0.000044, l2: 0.000075, l3: 0.000170, l4: 0.000544, l5: 0.001185, l6: 0.002159
[epoch: 902/1000, batch:   708/ 1052, ite: 237140] train loss: 0.003883, tar: 0.000043 
l0: 0.000014, l1: 0.000014, l2: 0.000030, l3: 0.000060, l4: 0.000287, l5: 0.000686, l6: 0.000734
[epoch: 902/1000, batch:   788/ 1052, ite: 237160] train loss: 0.003883, tar: 0.000043 
l0: 0.000079, l1: 0.000080, l2: 0.000104, l3: 0.000288, l4: 0.000758, l5: 0.001967, l6: 0.003219
[epoch: 902/1000, batch:   868/ 1052, ite: 237180] train loss: 0.003883, tar: 0.000043 
l0: 0.000016, l1: 0.000017, l2: 0.000039, l3: 0.000128, l4: 0.000522, l5: 0.001437, l6: 0.002366
[epoch: 902/1000, batch:   948/ 1052, ite: 237200] train loss: 0.003883, tar: 0.000043 
l0: 0.000006, l1: 0.000005, l2: 0.000027, l3: 0.000098, l4: 0.000434, l5: 0.001862, l6: 0.002224
[epoch: 902/1000, batch:  1028/ 1052, ite: 237220] train loss: 0.003884, tar: 0.000043 
[Epoch 902/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000092, l1: 0.000092, l2: 0.000132, l3: 0.000304, l4: 0.000490, l5: 0.000775, l6: 0.001534
[epoch: 903/1000, batch:    56/ 1052, ite: 237240] train loss: 0.003884, tar: 0.000043 
l0: 0.000009, l1: 0.000008, l2: 0.000057, l3: 0.000298, l4: 0.000868, l5: 0.001248, l6: 0.002565
[epoch: 903/1000, batch:   136/ 1052, ite: 237260] train loss: 0.003882, tar: 0.000043 
l0: 0.000001, l1: 0.000001, l2: 0.000014, l3: 0.000082, l4: 0.000231, l5: 0.000810, l6: 0.000920
[epoch: 903/1000, batch:   216/ 1052, ite: 237280] train loss: 0.003882, tar: 0.000043 
l0: 0.000074, l1: 0.000073, l2: 0.000100, l3: 0.000148, l4: 0.000340, l5: 0.000742, l6: 0.001802
[epoch: 903/1000, batch:   296/ 1052, ite: 237300] train loss: 0.003882, tar: 0.000043 
l0: 0.000016, l1: 0.000016, l2: 0.000025, l3: 0.000078, l4: 0.000362, l5: 0.000838, l6: 0.001364
[epoch: 903/1000, batch:   376/ 1052, ite: 237320] train loss: 0.003883, tar: 0.000043 
l0: 0.000006, l1: 0.000005, l2: 0.000032, l3: 0.000082, l4: 0.000288, l5: 0.000701, l6: 0.000885
[epoch: 903/1000, batch:   456/ 1052, ite: 237340] train loss: 0.003883, tar: 0.000043 
l0: 0.000041, l1: 0.000040, l2: 0.000130, l3: 0.000229, l4: 0.000410, l5: 0.001321, l6: 0.002083
[epoch: 903/1000, batch:   536/ 1052, ite: 237360] train loss: 0.003882, tar: 0.000043 
l0: 0.000080, l1: 0.000080, l2: 0.000100, l3: 0.000234, l4: 0.000553, l5: 0.001403, l6: 0.002709
[epoch: 903/1000, batch:   616/ 1052, ite: 237380] train loss: 0.003883, tar: 0.000043 
l0: 0.000151, l1: 0.000155, l2: 0.000185, l3: 0.000207, l4: 0.000493, l5: 0.001034, l6: 0.003217
[epoch: 903/1000, batch:   696/ 1052, ite: 237400] train loss: 0.003885, tar: 0.000043 
l0: 0.000012, l1: 0.000012, l2: 0.000028, l3: 0.000077, l4: 0.000242, l5: 0.000695, l6: 0.000893
[epoch: 903/1000, batch:   776/ 1052, ite: 237420] train loss: 0.003882, tar: 0.000043 
l0: 0.000108, l1: 0.000110, l2: 0.000119, l3: 0.000162, l4: 0.000452, l5: 0.001078, l6: 0.002247
[epoch: 903/1000, batch:   856/ 1052, ite: 237440] train loss: 0.003881, tar: 0.000043 
l0: 0.000033, l1: 0.000035, l2: 0.000050, l3: 0.000127, l4: 0.000403, l5: 0.001002, l6: 0.002251
[epoch: 903/1000, batch:   936/ 1052, ite: 237460] train loss: 0.003882, tar: 0.000043 
l0: 0.000037, l1: 0.000037, l2: 0.000068, l3: 0.000176, l4: 0.000459, l5: 0.001012, l6: 0.001917
[epoch: 903/1000, batch:  1016/ 1052, ite: 237480] train loss: 0.003881, tar: 0.000043 
[Epoch 903/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000030, l1: 0.000031, l2: 0.000048, l3: 0.000110, l4: 0.000414, l5: 0.000709, l6: 0.001447
[epoch: 904/1000, batch:    44/ 1052, ite: 237500] train loss: 0.003881, tar: 0.000043 
l0: 0.000061, l1: 0.000060, l2: 0.000091, l3: 0.000229, l4: 0.000436, l5: 0.001371, l6: 0.002850
[epoch: 904/1000, batch:   124/ 1052, ite: 237520] train loss: 0.003883, tar: 0.000043 
l0: 0.000112, l1: 0.000111, l2: 0.000138, l3: 0.000351, l4: 0.000711, l5: 0.001317, l6: 0.003190
[epoch: 904/1000, batch:   204/ 1052, ite: 237540] train loss: 0.003884, tar: 0.000043 
l0: 0.000066, l1: 0.000068, l2: 0.000083, l3: 0.000200, l4: 0.000398, l5: 0.000795, l6: 0.001593
[epoch: 904/1000, batch:   284/ 1052, ite: 237560] train loss: 0.003884, tar: 0.000043 
l0: 0.000057, l1: 0.000056, l2: 0.000090, l3: 0.000241, l4: 0.000819, l5: 0.001624, l6: 0.003670
[epoch: 904/1000, batch:   364/ 1052, ite: 237580] train loss: 0.003882, tar: 0.000043 
l0: 0.000110, l1: 0.000117, l2: 0.000186, l3: 0.000384, l4: 0.000755, l5: 0.001457, l6: 0.002168
[epoch: 904/1000, batch:   444/ 1052, ite: 237600] train loss: 0.003882, tar: 0.000043 
l0: 0.000033, l1: 0.000033, l2: 0.000087, l3: 0.000160, l4: 0.000430, l5: 0.000817, l6: 0.001288
[epoch: 904/1000, batch:   524/ 1052, ite: 237620] train loss: 0.003882, tar: 0.000043 
l0: 0.000017, l1: 0.000016, l2: 0.000023, l3: 0.000112, l4: 0.000366, l5: 0.000874, l6: 0.001415
[epoch: 904/1000, batch:   604/ 1052, ite: 237640] train loss: 0.003882, tar: 0.000043 
l0: 0.000005, l1: 0.000006, l2: 0.000019, l3: 0.000060, l4: 0.000302, l5: 0.000836, l6: 0.001595
[epoch: 904/1000, batch:   684/ 1052, ite: 237660] train loss: 0.003881, tar: 0.000043 
l0: 0.000016, l1: 0.000016, l2: 0.000065, l3: 0.000166, l4: 0.000534, l5: 0.001522, l6: 0.002397
[epoch: 904/1000, batch:   764/ 1052, ite: 237680] train loss: 0.003880, tar: 0.000043 
l0: 0.000001, l1: 0.000001, l2: 0.000007, l3: 0.000086, l4: 0.000311, l5: 0.000946, l6: 0.001456
[epoch: 904/1000, batch:   844/ 1052, ite: 237700] train loss: 0.003881, tar: 0.000043 
l0: 0.000046, l1: 0.000046, l2: 0.000060, l3: 0.000181, l4: 0.000308, l5: 0.001008, l6: 0.002048
[epoch: 904/1000, batch:   924/ 1052, ite: 237720] train loss: 0.003881, tar: 0.000043 
l0: 0.000138, l1: 0.000141, l2: 0.000171, l3: 0.000285, l4: 0.000713, l5: 0.001877, l6: 0.002944
[epoch: 904/1000, batch:  1004/ 1052, ite: 237740] train loss: 0.003881, tar: 0.000043 
[Epoch 904/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000023, l1: 0.000023, l2: 0.000058, l3: 0.000325, l4: 0.000900, l5: 0.001333, l6: 0.002544
[epoch: 905/1000, batch:    32/ 1052, ite: 237760] train loss: 0.003883, tar: 0.000043 
l0: 0.000072, l1: 0.000070, l2: 0.000119, l3: 0.000217, l4: 0.000523, l5: 0.001009, l6: 0.002247
[epoch: 905/1000, batch:   112/ 1052, ite: 237780] train loss: 0.003883, tar: 0.000043 
l0: 0.000010, l1: 0.000010, l2: 0.000031, l3: 0.000122, l4: 0.000396, l5: 0.000835, l6: 0.001247
[epoch: 905/1000, batch:   192/ 1052, ite: 237800] train loss: 0.003882, tar: 0.000043 
l0: 0.000021, l1: 0.000020, l2: 0.000044, l3: 0.000131, l4: 0.000301, l5: 0.000585, l6: 0.001139
[epoch: 905/1000, batch:   272/ 1052, ite: 237820] train loss: 0.003882, tar: 0.000042 
l0: 0.000042, l1: 0.000042, l2: 0.000050, l3: 0.000144, l4: 0.000365, l5: 0.000927, l6: 0.002123
[epoch: 905/1000, batch:   352/ 1052, ite: 237840] train loss: 0.003882, tar: 0.000042 
l0: 0.000121, l1: 0.000123, l2: 0.000172, l3: 0.000390, l4: 0.000809, l5: 0.001706, l6: 0.002386
[epoch: 905/1000, batch:   432/ 1052, ite: 237860] train loss: 0.003881, tar: 0.000042 
l0: 0.000041, l1: 0.000044, l2: 0.000062, l3: 0.000146, l4: 0.000515, l5: 0.001022, l6: 0.001568
[epoch: 905/1000, batch:   512/ 1052, ite: 237880] train loss: 0.003880, tar: 0.000042 
l0: 0.000053, l1: 0.000054, l2: 0.000112, l3: 0.000247, l4: 0.000976, l5: 0.001214, l6: 0.004256
[epoch: 905/1000, batch:   592/ 1052, ite: 237900] train loss: 0.003880, tar: 0.000042 
l0: 0.000009, l1: 0.000007, l2: 0.000023, l3: 0.000151, l4: 0.000462, l5: 0.001358, l6: 0.001851
[epoch: 905/1000, batch:   672/ 1052, ite: 237920] train loss: 0.003879, tar: 0.000042 
l0: 0.000098, l1: 0.000097, l2: 0.000127, l3: 0.000253, l4: 0.000603, l5: 0.000915, l6: 0.002183
[epoch: 905/1000, batch:   752/ 1052, ite: 237940] train loss: 0.003880, tar: 0.000042 
l0: 0.000003, l1: 0.000002, l2: 0.000033, l3: 0.000164, l4: 0.000754, l5: 0.001115, l6: 0.001867
[epoch: 905/1000, batch:   832/ 1052, ite: 237960] train loss: 0.003879, tar: 0.000042 
l0: 0.000005, l1: 0.000005, l2: 0.000006, l3: 0.000082, l4: 0.000169, l5: 0.000406, l6: 0.001301
[epoch: 905/1000, batch:   912/ 1052, ite: 237980] train loss: 0.003878, tar: 0.000042 
l0: 0.000021, l1: 0.000020, l2: 0.000061, l3: 0.000146, l4: 0.000390, l5: 0.001204, l6: 0.001815
[epoch: 905/1000, batch:   992/ 1052, ite: 238000] train loss: 0.003879, tar: 0.000042 
[Epoch 905/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000052, l1: 0.000052, l2: 0.000068, l3: 0.000143, l4: 0.000586, l5: 0.000826, l6: 0.001601
[epoch: 906/1000, batch:    20/ 1052, ite: 238020] train loss: 0.003880, tar: 0.000042 
l0: 0.000015, l1: 0.000014, l2: 0.000048, l3: 0.000145, l4: 0.000464, l5: 0.001529, l6: 0.001801
[epoch: 906/1000, batch:   100/ 1052, ite: 238040] train loss: 0.003879, tar: 0.000042 
l0: 0.000031, l1: 0.000032, l2: 0.000059, l3: 0.000263, l4: 0.000634, l5: 0.001863, l6: 0.003352
[epoch: 906/1000, batch:   180/ 1052, ite: 238060] train loss: 0.003878, tar: 0.000042 
l0: 0.000043, l1: 0.000043, l2: 0.000050, l3: 0.000325, l4: 0.000489, l5: 0.001058, l6: 0.002789
[epoch: 906/1000, batch:   260/ 1052, ite: 238080] train loss: 0.003880, tar: 0.000042 
l0: 0.000011, l1: 0.000010, l2: 0.000055, l3: 0.000130, l4: 0.000394, l5: 0.000890, l6: 0.000992
[epoch: 906/1000, batch:   340/ 1052, ite: 238100] train loss: 0.003881, tar: 0.000042 
l0: 0.000029, l1: 0.000029, l2: 0.000047, l3: 0.000180, l4: 0.000475, l5: 0.001242, l6: 0.001851
[epoch: 906/1000, batch:   420/ 1052, ite: 238120] train loss: 0.003881, tar: 0.000042 
l0: 0.000001, l1: 0.000002, l2: 0.000006, l3: 0.000105, l4: 0.000386, l5: 0.000793, l6: 0.001127
[epoch: 906/1000, batch:   500/ 1052, ite: 238140] train loss: 0.003883, tar: 0.000042 
l0: 0.000002, l1: 0.000001, l2: 0.000046, l3: 0.000130, l4: 0.000423, l5: 0.001115, l6: 0.001687
[epoch: 906/1000, batch:   580/ 1052, ite: 238160] train loss: 0.003882, tar: 0.000042 
l0: 0.000010, l1: 0.000010, l2: 0.000013, l3: 0.000098, l4: 0.000404, l5: 0.001091, l6: 0.001854
[epoch: 906/1000, batch:   660/ 1052, ite: 238180] train loss: 0.003883, tar: 0.000042 
l0: 0.000016, l1: 0.000015, l2: 0.000033, l3: 0.000089, l4: 0.000397, l5: 0.000845, l6: 0.001031
[epoch: 906/1000, batch:   740/ 1052, ite: 238200] train loss: 0.003882, tar: 0.000042 
l0: 0.000059, l1: 0.000058, l2: 0.000099, l3: 0.000292, l4: 0.000729, l5: 0.001919, l6: 0.002140
[epoch: 906/1000, batch:   820/ 1052, ite: 238220] train loss: 0.003882, tar: 0.000043 
l0: 0.000096, l1: 0.000092, l2: 0.000149, l3: 0.000373, l4: 0.000807, l5: 0.001361, l6: 0.002316
[epoch: 906/1000, batch:   900/ 1052, ite: 238240] train loss: 0.003884, tar: 0.000043 
l0: 0.000078, l1: 0.000080, l2: 0.000102, l3: 0.000167, l4: 0.000525, l5: 0.000717, l6: 0.001558
[epoch: 906/1000, batch:   980/ 1052, ite: 238260] train loss: 0.003883, tar: 0.000043 
[Epoch 906/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000009, l1: 0.000009, l2: 0.000039, l3: 0.000118, l4: 0.000384, l5: 0.001642, l6: 0.002850
[epoch: 907/1000, batch:     8/ 1052, ite: 238280] train loss: 0.003883, tar: 0.000042 
l0: 0.000008, l1: 0.000008, l2: 0.000022, l3: 0.000062, l4: 0.000305, l5: 0.000442, l6: 0.001289
[epoch: 907/1000, batch:    88/ 1052, ite: 238300] train loss: 0.003884, tar: 0.000043 
l0: 0.000003, l1: 0.000002, l2: 0.000013, l3: 0.000075, l4: 0.000279, l5: 0.000572, l6: 0.000755
[epoch: 907/1000, batch:   168/ 1052, ite: 238320] train loss: 0.003884, tar: 0.000043 
l0: 0.000015, l1: 0.000015, l2: 0.000019, l3: 0.000106, l4: 0.000483, l5: 0.000839, l6: 0.001570
[epoch: 907/1000, batch:   248/ 1052, ite: 238340] train loss: 0.003884, tar: 0.000043 
l0: 0.000017, l1: 0.000016, l2: 0.000050, l3: 0.000154, l4: 0.000572, l5: 0.002001, l6: 0.002489
[epoch: 907/1000, batch:   328/ 1052, ite: 238360] train loss: 0.003883, tar: 0.000042 
l0: 0.000006, l1: 0.000006, l2: 0.000045, l3: 0.000162, l4: 0.000509, l5: 0.001319, l6: 0.002070
[epoch: 907/1000, batch:   408/ 1052, ite: 238380] train loss: 0.003882, tar: 0.000042 
l0: 0.000082, l1: 0.000082, l2: 0.000100, l3: 0.000172, l4: 0.000326, l5: 0.001103, l6: 0.002317
[epoch: 907/1000, batch:   488/ 1052, ite: 238400] train loss: 0.003882, tar: 0.000042 
l0: 0.000192, l1: 0.000195, l2: 0.000186, l3: 0.000266, l4: 0.000364, l5: 0.001381, l6: 0.001639
[epoch: 907/1000, batch:   568/ 1052, ite: 238420] train loss: 0.003882, tar: 0.000042 
l0: 0.000006, l1: 0.000006, l2: 0.000027, l3: 0.000089, l4: 0.000325, l5: 0.000882, l6: 0.001312
[epoch: 907/1000, batch:   648/ 1052, ite: 238440] train loss: 0.003882, tar: 0.000043 
l0: 0.000026, l1: 0.000027, l2: 0.000034, l3: 0.000098, l4: 0.000363, l5: 0.000685, l6: 0.001148
[epoch: 907/1000, batch:   728/ 1052, ite: 238460] train loss: 0.003883, tar: 0.000043 
l0: 0.000024, l1: 0.000022, l2: 0.000037, l3: 0.000113, l4: 0.000348, l5: 0.000895, l6: 0.001305
[epoch: 907/1000, batch:   808/ 1052, ite: 238480] train loss: 0.003882, tar: 0.000043 
l0: 0.000001, l1: 0.000001, l2: 0.000030, l3: 0.000101, l4: 0.000215, l5: 0.000509, l6: 0.000968
[epoch: 907/1000, batch:   888/ 1052, ite: 238500] train loss: 0.003882, tar: 0.000042 
l0: 0.000005, l1: 0.000006, l2: 0.000020, l3: 0.000097, l4: 0.000308, l5: 0.000785, l6: 0.002908
[epoch: 907/1000, batch:   968/ 1052, ite: 238520] train loss: 0.003881, tar: 0.000042 
l0: 0.000059, l1: 0.000060, l2: 0.000100, l3: 0.000196, l4: 0.000297, l5: 0.001138, l6: 0.002620
[epoch: 907/1000, batch:  1048/ 1052, ite: 238540] train loss: 0.003882, tar: 0.000042 
[Epoch 907/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000010, l1: 0.000011, l2: 0.000016, l3: 0.000090, l4: 0.000344, l5: 0.001061, l6: 0.001336
[epoch: 908/1000, batch:    76/ 1052, ite: 238560] train loss: 0.003882, tar: 0.000043 
l0: 0.000070, l1: 0.000072, l2: 0.000106, l3: 0.000240, l4: 0.000466, l5: 0.001475, l6: 0.002522
[epoch: 908/1000, batch:   156/ 1052, ite: 238580] train loss: 0.003885, tar: 0.000043 
l0: 0.000038, l1: 0.000036, l2: 0.000079, l3: 0.000210, l4: 0.000336, l5: 0.001354, l6: 0.002281
[epoch: 908/1000, batch:   236/ 1052, ite: 238600] train loss: 0.003886, tar: 0.000043 
l0: 0.000019, l1: 0.000018, l2: 0.000060, l3: 0.000173, l4: 0.000399, l5: 0.000834, l6: 0.001728
[epoch: 908/1000, batch:   316/ 1052, ite: 238620] train loss: 0.003885, tar: 0.000042 
l0: 0.000035, l1: 0.000034, l2: 0.000078, l3: 0.000191, l4: 0.000425, l5: 0.000956, l6: 0.000893
[epoch: 908/1000, batch:   396/ 1052, ite: 238640] train loss: 0.003883, tar: 0.000042 
l0: 0.000054, l1: 0.000053, l2: 0.000110, l3: 0.000315, l4: 0.000669, l5: 0.001391, l6: 0.002815
[epoch: 908/1000, batch:   476/ 1052, ite: 238660] train loss: 0.003883, tar: 0.000042 
l0: 0.000035, l1: 0.000035, l2: 0.000055, l3: 0.000082, l4: 0.000232, l5: 0.000522, l6: 0.000757
[epoch: 908/1000, batch:   556/ 1052, ite: 238680] train loss: 0.003881, tar: 0.000042 
l0: 0.000042, l1: 0.000042, l2: 0.000061, l3: 0.000169, l4: 0.000359, l5: 0.001019, l6: 0.001315
[epoch: 908/1000, batch:   636/ 1052, ite: 238700] train loss: 0.003881, tar: 0.000042 
l0: 0.000015, l1: 0.000015, l2: 0.000033, l3: 0.000122, l4: 0.000326, l5: 0.001014, l6: 0.002167
[epoch: 908/1000, batch:   716/ 1052, ite: 238720] train loss: 0.003882, tar: 0.000042 
l0: 0.000020, l1: 0.000020, l2: 0.000026, l3: 0.000098, l4: 0.000343, l5: 0.000876, l6: 0.001565
[epoch: 908/1000, batch:   796/ 1052, ite: 238740] train loss: 0.003881, tar: 0.000042 
l0: 0.000079, l1: 0.000080, l2: 0.000108, l3: 0.000197, l4: 0.000490, l5: 0.001220, l6: 0.001878
[epoch: 908/1000, batch:   876/ 1052, ite: 238760] train loss: 0.003880, tar: 0.000042 
l0: 0.000103, l1: 0.000104, l2: 0.000121, l3: 0.000255, l4: 0.000503, l5: 0.001431, l6: 0.003454
[epoch: 908/1000, batch:   956/ 1052, ite: 238780] train loss: 0.003881, tar: 0.000042 
l0: 0.000010, l1: 0.000009, l2: 0.000031, l3: 0.000124, l4: 0.000356, l5: 0.000992, l6: 0.001705
[epoch: 908/1000, batch:  1036/ 1052, ite: 238800] train loss: 0.003881, tar: 0.000042 
[Epoch 908/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000001, l1: 0.000001, l2: 0.000015, l3: 0.000029, l4: 0.000196, l5: 0.000506, l6: 0.000836
[epoch: 909/1000, batch:    64/ 1052, ite: 238820] train loss: 0.003883, tar: 0.000043 
l0: 0.000032, l1: 0.000032, l2: 0.000069, l3: 0.000237, l4: 0.000577, l5: 0.000700, l6: 0.002496
[epoch: 909/1000, batch:   144/ 1052, ite: 238840] train loss: 0.003883, tar: 0.000042 
l0: 0.000099, l1: 0.000102, l2: 0.000123, l3: 0.000180, l4: 0.000457, l5: 0.000904, l6: 0.002084
[epoch: 909/1000, batch:   224/ 1052, ite: 238860] train loss: 0.003883, tar: 0.000042 
l0: 0.000001, l1: 0.000001, l2: 0.000008, l3: 0.000067, l4: 0.000463, l5: 0.001298, l6: 0.002106
[epoch: 909/1000, batch:   304/ 1052, ite: 238880] train loss: 0.003883, tar: 0.000042 
l0: 0.000094, l1: 0.000095, l2: 0.000100, l3: 0.000122, l4: 0.000291, l5: 0.001099, l6: 0.002107
[epoch: 909/1000, batch:   384/ 1052, ite: 238900] train loss: 0.003882, tar: 0.000042 
l0: 0.000043, l1: 0.000044, l2: 0.000093, l3: 0.000213, l4: 0.000581, l5: 0.001964, l6: 0.003081
[epoch: 909/1000, batch:   464/ 1052, ite: 238920] train loss: 0.003881, tar: 0.000042 
l0: 0.000008, l1: 0.000009, l2: 0.000013, l3: 0.000070, l4: 0.000250, l5: 0.000838, l6: 0.001343
[epoch: 909/1000, batch:   544/ 1052, ite: 238940] train loss: 0.003881, tar: 0.000042 
l0: 0.000030, l1: 0.000027, l2: 0.000052, l3: 0.000089, l4: 0.000400, l5: 0.001126, l6: 0.001385
[epoch: 909/1000, batch:   624/ 1052, ite: 238960] train loss: 0.003881, tar: 0.000042 
l0: 0.000067, l1: 0.000066, l2: 0.000118, l3: 0.000196, l4: 0.000548, l5: 0.001267, l6: 0.001948
[epoch: 909/1000, batch:   704/ 1052, ite: 238980] train loss: 0.003881, tar: 0.000042 
l0: 0.000077, l1: 0.000078, l2: 0.000088, l3: 0.000158, l4: 0.000414, l5: 0.000752, l6: 0.001446
[epoch: 909/1000, batch:   784/ 1052, ite: 239000] train loss: 0.003882, tar: 0.000042 
l0: 0.000002, l1: 0.000002, l2: 0.000015, l3: 0.000093, l4: 0.000524, l5: 0.001169, l6: 0.002054
[epoch: 909/1000, batch:   864/ 1052, ite: 239020] train loss: 0.003881, tar: 0.000042 
l0: 0.000057, l1: 0.000058, l2: 0.000071, l3: 0.000178, l4: 0.000472, l5: 0.001116, l6: 0.003458
[epoch: 909/1000, batch:   944/ 1052, ite: 239040] train loss: 0.003881, tar: 0.000042 
l0: 0.000004, l1: 0.000004, l2: 0.000010, l3: 0.000058, l4: 0.000317, l5: 0.000613, l6: 0.001500
[epoch: 909/1000, batch:  1024/ 1052, ite: 239060] train loss: 0.003881, tar: 0.000042 
[Epoch 909/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000030, l1: 0.000030, l2: 0.000044, l3: 0.000139, l4: 0.000282, l5: 0.000526, l6: 0.001252
[epoch: 910/1000, batch:    52/ 1052, ite: 239080] train loss: 0.003880, tar: 0.000042 
l0: 0.000093, l1: 0.000094, l2: 0.000141, l3: 0.000284, l4: 0.000536, l5: 0.001507, l6: 0.003150
[epoch: 910/1000, batch:   132/ 1052, ite: 239100] train loss: 0.003880, tar: 0.000042 
l0: 0.000024, l1: 0.000026, l2: 0.000037, l3: 0.000150, l4: 0.000426, l5: 0.000811, l6: 0.001102
[epoch: 910/1000, batch:   212/ 1052, ite: 239120] train loss: 0.003879, tar: 0.000042 
l0: 0.000020, l1: 0.000021, l2: 0.000031, l3: 0.000107, l4: 0.000226, l5: 0.000593, l6: 0.001256
[epoch: 910/1000, batch:   292/ 1052, ite: 239140] train loss: 0.003881, tar: 0.000042 
l0: 0.000070, l1: 0.000069, l2: 0.000097, l3: 0.000164, l4: 0.000510, l5: 0.001091, l6: 0.003553
[epoch: 910/1000, batch:   372/ 1052, ite: 239160] train loss: 0.003880, tar: 0.000042 
l0: 0.000004, l1: 0.000004, l2: 0.000009, l3: 0.000059, l4: 0.000236, l5: 0.000611, l6: 0.001179
[epoch: 910/1000, batch:   452/ 1052, ite: 239180] train loss: 0.003879, tar: 0.000042 
l0: 0.000036, l1: 0.000035, l2: 0.000105, l3: 0.000305, l4: 0.000762, l5: 0.003468, l6: 0.006104
[epoch: 910/1000, batch:   532/ 1052, ite: 239200] train loss: 0.003881, tar: 0.000042 
l0: 0.000071, l1: 0.000070, l2: 0.000102, l3: 0.000408, l4: 0.001323, l5: 0.002208, l6: 0.002814
[epoch: 910/1000, batch:   612/ 1052, ite: 239220] train loss: 0.003881, tar: 0.000042 
l0: 0.000068, l1: 0.000070, l2: 0.000101, l3: 0.000186, l4: 0.000596, l5: 0.001743, l6: 0.003306
[epoch: 910/1000, batch:   692/ 1052, ite: 239240] train loss: 0.003880, tar: 0.000042 
l0: 0.000026, l1: 0.000025, l2: 0.000046, l3: 0.000125, l4: 0.000277, l5: 0.000514, l6: 0.001074
[epoch: 910/1000, batch:   772/ 1052, ite: 239260] train loss: 0.003878, tar: 0.000042 
l0: 0.000046, l1: 0.000045, l2: 0.000070, l3: 0.000118, l4: 0.000270, l5: 0.001269, l6: 0.002944
[epoch: 910/1000, batch:   852/ 1052, ite: 239280] train loss: 0.003879, tar: 0.000042 
l0: 0.000007, l1: 0.000006, l2: 0.000026, l3: 0.000131, l4: 0.000487, l5: 0.001953, l6: 0.002843
[epoch: 910/1000, batch:   932/ 1052, ite: 239300] train loss: 0.003880, tar: 0.000042 
l0: 0.000013, l1: 0.000014, l2: 0.000027, l3: 0.000087, l4: 0.000346, l5: 0.000628, l6: 0.001367
[epoch: 910/1000, batch:  1012/ 1052, ite: 239320] train loss: 0.003880, tar: 0.000042 
[Epoch 910/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000193, l1: 0.000198, l2: 0.000244, l3: 0.000423, l4: 0.001126, l5: 0.001804, l6: 0.003581
[epoch: 911/1000, batch:    40/ 1052, ite: 239340] train loss: 0.003880, tar: 0.000042 
l0: 0.000049, l1: 0.000052, l2: 0.000060, l3: 0.000232, l4: 0.000564, l5: 0.002720, l6: 0.004063
[epoch: 911/1000, batch:   120/ 1052, ite: 239360] train loss: 0.003880, tar: 0.000042 
l0: 0.000003, l1: 0.000003, l2: 0.000035, l3: 0.000101, l4: 0.000425, l5: 0.001058, l6: 0.001453
[epoch: 911/1000, batch:   200/ 1052, ite: 239380] train loss: 0.003880, tar: 0.000042 
l0: 0.000046, l1: 0.000050, l2: 0.000062, l3: 0.000115, l4: 0.000640, l5: 0.000858, l6: 0.001413
[epoch: 911/1000, batch:   280/ 1052, ite: 239400] train loss: 0.003881, tar: 0.000042 
l0: 0.000005, l1: 0.000004, l2: 0.000026, l3: 0.000162, l4: 0.000402, l5: 0.001048, l6: 0.002514
[epoch: 911/1000, batch:   360/ 1052, ite: 239420] train loss: 0.003880, tar: 0.000042 
l0: 0.000004, l1: 0.000004, l2: 0.000044, l3: 0.000268, l4: 0.000745, l5: 0.001513, l6: 0.003118
[epoch: 911/1000, batch:   440/ 1052, ite: 239440] train loss: 0.003881, tar: 0.000042 
l0: 0.000028, l1: 0.000030, l2: 0.000043, l3: 0.000101, l4: 0.000209, l5: 0.000742, l6: 0.001568
[epoch: 911/1000, batch:   520/ 1052, ite: 239460] train loss: 0.003882, tar: 0.000042 
l0: 0.000022, l1: 0.000022, l2: 0.000064, l3: 0.000315, l4: 0.000701, l5: 0.001848, l6: 0.003065
[epoch: 911/1000, batch:   600/ 1052, ite: 239480] train loss: 0.003882, tar: 0.000042 
l0: 0.000090, l1: 0.000091, l2: 0.000129, l3: 0.000233, l4: 0.000565, l5: 0.001012, l6: 0.001534
[epoch: 911/1000, batch:   680/ 1052, ite: 239500] train loss: 0.003882, tar: 0.000042 
l0: 0.000053, l1: 0.000055, l2: 0.000083, l3: 0.000136, l4: 0.000607, l5: 0.000844, l6: 0.001733
[epoch: 911/1000, batch:   760/ 1052, ite: 239520] train loss: 0.003882, tar: 0.000042 
l0: 0.000006, l1: 0.000007, l2: 0.000014, l3: 0.000045, l4: 0.000278, l5: 0.000840, l6: 0.001191
[epoch: 911/1000, batch:   840/ 1052, ite: 239540] train loss: 0.003883, tar: 0.000042 
l0: 0.000047, l1: 0.000048, l2: 0.000077, l3: 0.000165, l4: 0.000424, l5: 0.001001, l6: 0.001921
[epoch: 911/1000, batch:   920/ 1052, ite: 239560] train loss: 0.003882, tar: 0.000042 
l0: 0.000012, l1: 0.000011, l2: 0.000035, l3: 0.000190, l4: 0.000503, l5: 0.000837, l6: 0.002308
[epoch: 911/1000, batch:  1000/ 1052, ite: 239580] train loss: 0.003882, tar: 0.000042 
[Epoch 911/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000030, l1: 0.000028, l2: 0.000080, l3: 0.000187, l4: 0.000845, l5: 0.001491, l6: 0.004018
[epoch: 912/1000, batch:    28/ 1052, ite: 239600] train loss: 0.003881, tar: 0.000042 
l0: 0.000085, l1: 0.000090, l2: 0.000096, l3: 0.000164, l4: 0.000334, l5: 0.000863, l6: 0.001401
[epoch: 912/1000, batch:   108/ 1052, ite: 239620] train loss: 0.003881, tar: 0.000042 
l0: 0.000026, l1: 0.000025, l2: 0.000055, l3: 0.000386, l4: 0.000732, l5: 0.001793, l6: 0.003974
[epoch: 912/1000, batch:   188/ 1052, ite: 239640] train loss: 0.003881, tar: 0.000042 
l0: 0.000006, l1: 0.000006, l2: 0.000106, l3: 0.000299, l4: 0.000497, l5: 0.000796, l6: 0.003171
[epoch: 912/1000, batch:   268/ 1052, ite: 239660] train loss: 0.003881, tar: 0.000042 
l0: 0.000022, l1: 0.000022, l2: 0.000034, l3: 0.000111, l4: 0.000499, l5: 0.000733, l6: 0.000965
[epoch: 912/1000, batch:   348/ 1052, ite: 239680] train loss: 0.003880, tar: 0.000042 
l0: 0.000057, l1: 0.000057, l2: 0.000119, l3: 0.000246, l4: 0.000694, l5: 0.000906, l6: 0.002626
[epoch: 912/1000, batch:   428/ 1052, ite: 239700] train loss: 0.003881, tar: 0.000042 
l0: 0.000050, l1: 0.000050, l2: 0.000097, l3: 0.000249, l4: 0.000693, l5: 0.001893, l6: 0.002415
[epoch: 912/1000, batch:   508/ 1052, ite: 239720] train loss: 0.003880, tar: 0.000042 
l0: 0.000177, l1: 0.000170, l2: 0.000195, l3: 0.000303, l4: 0.000544, l5: 0.001157, l6: 0.001131
[epoch: 912/1000, batch:   588/ 1052, ite: 239740] train loss: 0.003880, tar: 0.000042 
l0: 0.000004, l1: 0.000003, l2: 0.000032, l3: 0.000155, l4: 0.000530, l5: 0.001315, l6: 0.002112
[epoch: 912/1000, batch:   668/ 1052, ite: 239760] train loss: 0.003881, tar: 0.000042 
l0: 0.000003, l1: 0.000004, l2: 0.000027, l3: 0.000134, l4: 0.000444, l5: 0.000763, l6: 0.001420
[epoch: 912/1000, batch:   748/ 1052, ite: 239780] train loss: 0.003881, tar: 0.000042 
l0: 0.000098, l1: 0.000094, l2: 0.000126, l3: 0.000193, l4: 0.000518, l5: 0.001095, l6: 0.002038
[epoch: 912/1000, batch:   828/ 1052, ite: 239800] train loss: 0.003881, tar: 0.000042 
l0: 0.000047, l1: 0.000050, l2: 0.000102, l3: 0.000259, l4: 0.000460, l5: 0.001265, l6: 0.001994
[epoch: 912/1000, batch:   908/ 1052, ite: 239820] train loss: 0.003881, tar: 0.000042 
l0: 0.000004, l1: 0.000004, l2: 0.000026, l3: 0.000132, l4: 0.000321, l5: 0.000687, l6: 0.001173
[epoch: 912/1000, batch:   988/ 1052, ite: 239840] train loss: 0.003881, tar: 0.000042 
[Epoch 912/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000050, l1: 0.000053, l2: 0.000058, l3: 0.000114, l4: 0.000255, l5: 0.000402, l6: 0.000786
[epoch: 913/1000, batch:    16/ 1052, ite: 239860] train loss: 0.003881, tar: 0.000042 
l0: 0.000037, l1: 0.000037, l2: 0.000046, l3: 0.000089, l4: 0.000330, l5: 0.001072, l6: 0.000835
[epoch: 913/1000, batch:    96/ 1052, ite: 239880] train loss: 0.003881, tar: 0.000042 
l0: 0.000006, l1: 0.000005, l2: 0.000025, l3: 0.000144, l4: 0.000375, l5: 0.001157, l6: 0.001694
[epoch: 913/1000, batch:   176/ 1052, ite: 239900] train loss: 0.003882, tar: 0.000042 
l0: 0.000038, l1: 0.000040, l2: 0.000049, l3: 0.000179, l4: 0.000457, l5: 0.001191, l6: 0.002361
[epoch: 913/1000, batch:   256/ 1052, ite: 239920] train loss: 0.003882, tar: 0.000042 
l0: 0.000093, l1: 0.000093, l2: 0.000143, l3: 0.000241, l4: 0.000456, l5: 0.001371, l6: 0.003067
[epoch: 913/1000, batch:   336/ 1052, ite: 239940] train loss: 0.003882, tar: 0.000042 
l0: 0.000008, l1: 0.000010, l2: 0.000016, l3: 0.000100, l4: 0.000277, l5: 0.000960, l6: 0.001551
[epoch: 913/1000, batch:   416/ 1052, ite: 239960] train loss: 0.003882, tar: 0.000042 
l0: 0.000094, l1: 0.000096, l2: 0.000120, l3: 0.000170, l4: 0.000393, l5: 0.001392, l6: 0.001685
[epoch: 913/1000, batch:   496/ 1052, ite: 239980] train loss: 0.003882, tar: 0.000042 
l0: 0.000007, l1: 0.000008, l2: 0.000035, l3: 0.000055, l4: 0.000260, l5: 0.000540, l6: 0.000944
[epoch: 913/1000, batch:   576/ 1052, ite: 240000] train loss: 0.003883, tar: 0.000042 
l0: 0.000017, l1: 0.000017, l2: 0.000038, l3: 0.000133, l4: 0.000357, l5: 0.000683, l6: 0.001202
[epoch: 913/1000, batch:   656/ 1052, ite: 240020] train loss: 0.003883, tar: 0.000042 
l0: 0.000074, l1: 0.000077, l2: 0.000091, l3: 0.000179, l4: 0.000422, l5: 0.001155, l6: 0.001602
[epoch: 913/1000, batch:   736/ 1052, ite: 240040] train loss: 0.003882, tar: 0.000042 
l0: 0.000088, l1: 0.000088, l2: 0.000167, l3: 0.000334, l4: 0.000550, l5: 0.001882, l6: 0.004447
[epoch: 913/1000, batch:   816/ 1052, ite: 240060] train loss: 0.003882, tar: 0.000042 
l0: 0.000041, l1: 0.000041, l2: 0.000065, l3: 0.000177, l4: 0.000429, l5: 0.000884, l6: 0.001725
[epoch: 913/1000, batch:   896/ 1052, ite: 240080] train loss: 0.003882, tar: 0.000042 
l0: 0.000026, l1: 0.000026, l2: 0.000079, l3: 0.000180, l4: 0.000327, l5: 0.001319, l6: 0.002092
[epoch: 913/1000, batch:   976/ 1052, ite: 240100] train loss: 0.003881, tar: 0.000042 
[Epoch 913/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000024, l1: 0.000024, l2: 0.000081, l3: 0.000144, l4: 0.000392, l5: 0.001314, l6: 0.002124
[epoch: 914/1000, batch:     4/ 1052, ite: 240120] train loss: 0.003882, tar: 0.000042 
l0: 0.000003, l1: 0.000003, l2: 0.000014, l3: 0.000076, l4: 0.000378, l5: 0.001068, l6: 0.001793
[epoch: 914/1000, batch:    84/ 1052, ite: 240140] train loss: 0.003882, tar: 0.000042 
l0: 0.000010, l1: 0.000012, l2: 0.000024, l3: 0.000109, l4: 0.000636, l5: 0.001707, l6: 0.002377
[epoch: 914/1000, batch:   164/ 1052, ite: 240160] train loss: 0.003883, tar: 0.000042 
l0: 0.000032, l1: 0.000030, l2: 0.000046, l3: 0.000158, l4: 0.000318, l5: 0.000575, l6: 0.000940
[epoch: 914/1000, batch:   244/ 1052, ite: 240180] train loss: 0.003882, tar: 0.000042 
l0: 0.000054, l1: 0.000053, l2: 0.000107, l3: 0.000332, l4: 0.000616, l5: 0.001277, l6: 0.002061
[epoch: 914/1000, batch:   324/ 1052, ite: 240200] train loss: 0.003881, tar: 0.000042 
l0: 0.000019, l1: 0.000018, l2: 0.000031, l3: 0.000141, l4: 0.000295, l5: 0.000352, l6: 0.001010
[epoch: 914/1000, batch:   404/ 1052, ite: 240220] train loss: 0.003880, tar: 0.000042 
l0: 0.000094, l1: 0.000095, l2: 0.000109, l3: 0.000236, l4: 0.000409, l5: 0.001084, l6: 0.001613
[epoch: 914/1000, batch:   484/ 1052, ite: 240240] train loss: 0.003881, tar: 0.000042 
l0: 0.000071, l1: 0.000070, l2: 0.000125, l3: 0.000232, l4: 0.000537, l5: 0.001161, l6: 0.001249
[epoch: 914/1000, batch:   564/ 1052, ite: 240260] train loss: 0.003881, tar: 0.000042 
l0: 0.000052, l1: 0.000051, l2: 0.000094, l3: 0.000220, l4: 0.000420, l5: 0.001566, l6: 0.002571
[epoch: 914/1000, batch:   644/ 1052, ite: 240280] train loss: 0.003881, tar: 0.000042 
l0: 0.000026, l1: 0.000027, l2: 0.000059, l3: 0.000152, l4: 0.000225, l5: 0.000604, l6: 0.001076
[epoch: 914/1000, batch:   724/ 1052, ite: 240300] train loss: 0.003882, tar: 0.000042 
l0: 0.000058, l1: 0.000059, l2: 0.000106, l3: 0.000315, l4: 0.000713, l5: 0.001261, l6: 0.001961
[epoch: 914/1000, batch:   804/ 1052, ite: 240320] train loss: 0.003882, tar: 0.000042 
l0: 0.000019, l1: 0.000019, l2: 0.000038, l3: 0.000160, l4: 0.000324, l5: 0.001110, l6: 0.001948
[epoch: 914/1000, batch:   884/ 1052, ite: 240340] train loss: 0.003881, tar: 0.000042 
l0: 0.000045, l1: 0.000045, l2: 0.000095, l3: 0.000257, l4: 0.000650, l5: 0.002755, l6: 0.004681
[epoch: 914/1000, batch:   964/ 1052, ite: 240360] train loss: 0.003882, tar: 0.000042 
l0: 0.000024, l1: 0.000024, l2: 0.000033, l3: 0.000133, l4: 0.000262, l5: 0.000798, l6: 0.001335
[epoch: 914/1000, batch:  1044/ 1052, ite: 240380] train loss: 0.003882, tar: 0.000042 
[Epoch 914/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000007, l1: 0.000006, l2: 0.000037, l3: 0.000171, l4: 0.000431, l5: 0.000947, l6: 0.001922
[epoch: 915/1000, batch:    72/ 1052, ite: 240400] train loss: 0.003881, tar: 0.000042 
l0: 0.000007, l1: 0.000007, l2: 0.000034, l3: 0.000108, l4: 0.000268, l5: 0.000904, l6: 0.001078
[epoch: 915/1000, batch:   152/ 1052, ite: 240420] train loss: 0.003880, tar: 0.000042 
l0: 0.000038, l1: 0.000038, l2: 0.000104, l3: 0.000256, l4: 0.000542, l5: 0.001334, l6: 0.002916
[epoch: 915/1000, batch:   232/ 1052, ite: 240440] train loss: 0.003881, tar: 0.000042 
l0: 0.000035, l1: 0.000035, l2: 0.000063, l3: 0.000197, l4: 0.000543, l5: 0.001089, l6: 0.001957
[epoch: 915/1000, batch:   312/ 1052, ite: 240460] train loss: 0.003880, tar: 0.000042 
l0: 0.000001, l1: 0.000002, l2: 0.000005, l3: 0.000068, l4: 0.000147, l5: 0.000622, l6: 0.000956
[epoch: 915/1000, batch:   392/ 1052, ite: 240480] train loss: 0.003880, tar: 0.000042 
l0: 0.000031, l1: 0.000031, l2: 0.000058, l3: 0.000179, l4: 0.000367, l5: 0.001294, l6: 0.002072
[epoch: 915/1000, batch:   472/ 1052, ite: 240500] train loss: 0.003879, tar: 0.000042 
l0: 0.000063, l1: 0.000066, l2: 0.000076, l3: 0.000138, l4: 0.000433, l5: 0.000975, l6: 0.001664
[epoch: 915/1000, batch:   552/ 1052, ite: 240520] train loss: 0.003879, tar: 0.000042 
l0: 0.000054, l1: 0.000055, l2: 0.000063, l3: 0.000117, l4: 0.000461, l5: 0.000673, l6: 0.001897
[epoch: 915/1000, batch:   632/ 1052, ite: 240540] train loss: 0.003878, tar: 0.000042 
l0: 0.000061, l1: 0.000061, l2: 0.000080, l3: 0.000181, l4: 0.000486, l5: 0.001265, l6: 0.002317
[epoch: 915/1000, batch:   712/ 1052, ite: 240560] train loss: 0.003879, tar: 0.000042 
l0: 0.000030, l1: 0.000031, l2: 0.000042, l3: 0.000152, l4: 0.000509, l5: 0.001564, l6: 0.002043
[epoch: 915/1000, batch:   792/ 1052, ite: 240580] train loss: 0.003880, tar: 0.000042 
l0: 0.000011, l1: 0.000012, l2: 0.000010, l3: 0.000090, l4: 0.000293, l5: 0.000368, l6: 0.000973
[epoch: 915/1000, batch:   872/ 1052, ite: 240600] train loss: 0.003882, tar: 0.000042 
l0: 0.000074, l1: 0.000072, l2: 0.000156, l3: 0.000175, l4: 0.000434, l5: 0.000799, l6: 0.001774
[epoch: 915/1000, batch:   952/ 1052, ite: 240620] train loss: 0.003882, tar: 0.000042 
l0: 0.000016, l1: 0.000016, l2: 0.000046, l3: 0.000205, l4: 0.000553, l5: 0.000965, l6: 0.001201
[epoch: 915/1000, batch:  1032/ 1052, ite: 240640] train loss: 0.003882, tar: 0.000042 
[Epoch 915/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000021, l1: 0.000021, l2: 0.000034, l3: 0.000088, l4: 0.000468, l5: 0.000782, l6: 0.001433
[epoch: 916/1000, batch:    60/ 1052, ite: 240660] train loss: 0.003881, tar: 0.000042 
l0: 0.000001, l1: 0.000001, l2: 0.000017, l3: 0.000113, l4: 0.000510, l5: 0.000676, l6: 0.001580
[epoch: 916/1000, batch:   140/ 1052, ite: 240680] train loss: 0.003880, tar: 0.000042 
l0: 0.000061, l1: 0.000065, l2: 0.000074, l3: 0.000143, l4: 0.000463, l5: 0.001203, l6: 0.002330
[epoch: 916/1000, batch:   220/ 1052, ite: 240700] train loss: 0.003880, tar: 0.000042 
l0: 0.000059, l1: 0.000057, l2: 0.000093, l3: 0.000199, l4: 0.000447, l5: 0.001009, l6: 0.002118
[epoch: 916/1000, batch:   300/ 1052, ite: 240720] train loss: 0.003880, tar: 0.000042 
l0: 0.000002, l1: 0.000002, l2: 0.000016, l3: 0.000119, l4: 0.000263, l5: 0.000970, l6: 0.001625
[epoch: 916/1000, batch:   380/ 1052, ite: 240740] train loss: 0.003880, tar: 0.000042 
l0: 0.000068, l1: 0.000068, l2: 0.000080, l3: 0.000206, l4: 0.000485, l5: 0.001155, l6: 0.001934
[epoch: 916/1000, batch:   460/ 1052, ite: 240760] train loss: 0.003880, tar: 0.000042 
l0: 0.000005, l1: 0.000005, l2: 0.000014, l3: 0.000033, l4: 0.000262, l5: 0.000835, l6: 0.001264
[epoch: 916/1000, batch:   540/ 1052, ite: 240780] train loss: 0.003881, tar: 0.000042 
l0: 0.000024, l1: 0.000024, l2: 0.000053, l3: 0.000173, l4: 0.000464, l5: 0.001080, l6: 0.002114
[epoch: 916/1000, batch:   620/ 1052, ite: 240800] train loss: 0.003881, tar: 0.000042 
l0: 0.000014, l1: 0.000014, l2: 0.000024, l3: 0.000087, l4: 0.000342, l5: 0.000894, l6: 0.001103
[epoch: 916/1000, batch:   700/ 1052, ite: 240820] train loss: 0.003880, tar: 0.000042 
l0: 0.000032, l1: 0.000034, l2: 0.000063, l3: 0.000117, l4: 0.000369, l5: 0.000906, l6: 0.001203
[epoch: 916/1000, batch:   780/ 1052, ite: 240840] train loss: 0.003879, tar: 0.000042 
l0: 0.000047, l1: 0.000045, l2: 0.000098, l3: 0.000297, l4: 0.000832, l5: 0.001993, l6: 0.002185
[epoch: 916/1000, batch:   860/ 1052, ite: 240860] train loss: 0.003878, tar: 0.000042 
l0: 0.000002, l1: 0.000002, l2: 0.000013, l3: 0.000084, l4: 0.000489, l5: 0.000947, l6: 0.001297
[epoch: 916/1000, batch:   940/ 1052, ite: 240880] train loss: 0.003879, tar: 0.000042 
l0: 0.000017, l1: 0.000017, l2: 0.000031, l3: 0.000114, l4: 0.000333, l5: 0.000569, l6: 0.000996
[epoch: 916/1000, batch:  1020/ 1052, ite: 240900] train loss: 0.003880, tar: 0.000042 
[Epoch 916/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000016, l1: 0.000015, l2: 0.000065, l3: 0.000146, l4: 0.000417, l5: 0.000871, l6: 0.001240
[epoch: 917/1000, batch:    48/ 1052, ite: 240920] train loss: 0.003879, tar: 0.000042 
l0: 0.000054, l1: 0.000052, l2: 0.000084, l3: 0.000215, l4: 0.000525, l5: 0.001409, l6: 0.002093
[epoch: 917/1000, batch:   128/ 1052, ite: 240940] train loss: 0.003880, tar: 0.000042 
l0: 0.000043, l1: 0.000043, l2: 0.000095, l3: 0.000211, l4: 0.000515, l5: 0.001397, l6: 0.002541
[epoch: 917/1000, batch:   208/ 1052, ite: 240960] train loss: 0.003879, tar: 0.000042 
l0: 0.000002, l1: 0.000002, l2: 0.000016, l3: 0.000105, l4: 0.000427, l5: 0.001093, l6: 0.002168
[epoch: 917/1000, batch:   288/ 1052, ite: 240980] train loss: 0.003879, tar: 0.000042 
l0: 0.000065, l1: 0.000065, l2: 0.000077, l3: 0.000125, l4: 0.000246, l5: 0.001046, l6: 0.001845
[epoch: 917/1000, batch:   368/ 1052, ite: 241000] train loss: 0.003879, tar: 0.000042 
l0: 0.000060, l1: 0.000058, l2: 0.000096, l3: 0.000179, l4: 0.000459, l5: 0.000752, l6: 0.002049
[epoch: 917/1000, batch:   448/ 1052, ite: 241020] train loss: 0.003878, tar: 0.000042 
l0: 0.000012, l1: 0.000012, l2: 0.000037, l3: 0.000130, l4: 0.000342, l5: 0.000935, l6: 0.001499
[epoch: 917/1000, batch:   528/ 1052, ite: 241040] train loss: 0.003878, tar: 0.000042 
l0: 0.000112, l1: 0.000112, l2: 0.000123, l3: 0.000245, l4: 0.000455, l5: 0.001331, l6: 0.001711
[epoch: 917/1000, batch:   608/ 1052, ite: 241060] train loss: 0.003878, tar: 0.000042 
l0: 0.000058, l1: 0.000059, l2: 0.000080, l3: 0.000156, l4: 0.000333, l5: 0.000518, l6: 0.001434
[epoch: 917/1000, batch:   688/ 1052, ite: 241080] train loss: 0.003879, tar: 0.000042 
l0: 0.000089, l1: 0.000089, l2: 0.000150, l3: 0.000459, l4: 0.000887, l5: 0.002118, l6: 0.003055
[epoch: 917/1000, batch:   768/ 1052, ite: 241100] train loss: 0.003879, tar: 0.000042 
l0: 0.000003, l1: 0.000004, l2: 0.000021, l3: 0.000135, l4: 0.000435, l5: 0.001459, l6: 0.002353
[epoch: 917/1000, batch:   848/ 1052, ite: 241120] train loss: 0.003879, tar: 0.000042 
l0: 0.000040, l1: 0.000041, l2: 0.000080, l3: 0.000173, l4: 0.000555, l5: 0.001237, l6: 0.002169
[epoch: 917/1000, batch:   928/ 1052, ite: 241140] train loss: 0.003880, tar: 0.000042 
l0: 0.000000, l1: 0.000000, l2: 0.000002, l3: 0.000025, l4: 0.000289, l5: 0.000649, l6: 0.001112
[epoch: 917/1000, batch:  1008/ 1052, ite: 241160] train loss: 0.003880, tar: 0.000042 
[Epoch 917/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000012, l1: 0.000012, l2: 0.000032, l3: 0.000131, l4: 0.000355, l5: 0.000628, l6: 0.001716
[epoch: 918/1000, batch:    36/ 1052, ite: 241180] train loss: 0.003879, tar: 0.000042 
l0: 0.000046, l1: 0.000043, l2: 0.000074, l3: 0.000153, l4: 0.000379, l5: 0.000917, l6: 0.001528
[epoch: 918/1000, batch:   116/ 1052, ite: 241200] train loss: 0.003880, tar: 0.000042 
l0: 0.000001, l1: 0.000001, l2: 0.000018, l3: 0.000106, l4: 0.000259, l5: 0.000643, l6: 0.001473
[epoch: 918/1000, batch:   196/ 1052, ite: 241220] train loss: 0.003880, tar: 0.000042 
l0: 0.000005, l1: 0.000005, l2: 0.000021, l3: 0.000135, l4: 0.000573, l5: 0.000956, l6: 0.002479
[epoch: 918/1000, batch:   276/ 1052, ite: 241240] train loss: 0.003880, tar: 0.000042 
l0: 0.000001, l1: 0.000001, l2: 0.000011, l3: 0.000095, l4: 0.000429, l5: 0.000562, l6: 0.001105
[epoch: 918/1000, batch:   356/ 1052, ite: 241260] train loss: 0.003880, tar: 0.000042 
l0: 0.000109, l1: 0.000113, l2: 0.000154, l3: 0.000241, l4: 0.000470, l5: 0.001234, l6: 0.003281
[epoch: 918/1000, batch:   436/ 1052, ite: 241280] train loss: 0.003880, tar: 0.000042 
l0: 0.000076, l1: 0.000076, l2: 0.000090, l3: 0.000164, l4: 0.000324, l5: 0.000795, l6: 0.003114
[epoch: 918/1000, batch:   516/ 1052, ite: 241300] train loss: 0.003880, tar: 0.000042 
l0: 0.000016, l1: 0.000016, l2: 0.000062, l3: 0.000275, l4: 0.000954, l5: 0.002021, l6: 0.002692
[epoch: 918/1000, batch:   596/ 1052, ite: 241320] train loss: 0.003880, tar: 0.000042 
l0: 0.000024, l1: 0.000023, l2: 0.000036, l3: 0.000191, l4: 0.000559, l5: 0.001051, l6: 0.002731
[epoch: 918/1000, batch:   676/ 1052, ite: 241340] train loss: 0.003880, tar: 0.000042 
l0: 0.000116, l1: 0.000119, l2: 0.000139, l3: 0.000302, l4: 0.000647, l5: 0.001515, l6: 0.002594
[epoch: 918/1000, batch:   756/ 1052, ite: 241360] train loss: 0.003879, tar: 0.000042 
l0: 0.000046, l1: 0.000044, l2: 0.000129, l3: 0.000346, l4: 0.000775, l5: 0.002296, l6: 0.004596
[epoch: 918/1000, batch:   836/ 1052, ite: 241380] train loss: 0.003879, tar: 0.000042 
l0: 0.000148, l1: 0.000146, l2: 0.000186, l3: 0.000280, l4: 0.000645, l5: 0.001136, l6: 0.002418
[epoch: 918/1000, batch:   916/ 1052, ite: 241400] train loss: 0.003880, tar: 0.000042 
l0: 0.000009, l1: 0.000009, l2: 0.000023, l3: 0.000170, l4: 0.000297, l5: 0.001608, l6: 0.003048
[epoch: 918/1000, batch:   996/ 1052, ite: 241420] train loss: 0.003880, tar: 0.000042 
[Epoch 918/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000010, l1: 0.000010, l2: 0.000018, l3: 0.000056, l4: 0.000266, l5: 0.000493, l6: 0.001119
[epoch: 919/1000, batch:    24/ 1052, ite: 241440] train loss: 0.003880, tar: 0.000042 
l0: 0.000010, l1: 0.000009, l2: 0.000029, l3: 0.000170, l4: 0.000607, l5: 0.001127, l6: 0.001925
[epoch: 919/1000, batch:   104/ 1052, ite: 241460] train loss: 0.003879, tar: 0.000042 
l0: 0.000021, l1: 0.000020, l2: 0.000039, l3: 0.000296, l4: 0.000586, l5: 0.001033, l6: 0.002535
[epoch: 919/1000, batch:   184/ 1052, ite: 241480] train loss: 0.003879, tar: 0.000042 
l0: 0.000004, l1: 0.000004, l2: 0.000069, l3: 0.000218, l4: 0.000582, l5: 0.001307, l6: 0.001639
[epoch: 919/1000, batch:   264/ 1052, ite: 241500] train loss: 0.003880, tar: 0.000042 
l0: 0.000025, l1: 0.000024, l2: 0.000042, l3: 0.000131, l4: 0.000462, l5: 0.001070, l6: 0.002223
[epoch: 919/1000, batch:   344/ 1052, ite: 241520] train loss: 0.003880, tar: 0.000042 
l0: 0.000063, l1: 0.000068, l2: 0.000093, l3: 0.000219, l4: 0.000391, l5: 0.000659, l6: 0.001549
[epoch: 919/1000, batch:   424/ 1052, ite: 241540] train loss: 0.003880, tar: 0.000042 
l0: 0.000004, l1: 0.000004, l2: 0.000027, l3: 0.000149, l4: 0.000566, l5: 0.000645, l6: 0.001451
[epoch: 919/1000, batch:   504/ 1052, ite: 241560] train loss: 0.003879, tar: 0.000042 
l0: 0.000046, l1: 0.000046, l2: 0.000070, l3: 0.000188, l4: 0.000540, l5: 0.001025, l6: 0.002219
[epoch: 919/1000, batch:   584/ 1052, ite: 241580] train loss: 0.003878, tar: 0.000042 
l0: 0.000015, l1: 0.000014, l2: 0.000037, l3: 0.000442, l4: 0.000838, l5: 0.001752, l6: 0.002707
[epoch: 919/1000, batch:   664/ 1052, ite: 241600] train loss: 0.003878, tar: 0.000042 
l0: 0.000011, l1: 0.000011, l2: 0.000053, l3: 0.000245, l4: 0.000785, l5: 0.001824, l6: 0.002978
[epoch: 919/1000, batch:   744/ 1052, ite: 241620] train loss: 0.003878, tar: 0.000042 
l0: 0.000019, l1: 0.000018, l2: 0.000190, l3: 0.000499, l4: 0.001064, l5: 0.002015, l6: 0.004382
[epoch: 919/1000, batch:   824/ 1052, ite: 241640] train loss: 0.003878, tar: 0.000042 
l0: 0.000023, l1: 0.000022, l2: 0.000053, l3: 0.000159, l4: 0.000301, l5: 0.001024, l6: 0.001204
[epoch: 919/1000, batch:   904/ 1052, ite: 241660] train loss: 0.003879, tar: 0.000042 
l0: 0.000012, l1: 0.000012, l2: 0.000046, l3: 0.000155, l4: 0.000357, l5: 0.001147, l6: 0.001613
[epoch: 919/1000, batch:   984/ 1052, ite: 241680] train loss: 0.003880, tar: 0.000042 
[Epoch 919/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000044, l1: 0.000044, l2: 0.000057, l3: 0.000172, l4: 0.000451, l5: 0.000891, l6: 0.001549
[epoch: 920/1000, batch:    12/ 1052, ite: 241700] train loss: 0.003879, tar: 0.000042 
l0: 0.000008, l1: 0.000008, l2: 0.000013, l3: 0.000063, l4: 0.000138, l5: 0.000494, l6: 0.001360
[epoch: 920/1000, batch:    92/ 1052, ite: 241720] train loss: 0.003879, tar: 0.000042 
l0: 0.000027, l1: 0.000027, l2: 0.000045, l3: 0.000127, l4: 0.000384, l5: 0.000904, l6: 0.001587
[epoch: 920/1000, batch:   172/ 1052, ite: 241740] train loss: 0.003878, tar: 0.000042 
l0: 0.000102, l1: 0.000104, l2: 0.000124, l3: 0.000239, l4: 0.000692, l5: 0.001373, l6: 0.003261
[epoch: 920/1000, batch:   252/ 1052, ite: 241760] train loss: 0.003879, tar: 0.000042 
l0: 0.000096, l1: 0.000092, l2: 0.000273, l3: 0.000619, l4: 0.000957, l5: 0.001641, l6: 0.004669
[epoch: 920/1000, batch:   332/ 1052, ite: 241780] train loss: 0.003880, tar: 0.000042 
l0: 0.000043, l1: 0.000045, l2: 0.000045, l3: 0.000176, l4: 0.000688, l5: 0.001115, l6: 0.001945
[epoch: 920/1000, batch:   412/ 1052, ite: 241800] train loss: 0.003879, tar: 0.000042 
l0: 0.000125, l1: 0.000127, l2: 0.000148, l3: 0.000253, l4: 0.000633, l5: 0.001458, l6: 0.002868
[epoch: 920/1000, batch:   492/ 1052, ite: 241820] train loss: 0.003880, tar: 0.000042 
l0: 0.000023, l1: 0.000024, l2: 0.000033, l3: 0.000130, l4: 0.000402, l5: 0.000885, l6: 0.002644
[epoch: 920/1000, batch:   572/ 1052, ite: 241840] train loss: 0.003881, tar: 0.000042 
l0: 0.000062, l1: 0.000064, l2: 0.000081, l3: 0.000170, l4: 0.000492, l5: 0.000864, l6: 0.002013
[epoch: 920/1000, batch:   652/ 1052, ite: 241860] train loss: 0.003880, tar: 0.000042 
l0: 0.000000, l1: 0.000000, l2: 0.000009, l3: 0.000098, l4: 0.000282, l5: 0.000880, l6: 0.001446
[epoch: 920/1000, batch:   732/ 1052, ite: 241880] train loss: 0.003879, tar: 0.000042 
l0: 0.000116, l1: 0.000115, l2: 0.000183, l3: 0.000445, l4: 0.000917, l5: 0.001671, l6: 0.003120
[epoch: 920/1000, batch:   812/ 1052, ite: 241900] train loss: 0.003879, tar: 0.000042 
l0: 0.000034, l1: 0.000034, l2: 0.000062, l3: 0.000178, l4: 0.000524, l5: 0.001432, l6: 0.003114
[epoch: 920/1000, batch:   892/ 1052, ite: 241920] train loss: 0.003880, tar: 0.000042 
l0: 0.000040, l1: 0.000040, l2: 0.000049, l3: 0.000100, l4: 0.000195, l5: 0.000450, l6: 0.001358
[epoch: 920/1000, batch:   972/ 1052, ite: 241940] train loss: 0.003878, tar: 0.000042 
l0: 0.000139, l1: 0.000142, l2: 0.000156, l3: 0.000250, l4: 0.000540, l5: 0.001231, l6: 0.002939
[epoch: 920/1000, batch:  1052/ 1052, ite: 241960] train loss: 0.003879, tar: 0.000042 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_241960.pkl
[Epoch 920/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000016, l1: 0.000016, l2: 0.000066, l3: 0.000206, l4: 0.000420, l5: 0.001221, l6: 0.002725
[epoch: 921/1000, batch:    80/ 1052, ite: 241980] train loss: 0.003603, tar: 0.000039 
l0: 0.000175, l1: 0.000174, l2: 0.000245, l3: 0.000584, l4: 0.001134, l5: 0.002273, l6: 0.004132
[epoch: 921/1000, batch:   160/ 1052, ite: 242000] train loss: 0.003906, tar: 0.000042 
l0: 0.000196, l1: 0.000195, l2: 0.000256, l3: 0.000356, l4: 0.000547, l5: 0.001182, l6: 0.002207
[epoch: 921/1000, batch:   240/ 1052, ite: 242020] train loss: 0.003922, tar: 0.000041 
l0: 0.000004, l1: 0.000004, l2: 0.000029, l3: 0.000120, l4: 0.000700, l5: 0.001582, l6: 0.001971
[epoch: 921/1000, batch:   320/ 1052, ite: 242040] train loss: 0.003979, tar: 0.000046 
l0: 0.000003, l1: 0.000003, l2: 0.000014, l3: 0.000119, l4: 0.000296, l5: 0.001117, l6: 0.001487
[epoch: 921/1000, batch:   400/ 1052, ite: 242060] train loss: 0.003976, tar: 0.000045 
l0: 0.000021, l1: 0.000021, l2: 0.000057, l3: 0.000127, l4: 0.000398, l5: 0.001526, l6: 0.002079
[epoch: 921/1000, batch:   480/ 1052, ite: 242080] train loss: 0.003919, tar: 0.000044 
l0: 0.000211, l1: 0.000215, l2: 0.000255, l3: 0.000357, l4: 0.000902, l5: 0.002374, l6: 0.003486
[epoch: 921/1000, batch:   560/ 1052, ite: 242100] train loss: 0.003904, tar: 0.000044 
l0: 0.000024, l1: 0.000024, l2: 0.000033, l3: 0.000089, l4: 0.000247, l5: 0.000737, l6: 0.001610
[epoch: 921/1000, batch:   640/ 1052, ite: 242120] train loss: 0.003850, tar: 0.000042 
l0: 0.000115, l1: 0.000117, l2: 0.000146, l3: 0.000323, l4: 0.000806, l5: 0.001456, l6: 0.002586
[epoch: 921/1000, batch:   720/ 1052, ite: 242140] train loss: 0.003886, tar: 0.000043 
l0: 0.000002, l1: 0.000002, l2: 0.000018, l3: 0.000103, l4: 0.000573, l5: 0.000894, l6: 0.001461
[epoch: 921/1000, batch:   800/ 1052, ite: 242160] train loss: 0.003886, tar: 0.000042 
l0: 0.000014, l1: 0.000013, l2: 0.000025, l3: 0.000089, l4: 0.000475, l5: 0.000633, l6: 0.001449
[epoch: 921/1000, batch:   880/ 1052, ite: 242180] train loss: 0.003870, tar: 0.000041 
l0: 0.000066, l1: 0.000067, l2: 0.000083, l3: 0.000110, l4: 0.000253, l5: 0.000652, l6: 0.001964
[epoch: 921/1000, batch:   960/ 1052, ite: 242200] train loss: 0.003842, tar: 0.000040 
l0: 0.000008, l1: 0.000007, l2: 0.000025, l3: 0.000186, l4: 0.000343, l5: 0.000689, l6: 0.001480
[epoch: 921/1000, batch:  1040/ 1052, ite: 242220] train loss: 0.003850, tar: 0.000040 
[Epoch 921/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000019, l1: 0.000020, l2: 0.000081, l3: 0.000195, l4: 0.000688, l5: 0.002589, l6: 0.002882
[epoch: 922/1000, batch:    68/ 1052, ite: 242240] train loss: 0.003863, tar: 0.000040 
l0: 0.000044, l1: 0.000045, l2: 0.000077, l3: 0.000106, l4: 0.000241, l5: 0.000664, l6: 0.000850
[epoch: 922/1000, batch:   148/ 1052, ite: 242260] train loss: 0.003872, tar: 0.000040 
l0: 0.000005, l1: 0.000006, l2: 0.000011, l3: 0.000064, l4: 0.000284, l5: 0.000820, l6: 0.001356
[epoch: 922/1000, batch:   228/ 1052, ite: 242280] train loss: 0.003837, tar: 0.000039 
l0: 0.000015, l1: 0.000015, l2: 0.000026, l3: 0.000100, l4: 0.000317, l5: 0.000786, l6: 0.001506
[epoch: 922/1000, batch:   308/ 1052, ite: 242300] train loss: 0.003842, tar: 0.000040 
l0: 0.000018, l1: 0.000018, l2: 0.000014, l3: 0.000076, l4: 0.000230, l5: 0.001672, l6: 0.003144
[epoch: 922/1000, batch:   388/ 1052, ite: 242320] train loss: 0.003852, tar: 0.000040 
l0: 0.000007, l1: 0.000008, l2: 0.000029, l3: 0.000105, l4: 0.000364, l5: 0.000585, l6: 0.002151
[epoch: 922/1000, batch:   468/ 1052, ite: 242340] train loss: 0.003847, tar: 0.000040 
l0: 0.000025, l1: 0.000025, l2: 0.000041, l3: 0.000098, l4: 0.000378, l5: 0.000483, l6: 0.001246
[epoch: 922/1000, batch:   548/ 1052, ite: 242360] train loss: 0.003854, tar: 0.000040 
l0: 0.000039, l1: 0.000039, l2: 0.000055, l3: 0.000165, l4: 0.000237, l5: 0.001127, l6: 0.001826
[epoch: 922/1000, batch:   628/ 1052, ite: 242380] train loss: 0.003840, tar: 0.000040 
l0: 0.000002, l1: 0.000002, l2: 0.000004, l3: 0.000059, l4: 0.000283, l5: 0.000515, l6: 0.001209
[epoch: 922/1000, batch:   708/ 1052, ite: 242400] train loss: 0.003830, tar: 0.000039 
l0: 0.000076, l1: 0.000076, l2: 0.000117, l3: 0.000340, l4: 0.000541, l5: 0.001275, l6: 0.001911
[epoch: 922/1000, batch:   788/ 1052, ite: 242420] train loss: 0.003839, tar: 0.000039 
l0: 0.000242, l1: 0.000250, l2: 0.000226, l3: 0.000286, l4: 0.000944, l5: 0.001490, l6: 0.002054
[epoch: 922/1000, batch:   868/ 1052, ite: 242440] train loss: 0.003847, tar: 0.000040 
l0: 0.000228, l1: 0.000235, l2: 0.000226, l3: 0.000421, l4: 0.000930, l5: 0.001474, l6: 0.003837
[epoch: 922/1000, batch:   948/ 1052, ite: 242460] train loss: 0.003854, tar: 0.000040 
l0: 0.000019, l1: 0.000019, l2: 0.000028, l3: 0.000129, l4: 0.000478, l5: 0.001049, l6: 0.001209
[epoch: 922/1000, batch:  1028/ 1052, ite: 242480] train loss: 0.003851, tar: 0.000039 
[Epoch 922/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000017, l1: 0.000017, l2: 0.000036, l3: 0.000108, l4: 0.000339, l5: 0.000814, l6: 0.001739
[epoch: 923/1000, batch:    56/ 1052, ite: 242500] train loss: 0.003870, tar: 0.000039 
l0: 0.000019, l1: 0.000018, l2: 0.000026, l3: 0.000039, l4: 0.000331, l5: 0.000877, l6: 0.001299
[epoch: 923/1000, batch:   136/ 1052, ite: 242520] train loss: 0.003861, tar: 0.000039 
l0: 0.000005, l1: 0.000007, l2: 0.000010, l3: 0.000064, l4: 0.000335, l5: 0.000852, l6: 0.001085
[epoch: 923/1000, batch:   216/ 1052, ite: 242540] train loss: 0.003851, tar: 0.000039 
l0: 0.000009, l1: 0.000009, l2: 0.000034, l3: 0.000282, l4: 0.000503, l5: 0.001125, l6: 0.002398
[epoch: 923/1000, batch:   296/ 1052, ite: 242560] train loss: 0.003888, tar: 0.000039 
l0: 0.000004, l1: 0.000004, l2: 0.000006, l3: 0.000138, l4: 0.000354, l5: 0.001425, l6: 0.001750
[epoch: 923/1000, batch:   376/ 1052, ite: 242580] train loss: 0.003890, tar: 0.000039 
l0: 0.000019, l1: 0.000019, l2: 0.000044, l3: 0.000100, l4: 0.000369, l5: 0.000938, l6: 0.002123
[epoch: 923/1000, batch:   456/ 1052, ite: 242600] train loss: 0.003895, tar: 0.000040 
l0: 0.000121, l1: 0.000123, l2: 0.000177, l3: 0.000271, l4: 0.000512, l5: 0.001023, l6: 0.001223
[epoch: 923/1000, batch:   536/ 1052, ite: 242620] train loss: 0.003887, tar: 0.000040 
l0: 0.000031, l1: 0.000032, l2: 0.000045, l3: 0.000140, l4: 0.000407, l5: 0.000534, l6: 0.001232
[epoch: 923/1000, batch:   616/ 1052, ite: 242640] train loss: 0.003888, tar: 0.000040 
l0: 0.000012, l1: 0.000013, l2: 0.000030, l3: 0.000095, l4: 0.000281, l5: 0.000676, l6: 0.001693
[epoch: 923/1000, batch:   696/ 1052, ite: 242660] train loss: 0.003883, tar: 0.000039 
l0: 0.000019, l1: 0.000019, l2: 0.000054, l3: 0.000125, l4: 0.000449, l5: 0.001086, l6: 0.001347
[epoch: 923/1000, batch:   776/ 1052, ite: 242680] train loss: 0.003874, tar: 0.000039 
l0: 0.000069, l1: 0.000069, l2: 0.000069, l3: 0.000181, l4: 0.000347, l5: 0.000922, l6: 0.001717
[epoch: 923/1000, batch:   856/ 1052, ite: 242700] train loss: 0.003867, tar: 0.000039 
l0: 0.000030, l1: 0.000031, l2: 0.000059, l3: 0.000140, l4: 0.000216, l5: 0.000780, l6: 0.002076
[epoch: 923/1000, batch:   936/ 1052, ite: 242720] train loss: 0.003861, tar: 0.000039 
l0: 0.000020, l1: 0.000019, l2: 0.000028, l3: 0.000099, l4: 0.000206, l5: 0.000584, l6: 0.000823
[epoch: 923/1000, batch:  1016/ 1052, ite: 242740] train loss: 0.003846, tar: 0.000039 
[Epoch 923/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000003, l1: 0.000004, l2: 0.000008, l3: 0.000068, l4: 0.000118, l5: 0.000732, l6: 0.001342
[epoch: 924/1000, batch:    44/ 1052, ite: 242760] train loss: 0.003859, tar: 0.000039 
l0: 0.000034, l1: 0.000036, l2: 0.000067, l3: 0.000398, l4: 0.000724, l5: 0.001344, l6: 0.002211
[epoch: 924/1000, batch:   124/ 1052, ite: 242780] train loss: 0.003862, tar: 0.000039 
l0: 0.000023, l1: 0.000023, l2: 0.000035, l3: 0.000092, l4: 0.000330, l5: 0.000719, l6: 0.001809
[epoch: 924/1000, batch:   204/ 1052, ite: 242800] train loss: 0.003862, tar: 0.000039 
l0: 0.000018, l1: 0.000016, l2: 0.000050, l3: 0.000332, l4: 0.000912, l5: 0.001919, l6: 0.003011
[epoch: 924/1000, batch:   284/ 1052, ite: 242820] train loss: 0.003850, tar: 0.000038 
l0: 0.000008, l1: 0.000008, l2: 0.000032, l3: 0.000158, l4: 0.000525, l5: 0.000788, l6: 0.002355
[epoch: 924/1000, batch:   364/ 1052, ite: 242840] train loss: 0.003844, tar: 0.000038 
l0: 0.000038, l1: 0.000037, l2: 0.000051, l3: 0.000125, l4: 0.000473, l5: 0.001280, l6: 0.001235
[epoch: 924/1000, batch:   444/ 1052, ite: 242860] train loss: 0.003848, tar: 0.000039 
l0: 0.000009, l1: 0.000008, l2: 0.000018, l3: 0.000147, l4: 0.000496, l5: 0.001239, l6: 0.002035
[epoch: 924/1000, batch:   524/ 1052, ite: 242880] train loss: 0.003854, tar: 0.000040 
l0: 0.000021, l1: 0.000022, l2: 0.000027, l3: 0.000075, l4: 0.000261, l5: 0.000308, l6: 0.000874
[epoch: 924/1000, batch:   604/ 1052, ite: 242900] train loss: 0.003848, tar: 0.000040 
l0: 0.000053, l1: 0.000056, l2: 0.000064, l3: 0.000133, l4: 0.000387, l5: 0.000791, l6: 0.001784
[epoch: 924/1000, batch:   684/ 1052, ite: 242920] train loss: 0.003852, tar: 0.000040 
l0: 0.000025, l1: 0.000025, l2: 0.000034, l3: 0.000078, l4: 0.000256, l5: 0.000608, l6: 0.000700
[epoch: 924/1000, batch:   764/ 1052, ite: 242940] train loss: 0.003847, tar: 0.000040 
l0: 0.000048, l1: 0.000050, l2: 0.000050, l3: 0.000168, l4: 0.000474, l5: 0.001080, l6: 0.002470
[epoch: 924/1000, batch:   844/ 1052, ite: 242960] train loss: 0.003855, tar: 0.000040 
l0: 0.000032, l1: 0.000032, l2: 0.000052, l3: 0.000190, l4: 0.000441, l5: 0.001007, l6: 0.001216
[epoch: 924/1000, batch:   924/ 1052, ite: 242980] train loss: 0.003861, tar: 0.000040 
l0: 0.000044, l1: 0.000043, l2: 0.000063, l3: 0.000127, l4: 0.000395, l5: 0.000657, l6: 0.001342
[epoch: 924/1000, batch:  1004/ 1052, ite: 243000] train loss: 0.003855, tar: 0.000040 
[Epoch 924/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000032, l1: 0.000032, l2: 0.000079, l3: 0.000248, l4: 0.000900, l5: 0.004260, l6: 0.005884
[epoch: 925/1000, batch:    32/ 1052, ite: 243020] train loss: 0.003866, tar: 0.000040 
l0: 0.000014, l1: 0.000014, l2: 0.000093, l3: 0.000244, l4: 0.000671, l5: 0.001122, l6: 0.002237
[epoch: 925/1000, batch:   112/ 1052, ite: 243040] train loss: 0.003865, tar: 0.000040 
l0: 0.000008, l1: 0.000007, l2: 0.000038, l3: 0.000141, l4: 0.000367, l5: 0.001139, l6: 0.002072
[epoch: 925/1000, batch:   192/ 1052, ite: 243060] train loss: 0.003866, tar: 0.000040 
l0: 0.000015, l1: 0.000016, l2: 0.000062, l3: 0.000101, l4: 0.000231, l5: 0.000678, l6: 0.001220
[epoch: 925/1000, batch:   272/ 1052, ite: 243080] train loss: 0.003870, tar: 0.000040 
l0: 0.000023, l1: 0.000023, l2: 0.000069, l3: 0.000293, l4: 0.000823, l5: 0.001972, l6: 0.002903
[epoch: 925/1000, batch:   352/ 1052, ite: 243100] train loss: 0.003877, tar: 0.000040 
l0: 0.000023, l1: 0.000022, l2: 0.000037, l3: 0.000108, l4: 0.000351, l5: 0.000816, l6: 0.001006
[epoch: 925/1000, batch:   432/ 1052, ite: 243120] train loss: 0.003886, tar: 0.000040 
l0: 0.000010, l1: 0.000011, l2: 0.000031, l3: 0.000096, l4: 0.000351, l5: 0.001001, l6: 0.001255
[epoch: 925/1000, batch:   512/ 1052, ite: 243140] train loss: 0.003878, tar: 0.000040 
l0: 0.000062, l1: 0.000064, l2: 0.000071, l3: 0.000096, l4: 0.000289, l5: 0.000909, l6: 0.001751
[epoch: 925/1000, batch:   592/ 1052, ite: 243160] train loss: 0.003877, tar: 0.000040 
l0: 0.000160, l1: 0.000157, l2: 0.000202, l3: 0.000356, l4: 0.000784, l5: 0.001649, l6: 0.001869
[epoch: 925/1000, batch:   672/ 1052, ite: 243180] train loss: 0.003875, tar: 0.000040 
l0: 0.000040, l1: 0.000039, l2: 0.000061, l3: 0.000154, l4: 0.000365, l5: 0.000726, l6: 0.000967
[epoch: 925/1000, batch:   752/ 1052, ite: 243200] train loss: 0.003875, tar: 0.000040 
l0: 0.000003, l1: 0.000003, l2: 0.000016, l3: 0.000066, l4: 0.000282, l5: 0.000661, l6: 0.001304
[epoch: 925/1000, batch:   832/ 1052, ite: 243220] train loss: 0.003866, tar: 0.000040 
l0: 0.000014, l1: 0.000018, l2: 0.000020, l3: 0.000099, l4: 0.000248, l5: 0.000720, l6: 0.002251
[epoch: 925/1000, batch:   912/ 1052, ite: 243240] train loss: 0.003856, tar: 0.000040 
l0: 0.000018, l1: 0.000017, l2: 0.000069, l3: 0.000280, l4: 0.000759, l5: 0.001959, l6: 0.003765
[epoch: 925/1000, batch:   992/ 1052, ite: 243260] train loss: 0.003857, tar: 0.000040 
[Epoch 925/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000039, l1: 0.000038, l2: 0.000058, l3: 0.000189, l4: 0.000411, l5: 0.001345, l6: 0.001762
[epoch: 926/1000, batch:    20/ 1052, ite: 243280] train loss: 0.003865, tar: 0.000040 
l0: 0.000115, l1: 0.000114, l2: 0.000159, l3: 0.000465, l4: 0.001130, l5: 0.001759, l6: 0.004507
[epoch: 926/1000, batch:   100/ 1052, ite: 243300] train loss: 0.003862, tar: 0.000040 
l0: 0.000039, l1: 0.000039, l2: 0.000062, l3: 0.000146, l4: 0.000393, l5: 0.001074, l6: 0.002180
[epoch: 926/1000, batch:   180/ 1052, ite: 243320] train loss: 0.003863, tar: 0.000040 
l0: 0.000058, l1: 0.000058, l2: 0.000086, l3: 0.000229, l4: 0.000543, l5: 0.001384, l6: 0.002272
[epoch: 926/1000, batch:   260/ 1052, ite: 243340] train loss: 0.003868, tar: 0.000041 
l0: 0.000018, l1: 0.000017, l2: 0.000026, l3: 0.000079, l4: 0.000236, l5: 0.000643, l6: 0.000917
[epoch: 926/1000, batch:   340/ 1052, ite: 243360] train loss: 0.003866, tar: 0.000041 
l0: 0.000004, l1: 0.000004, l2: 0.000021, l3: 0.000089, l4: 0.000571, l5: 0.001003, l6: 0.002348
[epoch: 926/1000, batch:   420/ 1052, ite: 243380] train loss: 0.003865, tar: 0.000040 
l0: 0.000007, l1: 0.000008, l2: 0.000025, l3: 0.000109, l4: 0.000487, l5: 0.000846, l6: 0.001545
[epoch: 926/1000, batch:   500/ 1052, ite: 243400] train loss: 0.003862, tar: 0.000040 
l0: 0.000069, l1: 0.000070, l2: 0.000076, l3: 0.000192, l4: 0.000367, l5: 0.000891, l6: 0.002382
[epoch: 926/1000, batch:   580/ 1052, ite: 243420] train loss: 0.003866, tar: 0.000040 
l0: 0.000002, l1: 0.000002, l2: 0.000029, l3: 0.000119, l4: 0.000306, l5: 0.001109, l6: 0.001300
[epoch: 926/1000, batch:   660/ 1052, ite: 243440] train loss: 0.003861, tar: 0.000040 
l0: 0.000012, l1: 0.000010, l2: 0.000048, l3: 0.000282, l4: 0.000644, l5: 0.001269, l6: 0.002184
[epoch: 926/1000, batch:   740/ 1052, ite: 243460] train loss: 0.003863, tar: 0.000041 
l0: 0.000009, l1: 0.000008, l2: 0.000039, l3: 0.000127, l4: 0.000325, l5: 0.001293, l6: 0.001658
[epoch: 926/1000, batch:   820/ 1052, ite: 243480] train loss: 0.003862, tar: 0.000041 
l0: 0.000101, l1: 0.000103, l2: 0.000165, l3: 0.000302, l4: 0.000760, l5: 0.001524, l6: 0.003386
[epoch: 926/1000, batch:   900/ 1052, ite: 243500] train loss: 0.003872, tar: 0.000041 
l0: 0.000017, l1: 0.000017, l2: 0.000024, l3: 0.000078, l4: 0.000143, l5: 0.000623, l6: 0.001154
[epoch: 926/1000, batch:   980/ 1052, ite: 243520] train loss: 0.003870, tar: 0.000040 
[Epoch 926/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000020, l1: 0.000021, l2: 0.000031, l3: 0.000110, l4: 0.000372, l5: 0.000907, l6: 0.001376
[epoch: 927/1000, batch:     8/ 1052, ite: 243540] train loss: 0.003867, tar: 0.000040 
l0: 0.000019, l1: 0.000020, l2: 0.000024, l3: 0.000069, l4: 0.000175, l5: 0.000664, l6: 0.000921
[epoch: 927/1000, batch:    88/ 1052, ite: 243560] train loss: 0.003866, tar: 0.000040 
l0: 0.000011, l1: 0.000012, l2: 0.000032, l3: 0.000148, l4: 0.000431, l5: 0.001172, l6: 0.002392
[epoch: 927/1000, batch:   168/ 1052, ite: 243580] train loss: 0.003861, tar: 0.000040 
l0: 0.000078, l1: 0.000078, l2: 0.000116, l3: 0.000238, l4: 0.000608, l5: 0.001367, l6: 0.002808
[epoch: 927/1000, batch:   248/ 1052, ite: 243600] train loss: 0.003864, tar: 0.000040 
l0: 0.000044, l1: 0.000044, l2: 0.000068, l3: 0.000224, l4: 0.000601, l5: 0.000755, l6: 0.001860
[epoch: 927/1000, batch:   328/ 1052, ite: 243620] train loss: 0.003870, tar: 0.000040 
l0: 0.000010, l1: 0.000010, l2: 0.000017, l3: 0.000058, l4: 0.000272, l5: 0.000922, l6: 0.001871
[epoch: 927/1000, batch:   408/ 1052, ite: 243640] train loss: 0.003874, tar: 0.000040 
l0: 0.000043, l1: 0.000043, l2: 0.000083, l3: 0.000209, l4: 0.000328, l5: 0.000930, l6: 0.001463
[epoch: 927/1000, batch:   488/ 1052, ite: 243660] train loss: 0.003872, tar: 0.000040 
l0: 0.000001, l1: 0.000001, l2: 0.000010, l3: 0.000066, l4: 0.000141, l5: 0.000583, l6: 0.000961
[epoch: 927/1000, batch:   568/ 1052, ite: 243680] train loss: 0.003868, tar: 0.000040 
l0: 0.000017, l1: 0.000015, l2: 0.000065, l3: 0.000169, l4: 0.000244, l5: 0.001049, l6: 0.001923
[epoch: 927/1000, batch:   648/ 1052, ite: 243700] train loss: 0.003867, tar: 0.000040 
l0: 0.000045, l1: 0.000047, l2: 0.000056, l3: 0.000178, l4: 0.000732, l5: 0.001289, l6: 0.002313
[epoch: 927/1000, batch:   728/ 1052, ite: 243720] train loss: 0.003864, tar: 0.000040 
l0: 0.000025, l1: 0.000025, l2: 0.000027, l3: 0.000142, l4: 0.000372, l5: 0.000969, l6: 0.001971
[epoch: 927/1000, batch:   808/ 1052, ite: 243740] train loss: 0.003869, tar: 0.000040 
l0: 0.000012, l1: 0.000011, l2: 0.000067, l3: 0.000292, l4: 0.000659, l5: 0.001262, l6: 0.001922
[epoch: 927/1000, batch:   888/ 1052, ite: 243760] train loss: 0.003877, tar: 0.000041 
l0: 0.000048, l1: 0.000048, l2: 0.000078, l3: 0.000240, l4: 0.000416, l5: 0.000551, l6: 0.001284
[epoch: 927/1000, batch:   968/ 1052, ite: 243780] train loss: 0.003875, tar: 0.000041 
l0: 0.000033, l1: 0.000030, l2: 0.000049, l3: 0.000057, l4: 0.000209, l5: 0.000369, l6: 0.000416
[epoch: 927/1000, batch:  1048/ 1052, ite: 243800] train loss: 0.003871, tar: 0.000040 
[Epoch 927/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000033, l1: 0.000033, l2: 0.000088, l3: 0.000183, l4: 0.000587, l5: 0.001318, l6: 0.002382
[epoch: 928/1000, batch:    76/ 1052, ite: 243820] train loss: 0.003869, tar: 0.000040 
l0: 0.000058, l1: 0.000058, l2: 0.000064, l3: 0.000148, l4: 0.000389, l5: 0.001028, l6: 0.002030
[epoch: 928/1000, batch:   156/ 1052, ite: 243840] train loss: 0.003863, tar: 0.000040 
l0: 0.000015, l1: 0.000016, l2: 0.000045, l3: 0.000115, l4: 0.000177, l5: 0.000668, l6: 0.001357
[epoch: 928/1000, batch:   236/ 1052, ite: 243860] train loss: 0.003863, tar: 0.000040 
l0: 0.000053, l1: 0.000054, l2: 0.000071, l3: 0.000132, l4: 0.000347, l5: 0.001475, l6: 0.001272
[epoch: 928/1000, batch:   316/ 1052, ite: 243880] train loss: 0.003863, tar: 0.000040 
l0: 0.000007, l1: 0.000007, l2: 0.000054, l3: 0.000354, l4: 0.000760, l5: 0.001806, l6: 0.003431
[epoch: 928/1000, batch:   396/ 1052, ite: 243900] train loss: 0.003868, tar: 0.000040 
l0: 0.000071, l1: 0.000079, l2: 0.000091, l3: 0.000198, l4: 0.000561, l5: 0.001338, l6: 0.002164
[epoch: 928/1000, batch:   476/ 1052, ite: 243920] train loss: 0.003864, tar: 0.000040 
l0: 0.000028, l1: 0.000026, l2: 0.000084, l3: 0.000278, l4: 0.000798, l5: 0.001970, l6: 0.003850
[epoch: 928/1000, batch:   556/ 1052, ite: 243940] train loss: 0.003868, tar: 0.000041 
l0: 0.000057, l1: 0.000058, l2: 0.000067, l3: 0.000099, l4: 0.000312, l5: 0.000830, l6: 0.002019
[epoch: 928/1000, batch:   636/ 1052, ite: 243960] train loss: 0.003869, tar: 0.000040 
l0: 0.000014, l1: 0.000012, l2: 0.000044, l3: 0.000166, l4: 0.000419, l5: 0.001301, l6: 0.002478
[epoch: 928/1000, batch:   716/ 1052, ite: 243980] train loss: 0.003871, tar: 0.000041 
l0: 0.000022, l1: 0.000022, l2: 0.000067, l3: 0.000280, l4: 0.000672, l5: 0.002483, l6: 0.005245
[epoch: 928/1000, batch:   796/ 1052, ite: 244000] train loss: 0.003877, tar: 0.000041 
l0: 0.000023, l1: 0.000023, l2: 0.000030, l3: 0.000122, l4: 0.000238, l5: 0.000780, l6: 0.001256
[epoch: 928/1000, batch:   876/ 1052, ite: 244020] train loss: 0.003882, tar: 0.000041 
l0: 0.000009, l1: 0.000014, l2: 0.000021, l3: 0.000093, l4: 0.000285, l5: 0.001100, l6: 0.001562
[epoch: 928/1000, batch:   956/ 1052, ite: 244040] train loss: 0.003876, tar: 0.000041 
l0: 0.000005, l1: 0.000004, l2: 0.000022, l3: 0.000086, l4: 0.000477, l5: 0.000827, l6: 0.001489
[epoch: 928/1000, batch:  1036/ 1052, ite: 244060] train loss: 0.003875, tar: 0.000041 
[Epoch 928/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000223, l1: 0.000225, l2: 0.000307, l3: 0.000509, l4: 0.000856, l5: 0.001692, l6: 0.002706
[epoch: 929/1000, batch:    64/ 1052, ite: 244080] train loss: 0.003872, tar: 0.000041 
l0: 0.000065, l1: 0.000061, l2: 0.000138, l3: 0.000568, l4: 0.000955, l5: 0.001785, l6: 0.002658
[epoch: 929/1000, batch:   144/ 1052, ite: 244100] train loss: 0.003875, tar: 0.000041 
l0: 0.000013, l1: 0.000013, l2: 0.000025, l3: 0.000061, l4: 0.000379, l5: 0.000580, l6: 0.000927
[epoch: 929/1000, batch:   224/ 1052, ite: 244120] train loss: 0.003878, tar: 0.000041 
l0: 0.000010, l1: 0.000010, l2: 0.000030, l3: 0.000105, l4: 0.000374, l5: 0.000752, l6: 0.002721
[epoch: 929/1000, batch:   304/ 1052, ite: 244140] train loss: 0.003876, tar: 0.000041 
l0: 0.000013, l1: 0.000014, l2: 0.000042, l3: 0.000160, l4: 0.000530, l5: 0.001010, l6: 0.001571
[epoch: 929/1000, batch:   384/ 1052, ite: 244160] train loss: 0.003878, tar: 0.000041 
l0: 0.000014, l1: 0.000013, l2: 0.000027, l3: 0.000107, l4: 0.000265, l5: 0.000665, l6: 0.001662
[epoch: 929/1000, batch:   464/ 1052, ite: 244180] train loss: 0.003878, tar: 0.000041 
l0: 0.000006, l1: 0.000005, l2: 0.000017, l3: 0.000092, l4: 0.000381, l5: 0.000346, l6: 0.001702
[epoch: 929/1000, batch:   544/ 1052, ite: 244200] train loss: 0.003876, tar: 0.000041 
l0: 0.000059, l1: 0.000059, l2: 0.000089, l3: 0.000257, l4: 0.000540, l5: 0.001073, l6: 0.002001
[epoch: 929/1000, batch:   624/ 1052, ite: 244220] train loss: 0.003879, tar: 0.000041 
l0: 0.000032, l1: 0.000032, l2: 0.000095, l3: 0.000224, l4: 0.000689, l5: 0.001579, l6: 0.003041
[epoch: 929/1000, batch:   704/ 1052, ite: 244240] train loss: 0.003876, tar: 0.000041 
l0: 0.000016, l1: 0.000015, l2: 0.000022, l3: 0.000062, l4: 0.000271, l5: 0.000663, l6: 0.002032
[epoch: 929/1000, batch:   784/ 1052, ite: 244260] train loss: 0.003876, tar: 0.000041 
l0: 0.000009, l1: 0.000007, l2: 0.000020, l3: 0.000090, l4: 0.000225, l5: 0.001259, l6: 0.001363
[epoch: 929/1000, batch:   864/ 1052, ite: 244280] train loss: 0.003878, tar: 0.000041 
l0: 0.000037, l1: 0.000038, l2: 0.000056, l3: 0.000203, l4: 0.000634, l5: 0.001687, l6: 0.002613
[epoch: 929/1000, batch:   944/ 1052, ite: 244300] train loss: 0.003877, tar: 0.000041 
l0: 0.000033, l1: 0.000033, l2: 0.000074, l3: 0.000189, l4: 0.000469, l5: 0.001150, l6: 0.002399
[epoch: 929/1000, batch:  1024/ 1052, ite: 244320] train loss: 0.003880, tar: 0.000041 
[Epoch 929/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000005, l1: 0.000005, l2: 0.000038, l3: 0.000118, l4: 0.000451, l5: 0.001108, l6: 0.002011
[epoch: 930/1000, batch:    52/ 1052, ite: 244340] train loss: 0.003875, tar: 0.000041 
l0: 0.000004, l1: 0.000003, l2: 0.000076, l3: 0.000242, l4: 0.000720, l5: 0.001537, l6: 0.002127
[epoch: 930/1000, batch:   132/ 1052, ite: 244360] train loss: 0.003872, tar: 0.000041 
l0: 0.000011, l1: 0.000011, l2: 0.000031, l3: 0.000143, l4: 0.000354, l5: 0.001040, l6: 0.001810
[epoch: 930/1000, batch:   212/ 1052, ite: 244380] train loss: 0.003874, tar: 0.000041 
l0: 0.000059, l1: 0.000061, l2: 0.000077, l3: 0.000142, l4: 0.000283, l5: 0.000933, l6: 0.001414
[epoch: 930/1000, batch:   292/ 1052, ite: 244400] train loss: 0.003872, tar: 0.000041 
l0: 0.000027, l1: 0.000026, l2: 0.000048, l3: 0.000149, l4: 0.000312, l5: 0.000662, l6: 0.001683
[epoch: 930/1000, batch:   372/ 1052, ite: 244420] train loss: 0.003875, tar: 0.000041 
l0: 0.000069, l1: 0.000068, l2: 0.000100, l3: 0.000208, l4: 0.000384, l5: 0.000927, l6: 0.002411
[epoch: 930/1000, batch:   452/ 1052, ite: 244440] train loss: 0.003874, tar: 0.000041 
l0: 0.000075, l1: 0.000075, l2: 0.000090, l3: 0.000172, l4: 0.000386, l5: 0.000674, l6: 0.001261
[epoch: 930/1000, batch:   532/ 1052, ite: 244460] train loss: 0.003874, tar: 0.000041 
l0: 0.000072, l1: 0.000072, l2: 0.000158, l3: 0.000340, l4: 0.000580, l5: 0.001237, l6: 0.001607
[epoch: 930/1000, batch:   612/ 1052, ite: 244480] train loss: 0.003871, tar: 0.000041 
l0: 0.000042, l1: 0.000043, l2: 0.000051, l3: 0.000117, l4: 0.000350, l5: 0.000790, l6: 0.001398
[epoch: 930/1000, batch:   692/ 1052, ite: 244500] train loss: 0.003867, tar: 0.000041 
l0: 0.000044, l1: 0.000045, l2: 0.000080, l3: 0.000160, l4: 0.000262, l5: 0.000905, l6: 0.001096
[epoch: 930/1000, batch:   772/ 1052, ite: 244520] train loss: 0.003865, tar: 0.000041 
l0: 0.000003, l1: 0.000002, l2: 0.000011, l3: 0.000072, l4: 0.000387, l5: 0.000870, l6: 0.001661
[epoch: 930/1000, batch:   852/ 1052, ite: 244540] train loss: 0.003870, tar: 0.000041 
l0: 0.000021, l1: 0.000021, l2: 0.000054, l3: 0.000293, l4: 0.000710, l5: 0.001396, l6: 0.002502
[epoch: 930/1000, batch:   932/ 1052, ite: 244560] train loss: 0.003871, tar: 0.000040 
l0: 0.000051, l1: 0.000052, l2: 0.000080, l3: 0.000256, l4: 0.000964, l5: 0.001641, l6: 0.003344
[epoch: 930/1000, batch:  1012/ 1052, ite: 244580] train loss: 0.003874, tar: 0.000040 
[Epoch 930/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000035, l1: 0.000036, l2: 0.000116, l3: 0.000302, l4: 0.000825, l5: 0.001632, l6: 0.002667
[epoch: 931/1000, batch:    40/ 1052, ite: 244600] train loss: 0.003874, tar: 0.000040 
l0: 0.000087, l1: 0.000089, l2: 0.000143, l3: 0.000286, l4: 0.000827, l5: 0.001382, l6: 0.002966
[epoch: 931/1000, batch:   120/ 1052, ite: 244620] train loss: 0.003879, tar: 0.000041 
l0: 0.000010, l1: 0.000008, l2: 0.000033, l3: 0.000073, l4: 0.000294, l5: 0.000741, l6: 0.001004
[epoch: 931/1000, batch:   200/ 1052, ite: 244640] train loss: 0.003878, tar: 0.000040 
l0: 0.000052, l1: 0.000051, l2: 0.000127, l3: 0.000288, l4: 0.000487, l5: 0.000979, l6: 0.001737
[epoch: 931/1000, batch:   280/ 1052, ite: 244660] train loss: 0.003873, tar: 0.000040 
l0: 0.000172, l1: 0.000171, l2: 0.000221, l3: 0.000423, l4: 0.000872, l5: 0.002040, l6: 0.003584
[epoch: 931/1000, batch:   360/ 1052, ite: 244680] train loss: 0.003875, tar: 0.000041 
l0: 0.000015, l1: 0.000016, l2: 0.000025, l3: 0.000133, l4: 0.000278, l5: 0.000846, l6: 0.001547
[epoch: 931/1000, batch:   440/ 1052, ite: 244700] train loss: 0.003874, tar: 0.000041 
l0: 0.000032, l1: 0.000033, l2: 0.000059, l3: 0.000098, l4: 0.000254, l5: 0.000872, l6: 0.002130
[epoch: 931/1000, batch:   520/ 1052, ite: 244720] train loss: 0.003874, tar: 0.000041 
l0: 0.000021, l1: 0.000023, l2: 0.000047, l3: 0.000170, l4: 0.000261, l5: 0.001305, l6: 0.002186
[epoch: 931/1000, batch:   600/ 1052, ite: 244740] train loss: 0.003878, tar: 0.000041 
l0: 0.000044, l1: 0.000045, l2: 0.000087, l3: 0.000273, l4: 0.000643, l5: 0.001884, l6: 0.002096
[epoch: 931/1000, batch:   680/ 1052, ite: 244760] train loss: 0.003881, tar: 0.000041 
l0: 0.000062, l1: 0.000063, l2: 0.000097, l3: 0.000235, l4: 0.000644, l5: 0.001494, l6: 0.001880
[epoch: 931/1000, batch:   760/ 1052, ite: 244780] train loss: 0.003881, tar: 0.000041 
l0: 0.000045, l1: 0.000045, l2: 0.000080, l3: 0.000168, l4: 0.000350, l5: 0.000931, l6: 0.001875
[epoch: 931/1000, batch:   840/ 1052, ite: 244800] train loss: 0.003880, tar: 0.000041 
l0: 0.000006, l1: 0.000007, l2: 0.000012, l3: 0.000178, l4: 0.000383, l5: 0.000830, l6: 0.001426
[epoch: 931/1000, batch:   920/ 1052, ite: 244820] train loss: 0.003875, tar: 0.000041 
l0: 0.000060, l1: 0.000059, l2: 0.000084, l3: 0.000189, l4: 0.000501, l5: 0.001182, l6: 0.001651
[epoch: 931/1000, batch:  1000/ 1052, ite: 244840] train loss: 0.003874, tar: 0.000041 
[Epoch 931/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000041, l1: 0.000042, l2: 0.000077, l3: 0.000173, l4: 0.000434, l5: 0.000939, l6: 0.002762
[epoch: 932/1000, batch:    28/ 1052, ite: 244860] train loss: 0.003876, tar: 0.000041 
l0: 0.000041, l1: 0.000044, l2: 0.000065, l3: 0.000151, l4: 0.000602, l5: 0.000700, l6: 0.001397
[epoch: 932/1000, batch:   108/ 1052, ite: 244880] train loss: 0.003879, tar: 0.000041 
l0: 0.000046, l1: 0.000047, l2: 0.000068, l3: 0.000158, l4: 0.000437, l5: 0.000797, l6: 0.002038
[epoch: 932/1000, batch:   188/ 1052, ite: 244900] train loss: 0.003880, tar: 0.000041 
l0: 0.000035, l1: 0.000034, l2: 0.000091, l3: 0.000246, l4: 0.000572, l5: 0.001089, l6: 0.001715
[epoch: 932/1000, batch:   268/ 1052, ite: 244920] train loss: 0.003879, tar: 0.000041 
l0: 0.000014, l1: 0.000012, l2: 0.000058, l3: 0.000359, l4: 0.000922, l5: 0.001591, l6: 0.003262
[epoch: 932/1000, batch:   348/ 1052, ite: 244940] train loss: 0.003880, tar: 0.000041 
l0: 0.000081, l1: 0.000084, l2: 0.000136, l3: 0.000221, l4: 0.000378, l5: 0.001464, l6: 0.002626
[epoch: 932/1000, batch:   428/ 1052, ite: 244960] train loss: 0.003880, tar: 0.000041 
l0: 0.000128, l1: 0.000124, l2: 0.000166, l3: 0.000358, l4: 0.000865, l5: 0.001526, l6: 0.003632
[epoch: 932/1000, batch:   508/ 1052, ite: 244980] train loss: 0.003882, tar: 0.000041 
l0: 0.000008, l1: 0.000008, l2: 0.000016, l3: 0.000121, l4: 0.000388, l5: 0.001218, l6: 0.002323
[epoch: 932/1000, batch:   588/ 1052, ite: 245000] train loss: 0.003879, tar: 0.000040 
l0: 0.000007, l1: 0.000007, l2: 0.000027, l3: 0.000116, l4: 0.000489, l5: 0.001277, l6: 0.002000
[epoch: 932/1000, batch:   668/ 1052, ite: 245020] train loss: 0.003878, tar: 0.000040 
l0: 0.000085, l1: 0.000086, l2: 0.000091, l3: 0.000131, l4: 0.000440, l5: 0.001317, l6: 0.001864
[epoch: 932/1000, batch:   748/ 1052, ite: 245040] train loss: 0.003878, tar: 0.000040 
l0: 0.000015, l1: 0.000015, l2: 0.000025, l3: 0.000085, l4: 0.000268, l5: 0.001035, l6: 0.001680
[epoch: 932/1000, batch:   828/ 1052, ite: 245060] train loss: 0.003878, tar: 0.000040 
l0: 0.000096, l1: 0.000099, l2: 0.000117, l3: 0.000232, l4: 0.000603, l5: 0.000742, l6: 0.002770
[epoch: 932/1000, batch:   908/ 1052, ite: 245080] train loss: 0.003878, tar: 0.000040 
l0: 0.000023, l1: 0.000023, l2: 0.000040, l3: 0.000112, l4: 0.000626, l5: 0.001113, l6: 0.001764
[epoch: 932/1000, batch:   988/ 1052, ite: 245100] train loss: 0.003875, tar: 0.000040 
[Epoch 932/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000057, l1: 0.000060, l2: 0.000073, l3: 0.000168, l4: 0.000382, l5: 0.000471, l6: 0.002124
[epoch: 933/1000, batch:    16/ 1052, ite: 245120] train loss: 0.003876, tar: 0.000040 
l0: 0.000159, l1: 0.000163, l2: 0.000180, l3: 0.000284, l4: 0.000610, l5: 0.001532, l6: 0.002648
[epoch: 933/1000, batch:    96/ 1052, ite: 245140] train loss: 0.003878, tar: 0.000040 
l0: 0.000008, l1: 0.000008, l2: 0.000040, l3: 0.000182, l4: 0.000600, l5: 0.001733, l6: 0.002481
[epoch: 933/1000, batch:   176/ 1052, ite: 245160] train loss: 0.003879, tar: 0.000040 
l0: 0.000022, l1: 0.000022, l2: 0.000027, l3: 0.000080, l4: 0.000333, l5: 0.000655, l6: 0.001207
[epoch: 933/1000, batch:   256/ 1052, ite: 245180] train loss: 0.003879, tar: 0.000041 
l0: 0.000006, l1: 0.000006, l2: 0.000040, l3: 0.000087, l4: 0.000279, l5: 0.000677, l6: 0.001261
[epoch: 933/1000, batch:   336/ 1052, ite: 245200] train loss: 0.003878, tar: 0.000041 
l0: 0.000000, l1: 0.000000, l2: 0.000019, l3: 0.000265, l4: 0.000844, l5: 0.001292, l6: 0.002234
[epoch: 933/1000, batch:   416/ 1052, ite: 245220] train loss: 0.003876, tar: 0.000041 
l0: 0.000058, l1: 0.000059, l2: 0.000072, l3: 0.000165, l4: 0.000327, l5: 0.000906, l6: 0.001304
[epoch: 933/1000, batch:   496/ 1052, ite: 245240] train loss: 0.003875, tar: 0.000041 
l0: 0.000046, l1: 0.000046, l2: 0.000068, l3: 0.000210, l4: 0.000469, l5: 0.000649, l6: 0.001618
[epoch: 933/1000, batch:   576/ 1052, ite: 245260] train loss: 0.003872, tar: 0.000041 
l0: 0.000018, l1: 0.000018, l2: 0.000074, l3: 0.000226, l4: 0.000376, l5: 0.001362, l6: 0.002955
[epoch: 933/1000, batch:   656/ 1052, ite: 245280] train loss: 0.003875, tar: 0.000041 
l0: 0.000043, l1: 0.000044, l2: 0.000052, l3: 0.000128, l4: 0.000509, l5: 0.002016, l6: 0.003050
[epoch: 933/1000, batch:   736/ 1052, ite: 245300] train loss: 0.003872, tar: 0.000041 
l0: 0.000107, l1: 0.000106, l2: 0.000136, l3: 0.000219, l4: 0.000500, l5: 0.000871, l6: 0.002817
[epoch: 933/1000, batch:   816/ 1052, ite: 245320] train loss: 0.003869, tar: 0.000040 
l0: 0.000052, l1: 0.000053, l2: 0.000079, l3: 0.000136, l4: 0.000342, l5: 0.001187, l6: 0.001258
[epoch: 933/1000, batch:   896/ 1052, ite: 245340] train loss: 0.003870, tar: 0.000040 
l0: 0.000083, l1: 0.000083, l2: 0.000136, l3: 0.000236, l4: 0.000515, l5: 0.001330, l6: 0.001807
[epoch: 933/1000, batch:   976/ 1052, ite: 245360] train loss: 0.003872, tar: 0.000040 
[Epoch 933/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000079, l1: 0.000077, l2: 0.000129, l3: 0.000319, l4: 0.000689, l5: 0.001586, l6: 0.003073
[epoch: 934/1000, batch:     4/ 1052, ite: 245380] train loss: 0.003873, tar: 0.000040 
l0: 0.000014, l1: 0.000011, l2: 0.000054, l3: 0.000193, l4: 0.000620, l5: 0.001407, l6: 0.001657
[epoch: 934/1000, batch:    84/ 1052, ite: 245400] train loss: 0.003873, tar: 0.000040 
l0: 0.000036, l1: 0.000038, l2: 0.000054, l3: 0.000171, l4: 0.000271, l5: 0.000628, l6: 0.002037
[epoch: 934/1000, batch:   164/ 1052, ite: 245420] train loss: 0.003874, tar: 0.000040 
l0: 0.000034, l1: 0.000033, l2: 0.000046, l3: 0.000124, l4: 0.000342, l5: 0.000596, l6: 0.001107
[epoch: 934/1000, batch:   244/ 1052, ite: 245440] train loss: 0.003872, tar: 0.000040 
l0: 0.000021, l1: 0.000022, l2: 0.000054, l3: 0.000093, l4: 0.000204, l5: 0.000967, l6: 0.001802
[epoch: 934/1000, batch:   324/ 1052, ite: 245460] train loss: 0.003870, tar: 0.000040 
l0: 0.000008, l1: 0.000007, l2: 0.000033, l3: 0.000185, l4: 0.000416, l5: 0.001018, l6: 0.001492
[epoch: 934/1000, batch:   404/ 1052, ite: 245480] train loss: 0.003868, tar: 0.000040 
l0: 0.000046, l1: 0.000050, l2: 0.000083, l3: 0.000162, l4: 0.000362, l5: 0.001020, l6: 0.002738
[epoch: 934/1000, batch:   484/ 1052, ite: 245500] train loss: 0.003866, tar: 0.000040 
l0: 0.000026, l1: 0.000025, l2: 0.000068, l3: 0.000234, l4: 0.000617, l5: 0.001329, l6: 0.002309
[epoch: 934/1000, batch:   564/ 1052, ite: 245520] train loss: 0.003867, tar: 0.000040 
l0: 0.000110, l1: 0.000110, l2: 0.000131, l3: 0.000269, l4: 0.000693, l5: 0.001656, l6: 0.002047
[epoch: 934/1000, batch:   644/ 1052, ite: 245540] train loss: 0.003868, tar: 0.000040 
l0: 0.000015, l1: 0.000015, l2: 0.000039, l3: 0.000103, l4: 0.000281, l5: 0.001165, l6: 0.001824
[epoch: 934/1000, batch:   724/ 1052, ite: 245560] train loss: 0.003869, tar: 0.000040 
l0: 0.000002, l1: 0.000002, l2: 0.000015, l3: 0.000094, l4: 0.000216, l5: 0.000685, l6: 0.001302
[epoch: 934/1000, batch:   804/ 1052, ite: 245580] train loss: 0.003868, tar: 0.000040 
l0: 0.000009, l1: 0.000007, l2: 0.000048, l3: 0.000133, l4: 0.000376, l5: 0.001029, l6: 0.001458
[epoch: 934/1000, batch:   884/ 1052, ite: 245600] train loss: 0.003867, tar: 0.000040 
l0: 0.000008, l1: 0.000006, l2: 0.000046, l3: 0.000166, l4: 0.000571, l5: 0.000849, l6: 0.002173
[epoch: 934/1000, batch:   964/ 1052, ite: 245620] train loss: 0.003867, tar: 0.000040 
l0: 0.000015, l1: 0.000015, l2: 0.000048, l3: 0.000140, l4: 0.000377, l5: 0.001864, l6: 0.003093
[epoch: 934/1000, batch:  1044/ 1052, ite: 245640] train loss: 0.003871, tar: 0.000040 
[Epoch 934/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000056, l1: 0.000055, l2: 0.000095, l3: 0.000240, l4: 0.000509, l5: 0.001066, l6: 0.002554
[epoch: 935/1000, batch:    72/ 1052, ite: 245660] train loss: 0.003871, tar: 0.000041 
l0: 0.000004, l1: 0.000003, l2: 0.000039, l3: 0.000194, l4: 0.000774, l5: 0.001070, l6: 0.002172
[epoch: 935/1000, batch:   152/ 1052, ite: 245680] train loss: 0.003870, tar: 0.000041 
l0: 0.000049, l1: 0.000050, l2: 0.000070, l3: 0.000223, l4: 0.000679, l5: 0.001213, l6: 0.003245
[epoch: 935/1000, batch:   232/ 1052, ite: 245700] train loss: 0.003871, tar: 0.000041 
l0: 0.000066, l1: 0.000067, l2: 0.000079, l3: 0.000150, l4: 0.000714, l5: 0.001125, l6: 0.002612
[epoch: 935/1000, batch:   312/ 1052, ite: 245720] train loss: 0.003873, tar: 0.000041 
l0: 0.000017, l1: 0.000015, l2: 0.000051, l3: 0.000222, l4: 0.000655, l5: 0.001707, l6: 0.002564
[epoch: 935/1000, batch:   392/ 1052, ite: 245740] train loss: 0.003874, tar: 0.000041 
l0: 0.000002, l1: 0.000002, l2: 0.000024, l3: 0.000106, l4: 0.000302, l5: 0.001327, l6: 0.001867
[epoch: 935/1000, batch:   472/ 1052, ite: 245760] train loss: 0.003872, tar: 0.000040 
l0: 0.000006, l1: 0.000006, l2: 0.000017, l3: 0.000064, l4: 0.000278, l5: 0.000795, l6: 0.000921
[epoch: 935/1000, batch:   552/ 1052, ite: 245780] train loss: 0.003874, tar: 0.000040 
l0: 0.000034, l1: 0.000034, l2: 0.000050, l3: 0.000098, l4: 0.000225, l5: 0.000973, l6: 0.001224
[epoch: 935/1000, batch:   632/ 1052, ite: 245800] train loss: 0.003873, tar: 0.000041 
l0: 0.000007, l1: 0.000007, l2: 0.000072, l3: 0.000307, l4: 0.000807, l5: 0.001696, l6: 0.002501
[epoch: 935/1000, batch:   712/ 1052, ite: 245820] train loss: 0.003872, tar: 0.000041 
l0: 0.000020, l1: 0.000020, l2: 0.000043, l3: 0.000095, l4: 0.000295, l5: 0.000818, l6: 0.000876
[epoch: 935/1000, batch:   792/ 1052, ite: 245840] train loss: 0.003872, tar: 0.000041 
l0: 0.000001, l1: 0.000001, l2: 0.000009, l3: 0.000098, l4: 0.000368, l5: 0.000627, l6: 0.001651
[epoch: 935/1000, batch:   872/ 1052, ite: 245860] train loss: 0.003871, tar: 0.000040 
l0: 0.000071, l1: 0.000071, l2: 0.000080, l3: 0.000261, l4: 0.000456, l5: 0.001141, l6: 0.003446
[epoch: 935/1000, batch:   952/ 1052, ite: 245880] train loss: 0.003873, tar: 0.000041 
l0: 0.000009, l1: 0.000010, l2: 0.000029, l3: 0.000200, l4: 0.000435, l5: 0.000973, l6: 0.002779
[epoch: 935/1000, batch:  1032/ 1052, ite: 245900] train loss: 0.003872, tar: 0.000040 
[Epoch 935/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000159, l1: 0.000159, l2: 0.000230, l3: 0.000437, l4: 0.001026, l5: 0.002260, l6: 0.004882
[epoch: 936/1000, batch:    60/ 1052, ite: 245920] train loss: 0.003872, tar: 0.000041 
l0: 0.000019, l1: 0.000022, l2: 0.000040, l3: 0.000134, l4: 0.000338, l5: 0.000876, l6: 0.001946
[epoch: 936/1000, batch:   140/ 1052, ite: 245940] train loss: 0.003872, tar: 0.000041 
l0: 0.000018, l1: 0.000018, l2: 0.000069, l3: 0.000127, l4: 0.000325, l5: 0.000864, l6: 0.001852
[epoch: 936/1000, batch:   220/ 1052, ite: 245960] train loss: 0.003872, tar: 0.000041 
l0: 0.000116, l1: 0.000120, l2: 0.000114, l3: 0.000161, l4: 0.000456, l5: 0.000817, l6: 0.000905
[epoch: 936/1000, batch:   300/ 1052, ite: 245980] train loss: 0.003868, tar: 0.000040 
l0: 0.000003, l1: 0.000003, l2: 0.000016, l3: 0.000062, l4: 0.000374, l5: 0.001028, l6: 0.001191
[epoch: 936/1000, batch:   380/ 1052, ite: 246000] train loss: 0.003867, tar: 0.000040 
l0: 0.000025, l1: 0.000026, l2: 0.000043, l3: 0.000178, l4: 0.000420, l5: 0.001584, l6: 0.001622
[epoch: 936/1000, batch:   460/ 1052, ite: 246020] train loss: 0.003867, tar: 0.000040 
l0: 0.000090, l1: 0.000089, l2: 0.000118, l3: 0.000171, l4: 0.000460, l5: 0.001568, l6: 0.003768
[epoch: 936/1000, batch:   540/ 1052, ite: 246040] train loss: 0.003868, tar: 0.000040 
l0: 0.000104, l1: 0.000104, l2: 0.000148, l3: 0.000288, l4: 0.000582, l5: 0.001366, l6: 0.003070
[epoch: 936/1000, batch:   620/ 1052, ite: 246060] train loss: 0.003871, tar: 0.000040 
l0: 0.000012, l1: 0.000011, l2: 0.000041, l3: 0.000071, l4: 0.000163, l5: 0.000741, l6: 0.001054
[epoch: 936/1000, batch:   700/ 1052, ite: 246080] train loss: 0.003871, tar: 0.000040 
l0: 0.000020, l1: 0.000020, l2: 0.000049, l3: 0.000079, l4: 0.000145, l5: 0.000845, l6: 0.001015
[epoch: 936/1000, batch:   780/ 1052, ite: 246100] train loss: 0.003873, tar: 0.000040 
l0: 0.000011, l1: 0.000010, l2: 0.000028, l3: 0.000113, l4: 0.000237, l5: 0.000652, l6: 0.002184
[epoch: 936/1000, batch:   860/ 1052, ite: 246120] train loss: 0.003872, tar: 0.000040 
l0: 0.000009, l1: 0.000009, l2: 0.000016, l3: 0.000079, l4: 0.000288, l5: 0.000609, l6: 0.001169
[epoch: 936/1000, batch:   940/ 1052, ite: 246140] train loss: 0.003870, tar: 0.000040 
l0: 0.000025, l1: 0.000024, l2: 0.000041, l3: 0.000122, l4: 0.000504, l5: 0.000925, l6: 0.001676
[epoch: 936/1000, batch:  1020/ 1052, ite: 246160] train loss: 0.003871, tar: 0.000040 
[Epoch 936/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000099, l1: 0.000103, l2: 0.000123, l3: 0.000301, l4: 0.000643, l5: 0.003302, l6: 0.004433
[epoch: 937/1000, batch:    48/ 1052, ite: 246180] train loss: 0.003872, tar: 0.000040 
l0: 0.000127, l1: 0.000127, l2: 0.000155, l3: 0.000249, l4: 0.000589, l5: 0.002030, l6: 0.003145
[epoch: 937/1000, batch:   128/ 1052, ite: 246200] train loss: 0.003874, tar: 0.000040 
l0: 0.000022, l1: 0.000022, l2: 0.000083, l3: 0.000201, l4: 0.000341, l5: 0.001039, l6: 0.001541
[epoch: 937/1000, batch:   208/ 1052, ite: 246220] train loss: 0.003876, tar: 0.000040 
l0: 0.000080, l1: 0.000080, l2: 0.000109, l3: 0.000328, l4: 0.000419, l5: 0.001440, l6: 0.002275
[epoch: 937/1000, batch:   288/ 1052, ite: 246240] train loss: 0.003875, tar: 0.000040 
l0: 0.000067, l1: 0.000068, l2: 0.000075, l3: 0.000127, l4: 0.000292, l5: 0.000861, l6: 0.001579
[epoch: 937/1000, batch:   368/ 1052, ite: 246260] train loss: 0.003876, tar: 0.000040 
l0: 0.000013, l1: 0.000014, l2: 0.000015, l3: 0.000079, l4: 0.000213, l5: 0.000399, l6: 0.000585
[epoch: 937/1000, batch:   448/ 1052, ite: 246280] train loss: 0.003876, tar: 0.000040 
l0: 0.000009, l1: 0.000009, l2: 0.000032, l3: 0.000136, l4: 0.000249, l5: 0.000430, l6: 0.001284
[epoch: 937/1000, batch:   528/ 1052, ite: 246300] train loss: 0.003875, tar: 0.000040 
l0: 0.000009, l1: 0.000011, l2: 0.000028, l3: 0.000134, l4: 0.000222, l5: 0.000849, l6: 0.001591
[epoch: 937/1000, batch:   608/ 1052, ite: 246320] train loss: 0.003875, tar: 0.000040 
l0: 0.000007, l1: 0.000009, l2: 0.000010, l3: 0.000089, l4: 0.000350, l5: 0.000767, l6: 0.002023
[epoch: 937/1000, batch:   688/ 1052, ite: 246340] train loss: 0.003875, tar: 0.000040 
l0: 0.000012, l1: 0.000012, l2: 0.000040, l3: 0.000091, l4: 0.000366, l5: 0.001180, l6: 0.001867
[epoch: 937/1000, batch:   768/ 1052, ite: 246360] train loss: 0.003874, tar: 0.000040 
l0: 0.000165, l1: 0.000166, l2: 0.000189, l3: 0.000382, l4: 0.000713, l5: 0.002124, l6: 0.003182
[epoch: 937/1000, batch:   848/ 1052, ite: 246380] train loss: 0.003873, tar: 0.000040 
l0: 0.000002, l1: 0.000002, l2: 0.000004, l3: 0.000043, l4: 0.000169, l5: 0.000574, l6: 0.000920
[epoch: 937/1000, batch:   928/ 1052, ite: 246400] train loss: 0.003873, tar: 0.000040 
l0: 0.000008, l1: 0.000009, l2: 0.000047, l3: 0.000276, l4: 0.000566, l5: 0.001431, l6: 0.002678
[epoch: 937/1000, batch:  1008/ 1052, ite: 246420] train loss: 0.003871, tar: 0.000040 
[Epoch 937/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000048, l1: 0.000049, l2: 0.000061, l3: 0.000127, l4: 0.000610, l5: 0.001274, l6: 0.002098
[epoch: 938/1000, batch:    36/ 1052, ite: 246440] train loss: 0.003869, tar: 0.000040 
l0: 0.000016, l1: 0.000017, l2: 0.000027, l3: 0.000067, l4: 0.000376, l5: 0.000675, l6: 0.001797
[epoch: 938/1000, batch:   116/ 1052, ite: 246460] train loss: 0.003871, tar: 0.000040 
l0: 0.000011, l1: 0.000011, l2: 0.000027, l3: 0.000066, l4: 0.000275, l5: 0.000679, l6: 0.001500
[epoch: 938/1000, batch:   196/ 1052, ite: 246480] train loss: 0.003871, tar: 0.000040 
l0: 0.000010, l1: 0.000009, l2: 0.000042, l3: 0.000255, l4: 0.000664, l5: 0.001923, l6: 0.002764
[epoch: 938/1000, batch:   276/ 1052, ite: 246500] train loss: 0.003871, tar: 0.000040 
l0: 0.000019, l1: 0.000019, l2: 0.000026, l3: 0.000056, l4: 0.000228, l5: 0.000491, l6: 0.001419
[epoch: 938/1000, batch:   356/ 1052, ite: 246520] train loss: 0.003870, tar: 0.000040 
l0: 0.000114, l1: 0.000119, l2: 0.000126, l3: 0.000196, l4: 0.000516, l5: 0.000974, l6: 0.002617
[epoch: 938/1000, batch:   436/ 1052, ite: 246540] train loss: 0.003869, tar: 0.000040 
l0: 0.000069, l1: 0.000065, l2: 0.000136, l3: 0.000196, l4: 0.000348, l5: 0.000914, l6: 0.000985
[epoch: 938/1000, batch:   516/ 1052, ite: 246560] train loss: 0.003869, tar: 0.000040 
l0: 0.000064, l1: 0.000060, l2: 0.000109, l3: 0.000205, l4: 0.000467, l5: 0.001334, l6: 0.001352
[epoch: 938/1000, batch:   596/ 1052, ite: 246580] train loss: 0.003870, tar: 0.000040 
l0: 0.000034, l1: 0.000037, l2: 0.000058, l3: 0.000260, l4: 0.000454, l5: 0.001049, l6: 0.002251
[epoch: 938/1000, batch:   676/ 1052, ite: 246600] train loss: 0.003869, tar: 0.000040 
l0: 0.000003, l1: 0.000003, l2: 0.000016, l3: 0.000083, l4: 0.000240, l5: 0.000512, l6: 0.000861
[epoch: 938/1000, batch:   756/ 1052, ite: 246620] train loss: 0.003867, tar: 0.000040 
l0: 0.000007, l1: 0.000011, l2: 0.000016, l3: 0.000110, l4: 0.000265, l5: 0.001201, l6: 0.002098
[epoch: 938/1000, batch:   836/ 1052, ite: 246640] train loss: 0.003867, tar: 0.000040 
l0: 0.000012, l1: 0.000012, l2: 0.000054, l3: 0.000112, l4: 0.000470, l5: 0.000898, l6: 0.001645
[epoch: 938/1000, batch:   916/ 1052, ite: 246660] train loss: 0.003866, tar: 0.000040 
l0: 0.000007, l1: 0.000007, l2: 0.000021, l3: 0.000087, l4: 0.000358, l5: 0.000926, l6: 0.001494
[epoch: 938/1000, batch:   996/ 1052, ite: 246680] train loss: 0.003867, tar: 0.000040 
[Epoch 938/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000057, l1: 0.000057, l2: 0.000083, l3: 0.000157, l4: 0.000368, l5: 0.001076, l6: 0.001072
[epoch: 939/1000, batch:    24/ 1052, ite: 246700] train loss: 0.003868, tar: 0.000040 
l0: 0.000024, l1: 0.000024, l2: 0.000044, l3: 0.000131, l4: 0.000301, l5: 0.000838, l6: 0.001257
[epoch: 939/1000, batch:   104/ 1052, ite: 246720] train loss: 0.003865, tar: 0.000040 
l0: 0.000039, l1: 0.000039, l2: 0.000063, l3: 0.000210, l4: 0.000466, l5: 0.001184, l6: 0.002075
[epoch: 939/1000, batch:   184/ 1052, ite: 246740] train loss: 0.003866, tar: 0.000040 
l0: 0.000142, l1: 0.000141, l2: 0.000198, l3: 0.000410, l4: 0.000674, l5: 0.001605, l6: 0.003984
[epoch: 939/1000, batch:   264/ 1052, ite: 246760] train loss: 0.003865, tar: 0.000040 
l0: 0.000012, l1: 0.000012, l2: 0.000043, l3: 0.000144, l4: 0.000465, l5: 0.001107, l6: 0.001660
[epoch: 939/1000, batch:   344/ 1052, ite: 246780] train loss: 0.003867, tar: 0.000040 
l0: 0.000015, l1: 0.000015, l2: 0.000048, l3: 0.000191, l4: 0.000509, l5: 0.001015, l6: 0.002300
[epoch: 939/1000, batch:   424/ 1052, ite: 246800] train loss: 0.003867, tar: 0.000040 
l0: 0.000020, l1: 0.000019, l2: 0.000045, l3: 0.000154, l4: 0.000412, l5: 0.001827, l6: 0.003168
[epoch: 939/1000, batch:   504/ 1052, ite: 246820] train loss: 0.003869, tar: 0.000040 
l0: 0.000072, l1: 0.000074, l2: 0.000104, l3: 0.000290, l4: 0.000651, l5: 0.001688, l6: 0.002522
[epoch: 939/1000, batch:   584/ 1052, ite: 246840] train loss: 0.003867, tar: 0.000040 
l0: 0.000020, l1: 0.000022, l2: 0.000026, l3: 0.000140, l4: 0.000535, l5: 0.000322, l6: 0.001144
[epoch: 939/1000, batch:   664/ 1052, ite: 246860] train loss: 0.003866, tar: 0.000040 
l0: 0.000063, l1: 0.000063, l2: 0.000078, l3: 0.000143, l4: 0.000340, l5: 0.000909, l6: 0.001198
[epoch: 939/1000, batch:   744/ 1052, ite: 246880] train loss: 0.003867, tar: 0.000040 
l0: 0.000016, l1: 0.000015, l2: 0.000028, l3: 0.000191, l4: 0.000362, l5: 0.001193, l6: 0.002111
[epoch: 939/1000, batch:   824/ 1052, ite: 246900] train loss: 0.003868, tar: 0.000040 
l0: 0.000056, l1: 0.000055, l2: 0.000106, l3: 0.000300, l4: 0.000763, l5: 0.002054, l6: 0.003075
[epoch: 939/1000, batch:   904/ 1052, ite: 246920] train loss: 0.003868, tar: 0.000040 
l0: 0.000008, l1: 0.000008, l2: 0.000050, l3: 0.000184, l4: 0.000375, l5: 0.001013, l6: 0.001780
[epoch: 939/1000, batch:   984/ 1052, ite: 246940] train loss: 0.003869, tar: 0.000040 
[Epoch 939/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000070, l1: 0.000069, l2: 0.000100, l3: 0.000158, l4: 0.000332, l5: 0.000920, l6: 0.001143
[epoch: 940/1000, batch:    12/ 1052, ite: 246960] train loss: 0.003867, tar: 0.000040 
l0: 0.000025, l1: 0.000025, l2: 0.000051, l3: 0.000129, l4: 0.000344, l5: 0.000823, l6: 0.000967
[epoch: 940/1000, batch:    92/ 1052, ite: 246980] train loss: 0.003866, tar: 0.000040 
l0: 0.000009, l1: 0.000009, l2: 0.000025, l3: 0.000323, l4: 0.000757, l5: 0.001375, l6: 0.002488
[epoch: 940/1000, batch:   172/ 1052, ite: 247000] train loss: 0.003867, tar: 0.000040 
l0: 0.000021, l1: 0.000020, l2: 0.000094, l3: 0.000321, l4: 0.000521, l5: 0.001734, l6: 0.004226
[epoch: 940/1000, batch:   252/ 1052, ite: 247020] train loss: 0.003868, tar: 0.000040 
l0: 0.000011, l1: 0.000013, l2: 0.000028, l3: 0.000087, l4: 0.000365, l5: 0.000794, l6: 0.001803
[epoch: 940/1000, batch:   332/ 1052, ite: 247040] train loss: 0.003868, tar: 0.000040 
l0: 0.000018, l1: 0.000019, l2: 0.000043, l3: 0.000132, l4: 0.000314, l5: 0.000977, l6: 0.001652
[epoch: 940/1000, batch:   412/ 1052, ite: 247060] train loss: 0.003867, tar: 0.000040 
l0: 0.000037, l1: 0.000040, l2: 0.000078, l3: 0.000237, l4: 0.000539, l5: 0.001586, l6: 0.003599
[epoch: 940/1000, batch:   492/ 1052, ite: 247080] train loss: 0.003868, tar: 0.000040 
l0: 0.000003, l1: 0.000003, l2: 0.000018, l3: 0.000074, l4: 0.000303, l5: 0.000908, l6: 0.001190
[epoch: 940/1000, batch:   572/ 1052, ite: 247100] train loss: 0.003868, tar: 0.000040 
l0: 0.000019, l1: 0.000020, l2: 0.000062, l3: 0.000398, l4: 0.000729, l5: 0.001733, l6: 0.003608
[epoch: 940/1000, batch:   652/ 1052, ite: 247120] train loss: 0.003869, tar: 0.000040 
l0: 0.000001, l1: 0.000001, l2: 0.000016, l3: 0.000079, l4: 0.000295, l5: 0.000652, l6: 0.001303
[epoch: 940/1000, batch:   732/ 1052, ite: 247140] train loss: 0.003868, tar: 0.000040 
l0: 0.000001, l1: 0.000001, l2: 0.000007, l3: 0.000068, l4: 0.000311, l5: 0.001083, l6: 0.001449
[epoch: 940/1000, batch:   812/ 1052, ite: 247160] train loss: 0.003869, tar: 0.000040 
l0: 0.000002, l1: 0.000001, l2: 0.000018, l3: 0.000163, l4: 0.000430, l5: 0.001008, l6: 0.002047
[epoch: 940/1000, batch:   892/ 1052, ite: 247180] train loss: 0.003868, tar: 0.000040 
l0: 0.000014, l1: 0.000012, l2: 0.000041, l3: 0.000078, l4: 0.000321, l5: 0.000870, l6: 0.001357
[epoch: 940/1000, batch:   972/ 1052, ite: 247200] train loss: 0.003867, tar: 0.000040 
l0: 0.000240, l1: 0.000244, l2: 0.000287, l3: 0.000371, l4: 0.000621, l5: 0.001441, l6: 0.003661
[epoch: 940/1000, batch:  1052/ 1052, ite: 247220] train loss: 0.003868, tar: 0.000040 
[Epoch 940/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000064, l1: 0.000065, l2: 0.000089, l3: 0.000187, l4: 0.000373, l5: 0.001107, l6: 0.002298
[epoch: 941/1000, batch:    80/ 1052, ite: 247240] train loss: 0.003867, tar: 0.000040 
l0: 0.000002, l1: 0.000002, l2: 0.000015, l3: 0.000068, l4: 0.000462, l5: 0.001006, l6: 0.001662
[epoch: 941/1000, batch:   160/ 1052, ite: 247260] train loss: 0.003866, tar: 0.000040 
l0: 0.000050, l1: 0.000050, l2: 0.000057, l3: 0.000169, l4: 0.000543, l5: 0.001240, l6: 0.001808
[epoch: 941/1000, batch:   240/ 1052, ite: 247280] train loss: 0.003864, tar: 0.000040 
l0: 0.000012, l1: 0.000012, l2: 0.000044, l3: 0.000173, l4: 0.000388, l5: 0.001546, l6: 0.001865
[epoch: 941/1000, batch:   320/ 1052, ite: 247300] train loss: 0.003867, tar: 0.000040 
l0: 0.000018, l1: 0.000018, l2: 0.000039, l3: 0.000178, l4: 0.000432, l5: 0.000879, l6: 0.001969
[epoch: 941/1000, batch:   400/ 1052, ite: 247320] train loss: 0.003867, tar: 0.000040 
l0: 0.000014, l1: 0.000013, l2: 0.000061, l3: 0.000231, l4: 0.000489, l5: 0.001405, l6: 0.002642
[epoch: 941/1000, batch:   480/ 1052, ite: 247340] train loss: 0.003867, tar: 0.000040 
l0: 0.000001, l1: 0.000001, l2: 0.000018, l3: 0.000084, l4: 0.000147, l5: 0.000622, l6: 0.000943
[epoch: 941/1000, batch:   560/ 1052, ite: 247360] train loss: 0.003867, tar: 0.000040 
l0: 0.000073, l1: 0.000077, l2: 0.000089, l3: 0.000296, l4: 0.000505, l5: 0.001366, l6: 0.001934
[epoch: 941/1000, batch:   640/ 1052, ite: 247380] train loss: 0.003868, tar: 0.000040 
l0: 0.000035, l1: 0.000034, l2: 0.000058, l3: 0.000123, l4: 0.000167, l5: 0.000601, l6: 0.000812
[epoch: 941/1000, batch:   720/ 1052, ite: 247400] train loss: 0.003867, tar: 0.000040 
l0: 0.000224, l1: 0.000229, l2: 0.000277, l3: 0.000380, l4: 0.001061, l5: 0.002687, l6: 0.005519
[epoch: 941/1000, batch:   800/ 1052, ite: 247420] train loss: 0.003868, tar: 0.000040 
l0: 0.000043, l1: 0.000043, l2: 0.000087, l3: 0.000253, l4: 0.000515, l5: 0.000938, l6: 0.001585
[epoch: 941/1000, batch:   880/ 1052, ite: 247440] train loss: 0.003868, tar: 0.000040 
l0: 0.000049, l1: 0.000045, l2: 0.000092, l3: 0.000302, l4: 0.000523, l5: 0.001135, l6: 0.002153
[epoch: 941/1000, batch:   960/ 1052, ite: 247460] train loss: 0.003868, tar: 0.000040 
l0: 0.000093, l1: 0.000095, l2: 0.000117, l3: 0.000201, l4: 0.000460, l5: 0.000790, l6: 0.001340
[epoch: 941/1000, batch:  1040/ 1052, ite: 247480] train loss: 0.003869, tar: 0.000040 
[Epoch 941/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000004, l1: 0.000004, l2: 0.000038, l3: 0.000255, l4: 0.000772, l5: 0.001552, l6: 0.002768
[epoch: 942/1000, batch:    68/ 1052, ite: 247500] train loss: 0.003868, tar: 0.000040 
l0: 0.000175, l1: 0.000177, l2: 0.000205, l3: 0.000388, l4: 0.000656, l5: 0.001666, l6: 0.002794
[epoch: 942/1000, batch:   148/ 1052, ite: 247520] train loss: 0.003868, tar: 0.000040 
l0: 0.000059, l1: 0.000058, l2: 0.000115, l3: 0.000220, l4: 0.000540, l5: 0.000741, l6: 0.002205
[epoch: 942/1000, batch:   228/ 1052, ite: 247540] train loss: 0.003869, tar: 0.000040 
l0: 0.000106, l1: 0.000107, l2: 0.000135, l3: 0.000287, l4: 0.000947, l5: 0.001721, l6: 0.003054
[epoch: 942/1000, batch:   308/ 1052, ite: 247560] train loss: 0.003869, tar: 0.000040 
l0: 0.000005, l1: 0.000005, l2: 0.000025, l3: 0.000123, l4: 0.000423, l5: 0.001027, l6: 0.001408
[epoch: 942/1000, batch:   388/ 1052, ite: 247580] train loss: 0.003869, tar: 0.000040 
l0: 0.000146, l1: 0.000147, l2: 0.000221, l3: 0.000547, l4: 0.001004, l5: 0.001789, l6: 0.003651
[epoch: 942/1000, batch:   468/ 1052, ite: 247600] train loss: 0.003870, tar: 0.000040 
l0: 0.000130, l1: 0.000130, l2: 0.000177, l3: 0.000358, l4: 0.000795, l5: 0.002103, l6: 0.003223
[epoch: 942/1000, batch:   548/ 1052, ite: 247620] train loss: 0.003872, tar: 0.000040 
l0: 0.000010, l1: 0.000010, l2: 0.000030, l3: 0.000084, l4: 0.000328, l5: 0.001086, l6: 0.001244
[epoch: 942/1000, batch:   628/ 1052, ite: 247640] train loss: 0.003872, tar: 0.000040 
l0: 0.000002, l1: 0.000002, l2: 0.000032, l3: 0.000113, l4: 0.000220, l5: 0.000659, l6: 0.001623
[epoch: 942/1000, batch:   708/ 1052, ite: 247660] train loss: 0.003870, tar: 0.000040 
l0: 0.000059, l1: 0.000060, l2: 0.000076, l3: 0.000166, l4: 0.000347, l5: 0.001188, l6: 0.001728
[epoch: 942/1000, batch:   788/ 1052, ite: 247680] train loss: 0.003870, tar: 0.000040 
l0: 0.000003, l1: 0.000002, l2: 0.000018, l3: 0.000086, l4: 0.000140, l5: 0.000630, l6: 0.001100
[epoch: 942/1000, batch:   868/ 1052, ite: 247700] train loss: 0.003871, tar: 0.000040 
l0: 0.000077, l1: 0.000075, l2: 0.000114, l3: 0.000179, l4: 0.000540, l5: 0.000932, l6: 0.002591
[epoch: 942/1000, batch:   948/ 1052, ite: 247720] train loss: 0.003871, tar: 0.000040 
l0: 0.000009, l1: 0.000009, l2: 0.000040, l3: 0.000342, l4: 0.000928, l5: 0.001742, l6: 0.003618
[epoch: 942/1000, batch:  1028/ 1052, ite: 247740] train loss: 0.003869, tar: 0.000040 
[Epoch 942/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000115, l1: 0.000114, l2: 0.000155, l3: 0.000336, l4: 0.000976, l5: 0.002412, l6: 0.003940
[epoch: 943/1000, batch:    56/ 1052, ite: 247760] train loss: 0.003870, tar: 0.000040 
l0: 0.000006, l1: 0.000006, l2: 0.000022, l3: 0.000067, l4: 0.000329, l5: 0.000877, l6: 0.001345
[epoch: 943/1000, batch:   136/ 1052, ite: 247780] train loss: 0.003871, tar: 0.000040 
l0: 0.000006, l1: 0.000006, l2: 0.000026, l3: 0.000188, l4: 0.000851, l5: 0.001430, l6: 0.002939
[epoch: 943/1000, batch:   216/ 1052, ite: 247800] train loss: 0.003872, tar: 0.000040 
l0: 0.000116, l1: 0.000118, l2: 0.000121, l3: 0.000149, l4: 0.000322, l5: 0.000964, l6: 0.001455
[epoch: 943/1000, batch:   296/ 1052, ite: 247820] train loss: 0.003871, tar: 0.000040 
l0: 0.000012, l1: 0.000011, l2: 0.000062, l3: 0.000260, l4: 0.000500, l5: 0.001043, l6: 0.001429
[epoch: 943/1000, batch:   376/ 1052, ite: 247840] train loss: 0.003870, tar: 0.000040 
l0: 0.000067, l1: 0.000067, l2: 0.000083, l3: 0.000174, l4: 0.000537, l5: 0.001345, l6: 0.001768
[epoch: 943/1000, batch:   456/ 1052, ite: 247860] train loss: 0.003871, tar: 0.000040 
l0: 0.000008, l1: 0.000007, l2: 0.000046, l3: 0.000128, l4: 0.000458, l5: 0.000847, l6: 0.001844
[epoch: 943/1000, batch:   536/ 1052, ite: 247880] train loss: 0.003873, tar: 0.000040 
l0: 0.000030, l1: 0.000029, l2: 0.000068, l3: 0.000179, l4: 0.000411, l5: 0.000889, l6: 0.001662
[epoch: 943/1000, batch:   616/ 1052, ite: 247900] train loss: 0.003871, tar: 0.000040 
l0: 0.000009, l1: 0.000009, l2: 0.000011, l3: 0.000052, l4: 0.000254, l5: 0.000670, l6: 0.000916
[epoch: 943/1000, batch:   696/ 1052, ite: 247920] train loss: 0.003870, tar: 0.000040 
l0: 0.000079, l1: 0.000082, l2: 0.000118, l3: 0.000330, l4: 0.000604, l5: 0.001588, l6: 0.001839
[epoch: 943/1000, batch:   776/ 1052, ite: 247940] train loss: 0.003869, tar: 0.000040 
l0: 0.000003, l1: 0.000002, l2: 0.000009, l3: 0.000083, l4: 0.000255, l5: 0.000583, l6: 0.000992
[epoch: 943/1000, batch:   856/ 1052, ite: 247960] train loss: 0.003870, tar: 0.000040 
l0: 0.000060, l1: 0.000061, l2: 0.000098, l3: 0.000236, l4: 0.000432, l5: 0.001124, l6: 0.001494
[epoch: 943/1000, batch:   936/ 1052, ite: 247980] train loss: 0.003870, tar: 0.000040 
l0: 0.000051, l1: 0.000055, l2: 0.000068, l3: 0.000203, l4: 0.000395, l5: 0.000496, l6: 0.001240
[epoch: 943/1000, batch:  1016/ 1052, ite: 248000] train loss: 0.003869, tar: 0.000040 
[Epoch 943/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000002, l1: 0.000002, l2: 0.000021, l3: 0.000351, l4: 0.001167, l5: 0.002247, l6: 0.004096
[epoch: 944/1000, batch:    44/ 1052, ite: 248020] train loss: 0.003870, tar: 0.000040 
l0: 0.000045, l1: 0.000049, l2: 0.000046, l3: 0.000083, l4: 0.000315, l5: 0.000573, l6: 0.001128
[epoch: 944/1000, batch:   124/ 1052, ite: 248040] train loss: 0.003871, tar: 0.000040 
l0: 0.000024, l1: 0.000023, l2: 0.000028, l3: 0.000078, l4: 0.000265, l5: 0.000592, l6: 0.001181
[epoch: 944/1000, batch:   204/ 1052, ite: 248060] train loss: 0.003871, tar: 0.000040 
l0: 0.000007, l1: 0.000007, l2: 0.000019, l3: 0.000048, l4: 0.000177, l5: 0.000984, l6: 0.001120
[epoch: 944/1000, batch:   284/ 1052, ite: 248080] train loss: 0.003871, tar: 0.000040 
l0: 0.000122, l1: 0.000125, l2: 0.000130, l3: 0.000254, l4: 0.000493, l5: 0.001458, l6: 0.002792
[epoch: 944/1000, batch:   364/ 1052, ite: 248100] train loss: 0.003871, tar: 0.000041 
l0: 0.000030, l1: 0.000031, l2: 0.000045, l3: 0.000078, l4: 0.000254, l5: 0.000693, l6: 0.000860
[epoch: 944/1000, batch:   444/ 1052, ite: 248120] train loss: 0.003871, tar: 0.000041 
l0: 0.000160, l1: 0.000163, l2: 0.000212, l3: 0.000372, l4: 0.000689, l5: 0.001146, l6: 0.002607
[epoch: 944/1000, batch:   524/ 1052, ite: 248140] train loss: 0.003872, tar: 0.000041 
l0: 0.000035, l1: 0.000036, l2: 0.000063, l3: 0.000073, l4: 0.000281, l5: 0.000560, l6: 0.001258
[epoch: 944/1000, batch:   604/ 1052, ite: 248160] train loss: 0.003872, tar: 0.000041 
l0: 0.000064, l1: 0.000063, l2: 0.000101, l3: 0.000165, l4: 0.000404, l5: 0.001151, l6: 0.002367
[epoch: 944/1000, batch:   684/ 1052, ite: 248180] train loss: 0.003872, tar: 0.000041 
l0: 0.000008, l1: 0.000008, l2: 0.000012, l3: 0.000064, l4: 0.000296, l5: 0.000649, l6: 0.001630
[epoch: 944/1000, batch:   764/ 1052, ite: 248200] train loss: 0.003872, tar: 0.000041 
l0: 0.000003, l1: 0.000002, l2: 0.000015, l3: 0.000103, l4: 0.000346, l5: 0.000345, l6: 0.000964
[epoch: 944/1000, batch:   844/ 1052, ite: 248220] train loss: 0.003870, tar: 0.000041 
l0: 0.000035, l1: 0.000035, l2: 0.000082, l3: 0.000323, l4: 0.000738, l5: 0.001346, l6: 0.001733
[epoch: 944/1000, batch:   924/ 1052, ite: 248240] train loss: 0.003871, tar: 0.000041 
l0: 0.000006, l1: 0.000007, l2: 0.000016, l3: 0.000104, l4: 0.000350, l5: 0.000934, l6: 0.001697
[epoch: 944/1000, batch:  1004/ 1052, ite: 248260] train loss: 0.003872, tar: 0.000041 
[Epoch 944/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000041, l1: 0.000043, l2: 0.000104, l3: 0.000368, l4: 0.001020, l5: 0.002021, l6: 0.004901
[epoch: 945/1000, batch:    32/ 1052, ite: 248280] train loss: 0.003873, tar: 0.000041 
l0: 0.000035, l1: 0.000036, l2: 0.000040, l3: 0.000061, l4: 0.000206, l5: 0.001199, l6: 0.001504
[epoch: 945/1000, batch:   112/ 1052, ite: 248300] train loss: 0.003872, tar: 0.000041 
l0: 0.000004, l1: 0.000004, l2: 0.000014, l3: 0.000041, l4: 0.000271, l5: 0.000857, l6: 0.000834
[epoch: 945/1000, batch:   192/ 1052, ite: 248320] train loss: 0.003871, tar: 0.000041 
l0: 0.000100, l1: 0.000101, l2: 0.000150, l3: 0.000233, l4: 0.000729, l5: 0.001240, l6: 0.003001
[epoch: 945/1000, batch:   272/ 1052, ite: 248340] train loss: 0.003874, tar: 0.000041 
l0: 0.000035, l1: 0.000033, l2: 0.000067, l3: 0.000392, l4: 0.000690, l5: 0.001497, l6: 0.002333
[epoch: 945/1000, batch:   352/ 1052, ite: 248360] train loss: 0.003873, tar: 0.000041 
l0: 0.000051, l1: 0.000050, l2: 0.000082, l3: 0.000162, l4: 0.000425, l5: 0.000922, l6: 0.001334
[epoch: 945/1000, batch:   432/ 1052, ite: 248380] train loss: 0.003872, tar: 0.000041 
l0: 0.000029, l1: 0.000030, l2: 0.000034, l3: 0.000129, l4: 0.000511, l5: 0.001197, l6: 0.002342
[epoch: 945/1000, batch:   512/ 1052, ite: 248400] train loss: 0.003871, tar: 0.000041 
l0: 0.000005, l1: 0.000004, l2: 0.000038, l3: 0.000191, l4: 0.000362, l5: 0.001164, l6: 0.002149
[epoch: 945/1000, batch:   592/ 1052, ite: 248420] train loss: 0.003871, tar: 0.000041 
l0: 0.000031, l1: 0.000031, l2: 0.000043, l3: 0.000197, l4: 0.000469, l5: 0.001197, l6: 0.001212
[epoch: 945/1000, batch:   672/ 1052, ite: 248440] train loss: 0.003871, tar: 0.000041 
l0: 0.000018, l1: 0.000017, l2: 0.000083, l3: 0.000332, l4: 0.001010, l5: 0.001476, l6: 0.003176
[epoch: 945/1000, batch:   752/ 1052, ite: 248460] train loss: 0.003872, tar: 0.000041 
l0: 0.000044, l1: 0.000045, l2: 0.000082, l3: 0.000189, l4: 0.000417, l5: 0.000870, l6: 0.001450
[epoch: 945/1000, batch:   832/ 1052, ite: 248480] train loss: 0.003872, tar: 0.000041 
l0: 0.000033, l1: 0.000034, l2: 0.000064, l3: 0.000238, l4: 0.000484, l5: 0.001478, l6: 0.001623
[epoch: 945/1000, batch:   912/ 1052, ite: 248500] train loss: 0.003875, tar: 0.000041 
l0: 0.000000, l1: 0.000000, l2: 0.000003, l3: 0.000024, l4: 0.000273, l5: 0.000713, l6: 0.001301
[epoch: 945/1000, batch:   992/ 1052, ite: 248520] train loss: 0.003874, tar: 0.000041 
[Epoch 945/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000014, l1: 0.000014, l2: 0.000023, l3: 0.000127, l4: 0.000189, l5: 0.000659, l6: 0.001122
[epoch: 946/1000, batch:    20/ 1052, ite: 248540] train loss: 0.003872, tar: 0.000041 
l0: 0.000130, l1: 0.000132, l2: 0.000209, l3: 0.000405, l4: 0.000854, l5: 0.001461, l6: 0.002581
[epoch: 946/1000, batch:   100/ 1052, ite: 248560] train loss: 0.003872, tar: 0.000041 
l0: 0.000003, l1: 0.000002, l2: 0.000024, l3: 0.000232, l4: 0.000560, l5: 0.001342, l6: 0.002194
[epoch: 946/1000, batch:   180/ 1052, ite: 248580] train loss: 0.003872, tar: 0.000041 
l0: 0.000095, l1: 0.000098, l2: 0.000114, l3: 0.000154, l4: 0.000490, l5: 0.001267, l6: 0.002559
[epoch: 946/1000, batch:   260/ 1052, ite: 248600] train loss: 0.003872, tar: 0.000041 
l0: 0.000010, l1: 0.000010, l2: 0.000044, l3: 0.000260, l4: 0.000711, l5: 0.001543, l6: 0.002722
[epoch: 946/1000, batch:   340/ 1052, ite: 248620] train loss: 0.003874, tar: 0.000041 
l0: 0.000009, l1: 0.000009, l2: 0.000038, l3: 0.000241, l4: 0.000760, l5: 0.001267, l6: 0.002203
[epoch: 946/1000, batch:   420/ 1052, ite: 248640] train loss: 0.003873, tar: 0.000041 
l0: 0.000030, l1: 0.000041, l2: 0.000043, l3: 0.000131, l4: 0.000348, l5: 0.000641, l6: 0.001495
[epoch: 946/1000, batch:   500/ 1052, ite: 248660] train loss: 0.003873, tar: 0.000041 
l0: 0.000003, l1: 0.000003, l2: 0.000011, l3: 0.000059, l4: 0.000264, l5: 0.000575, l6: 0.001143
[epoch: 946/1000, batch:   580/ 1052, ite: 248680] train loss: 0.003873, tar: 0.000041 
l0: 0.000032, l1: 0.000031, l2: 0.000066, l3: 0.000173, l4: 0.000476, l5: 0.000931, l6: 0.002172
[epoch: 946/1000, batch:   660/ 1052, ite: 248700] train loss: 0.003873, tar: 0.000041 
l0: 0.000038, l1: 0.000041, l2: 0.000043, l3: 0.000127, l4: 0.000535, l5: 0.000980, l6: 0.001437
[epoch: 946/1000, batch:   740/ 1052, ite: 248720] train loss: 0.003874, tar: 0.000041 
l0: 0.000135, l1: 0.000135, l2: 0.000173, l3: 0.000280, l4: 0.000627, l5: 0.001098, l6: 0.002209
[epoch: 946/1000, batch:   820/ 1052, ite: 248740] train loss: 0.003874, tar: 0.000041 
l0: 0.000042, l1: 0.000042, l2: 0.000054, l3: 0.000126, l4: 0.000357, l5: 0.000964, l6: 0.000791
[epoch: 946/1000, batch:   900/ 1052, ite: 248760] train loss: 0.003874, tar: 0.000041 
l0: 0.000013, l1: 0.000012, l2: 0.000048, l3: 0.000300, l4: 0.000683, l5: 0.001011, l6: 0.002357
[epoch: 946/1000, batch:   980/ 1052, ite: 248780] train loss: 0.003874, tar: 0.000041 
[Epoch 946/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000006, l1: 0.000007, l2: 0.000026, l3: 0.000158, l4: 0.000546, l5: 0.001043, l6: 0.001691
[epoch: 947/1000, batch:     8/ 1052, ite: 248800] train loss: 0.003875, tar: 0.000041 
l0: 0.000018, l1: 0.000021, l2: 0.000042, l3: 0.000166, l4: 0.000447, l5: 0.001037, l6: 0.001226
[epoch: 947/1000, batch:    88/ 1052, ite: 248820] train loss: 0.003873, tar: 0.000041 
l0: 0.000039, l1: 0.000041, l2: 0.000048, l3: 0.000082, l4: 0.000333, l5: 0.000873, l6: 0.001497
[epoch: 947/1000, batch:   168/ 1052, ite: 248840] train loss: 0.003874, tar: 0.000041 
l0: 0.000045, l1: 0.000046, l2: 0.000066, l3: 0.000185, l4: 0.000524, l5: 0.000954, l6: 0.001944
[epoch: 947/1000, batch:   248/ 1052, ite: 248860] train loss: 0.003873, tar: 0.000041 
l0: 0.000034, l1: 0.000034, l2: 0.000050, l3: 0.000196, l4: 0.000301, l5: 0.000834, l6: 0.001390
[epoch: 947/1000, batch:   328/ 1052, ite: 248880] train loss: 0.003873, tar: 0.000041 
l0: 0.000007, l1: 0.000007, l2: 0.000045, l3: 0.000173, l4: 0.000543, l5: 0.001476, l6: 0.001811
[epoch: 947/1000, batch:   408/ 1052, ite: 248900] train loss: 0.003872, tar: 0.000041 
l0: 0.000022, l1: 0.000020, l2: 0.000035, l3: 0.000123, l4: 0.000333, l5: 0.000697, l6: 0.000874
[epoch: 947/1000, batch:   488/ 1052, ite: 248920] train loss: 0.003873, tar: 0.000041 
l0: 0.000027, l1: 0.000027, l2: 0.000058, l3: 0.000158, l4: 0.000446, l5: 0.001302, l6: 0.002463
[epoch: 947/1000, batch:   568/ 1052, ite: 248940] train loss: 0.003874, tar: 0.000041 
l0: 0.000046, l1: 0.000046, l2: 0.000055, l3: 0.000118, l4: 0.000270, l5: 0.000779, l6: 0.001396
[epoch: 947/1000, batch:   648/ 1052, ite: 248960] train loss: 0.003874, tar: 0.000041 
l0: 0.000005, l1: 0.000004, l2: 0.000032, l3: 0.000224, l4: 0.000387, l5: 0.000918, l6: 0.001110
[epoch: 947/1000, batch:   728/ 1052, ite: 248980] train loss: 0.003874, tar: 0.000041 
l0: 0.000001, l1: 0.000001, l2: 0.000006, l3: 0.000065, l4: 0.000352, l5: 0.000860, l6: 0.001070
[epoch: 947/1000, batch:   808/ 1052, ite: 249000] train loss: 0.003874, tar: 0.000041 
l0: 0.000006, l1: 0.000005, l2: 0.000033, l3: 0.000153, l4: 0.000427, l5: 0.001403, l6: 0.003178
[epoch: 947/1000, batch:   888/ 1052, ite: 249020] train loss: 0.003874, tar: 0.000041 
l0: 0.000078, l1: 0.000081, l2: 0.000088, l3: 0.000201, l4: 0.000414, l5: 0.000705, l6: 0.001346
[epoch: 947/1000, batch:   968/ 1052, ite: 249040] train loss: 0.003874, tar: 0.000041 
l0: 0.000026, l1: 0.000024, l2: 0.000038, l3: 0.000168, l4: 0.000389, l5: 0.000674, l6: 0.000971
[epoch: 947/1000, batch:  1048/ 1052, ite: 249060] train loss: 0.003874, tar: 0.000041 
[Epoch 947/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000020, l1: 0.000020, l2: 0.000039, l3: 0.000107, l4: 0.000277, l5: 0.000691, l6: 0.001193
[epoch: 948/1000, batch:    76/ 1052, ite: 249080] train loss: 0.003875, tar: 0.000041 
l0: 0.000008, l1: 0.000008, l2: 0.000015, l3: 0.000092, l4: 0.000298, l5: 0.001071, l6: 0.002014
[epoch: 948/1000, batch:   156/ 1052, ite: 249100] train loss: 0.003873, tar: 0.000041 
l0: 0.000008, l1: 0.000008, l2: 0.000024, l3: 0.000127, l4: 0.000509, l5: 0.001194, l6: 0.002056
[epoch: 948/1000, batch:   236/ 1052, ite: 249120] train loss: 0.003872, tar: 0.000041 
l0: 0.000066, l1: 0.000067, l2: 0.000077, l3: 0.000197, l4: 0.000555, l5: 0.001427, l6: 0.002573
[epoch: 948/1000, batch:   316/ 1052, ite: 249140] train loss: 0.003872, tar: 0.000041 
l0: 0.000025, l1: 0.000027, l2: 0.000045, l3: 0.000176, l4: 0.000617, l5: 0.001150, l6: 0.002099
[epoch: 948/1000, batch:   396/ 1052, ite: 249160] train loss: 0.003872, tar: 0.000041 
l0: 0.000018, l1: 0.000017, l2: 0.000046, l3: 0.000136, l4: 0.000446, l5: 0.000889, l6: 0.001807
[epoch: 948/1000, batch:   476/ 1052, ite: 249180] train loss: 0.003873, tar: 0.000041 
l0: 0.000021, l1: 0.000022, l2: 0.000026, l3: 0.000044, l4: 0.000264, l5: 0.000722, l6: 0.000406
[epoch: 948/1000, batch:   556/ 1052, ite: 249200] train loss: 0.003873, tar: 0.000041 
l0: 0.000006, l1: 0.000006, l2: 0.000035, l3: 0.000156, l4: 0.000338, l5: 0.001103, l6: 0.001638
[epoch: 948/1000, batch:   636/ 1052, ite: 249220] train loss: 0.003873, tar: 0.000041 
l0: 0.000003, l1: 0.000003, l2: 0.000010, l3: 0.000101, l4: 0.000339, l5: 0.000821, l6: 0.000997
[epoch: 948/1000, batch:   716/ 1052, ite: 249240] train loss: 0.003873, tar: 0.000041 
l0: 0.000165, l1: 0.000167, l2: 0.000197, l3: 0.000260, l4: 0.000593, l5: 0.000987, l6: 0.003542
[epoch: 948/1000, batch:   796/ 1052, ite: 249260] train loss: 0.003873, tar: 0.000041 
l0: 0.000029, l1: 0.000029, l2: 0.000048, l3: 0.000110, l4: 0.000260, l5: 0.000347, l6: 0.000912
[epoch: 948/1000, batch:   876/ 1052, ite: 249280] train loss: 0.003874, tar: 0.000041 
l0: 0.000002, l1: 0.000002, l2: 0.000010, l3: 0.000052, l4: 0.000299, l5: 0.000570, l6: 0.001769
[epoch: 948/1000, batch:   956/ 1052, ite: 249300] train loss: 0.003876, tar: 0.000041 
l0: 0.000072, l1: 0.000074, l2: 0.000103, l3: 0.000259, l4: 0.000749, l5: 0.001301, l6: 0.001927
[epoch: 948/1000, batch:  1036/ 1052, ite: 249320] train loss: 0.003875, tar: 0.000041 
[Epoch 948/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000050, l1: 0.000048, l2: 0.000094, l3: 0.000174, l4: 0.000278, l5: 0.000757, l6: 0.001704
[epoch: 949/1000, batch:    64/ 1052, ite: 249340] train loss: 0.003875, tar: 0.000041 
l0: 0.000035, l1: 0.000035, l2: 0.000036, l3: 0.000220, l4: 0.000465, l5: 0.000727, l6: 0.001264
[epoch: 949/1000, batch:   144/ 1052, ite: 249360] train loss: 0.003875, tar: 0.000041 
l0: 0.000317, l1: 0.000317, l2: 0.000446, l3: 0.000530, l4: 0.001039, l5: 0.002447, l6: 0.004848
[epoch: 949/1000, batch:   224/ 1052, ite: 249380] train loss: 0.003880, tar: 0.000041 
l0: 0.000172, l1: 0.000186, l2: 0.000329, l3: 0.000389, l4: 0.000871, l5: 0.001473, l6: 0.004713
[epoch: 949/1000, batch:   304/ 1052, ite: 249400] train loss: 0.003886, tar: 0.000043 
l0: 0.000107, l1: 0.000110, l2: 0.000151, l3: 0.000189, l4: 0.000563, l5: 0.001357, l6: 0.001555
[epoch: 949/1000, batch:   384/ 1052, ite: 249420] train loss: 0.003889, tar: 0.000043 
l0: 0.000085, l1: 0.000082, l2: 0.000124, l3: 0.000209, l4: 0.000360, l5: 0.000904, l6: 0.001200
[epoch: 949/1000, batch:   464/ 1052, ite: 249440] train loss: 0.003890, tar: 0.000044 
l0: 0.000170, l1: 0.000186, l2: 0.000253, l3: 0.000308, l4: 0.000533, l5: 0.001983, l6: 0.003788
[epoch: 949/1000, batch:   544/ 1052, ite: 249460] train loss: 0.003890, tar: 0.000044 
l0: 0.000129, l1: 0.000148, l2: 0.000151, l3: 0.000197, l4: 0.000393, l5: 0.001265, l6: 0.001861
[epoch: 949/1000, batch:   624/ 1052, ite: 249480] train loss: 0.003891, tar: 0.000044 
l0: 0.000057, l1: 0.000056, l2: 0.000102, l3: 0.000200, l4: 0.000558, l5: 0.001192, l6: 0.002127
[epoch: 949/1000, batch:   704/ 1052, ite: 249500] train loss: 0.003891, tar: 0.000044 
l0: 0.000089, l1: 0.000088, l2: 0.000091, l3: 0.000161, l4: 0.000511, l5: 0.001216, l6: 0.002196
[epoch: 949/1000, batch:   784/ 1052, ite: 249520] train loss: 0.003891, tar: 0.000044 
l0: 0.000038, l1: 0.000039, l2: 0.000068, l3: 0.000342, l4: 0.000777, l5: 0.001755, l6: 0.002208
[epoch: 949/1000, batch:   864/ 1052, ite: 249540] train loss: 0.003892, tar: 0.000044 
l0: 0.000165, l1: 0.000168, l2: 0.000210, l3: 0.000251, l4: 0.000491, l5: 0.001336, l6: 0.001407
[epoch: 949/1000, batch:   944/ 1052, ite: 249560] train loss: 0.003892, tar: 0.000044 
l0: 0.000072, l1: 0.000075, l2: 0.000132, l3: 0.000167, l4: 0.000253, l5: 0.000345, l6: 0.000889
[epoch: 949/1000, batch:  1024/ 1052, ite: 249580] train loss: 0.003894, tar: 0.000045 
[Epoch 949/1000] Test loss: 0.012, Test target loss: 0.002
l0: 0.000087, l1: 0.000085, l2: 0.000168, l3: 0.000237, l4: 0.000561, l5: 0.001289, l6: 0.002124
[epoch: 950/1000, batch:    52/ 1052, ite: 249600] train loss: 0.003895, tar: 0.000045 
l0: 0.000116, l1: 0.000113, l2: 0.000152, l3: 0.000275, l4: 0.000541, l5: 0.001301, l6: 0.002394
[epoch: 950/1000, batch:   132/ 1052, ite: 249620] train loss: 0.003896, tar: 0.000045 
l0: 0.000008, l1: 0.000007, l2: 0.000031, l3: 0.000222, l4: 0.000349, l5: 0.000769, l6: 0.001269
[epoch: 950/1000, batch:   212/ 1052, ite: 249640] train loss: 0.003897, tar: 0.000045 
l0: 0.000114, l1: 0.000114, l2: 0.000137, l3: 0.000339, l4: 0.000636, l5: 0.000969, l6: 0.001597
[epoch: 950/1000, batch:   292/ 1052, ite: 249660] train loss: 0.003898, tar: 0.000045 
l0: 0.000132, l1: 0.000138, l2: 0.000190, l3: 0.000482, l4: 0.001029, l5: 0.002178, l6: 0.005045
[epoch: 950/1000, batch:   372/ 1052, ite: 249680] train loss: 0.003899, tar: 0.000045 
l0: 0.000049, l1: 0.000050, l2: 0.000082, l3: 0.000165, l4: 0.000284, l5: 0.001147, l6: 0.001652
[epoch: 950/1000, batch:   452/ 1052, ite: 249700] train loss: 0.003899, tar: 0.000045 
l0: 0.000183, l1: 0.000190, l2: 0.000221, l3: 0.000348, l4: 0.000668, l5: 0.001414, l6: 0.003490
[epoch: 950/1000, batch:   532/ 1052, ite: 249720] train loss: 0.003899, tar: 0.000045 
l0: 0.000096, l1: 0.000099, l2: 0.000151, l3: 0.000250, l4: 0.000737, l5: 0.001413, l6: 0.002098
[epoch: 950/1000, batch:   612/ 1052, ite: 249740] train loss: 0.003898, tar: 0.000045 
l0: 0.000038, l1: 0.000039, l2: 0.000075, l3: 0.000167, l4: 0.000490, l5: 0.000793, l6: 0.001573
[epoch: 950/1000, batch:   692/ 1052, ite: 249760] train loss: 0.003899, tar: 0.000045 
l0: 0.000073, l1: 0.000073, l2: 0.000172, l3: 0.000335, l4: 0.000748, l5: 0.001452, l6: 0.003379
[epoch: 950/1000, batch:   772/ 1052, ite: 249780] train loss: 0.003900, tar: 0.000045 
l0: 0.000009, l1: 0.000008, l2: 0.000019, l3: 0.000039, l4: 0.000268, l5: 0.000433, l6: 0.001022
[epoch: 950/1000, batch:   852/ 1052, ite: 249800] train loss: 0.003902, tar: 0.000045 
l0: 0.000071, l1: 0.000077, l2: 0.000115, l3: 0.000225, l4: 0.000356, l5: 0.000937, l6: 0.001989
[epoch: 950/1000, batch:   932/ 1052, ite: 249820] train loss: 0.003902, tar: 0.000045 
l0: 0.000092, l1: 0.000082, l2: 0.000135, l3: 0.000226, l4: 0.000699, l5: 0.001059, l6: 0.001588
[epoch: 950/1000, batch:  1012/ 1052, ite: 249840] train loss: 0.003902, tar: 0.000046 
[Epoch 950/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000091, l1: 0.000095, l2: 0.000120, l3: 0.000208, l4: 0.000476, l5: 0.001257, l6: 0.002794
[epoch: 951/1000, batch:    40/ 1052, ite: 249860] train loss: 0.003904, tar: 0.000046 
l0: 0.000152, l1: 0.000156, l2: 0.000194, l3: 0.000257, l4: 0.000579, l5: 0.001673, l6: 0.002487
[epoch: 951/1000, batch:   120/ 1052, ite: 249880] train loss: 0.003905, tar: 0.000046 
l0: 0.000079, l1: 0.000078, l2: 0.000122, l3: 0.000208, l4: 0.000581, l5: 0.001697, l6: 0.002975
[epoch: 951/1000, batch:   200/ 1052, ite: 249900] train loss: 0.003905, tar: 0.000046 
l0: 0.000053, l1: 0.000055, l2: 0.000084, l3: 0.000167, l4: 0.000405, l5: 0.001025, l6: 0.002488
[epoch: 951/1000, batch:   280/ 1052, ite: 249920] train loss: 0.003904, tar: 0.000046 
l0: 0.000021, l1: 0.000020, l2: 0.000036, l3: 0.000108, l4: 0.000313, l5: 0.000638, l6: 0.001141
[epoch: 951/1000, batch:   360/ 1052, ite: 249940] train loss: 0.003904, tar: 0.000046 
l0: 0.000151, l1: 0.000147, l2: 0.000201, l3: 0.000313, l4: 0.000591, l5: 0.001075, l6: 0.002513
[epoch: 951/1000, batch:   440/ 1052, ite: 249960] train loss: 0.003903, tar: 0.000046 
l0: 0.000009, l1: 0.000009, l2: 0.000028, l3: 0.000111, l4: 0.000206, l5: 0.000497, l6: 0.001548
[epoch: 951/1000, batch:   520/ 1052, ite: 249980] train loss: 0.003902, tar: 0.000046 
l0: 0.000127, l1: 0.000132, l2: 0.000154, l3: 0.000195, l4: 0.000402, l5: 0.001500, l6: 0.001967
[epoch: 951/1000, batch:   600/ 1052, ite: 250000] train loss: 0.003903, tar: 0.000046 
l0: 0.000067, l1: 0.000069, l2: 0.000105, l3: 0.000340, l4: 0.000841, l5: 0.001173, l6: 0.003415
[epoch: 951/1000, batch:   680/ 1052, ite: 250020] train loss: 0.003902, tar: 0.000046 
l0: 0.000094, l1: 0.000096, l2: 0.000116, l3: 0.000189, l4: 0.000512, l5: 0.001077, l6: 0.002091
[epoch: 951/1000, batch:   760/ 1052, ite: 250040] train loss: 0.003904, tar: 0.000046 
l0: 0.000008, l1: 0.000006, l2: 0.000026, l3: 0.000111, l4: 0.000329, l5: 0.001039, l6: 0.001355
[epoch: 951/1000, batch:   840/ 1052, ite: 250060] train loss: 0.003904, tar: 0.000046 
l0: 0.000043, l1: 0.000043, l2: 0.000075, l3: 0.000146, l4: 0.000391, l5: 0.000905, l6: 0.001846
[epoch: 951/1000, batch:   920/ 1052, ite: 250080] train loss: 0.003904, tar: 0.000046 
l0: 0.000017, l1: 0.000016, l2: 0.000062, l3: 0.000273, l4: 0.000568, l5: 0.001105, l6: 0.001906
[epoch: 951/1000, batch:  1000/ 1052, ite: 250100] train loss: 0.003904, tar: 0.000046 
[Epoch 951/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000057, l1: 0.000057, l2: 0.000082, l3: 0.000196, l4: 0.000319, l5: 0.000812, l6: 0.001504
[epoch: 952/1000, batch:    28/ 1052, ite: 250120] train loss: 0.003905, tar: 0.000046 
l0: 0.000009, l1: 0.000010, l2: 0.000031, l3: 0.000147, l4: 0.000327, l5: 0.000815, l6: 0.001997
[epoch: 952/1000, batch:   108/ 1052, ite: 250140] train loss: 0.003905, tar: 0.000046 
l0: 0.000012, l1: 0.000011, l2: 0.000033, l3: 0.000227, l4: 0.000667, l5: 0.001537, l6: 0.002969
[epoch: 952/1000, batch:   188/ 1052, ite: 250160] train loss: 0.003905, tar: 0.000046 
l0: 0.000065, l1: 0.000064, l2: 0.000103, l3: 0.000219, l4: 0.000491, l5: 0.000807, l6: 0.001603
[epoch: 952/1000, batch:   268/ 1052, ite: 250180] train loss: 0.003906, tar: 0.000046 
l0: 0.000027, l1: 0.000026, l2: 0.000061, l3: 0.000163, l4: 0.000438, l5: 0.001502, l6: 0.001877
[epoch: 952/1000, batch:   348/ 1052, ite: 250200] train loss: 0.003905, tar: 0.000046 
l0: 0.000021, l1: 0.000021, l2: 0.000030, l3: 0.000096, l4: 0.000321, l5: 0.000542, l6: 0.000921
[epoch: 952/1000, batch:   428/ 1052, ite: 250220] train loss: 0.003905, tar: 0.000046 
l0: 0.000201, l1: 0.000204, l2: 0.000226, l3: 0.000308, l4: 0.000790, l5: 0.001630, l6: 0.002683
[epoch: 952/1000, batch:   508/ 1052, ite: 250240] train loss: 0.003906, tar: 0.000046 
l0: 0.000014, l1: 0.000015, l2: 0.000020, l3: 0.000073, l4: 0.000312, l5: 0.000682, l6: 0.001183
[epoch: 952/1000, batch:   588/ 1052, ite: 250260] train loss: 0.003906, tar: 0.000046 
l0: 0.000008, l1: 0.000008, l2: 0.000015, l3: 0.000047, l4: 0.000345, l5: 0.000833, l6: 0.001341
[epoch: 952/1000, batch:   668/ 1052, ite: 250280] train loss: 0.003906, tar: 0.000046 
l0: 0.000020, l1: 0.000021, l2: 0.000039, l3: 0.000109, l4: 0.000220, l5: 0.000807, l6: 0.001276
[epoch: 952/1000, batch:   748/ 1052, ite: 250300] train loss: 0.003906, tar: 0.000046 
l0: 0.000043, l1: 0.000045, l2: 0.000083, l3: 0.000190, l4: 0.000379, l5: 0.001132, l6: 0.002156
[epoch: 952/1000, batch:   828/ 1052, ite: 250320] train loss: 0.003906, tar: 0.000046 
l0: 0.000011, l1: 0.000011, l2: 0.000022, l3: 0.000130, l4: 0.000575, l5: 0.001056, l6: 0.001733
[epoch: 952/1000, batch:   908/ 1052, ite: 250340] train loss: 0.003906, tar: 0.000046 
l0: 0.000077, l1: 0.000078, l2: 0.000116, l3: 0.000177, l4: 0.000497, l5: 0.000932, l6: 0.001615
[epoch: 952/1000, batch:   988/ 1052, ite: 250360] train loss: 0.003907, tar: 0.000046 
[Epoch 952/1000] Test loss: 0.013, Test target loss: 0.002
l0: 0.000035, l1: 0.000036, l2: 0.000064, l3: 0.000174, l4: 0.000393, l5: 0.000525, l6: 0.001392
[epoch: 953/1000, batch:    16/ 1052, ite: 250380] train loss: 0.003906, tar: 0.000046 
l0: 0.000007, l1: 0.000007, l2: 0.000017, l3: 0.000193, l4: 0.000603, l5: 0.001563, l6: 0.002581
[epoch: 953/1000, batch:    96/ 1052, ite: 250400] train loss: 0.003908, tar: 0.000046 
l0: 0.000016, l1: 0.000015, l2: 0.000026, l3: 0.000057, l4: 0.000167, l5: 0.000667, l6: 0.001341
[epoch: 953/1000, batch:   176/ 1052, ite: 250420] train loss: 0.003907, tar: 0.000046 
l0: 0.000064, l1: 0.000067, l2: 0.000079, l3: 0.000150, l4: 0.000467, l5: 0.001390, l6: 0.001224
[epoch: 953/1000, batch:   256/ 1052, ite: 250440] train loss: 0.003906, tar: 0.000046 
l0: 0.000094, l1: 0.000097, l2: 0.000111, l3: 0.000169, l4: 0.000482, l5: 0.001324, l6: 0.002453
[epoch: 953/1000, batch:   336/ 1052, ite: 250460] train loss: 0.003906, tar: 0.000046 
l0: 0.000010, l1: 0.000010, l2: 0.000047, l3: 0.000137, l4: 0.000537, l5: 0.000781, l6: 0.001537
[epoch: 953/1000, batch:   416/ 1052, ite: 250480] train loss: 0.003906, tar: 0.000046 
l0: 0.000006, l1: 0.000007, l2: 0.000024, l3: 0.000198, l4: 0.000433, l5: 0.000787, l6: 0.002157
[epoch: 953/1000, batch:   496/ 1052, ite: 250500] train loss: 0.003907, tar: 0.000046 
l0: 0.000016, l1: 0.000015, l2: 0.000048, l3: 0.000173, l4: 0.000753, l5: 0.001538, l6: 0.002497
[epoch: 953/1000, batch:   576/ 1052, ite: 250520] train loss: 0.003907, tar: 0.000046 
l0: 0.000004, l1: 0.000002, l2: 0.000026, l3: 0.000194, l4: 0.000454, l5: 0.000973, l6: 0.002044
[epoch: 953/1000, batch:   656/ 1052, ite: 250540] train loss: 0.003907, tar: 0.000046 
l0: 0.000056, l1: 0.000058, l2: 0.000077, l3: 0.000292, l4: 0.000939, l5: 0.001908, l6: 0.001977
[epoch: 953/1000, batch:   736/ 1052, ite: 250560] train loss: 0.003906, tar: 0.000046 
l0: 0.000019, l1: 0.000017, l2: 0.000102, l3: 0.000184, l4: 0.000438, l5: 0.000883, l6: 0.002488
[epoch: 953/1000, batch:   816/ 1052, ite: 250580] train loss: 0.003906, tar: 0.000046 
l0: 0.000043, l1: 0.000044, l2: 0.000089, l3: 0.000277, l4: 0.000675, l5: 0.001193, l6: 0.002739
[epoch: 953/1000, batch:   896/ 1052, ite: 250600] train loss: 0.003906, tar: 0.000046 
l0: 0.000008, l1: 0.000007, l2: 0.000031, l3: 0.000264, l4: 0.000430, l5: 0.000822, l6: 0.001970
[epoch: 953/1000, batch:   976/ 1052, ite: 250620] train loss: 0.003907, tar: 0.000046 
[Epoch 953/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000033, l1: 0.000029, l2: 0.000090, l3: 0.000403, l4: 0.000611, l5: 0.001594, l6: 0.005113
[epoch: 954/1000, batch:     4/ 1052, ite: 250640] train loss: 0.003907, tar: 0.000046 
l0: 0.000044, l1: 0.000046, l2: 0.000051, l3: 0.000088, l4: 0.000340, l5: 0.000794, l6: 0.001502
[epoch: 954/1000, batch:    84/ 1052, ite: 250660] train loss: 0.003907, tar: 0.000046 
l0: 0.000031, l1: 0.000031, l2: 0.000086, l3: 0.000432, l4: 0.000901, l5: 0.001336, l6: 0.003798
[epoch: 954/1000, batch:   164/ 1052, ite: 250680] train loss: 0.003908, tar: 0.000046 
l0: 0.000039, l1: 0.000039, l2: 0.000045, l3: 0.000112, l4: 0.000440, l5: 0.001306, l6: 0.001930
[epoch: 954/1000, batch:   244/ 1052, ite: 250700] train loss: 0.003908, tar: 0.000046 
l0: 0.000018, l1: 0.000018, l2: 0.000027, l3: 0.000100, l4: 0.000202, l5: 0.000691, l6: 0.001256
[epoch: 954/1000, batch:   324/ 1052, ite: 250720] train loss: 0.003907, tar: 0.000046 
l0: 0.000067, l1: 0.000072, l2: 0.000080, l3: 0.000178, l4: 0.000285, l5: 0.001305, l6: 0.002286
[epoch: 954/1000, batch:   404/ 1052, ite: 250740] train loss: 0.003908, tar: 0.000046 
l0: 0.000063, l1: 0.000065, l2: 0.000078, l3: 0.000163, l4: 0.000399, l5: 0.000803, l6: 0.001596
[epoch: 954/1000, batch:   484/ 1052, ite: 250760] train loss: 0.003908, tar: 0.000046 
l0: 0.000010, l1: 0.000008, l2: 0.000044, l3: 0.000156, l4: 0.000413, l5: 0.001219, l6: 0.002719
[epoch: 954/1000, batch:   564/ 1052, ite: 250780] train loss: 0.003908, tar: 0.000046 
l0: 0.000004, l1: 0.000005, l2: 0.000012, l3: 0.000074, l4: 0.000211, l5: 0.000709, l6: 0.001147
[epoch: 954/1000, batch:   644/ 1052, ite: 250800] train loss: 0.003907, tar: 0.000046 
l0: 0.000009, l1: 0.000009, l2: 0.000049, l3: 0.000129, l4: 0.000340, l5: 0.000798, l6: 0.002201
[epoch: 954/1000, batch:   724/ 1052, ite: 250820] train loss: 0.003908, tar: 0.000046 
l0: 0.000003, l1: 0.000002, l2: 0.000023, l3: 0.000109, l4: 0.000254, l5: 0.000892, l6: 0.002696
[epoch: 954/1000, batch:   804/ 1052, ite: 250840] train loss: 0.003909, tar: 0.000046 
l0: 0.000005, l1: 0.000005, l2: 0.000024, l3: 0.000063, l4: 0.000181, l5: 0.000568, l6: 0.001149
[epoch: 954/1000, batch:   884/ 1052, ite: 250860] train loss: 0.003907, tar: 0.000046 
l0: 0.000007, l1: 0.000006, l2: 0.000009, l3: 0.000090, l4: 0.000250, l5: 0.000729, l6: 0.001144
[epoch: 954/1000, batch:   964/ 1052, ite: 250880] train loss: 0.003906, tar: 0.000046 
l0: 0.000010, l1: 0.000010, l2: 0.000021, l3: 0.000108, l4: 0.000497, l5: 0.000836, l6: 0.001201
[epoch: 954/1000, batch:  1044/ 1052, ite: 250900] train loss: 0.003905, tar: 0.000046 
[Epoch 954/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000092, l1: 0.000092, l2: 0.000122, l3: 0.000268, l4: 0.001066, l5: 0.002108, l6: 0.003749
[epoch: 955/1000, batch:    72/ 1052, ite: 250920] train loss: 0.003906, tar: 0.000046 
l0: 0.000104, l1: 0.000108, l2: 0.000141, l3: 0.000225, l4: 0.000515, l5: 0.000941, l6: 0.001495
[epoch: 955/1000, batch:   152/ 1052, ite: 250940] train loss: 0.003905, tar: 0.000046 
l0: 0.000044, l1: 0.000043, l2: 0.000108, l3: 0.000209, l4: 0.000460, l5: 0.001152, l6: 0.002721
[epoch: 955/1000, batch:   232/ 1052, ite: 250960] train loss: 0.003906, tar: 0.000046 
l0: 0.000018, l1: 0.000019, l2: 0.000051, l3: 0.000095, l4: 0.000409, l5: 0.001005, l6: 0.001533
[epoch: 955/1000, batch:   312/ 1052, ite: 250980] train loss: 0.003905, tar: 0.000046 
l0: 0.000021, l1: 0.000024, l2: 0.000064, l3: 0.000273, l4: 0.000440, l5: 0.000787, l6: 0.001291
[epoch: 955/1000, batch:   392/ 1052, ite: 251000] train loss: 0.003906, tar: 0.000046 
l0: 0.000020, l1: 0.000017, l2: 0.000063, l3: 0.000151, l4: 0.000516, l5: 0.001143, l6: 0.002631
[epoch: 955/1000, batch:   472/ 1052, ite: 251020] train loss: 0.003906, tar: 0.000046 
l0: 0.000018, l1: 0.000018, l2: 0.000033, l3: 0.000089, l4: 0.000364, l5: 0.001107, l6: 0.001401
[epoch: 955/1000, batch:   552/ 1052, ite: 251040] train loss: 0.003905, tar: 0.000046 
l0: 0.000004, l1: 0.000003, l2: 0.000015, l3: 0.000151, l4: 0.000229, l5: 0.001025, l6: 0.001711
[epoch: 955/1000, batch:   632/ 1052, ite: 251060] train loss: 0.003906, tar: 0.000046 
l0: 0.000015, l1: 0.000014, l2: 0.000058, l3: 0.000189, l4: 0.000547, l5: 0.001167, l6: 0.001533
[epoch: 955/1000, batch:   712/ 1052, ite: 251080] train loss: 0.003906, tar: 0.000046 
l0: 0.000139, l1: 0.000144, l2: 0.000156, l3: 0.000268, l4: 0.000532, l5: 0.001288, l6: 0.002608
[epoch: 955/1000, batch:   792/ 1052, ite: 251100] train loss: 0.003905, tar: 0.000046 
l0: 0.000090, l1: 0.000092, l2: 0.000107, l3: 0.000182, l4: 0.000326, l5: 0.000520, l6: 0.001667
[epoch: 955/1000, batch:   872/ 1052, ite: 251120] train loss: 0.003905, tar: 0.000046 
l0: 0.000072, l1: 0.000072, l2: 0.000107, l3: 0.000283, l4: 0.000784, l5: 0.001443, l6: 0.003018
[epoch: 955/1000, batch:   952/ 1052, ite: 251140] train loss: 0.003905, tar: 0.000046 
l0: 0.000070, l1: 0.000068, l2: 0.000094, l3: 0.000218, l4: 0.000393, l5: 0.001330, l6: 0.001801
[epoch: 955/1000, batch:  1032/ 1052, ite: 251160] train loss: 0.003904, tar: 0.000046 
[Epoch 955/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000005, l1: 0.000004, l2: 0.000025, l3: 0.000104, l4: 0.000250, l5: 0.001022, l6: 0.001894
[epoch: 956/1000, batch:    60/ 1052, ite: 251180] train loss: 0.003905, tar: 0.000046 
l0: 0.000014, l1: 0.000016, l2: 0.000034, l3: 0.000134, l4: 0.000595, l5: 0.000995, l6: 0.002662
[epoch: 956/1000, batch:   140/ 1052, ite: 251200] train loss: 0.003905, tar: 0.000046 
l0: 0.000003, l1: 0.000002, l2: 0.000035, l3: 0.000104, l4: 0.000398, l5: 0.001108, l6: 0.001941
[epoch: 956/1000, batch:   220/ 1052, ite: 251220] train loss: 0.003904, tar: 0.000045 
l0: 0.000004, l1: 0.000003, l2: 0.000044, l3: 0.000244, l4: 0.000603, l5: 0.001199, l6: 0.002256
[epoch: 956/1000, batch:   300/ 1052, ite: 251240] train loss: 0.003904, tar: 0.000045 
l0: 0.000016, l1: 0.000018, l2: 0.000024, l3: 0.000048, l4: 0.000261, l5: 0.000708, l6: 0.001198
[epoch: 956/1000, batch:   380/ 1052, ite: 251260] train loss: 0.003903, tar: 0.000045 
l0: 0.000117, l1: 0.000119, l2: 0.000154, l3: 0.000208, l4: 0.000487, l5: 0.001651, l6: 0.002779
[epoch: 956/1000, batch:   460/ 1052, ite: 251280] train loss: 0.003904, tar: 0.000045 
l0: 0.000022, l1: 0.000023, l2: 0.000039, l3: 0.000140, l4: 0.000266, l5: 0.001135, l6: 0.002527
[epoch: 956/1000, batch:   540/ 1052, ite: 251300] train loss: 0.003904, tar: 0.000045 
l0: 0.000021, l1: 0.000021, l2: 0.000029, l3: 0.000080, l4: 0.000280, l5: 0.001063, l6: 0.002128
[epoch: 956/1000, batch:   620/ 1052, ite: 251320] train loss: 0.003904, tar: 0.000045 
l0: 0.000013, l1: 0.000013, l2: 0.000022, l3: 0.000106, l4: 0.000315, l5: 0.000716, l6: 0.002218
[epoch: 956/1000, batch:   700/ 1052, ite: 251340] train loss: 0.003905, tar: 0.000045 
l0: 0.000003, l1: 0.000003, l2: 0.000012, l3: 0.000043, l4: 0.000132, l5: 0.000746, l6: 0.001299
[epoch: 956/1000, batch:   780/ 1052, ite: 251360] train loss: 0.003905, tar: 0.000045 
l0: 0.000000, l1: 0.000000, l2: 0.000002, l3: 0.000029, l4: 0.000120, l5: 0.000505, l6: 0.000940
[epoch: 956/1000, batch:   860/ 1052, ite: 251380] train loss: 0.003904, tar: 0.000045 
l0: 0.000027, l1: 0.000026, l2: 0.000044, l3: 0.000117, l4: 0.000500, l5: 0.002157, l6: 0.003134
[epoch: 956/1000, batch:   940/ 1052, ite: 251400] train loss: 0.003903, tar: 0.000045 
l0: 0.000002, l1: 0.000002, l2: 0.000016, l3: 0.000153, l4: 0.000429, l5: 0.000957, l6: 0.001493
[epoch: 956/1000, batch:  1020/ 1052, ite: 251420] train loss: 0.003903, tar: 0.000045 
[Epoch 956/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000001, l1: 0.000001, l2: 0.000041, l3: 0.000136, l4: 0.000384, l5: 0.001481, l6: 0.001816
[epoch: 957/1000, batch:    48/ 1052, ite: 251440] train loss: 0.003904, tar: 0.000045 
l0: 0.000001, l1: 0.000001, l2: 0.000008, l3: 0.000069, l4: 0.000290, l5: 0.000914, l6: 0.001061
[epoch: 957/1000, batch:   128/ 1052, ite: 251460] train loss: 0.003903, tar: 0.000045 
l0: 0.000009, l1: 0.000010, l2: 0.000035, l3: 0.000113, l4: 0.000476, l5: 0.000731, l6: 0.001725
[epoch: 957/1000, batch:   208/ 1052, ite: 251480] train loss: 0.003903, tar: 0.000045 
l0: 0.000088, l1: 0.000089, l2: 0.000149, l3: 0.000275, l4: 0.000599, l5: 0.001238, l6: 0.001909
[epoch: 957/1000, batch:   288/ 1052, ite: 251500] train loss: 0.003902, tar: 0.000045 
l0: 0.000055, l1: 0.000054, l2: 0.000092, l3: 0.000245, l4: 0.000481, l5: 0.000747, l6: 0.001598
[epoch: 957/1000, batch:   368/ 1052, ite: 251520] train loss: 0.003902, tar: 0.000045 
l0: 0.000014, l1: 0.000013, l2: 0.000061, l3: 0.000309, l4: 0.000791, l5: 0.002268, l6: 0.004355
[epoch: 957/1000, batch:   448/ 1052, ite: 251540] train loss: 0.003903, tar: 0.000045 
l0: 0.000018, l1: 0.000019, l2: 0.000034, l3: 0.000164, l4: 0.000270, l5: 0.000754, l6: 0.001384
[epoch: 957/1000, batch:   528/ 1052, ite: 251560] train loss: 0.003904, tar: 0.000045 
l0: 0.000001, l1: 0.000001, l2: 0.000003, l3: 0.000072, l4: 0.000342, l5: 0.000665, l6: 0.001622
[epoch: 957/1000, batch:   608/ 1052, ite: 251580] train loss: 0.003905, tar: 0.000045 
l0: 0.000047, l1: 0.000047, l2: 0.000069, l3: 0.000203, l4: 0.000392, l5: 0.000743, l6: 0.001798
[epoch: 957/1000, batch:   688/ 1052, ite: 251600] train loss: 0.003904, tar: 0.000045 
l0: 0.000012, l1: 0.000011, l2: 0.000038, l3: 0.000132, l4: 0.000465, l5: 0.001063, l6: 0.002139
[epoch: 957/1000, batch:   768/ 1052, ite: 251620] train loss: 0.003903, tar: 0.000045 
l0: 0.000021, l1: 0.000022, l2: 0.000025, l3: 0.000087, l4: 0.000397, l5: 0.000858, l6: 0.001747
[epoch: 957/1000, batch:   848/ 1052, ite: 251640] train loss: 0.003902, tar: 0.000045 
l0: 0.000101, l1: 0.000098, l2: 0.000126, l3: 0.000295, l4: 0.000517, l5: 0.001208, l6: 0.003125
[epoch: 957/1000, batch:   928/ 1052, ite: 251660] train loss: 0.003903, tar: 0.000045 
l0: 0.000074, l1: 0.000074, l2: 0.000093, l3: 0.000158, l4: 0.000535, l5: 0.001161, l6: 0.002039
[epoch: 957/1000, batch:  1008/ 1052, ite: 251680] train loss: 0.003902, tar: 0.000045 
[Epoch 957/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000007, l1: 0.000007, l2: 0.000024, l3: 0.000149, l4: 0.000325, l5: 0.001085, l6: 0.002167
[epoch: 958/1000, batch:    36/ 1052, ite: 251700] train loss: 0.003902, tar: 0.000045 
l0: 0.000027, l1: 0.000028, l2: 0.000048, l3: 0.000133, l4: 0.000367, l5: 0.001115, l6: 0.001466
[epoch: 958/1000, batch:   116/ 1052, ite: 251720] train loss: 0.003901, tar: 0.000045 
l0: 0.000060, l1: 0.000061, l2: 0.000097, l3: 0.000270, l4: 0.000599, l5: 0.000994, l6: 0.001358
[epoch: 958/1000, batch:   196/ 1052, ite: 251740] train loss: 0.003901, tar: 0.000045 
l0: 0.000042, l1: 0.000042, l2: 0.000055, l3: 0.000165, l4: 0.000471, l5: 0.000770, l6: 0.001241
[epoch: 958/1000, batch:   276/ 1052, ite: 251760] train loss: 0.003900, tar: 0.000045 
l0: 0.000007, l1: 0.000006, l2: 0.000049, l3: 0.000152, l4: 0.000377, l5: 0.000924, l6: 0.001397
[epoch: 958/1000, batch:   356/ 1052, ite: 251780] train loss: 0.003900, tar: 0.000045 
l0: 0.000131, l1: 0.000135, l2: 0.000165, l3: 0.000335, l4: 0.000660, l5: 0.001650, l6: 0.003565
[epoch: 958/1000, batch:   436/ 1052, ite: 251800] train loss: 0.003900, tar: 0.000045 
l0: 0.000005, l1: 0.000006, l2: 0.000018, l3: 0.000046, l4: 0.000447, l5: 0.000472, l6: 0.001725
[epoch: 958/1000, batch:   516/ 1052, ite: 251820] train loss: 0.003900, tar: 0.000045 
l0: 0.000007, l1: 0.000007, l2: 0.000037, l3: 0.000212, l4: 0.000657, l5: 0.001577, l6: 0.001974
[epoch: 958/1000, batch:   596/ 1052, ite: 251840] train loss: 0.003901, tar: 0.000045 
l0: 0.000050, l1: 0.000047, l2: 0.000104, l3: 0.000312, l4: 0.000787, l5: 0.001484, l6: 0.003209
[epoch: 958/1000, batch:   676/ 1052, ite: 251860] train loss: 0.003902, tar: 0.000045 
l0: 0.000044, l1: 0.000044, l2: 0.000073, l3: 0.000193, l4: 0.000448, l5: 0.000857, l6: 0.001561
[epoch: 958/1000, batch:   756/ 1052, ite: 251880] train loss: 0.003900, tar: 0.000045 
l0: 0.000018, l1: 0.000018, l2: 0.000052, l3: 0.000148, l4: 0.000348, l5: 0.001368, l6: 0.002564
[epoch: 958/1000, batch:   836/ 1052, ite: 251900] train loss: 0.003901, tar: 0.000045 
l0: 0.000003, l1: 0.000004, l2: 0.000018, l3: 0.000210, l4: 0.000651, l5: 0.001464, l6: 0.002872
[epoch: 958/1000, batch:   916/ 1052, ite: 251920] train loss: 0.003901, tar: 0.000045 
l0: 0.000012, l1: 0.000012, l2: 0.000091, l3: 0.000318, l4: 0.000470, l5: 0.001341, l6: 0.003500
[epoch: 958/1000, batch:   996/ 1052, ite: 251940] train loss: 0.003901, tar: 0.000045 
[Epoch 958/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000037, l1: 0.000037, l2: 0.000126, l3: 0.000282, l4: 0.000420, l5: 0.001169, l6: 0.002796
[epoch: 959/1000, batch:    24/ 1052, ite: 251960] train loss: 0.003901, tar: 0.000045 
l0: 0.000007, l1: 0.000009, l2: 0.000010, l3: 0.000102, l4: 0.000201, l5: 0.000509, l6: 0.001568
[epoch: 959/1000, batch:   104/ 1052, ite: 251980] train loss: 0.003901, tar: 0.000045 
l0: 0.000002, l1: 0.000002, l2: 0.000020, l3: 0.000081, l4: 0.000272, l5: 0.000986, l6: 0.001363
[epoch: 959/1000, batch:   184/ 1052, ite: 252000] train loss: 0.003901, tar: 0.000045 
l0: 0.000021, l1: 0.000021, l2: 0.000032, l3: 0.000097, l4: 0.000408, l5: 0.000604, l6: 0.000865
[epoch: 959/1000, batch:   264/ 1052, ite: 252020] train loss: 0.003901, tar: 0.000045 
l0: 0.000014, l1: 0.000014, l2: 0.000032, l3: 0.000080, l4: 0.000280, l5: 0.000361, l6: 0.001013
[epoch: 959/1000, batch:   344/ 1052, ite: 252040] train loss: 0.003901, tar: 0.000045 
l0: 0.000015, l1: 0.000013, l2: 0.000042, l3: 0.000109, l4: 0.000279, l5: 0.000713, l6: 0.001583
[epoch: 959/1000, batch:   424/ 1052, ite: 252060] train loss: 0.003901, tar: 0.000045 
l0: 0.000047, l1: 0.000048, l2: 0.000079, l3: 0.000149, l4: 0.000382, l5: 0.001084, l6: 0.002162
[epoch: 959/1000, batch:   504/ 1052, ite: 252080] train loss: 0.003900, tar: 0.000045 
l0: 0.000006, l1: 0.000007, l2: 0.000025, l3: 0.000228, l4: 0.000805, l5: 0.001851, l6: 0.003348
[epoch: 959/1000, batch:   584/ 1052, ite: 252100] train loss: 0.003900, tar: 0.000045 
l0: 0.000030, l1: 0.000030, l2: 0.000070, l3: 0.000141, l4: 0.000434, l5: 0.000825, l6: 0.001060
[epoch: 959/1000, batch:   664/ 1052, ite: 252120] train loss: 0.003901, tar: 0.000045 
l0: 0.000090, l1: 0.000091, l2: 0.000119, l3: 0.000249, l4: 0.000524, l5: 0.000982, l6: 0.002686
[epoch: 959/1000, batch:   744/ 1052, ite: 252140] train loss: 0.003900, tar: 0.000045 
l0: 0.000020, l1: 0.000020, l2: 0.000036, l3: 0.000154, l4: 0.000304, l5: 0.000694, l6: 0.001577
[epoch: 959/1000, batch:   824/ 1052, ite: 252160] train loss: 0.003899, tar: 0.000045 
l0: 0.000066, l1: 0.000065, l2: 0.000112, l3: 0.000211, l4: 0.000945, l5: 0.001719, l6: 0.004350
[epoch: 959/1000, batch:   904/ 1052, ite: 252180] train loss: 0.003900, tar: 0.000045 
l0: 0.000074, l1: 0.000074, l2: 0.000103, l3: 0.000186, l4: 0.000459, l5: 0.000998, l6: 0.001752
[epoch: 959/1000, batch:   984/ 1052, ite: 252200] train loss: 0.003899, tar: 0.000045 
[Epoch 959/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000133, l1: 0.000138, l2: 0.000171, l3: 0.000282, l4: 0.000633, l5: 0.001219, l6: 0.002339
[epoch: 960/1000, batch:    12/ 1052, ite: 252220] train loss: 0.003899, tar: 0.000045 
l0: 0.000064, l1: 0.000066, l2: 0.000089, l3: 0.000197, l4: 0.000529, l5: 0.001401, l6: 0.002894
[epoch: 960/1000, batch:    92/ 1052, ite: 252240] train loss: 0.003899, tar: 0.000045 
l0: 0.000011, l1: 0.000011, l2: 0.000035, l3: 0.000105, l4: 0.000266, l5: 0.000625, l6: 0.002413
[epoch: 960/1000, batch:   172/ 1052, ite: 252260] train loss: 0.003899, tar: 0.000045 
l0: 0.000077, l1: 0.000068, l2: 0.000131, l3: 0.000469, l4: 0.001177, l5: 0.001785, l6: 0.003631
[epoch: 960/1000, batch:   252/ 1052, ite: 252280] train loss: 0.003899, tar: 0.000045 
l0: 0.000083, l1: 0.000084, l2: 0.000125, l3: 0.000313, l4: 0.000940, l5: 0.001883, l6: 0.003083
[epoch: 960/1000, batch:   332/ 1052, ite: 252300] train loss: 0.003898, tar: 0.000044 
l0: 0.000021, l1: 0.000022, l2: 0.000025, l3: 0.000109, l4: 0.000222, l5: 0.000496, l6: 0.001601
[epoch: 960/1000, batch:   412/ 1052, ite: 252320] train loss: 0.003897, tar: 0.000044 
l0: 0.000013, l1: 0.000012, l2: 0.000057, l3: 0.000209, l4: 0.000403, l5: 0.001590, l6: 0.003427
[epoch: 960/1000, batch:   492/ 1052, ite: 252340] train loss: 0.003898, tar: 0.000044 
l0: 0.000059, l1: 0.000060, l2: 0.000083, l3: 0.000183, l4: 0.000547, l5: 0.001410, l6: 0.002044
[epoch: 960/1000, batch:   572/ 1052, ite: 252360] train loss: 0.003897, tar: 0.000044 
l0: 0.000050, l1: 0.000053, l2: 0.000063, l3: 0.000143, l4: 0.000440, l5: 0.001208, l6: 0.002280
[epoch: 960/1000, batch:   652/ 1052, ite: 252380] train loss: 0.003898, tar: 0.000044 
l0: 0.000003, l1: 0.000003, l2: 0.000017, l3: 0.000096, l4: 0.000415, l5: 0.001169, l6: 0.002559
[epoch: 960/1000, batch:   732/ 1052, ite: 252400] train loss: 0.003898, tar: 0.000044 
l0: 0.000000, l1: 0.000000, l2: 0.000003, l3: 0.000036, l4: 0.000266, l5: 0.000363, l6: 0.000965
[epoch: 960/1000, batch:   812/ 1052, ite: 252420] train loss: 0.003898, tar: 0.000044 
l0: 0.000003, l1: 0.000002, l2: 0.000026, l3: 0.000082, l4: 0.000389, l5: 0.000435, l6: 0.000819
[epoch: 960/1000, batch:   892/ 1052, ite: 252440] train loss: 0.003898, tar: 0.000044 
l0: 0.000085, l1: 0.000086, l2: 0.000122, l3: 0.000250, l4: 0.000515, l5: 0.001119, l6: 0.001427
[epoch: 960/1000, batch:   972/ 1052, ite: 252460] train loss: 0.003898, tar: 0.000044 
l0: 0.000011, l1: 0.000010, l2: 0.000021, l3: 0.000073, l4: 0.000253, l5: 0.000591, l6: 0.000838
[epoch: 960/1000, batch:  1052/ 1052, ite: 252480] train loss: 0.003898, tar: 0.000044 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_252480.pkl
[Epoch 960/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000008, l1: 0.000008, l2: 0.000031, l3: 0.000136, l4: 0.000328, l5: 0.001199, l6: 0.001197
[epoch: 961/1000, batch:    80/ 1052, ite: 252500] train loss: 0.003631, tar: 0.000028 
l0: 0.000059, l1: 0.000060, l2: 0.000083, l3: 0.000164, l4: 0.000304, l5: 0.001091, l6: 0.001703
[epoch: 961/1000, batch:   160/ 1052, ite: 252520] train loss: 0.003645, tar: 0.000041 
l0: 0.000037, l1: 0.000036, l2: 0.000064, l3: 0.000232, l4: 0.000446, l5: 0.001054, l6: 0.001332
[epoch: 961/1000, batch:   240/ 1052, ite: 252540] train loss: 0.003998, tar: 0.000042 
l0: 0.000011, l1: 0.000011, l2: 0.000046, l3: 0.000215, l4: 0.000365, l5: 0.000497, l6: 0.001351
[epoch: 961/1000, batch:   320/ 1052, ite: 252560] train loss: 0.004063, tar: 0.000040 
l0: 0.000005, l1: 0.000005, l2: 0.000039, l3: 0.000137, l4: 0.000355, l5: 0.001279, l6: 0.002224
[epoch: 961/1000, batch:   400/ 1052, ite: 252580] train loss: 0.003963, tar: 0.000039 
l0: 0.000019, l1: 0.000021, l2: 0.000036, l3: 0.000061, l4: 0.000422, l5: 0.000827, l6: 0.001486
[epoch: 961/1000, batch:   480/ 1052, ite: 252600] train loss: 0.004018, tar: 0.000039 
l0: 0.000042, l1: 0.000040, l2: 0.000091, l3: 0.000296, l4: 0.000745, l5: 0.001410, l6: 0.002680
[epoch: 961/1000, batch:   560/ 1052, ite: 252620] train loss: 0.003949, tar: 0.000037 
l0: 0.000004, l1: 0.000004, l2: 0.000038, l3: 0.000154, l4: 0.000375, l5: 0.001362, l6: 0.001464
[epoch: 961/1000, batch:   640/ 1052, ite: 252640] train loss: 0.003963, tar: 0.000037 
l0: 0.000012, l1: 0.000012, l2: 0.000014, l3: 0.000059, l4: 0.000121, l5: 0.000309, l6: 0.001153
[epoch: 961/1000, batch:   720/ 1052, ite: 252660] train loss: 0.003939, tar: 0.000037 
l0: 0.000049, l1: 0.000050, l2: 0.000094, l3: 0.000231, l4: 0.000573, l5: 0.001493, l6: 0.001640
[epoch: 961/1000, batch:   800/ 1052, ite: 252680] train loss: 0.003902, tar: 0.000036 
l0: 0.000026, l1: 0.000026, l2: 0.000075, l3: 0.000155, l4: 0.000510, l5: 0.000921, l6: 0.001617
[epoch: 961/1000, batch:   880/ 1052, ite: 252700] train loss: 0.003886, tar: 0.000036 
l0: 0.000098, l1: 0.000097, l2: 0.000107, l3: 0.000194, l4: 0.000467, l5: 0.001260, l6: 0.003480
[epoch: 961/1000, batch:   960/ 1052, ite: 252720] train loss: 0.003900, tar: 0.000037 
l0: 0.000002, l1: 0.000001, l2: 0.000025, l3: 0.000106, l4: 0.000447, l5: 0.000899, l6: 0.001476
[epoch: 961/1000, batch:  1040/ 1052, ite: 252740] train loss: 0.003880, tar: 0.000037 
[Epoch 961/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000004, l1: 0.000004, l2: 0.000021, l3: 0.000092, l4: 0.000243, l5: 0.001264, l6: 0.001440
[epoch: 962/1000, batch:    68/ 1052, ite: 252760] train loss: 0.003874, tar: 0.000037 
l0: 0.000029, l1: 0.000029, l2: 0.000044, l3: 0.000072, l4: 0.000192, l5: 0.000734, l6: 0.000880
[epoch: 962/1000, batch:   148/ 1052, ite: 252780] train loss: 0.003882, tar: 0.000038 
l0: 0.000052, l1: 0.000053, l2: 0.000085, l3: 0.000223, l4: 0.000606, l5: 0.001744, l6: 0.001927
[epoch: 962/1000, batch:   228/ 1052, ite: 252800] train loss: 0.003872, tar: 0.000037 
l0: 0.000050, l1: 0.000050, l2: 0.000088, l3: 0.000205, l4: 0.000591, l5: 0.000822, l6: 0.001727
[epoch: 962/1000, batch:   308/ 1052, ite: 252820] train loss: 0.003855, tar: 0.000037 
l0: 0.000033, l1: 0.000035, l2: 0.000064, l3: 0.000170, l4: 0.000394, l5: 0.001117, l6: 0.001949
[epoch: 962/1000, batch:   388/ 1052, ite: 252840] train loss: 0.003834, tar: 0.000036 
l0: 0.000021, l1: 0.000022, l2: 0.000038, l3: 0.000124, l4: 0.000366, l5: 0.000867, l6: 0.002238
[epoch: 962/1000, batch:   468/ 1052, ite: 252860] train loss: 0.003822, tar: 0.000036 
l0: 0.000025, l1: 0.000025, l2: 0.000039, l3: 0.000095, l4: 0.000234, l5: 0.000787, l6: 0.001446
[epoch: 962/1000, batch:   548/ 1052, ite: 252880] train loss: 0.003794, tar: 0.000035 
l0: 0.000092, l1: 0.000091, l2: 0.000135, l3: 0.000230, l4: 0.000531, l5: 0.001103, l6: 0.002121
[epoch: 962/1000, batch:   628/ 1052, ite: 252900] train loss: 0.003785, tar: 0.000035 
l0: 0.000018, l1: 0.000019, l2: 0.000030, l3: 0.000157, l4: 0.000416, l5: 0.000924, l6: 0.001333
[epoch: 962/1000, batch:   708/ 1052, ite: 252920] train loss: 0.003795, tar: 0.000035 
l0: 0.000021, l1: 0.000022, l2: 0.000035, l3: 0.000159, l4: 0.000515, l5: 0.000552, l6: 0.001198
[epoch: 962/1000, batch:   788/ 1052, ite: 252940] train loss: 0.003800, tar: 0.000035 
l0: 0.000059, l1: 0.000060, l2: 0.000139, l3: 0.000397, l4: 0.001198, l5: 0.003466, l6: 0.004198
[epoch: 962/1000, batch:   868/ 1052, ite: 252960] train loss: 0.003825, tar: 0.000035 
l0: 0.000020, l1: 0.000022, l2: 0.000043, l3: 0.000165, l4: 0.000391, l5: 0.001250, l6: 0.002628
[epoch: 962/1000, batch:   948/ 1052, ite: 252980] train loss: 0.003832, tar: 0.000035 
l0: 0.000095, l1: 0.000095, l2: 0.000121, l3: 0.000177, l4: 0.000369, l5: 0.000932, l6: 0.002504
[epoch: 962/1000, batch:  1028/ 1052, ite: 253000] train loss: 0.003837, tar: 0.000035 
[Epoch 962/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000019, l1: 0.000019, l2: 0.000029, l3: 0.000154, l4: 0.000406, l5: 0.000967, l6: 0.002200
[epoch: 963/1000, batch:    56/ 1052, ite: 253020] train loss: 0.003823, tar: 0.000035 
l0: 0.000085, l1: 0.000087, l2: 0.000096, l3: 0.000202, l4: 0.000564, l5: 0.001307, l6: 0.002136
[epoch: 963/1000, batch:   136/ 1052, ite: 253040] train loss: 0.003827, tar: 0.000036 
l0: 0.000007, l1: 0.000007, l2: 0.000016, l3: 0.000174, l4: 0.000477, l5: 0.001200, l6: 0.002021
[epoch: 963/1000, batch:   216/ 1052, ite: 253060] train loss: 0.003811, tar: 0.000035 
l0: 0.000064, l1: 0.000065, l2: 0.000076, l3: 0.000121, l4: 0.000423, l5: 0.000937, l6: 0.002608
[epoch: 963/1000, batch:   296/ 1052, ite: 253080] train loss: 0.003800, tar: 0.000035 
l0: 0.000002, l1: 0.000001, l2: 0.000021, l3: 0.000148, l4: 0.000411, l5: 0.000912, l6: 0.002144
[epoch: 963/1000, batch:   376/ 1052, ite: 253100] train loss: 0.003801, tar: 0.000035 
l0: 0.000003, l1: 0.000003, l2: 0.000010, l3: 0.000122, l4: 0.000274, l5: 0.000803, l6: 0.001737
[epoch: 963/1000, batch:   456/ 1052, ite: 253120] train loss: 0.003796, tar: 0.000035 
l0: 0.000018, l1: 0.000018, l2: 0.000043, l3: 0.000136, l4: 0.000352, l5: 0.000730, l6: 0.001375
[epoch: 963/1000, batch:   536/ 1052, ite: 253140] train loss: 0.003811, tar: 0.000035 
l0: 0.000063, l1: 0.000063, l2: 0.000094, l3: 0.000242, l4: 0.000839, l5: 0.001535, l6: 0.002608
[epoch: 963/1000, batch:   616/ 1052, ite: 253160] train loss: 0.003818, tar: 0.000035 
l0: 0.000008, l1: 0.000007, l2: 0.000025, l3: 0.000098, l4: 0.000292, l5: 0.000706, l6: 0.001359
[epoch: 963/1000, batch:   696/ 1052, ite: 253180] train loss: 0.003821, tar: 0.000035 
l0: 0.000004, l1: 0.000004, l2: 0.000011, l3: 0.000079, l4: 0.000377, l5: 0.001069, l6: 0.001567
[epoch: 963/1000, batch:   776/ 1052, ite: 253200] train loss: 0.003827, tar: 0.000035 
l0: 0.000009, l1: 0.000009, l2: 0.000028, l3: 0.000133, l4: 0.000411, l5: 0.000936, l6: 0.001803
[epoch: 963/1000, batch:   856/ 1052, ite: 253220] train loss: 0.003840, tar: 0.000035 
l0: 0.000127, l1: 0.000129, l2: 0.000144, l3: 0.000204, l4: 0.000496, l5: 0.001017, l6: 0.002620
[epoch: 963/1000, batch:   936/ 1052, ite: 253240] train loss: 0.003833, tar: 0.000035 
l0: 0.000046, l1: 0.000047, l2: 0.000054, l3: 0.000193, l4: 0.000545, l5: 0.000768, l6: 0.001801
[epoch: 963/1000, batch:  1016/ 1052, ite: 253260] train loss: 0.003842, tar: 0.000035 
[Epoch 963/1000] Test loss: 0.016, Test target loss: 0.002
l0: 0.000050, l1: 0.000049, l2: 0.000064, l3: 0.000097, l4: 0.000219, l5: 0.000560, l6: 0.000857
[epoch: 964/1000, batch:    44/ 1052, ite: 253280] train loss: 0.003839, tar: 0.000035 
l0: 0.000027, l1: 0.000027, l2: 0.000041, l3: 0.000106, l4: 0.000184, l5: 0.000583, l6: 0.001583
[epoch: 964/1000, batch:   124/ 1052, ite: 253300] train loss: 0.003834, tar: 0.000035 
l0: 0.000020, l1: 0.000021, l2: 0.000032, l3: 0.000100, l4: 0.000319, l5: 0.001186, l6: 0.001764
[epoch: 964/1000, batch:   204/ 1052, ite: 253320] train loss: 0.003831, tar: 0.000035 
l0: 0.000105, l1: 0.000104, l2: 0.000112, l3: 0.000184, l4: 0.000425, l5: 0.001303, l6: 0.002810
[epoch: 964/1000, batch:   284/ 1052, ite: 253340] train loss: 0.003832, tar: 0.000035 
l0: 0.000135, l1: 0.000133, l2: 0.000181, l3: 0.000384, l4: 0.000780, l5: 0.001111, l6: 0.002620
[epoch: 964/1000, batch:   364/ 1052, ite: 253360] train loss: 0.003843, tar: 0.000035 
l0: 0.000023, l1: 0.000022, l2: 0.000031, l3: 0.000103, l4: 0.000330, l5: 0.000945, l6: 0.001660
[epoch: 964/1000, batch:   444/ 1052, ite: 253380] train loss: 0.003834, tar: 0.000035 
l0: 0.000042, l1: 0.000044, l2: 0.000075, l3: 0.000169, l4: 0.000391, l5: 0.000766, l6: 0.001666
[epoch: 964/1000, batch:   524/ 1052, ite: 253400] train loss: 0.003836, tar: 0.000035 
l0: 0.000018, l1: 0.000019, l2: 0.000080, l3: 0.000413, l4: 0.000890, l5: 0.002647, l6: 0.004495
[epoch: 964/1000, batch:   604/ 1052, ite: 253420] train loss: 0.003841, tar: 0.000035 
l0: 0.000051, l1: 0.000052, l2: 0.000074, l3: 0.000259, l4: 0.000557, l5: 0.001545, l6: 0.001776
[epoch: 964/1000, batch:   684/ 1052, ite: 253440] train loss: 0.003833, tar: 0.000035 
l0: 0.000054, l1: 0.000053, l2: 0.000077, l3: 0.000166, l4: 0.000432, l5: 0.001213, l6: 0.003591
[epoch: 964/1000, batch:   764/ 1052, ite: 253460] train loss: 0.003840, tar: 0.000035 
l0: 0.000086, l1: 0.000087, l2: 0.000111, l3: 0.000299, l4: 0.000679, l5: 0.000841, l6: 0.001633
[epoch: 964/1000, batch:   844/ 1052, ite: 253480] train loss: 0.003841, tar: 0.000035 
l0: 0.000000, l1: 0.000000, l2: 0.000011, l3: 0.000092, l4: 0.000226, l5: 0.000777, l6: 0.000815
[epoch: 964/1000, batch:   924/ 1052, ite: 253500] train loss: 0.003837, tar: 0.000035 
l0: 0.000010, l1: 0.000009, l2: 0.000046, l3: 0.000167, l4: 0.000356, l5: 0.001205, l6: 0.002890
[epoch: 964/1000, batch:  1004/ 1052, ite: 253520] train loss: 0.003842, tar: 0.000035 
[Epoch 964/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000019, l1: 0.000017, l2: 0.000083, l3: 0.000278, l4: 0.000734, l5: 0.001193, l6: 0.002813
[epoch: 965/1000, batch:    32/ 1052, ite: 253540] train loss: 0.003847, tar: 0.000035 
l0: 0.000029, l1: 0.000030, l2: 0.000051, l3: 0.000182, l4: 0.000563, l5: 0.001331, l6: 0.001855
[epoch: 965/1000, batch:   112/ 1052, ite: 253560] train loss: 0.003871, tar: 0.000036 
l0: 0.000002, l1: 0.000002, l2: 0.000017, l3: 0.000111, l4: 0.000291, l5: 0.000805, l6: 0.001874
[epoch: 965/1000, batch:   192/ 1052, ite: 253580] train loss: 0.003865, tar: 0.000036 
l0: 0.000006, l1: 0.000006, l2: 0.000024, l3: 0.000089, l4: 0.000375, l5: 0.000796, l6: 0.001453
[epoch: 965/1000, batch:   272/ 1052, ite: 253600] train loss: 0.003872, tar: 0.000036 
l0: 0.000008, l1: 0.000008, l2: 0.000020, l3: 0.000045, l4: 0.000216, l5: 0.000680, l6: 0.001284
[epoch: 965/1000, batch:   352/ 1052, ite: 253620] train loss: 0.003861, tar: 0.000036 
l0: 0.000074, l1: 0.000073, l2: 0.000113, l3: 0.000351, l4: 0.000903, l5: 0.001577, l6: 0.002332
[epoch: 965/1000, batch:   432/ 1052, ite: 253640] train loss: 0.003858, tar: 0.000036 
l0: 0.000004, l1: 0.000004, l2: 0.000013, l3: 0.000086, l4: 0.000409, l5: 0.000984, l6: 0.001667
[epoch: 965/1000, batch:   512/ 1052, ite: 253660] train loss: 0.003851, tar: 0.000035 
l0: 0.000054, l1: 0.000056, l2: 0.000066, l3: 0.000205, l4: 0.000554, l5: 0.001531, l6: 0.002385
[epoch: 965/1000, batch:   592/ 1052, ite: 253680] train loss: 0.003850, tar: 0.000035 
l0: 0.000016, l1: 0.000017, l2: 0.000017, l3: 0.000148, l4: 0.000514, l5: 0.001240, l6: 0.001798
[epoch: 965/1000, batch:   672/ 1052, ite: 253700] train loss: 0.003852, tar: 0.000035 
l0: 0.000014, l1: 0.000015, l2: 0.000025, l3: 0.000105, l4: 0.000510, l5: 0.000862, l6: 0.001206
[epoch: 965/1000, batch:   752/ 1052, ite: 253720] train loss: 0.003855, tar: 0.000036 
l0: 0.000010, l1: 0.000010, l2: 0.000027, l3: 0.000081, l4: 0.000344, l5: 0.000986, l6: 0.001713
[epoch: 965/1000, batch:   832/ 1052, ite: 253740] train loss: 0.003854, tar: 0.000036 
l0: 0.000012, l1: 0.000012, l2: 0.000074, l3: 0.000347, l4: 0.000932, l5: 0.001352, l6: 0.001962
[epoch: 965/1000, batch:   912/ 1052, ite: 253760] train loss: 0.003845, tar: 0.000035 
l0: 0.000092, l1: 0.000089, l2: 0.000099, l3: 0.000334, l4: 0.000789, l5: 0.001529, l6: 0.003353
[epoch: 965/1000, batch:   992/ 1052, ite: 253780] train loss: 0.003848, tar: 0.000035 
[Epoch 965/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000008, l1: 0.000007, l2: 0.000065, l3: 0.000290, l4: 0.000723, l5: 0.001398, l6: 0.002352
[epoch: 966/1000, batch:    20/ 1052, ite: 253800] train loss: 0.003851, tar: 0.000035 
l0: 0.000008, l1: 0.000008, l2: 0.000034, l3: 0.000160, l4: 0.000451, l5: 0.001476, l6: 0.002774
[epoch: 966/1000, batch:   100/ 1052, ite: 253820] train loss: 0.003853, tar: 0.000036 
l0: 0.000067, l1: 0.000065, l2: 0.000107, l3: 0.000204, l4: 0.000378, l5: 0.001274, l6: 0.001837
[epoch: 966/1000, batch:   180/ 1052, ite: 253840] train loss: 0.003851, tar: 0.000035 
l0: 0.000043, l1: 0.000044, l2: 0.000101, l3: 0.000187, l4: 0.000575, l5: 0.001171, l6: 0.002639
[epoch: 966/1000, batch:   260/ 1052, ite: 253860] train loss: 0.003855, tar: 0.000035 
l0: 0.000026, l1: 0.000026, l2: 0.000035, l3: 0.000125, l4: 0.000398, l5: 0.001041, l6: 0.002286
[epoch: 966/1000, batch:   340/ 1052, ite: 253880] train loss: 0.003851, tar: 0.000035 
l0: 0.000068, l1: 0.000069, l2: 0.000103, l3: 0.000211, l4: 0.000648, l5: 0.001315, l6: 0.002269
[epoch: 966/1000, batch:   420/ 1052, ite: 253900] train loss: 0.003855, tar: 0.000035 
l0: 0.000020, l1: 0.000019, l2: 0.000032, l3: 0.000156, l4: 0.000444, l5: 0.001139, l6: 0.001932
[epoch: 966/1000, batch:   500/ 1052, ite: 253920] train loss: 0.003856, tar: 0.000035 
l0: 0.000054, l1: 0.000055, l2: 0.000059, l3: 0.000112, l4: 0.000259, l5: 0.001131, l6: 0.001861
[epoch: 966/1000, batch:   580/ 1052, ite: 253940] train loss: 0.003853, tar: 0.000035 
l0: 0.000007, l1: 0.000007, l2: 0.000016, l3: 0.000125, l4: 0.000570, l5: 0.001139, l6: 0.002305
[epoch: 966/1000, batch:   660/ 1052, ite: 253960] train loss: 0.003853, tar: 0.000035 
l0: 0.000006, l1: 0.000005, l2: 0.000037, l3: 0.000105, l4: 0.000370, l5: 0.000972, l6: 0.002178
[epoch: 966/1000, batch:   740/ 1052, ite: 253980] train loss: 0.003854, tar: 0.000035 
l0: 0.000007, l1: 0.000007, l2: 0.000026, l3: 0.000121, l4: 0.000548, l5: 0.001047, l6: 0.001603
[epoch: 966/1000, batch:   820/ 1052, ite: 254000] train loss: 0.003851, tar: 0.000035 
l0: 0.000107, l1: 0.000101, l2: 0.000189, l3: 0.000277, l4: 0.000942, l5: 0.001629, l6: 0.001990
[epoch: 966/1000, batch:   900/ 1052, ite: 254020] train loss: 0.003852, tar: 0.000035 
l0: 0.000002, l1: 0.000002, l2: 0.000008, l3: 0.000090, l4: 0.000304, l5: 0.000802, l6: 0.001842
[epoch: 966/1000, batch:   980/ 1052, ite: 254040] train loss: 0.003845, tar: 0.000035 
[Epoch 966/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000019, l1: 0.000019, l2: 0.000044, l3: 0.000163, l4: 0.000533, l5: 0.001247, l6: 0.002026
[epoch: 967/1000, batch:     8/ 1052, ite: 254060] train loss: 0.003846, tar: 0.000035 
l0: 0.000035, l1: 0.000035, l2: 0.000070, l3: 0.000340, l4: 0.000808, l5: 0.001447, l6: 0.001949
[epoch: 967/1000, batch:    88/ 1052, ite: 254080] train loss: 0.003847, tar: 0.000036 
l0: 0.000014, l1: 0.000014, l2: 0.000070, l3: 0.000410, l4: 0.000931, l5: 0.002128, l6: 0.003349
[epoch: 967/1000, batch:   168/ 1052, ite: 254100] train loss: 0.003845, tar: 0.000036 
l0: 0.000002, l1: 0.000002, l2: 0.000019, l3: 0.000115, l4: 0.000474, l5: 0.001268, l6: 0.002608
[epoch: 967/1000, batch:   248/ 1052, ite: 254120] train loss: 0.003845, tar: 0.000036 
l0: 0.000016, l1: 0.000015, l2: 0.000041, l3: 0.000175, l4: 0.000599, l5: 0.001401, l6: 0.002075
[epoch: 967/1000, batch:   328/ 1052, ite: 254140] train loss: 0.003845, tar: 0.000036 
l0: 0.000005, l1: 0.000005, l2: 0.000018, l3: 0.000117, l4: 0.000294, l5: 0.001060, l6: 0.001383
[epoch: 967/1000, batch:   408/ 1052, ite: 254160] train loss: 0.003846, tar: 0.000036 
l0: 0.000020, l1: 0.000021, l2: 0.000063, l3: 0.000166, l4: 0.000743, l5: 0.000956, l6: 0.001819
[epoch: 967/1000, batch:   488/ 1052, ite: 254180] train loss: 0.003849, tar: 0.000036 
l0: 0.000068, l1: 0.000069, l2: 0.000104, l3: 0.000284, l4: 0.000713, l5: 0.001838, l6: 0.002069
[epoch: 967/1000, batch:   568/ 1052, ite: 254200] train loss: 0.003850, tar: 0.000036 
l0: 0.000167, l1: 0.000172, l2: 0.000206, l3: 0.000344, l4: 0.001134, l5: 0.001727, l6: 0.003351
[epoch: 967/1000, batch:   648/ 1052, ite: 254220] train loss: 0.003853, tar: 0.000036 
l0: 0.000018, l1: 0.000018, l2: 0.000044, l3: 0.000151, l4: 0.000341, l5: 0.000863, l6: 0.001895
[epoch: 967/1000, batch:   728/ 1052, ite: 254240] train loss: 0.003848, tar: 0.000036 
l0: 0.000081, l1: 0.000083, l2: 0.000078, l3: 0.000125, l4: 0.000342, l5: 0.000819, l6: 0.001992
[epoch: 967/1000, batch:   808/ 1052, ite: 254260] train loss: 0.003844, tar: 0.000036 
l0: 0.000008, l1: 0.000008, l2: 0.000032, l3: 0.000118, l4: 0.000276, l5: 0.001000, l6: 0.001890
[epoch: 967/1000, batch:   888/ 1052, ite: 254280] train loss: 0.003847, tar: 0.000036 
l0: 0.000063, l1: 0.000064, l2: 0.000101, l3: 0.000179, l4: 0.000533, l5: 0.001074, l6: 0.001999
[epoch: 967/1000, batch:   968/ 1052, ite: 254300] train loss: 0.003843, tar: 0.000036 
l0: 0.000005, l1: 0.000005, l2: 0.000033, l3: 0.000105, l4: 0.000381, l5: 0.001146, l6: 0.002104
[epoch: 967/1000, batch:  1048/ 1052, ite: 254320] train loss: 0.003842, tar: 0.000036 
[Epoch 967/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000066, l1: 0.000067, l2: 0.000094, l3: 0.000138, l4: 0.000488, l5: 0.000930, l6: 0.002480
[epoch: 968/1000, batch:    76/ 1052, ite: 254340] train loss: 0.003839, tar: 0.000036 
l0: 0.000036, l1: 0.000036, l2: 0.000053, l3: 0.000109, l4: 0.000317, l5: 0.000511, l6: 0.001410
[epoch: 968/1000, batch:   156/ 1052, ite: 254360] train loss: 0.003838, tar: 0.000036 
l0: 0.000008, l1: 0.000009, l2: 0.000043, l3: 0.000131, l4: 0.000235, l5: 0.000894, l6: 0.001382
[epoch: 968/1000, batch:   236/ 1052, ite: 254380] train loss: 0.003844, tar: 0.000037 
l0: 0.000007, l1: 0.000007, l2: 0.000045, l3: 0.000273, l4: 0.000784, l5: 0.001121, l6: 0.002411
[epoch: 968/1000, batch:   316/ 1052, ite: 254400] train loss: 0.003847, tar: 0.000037 
l0: 0.000038, l1: 0.000038, l2: 0.000057, l3: 0.000173, l4: 0.000347, l5: 0.000688, l6: 0.001139
[epoch: 968/1000, batch:   396/ 1052, ite: 254420] train loss: 0.003846, tar: 0.000037 
l0: 0.000056, l1: 0.000057, l2: 0.000077, l3: 0.000155, l4: 0.000359, l5: 0.001001, l6: 0.002602
[epoch: 968/1000, batch:   476/ 1052, ite: 254440] train loss: 0.003845, tar: 0.000037 
l0: 0.000030, l1: 0.000035, l2: 0.000030, l3: 0.000145, l4: 0.000317, l5: 0.000918, l6: 0.000962
[epoch: 968/1000, batch:   556/ 1052, ite: 254460] train loss: 0.003847, tar: 0.000037 
l0: 0.000029, l1: 0.000030, l2: 0.000054, l3: 0.000101, l4: 0.000474, l5: 0.001002, l6: 0.001552
[epoch: 968/1000, batch:   636/ 1052, ite: 254480] train loss: 0.003846, tar: 0.000037 
l0: 0.000015, l1: 0.000014, l2: 0.000041, l3: 0.000157, l4: 0.000389, l5: 0.001378, l6: 0.002495
[epoch: 968/1000, batch:   716/ 1052, ite: 254500] train loss: 0.003846, tar: 0.000037 
l0: 0.000008, l1: 0.000010, l2: 0.000030, l3: 0.000108, l4: 0.000394, l5: 0.001436, l6: 0.002172
[epoch: 968/1000, batch:   796/ 1052, ite: 254520] train loss: 0.003845, tar: 0.000037 
l0: 0.000017, l1: 0.000017, l2: 0.000057, l3: 0.000145, l4: 0.000384, l5: 0.001219, l6: 0.002453
[epoch: 968/1000, batch:   876/ 1052, ite: 254540] train loss: 0.003841, tar: 0.000037 
l0: 0.000030, l1: 0.000030, l2: 0.000037, l3: 0.000083, l4: 0.000324, l5: 0.000728, l6: 0.001973
[epoch: 968/1000, batch:   956/ 1052, ite: 254560] train loss: 0.003839, tar: 0.000036 
l0: 0.000004, l1: 0.000004, l2: 0.000025, l3: 0.000070, l4: 0.000318, l5: 0.001164, l6: 0.001927
[epoch: 968/1000, batch:  1036/ 1052, ite: 254580] train loss: 0.003838, tar: 0.000036 
[Epoch 968/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000071, l1: 0.000070, l2: 0.000097, l3: 0.000167, l4: 0.000505, l5: 0.000952, l6: 0.002343
[epoch: 969/1000, batch:    64/ 1052, ite: 254600] train loss: 0.003840, tar: 0.000036 
l0: 0.000001, l1: 0.000001, l2: 0.000003, l3: 0.000030, l4: 0.000113, l5: 0.000414, l6: 0.001071
[epoch: 969/1000, batch:   144/ 1052, ite: 254620] train loss: 0.003840, tar: 0.000036 
l0: 0.000006, l1: 0.000006, l2: 0.000015, l3: 0.000125, l4: 0.000404, l5: 0.000886, l6: 0.001152
[epoch: 969/1000, batch:   224/ 1052, ite: 254640] train loss: 0.003838, tar: 0.000036 
l0: 0.000003, l1: 0.000003, l2: 0.000031, l3: 0.000137, l4: 0.000288, l5: 0.001148, l6: 0.003122
[epoch: 969/1000, batch:   304/ 1052, ite: 254660] train loss: 0.003833, tar: 0.000036 
l0: 0.000001, l1: 0.000001, l2: 0.000038, l3: 0.000131, l4: 0.000352, l5: 0.000585, l6: 0.001317
[epoch: 969/1000, batch:   384/ 1052, ite: 254680] train loss: 0.003836, tar: 0.000036 
l0: 0.000015, l1: 0.000015, l2: 0.000022, l3: 0.000072, l4: 0.000418, l5: 0.000983, l6: 0.001115
[epoch: 969/1000, batch:   464/ 1052, ite: 254700] train loss: 0.003832, tar: 0.000036 
l0: 0.000017, l1: 0.000018, l2: 0.000028, l3: 0.000105, l4: 0.000423, l5: 0.001715, l6: 0.001837
[epoch: 969/1000, batch:   544/ 1052, ite: 254720] train loss: 0.003837, tar: 0.000036 
l0: 0.000105, l1: 0.000108, l2: 0.000133, l3: 0.000239, l4: 0.000489, l5: 0.001021, l6: 0.002358
[epoch: 969/1000, batch:   624/ 1052, ite: 254740] train loss: 0.003839, tar: 0.000036 
l0: 0.000016, l1: 0.000016, l2: 0.000054, l3: 0.000185, l4: 0.000810, l5: 0.001343, l6: 0.002408
[epoch: 969/1000, batch:   704/ 1052, ite: 254760] train loss: 0.003839, tar: 0.000037 
l0: 0.000047, l1: 0.000045, l2: 0.000083, l3: 0.000178, l4: 0.000404, l5: 0.001703, l6: 0.002283
[epoch: 969/1000, batch:   784/ 1052, ite: 254780] train loss: 0.003840, tar: 0.000037 
l0: 0.000020, l1: 0.000020, l2: 0.000054, l3: 0.000179, l4: 0.000619, l5: 0.001069, l6: 0.002121
[epoch: 969/1000, batch:   864/ 1052, ite: 254800] train loss: 0.003838, tar: 0.000037 
l0: 0.000034, l1: 0.000036, l2: 0.000056, l3: 0.000140, l4: 0.000464, l5: 0.001228, l6: 0.001632
[epoch: 969/1000, batch:   944/ 1052, ite: 254820] train loss: 0.003834, tar: 0.000037 
l0: 0.000013, l1: 0.000012, l2: 0.000042, l3: 0.000214, l4: 0.000641, l5: 0.001670, l6: 0.002290
[epoch: 969/1000, batch:  1024/ 1052, ite: 254840] train loss: 0.003839, tar: 0.000037 
[Epoch 969/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000004, l1: 0.000004, l2: 0.000016, l3: 0.000054, l4: 0.000273, l5: 0.000854, l6: 0.001330
[epoch: 970/1000, batch:    52/ 1052, ite: 254860] train loss: 0.003844, tar: 0.000037 
l0: 0.000001, l1: 0.000001, l2: 0.000021, l3: 0.000125, l4: 0.000508, l5: 0.000841, l6: 0.001736
[epoch: 970/1000, batch:   132/ 1052, ite: 254880] train loss: 0.003852, tar: 0.000037 
l0: 0.000002, l1: 0.000001, l2: 0.000009, l3: 0.000064, l4: 0.000319, l5: 0.000718, l6: 0.001326
[epoch: 970/1000, batch:   212/ 1052, ite: 254900] train loss: 0.003848, tar: 0.000037 
l0: 0.000005, l1: 0.000006, l2: 0.000010, l3: 0.000047, l4: 0.000268, l5: 0.000813, l6: 0.000967
[epoch: 970/1000, batch:   292/ 1052, ite: 254920] train loss: 0.003848, tar: 0.000037 
l0: 0.000015, l1: 0.000016, l2: 0.000029, l3: 0.000030, l4: 0.000344, l5: 0.000513, l6: 0.000926
[epoch: 970/1000, batch:   372/ 1052, ite: 254940] train loss: 0.003850, tar: 0.000037 
l0: 0.000068, l1: 0.000069, l2: 0.000094, l3: 0.000204, l4: 0.000458, l5: 0.001602, l6: 0.002894
[epoch: 970/1000, batch:   452/ 1052, ite: 254960] train loss: 0.003850, tar: 0.000037 
l0: 0.000096, l1: 0.000099, l2: 0.000104, l3: 0.000136, l4: 0.000342, l5: 0.000783, l6: 0.001762
[epoch: 970/1000, batch:   532/ 1052, ite: 254980] train loss: 0.003844, tar: 0.000037 
l0: 0.000071, l1: 0.000070, l2: 0.000108, l3: 0.000193, l4: 0.000465, l5: 0.000641, l6: 0.001095
[epoch: 970/1000, batch:   612/ 1052, ite: 255000] train loss: 0.003842, tar: 0.000037 
l0: 0.000044, l1: 0.000041, l2: 0.000119, l3: 0.000414, l4: 0.000819, l5: 0.002412, l6: 0.004216
[epoch: 970/1000, batch:   692/ 1052, ite: 255020] train loss: 0.003847, tar: 0.000037 
l0: 0.000005, l1: 0.000005, l2: 0.000017, l3: 0.000044, l4: 0.000225, l5: 0.000685, l6: 0.001161
[epoch: 970/1000, batch:   772/ 1052, ite: 255040] train loss: 0.003846, tar: 0.000037 
l0: 0.000007, l1: 0.000007, l2: 0.000023, l3: 0.000102, l4: 0.000435, l5: 0.001402, l6: 0.002178
[epoch: 970/1000, batch:   852/ 1052, ite: 255060] train loss: 0.003845, tar: 0.000037 
l0: 0.000056, l1: 0.000056, l2: 0.000078, l3: 0.000126, l4: 0.000294, l5: 0.001236, l6: 0.001302
[epoch: 970/1000, batch:   932/ 1052, ite: 255080] train loss: 0.003846, tar: 0.000037 
l0: 0.000048, l1: 0.000050, l2: 0.000060, l3: 0.000160, l4: 0.000364, l5: 0.000829, l6: 0.001582
[epoch: 970/1000, batch:  1012/ 1052, ite: 255100] train loss: 0.003847, tar: 0.000037 
[Epoch 970/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000043, l1: 0.000043, l2: 0.000065, l3: 0.000152, l4: 0.000431, l5: 0.000901, l6: 0.000998
[epoch: 971/1000, batch:    40/ 1052, ite: 255120] train loss: 0.003845, tar: 0.000037 
l0: 0.000039, l1: 0.000038, l2: 0.000062, l3: 0.000203, l4: 0.000520, l5: 0.001158, l6: 0.002070
[epoch: 971/1000, batch:   120/ 1052, ite: 255140] train loss: 0.003843, tar: 0.000037 
l0: 0.000117, l1: 0.000118, l2: 0.000145, l3: 0.000424, l4: 0.000938, l5: 0.001890, l6: 0.003733
[epoch: 971/1000, batch:   200/ 1052, ite: 255160] train loss: 0.003846, tar: 0.000037 
l0: 0.000010, l1: 0.000009, l2: 0.000024, l3: 0.000129, l4: 0.000782, l5: 0.002229, l6: 0.003071
[epoch: 971/1000, batch:   280/ 1052, ite: 255180] train loss: 0.003846, tar: 0.000037 
l0: 0.000036, l1: 0.000035, l2: 0.000055, l3: 0.000169, l4: 0.000495, l5: 0.001501, l6: 0.002167
[epoch: 971/1000, batch:   360/ 1052, ite: 255200] train loss: 0.003848, tar: 0.000037 
l0: 0.000017, l1: 0.000017, l2: 0.000056, l3: 0.000194, l4: 0.000344, l5: 0.001275, l6: 0.002655
[epoch: 971/1000, batch:   440/ 1052, ite: 255220] train loss: 0.003855, tar: 0.000037 
l0: 0.000131, l1: 0.000132, l2: 0.000178, l3: 0.000369, l4: 0.000807, l5: 0.001295, l6: 0.002439
[epoch: 971/1000, batch:   520/ 1052, ite: 255240] train loss: 0.003855, tar: 0.000038 
l0: 0.000014, l1: 0.000014, l2: 0.000038, l3: 0.000121, l4: 0.000357, l5: 0.000729, l6: 0.001574
[epoch: 971/1000, batch:   600/ 1052, ite: 255260] train loss: 0.003856, tar: 0.000038 
l0: 0.000014, l1: 0.000015, l2: 0.000031, l3: 0.000205, l4: 0.000475, l5: 0.001382, l6: 0.002679
[epoch: 971/1000, batch:   680/ 1052, ite: 255280] train loss: 0.003855, tar: 0.000038 
l0: 0.000048, l1: 0.000049, l2: 0.000059, l3: 0.000148, l4: 0.000377, l5: 0.000900, l6: 0.001924
[epoch: 971/1000, batch:   760/ 1052, ite: 255300] train loss: 0.003850, tar: 0.000037 
l0: 0.000004, l1: 0.000005, l2: 0.000018, l3: 0.000121, l4: 0.000501, l5: 0.000914, l6: 0.001733
[epoch: 971/1000, batch:   840/ 1052, ite: 255320] train loss: 0.003851, tar: 0.000037 
l0: 0.000023, l1: 0.000024, l2: 0.000075, l3: 0.000235, l4: 0.000548, l5: 0.001062, l6: 0.002094
[epoch: 971/1000, batch:   920/ 1052, ite: 255340] train loss: 0.003848, tar: 0.000037 
l0: 0.000003, l1: 0.000002, l2: 0.000025, l3: 0.000169, l4: 0.000484, l5: 0.001198, l6: 0.002133
[epoch: 971/1000, batch:  1000/ 1052, ite: 255360] train loss: 0.003849, tar: 0.000037 
[Epoch 971/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000030, l1: 0.000028, l2: 0.000061, l3: 0.000133, l4: 0.000521, l5: 0.001346, l6: 0.001967
[epoch: 972/1000, batch:    28/ 1052, ite: 255380] train loss: 0.003850, tar: 0.000037 
l0: 0.000007, l1: 0.000007, l2: 0.000024, l3: 0.000153, l4: 0.000727, l5: 0.002348, l6: 0.002908
[epoch: 972/1000, batch:   108/ 1052, ite: 255400] train loss: 0.003850, tar: 0.000037 
l0: 0.000001, l1: 0.000001, l2: 0.000039, l3: 0.000068, l4: 0.000254, l5: 0.000690, l6: 0.001575
[epoch: 972/1000, batch:   188/ 1052, ite: 255420] train loss: 0.003850, tar: 0.000037 
l0: 0.000012, l1: 0.000013, l2: 0.000080, l3: 0.000338, l4: 0.000728, l5: 0.001761, l6: 0.002865
[epoch: 972/1000, batch:   268/ 1052, ite: 255440] train loss: 0.003852, tar: 0.000037 
l0: 0.000049, l1: 0.000051, l2: 0.000078, l3: 0.000208, l4: 0.000412, l5: 0.000748, l6: 0.001259
[epoch: 972/1000, batch:   348/ 1052, ite: 255460] train loss: 0.003852, tar: 0.000037 
l0: 0.000003, l1: 0.000003, l2: 0.000014, l3: 0.000082, l4: 0.000312, l5: 0.000609, l6: 0.001230
[epoch: 972/1000, batch:   428/ 1052, ite: 255480] train loss: 0.003848, tar: 0.000037 
l0: 0.000065, l1: 0.000065, l2: 0.000092, l3: 0.000165, l4: 0.000632, l5: 0.001533, l6: 0.001927
[epoch: 972/1000, batch:   508/ 1052, ite: 255500] train loss: 0.003850, tar: 0.000037 
l0: 0.000060, l1: 0.000065, l2: 0.000059, l3: 0.000076, l4: 0.000358, l5: 0.000671, l6: 0.001623
[epoch: 972/1000, batch:   588/ 1052, ite: 255520] train loss: 0.003850, tar: 0.000037 
l0: 0.000026, l1: 0.000027, l2: 0.000054, l3: 0.000161, l4: 0.000422, l5: 0.000728, l6: 0.001803
[epoch: 972/1000, batch:   668/ 1052, ite: 255540] train loss: 0.003849, tar: 0.000037 
l0: 0.000039, l1: 0.000039, l2: 0.000068, l3: 0.000152, l4: 0.000303, l5: 0.000842, l6: 0.001500
[epoch: 972/1000, batch:   748/ 1052, ite: 255560] train loss: 0.003849, tar: 0.000037 
l0: 0.000018, l1: 0.000019, l2: 0.000030, l3: 0.000126, l4: 0.000571, l5: 0.001357, l6: 0.001947
[epoch: 972/1000, batch:   828/ 1052, ite: 255580] train loss: 0.003848, tar: 0.000037 
l0: 0.000144, l1: 0.000144, l2: 0.000175, l3: 0.000300, l4: 0.000848, l5: 0.001810, l6: 0.002719
[epoch: 972/1000, batch:   908/ 1052, ite: 255600] train loss: 0.003848, tar: 0.000037 
l0: 0.000074, l1: 0.000074, l2: 0.000165, l3: 0.000186, l4: 0.000339, l5: 0.001176, l6: 0.001530
[epoch: 972/1000, batch:   988/ 1052, ite: 255620] train loss: 0.003847, tar: 0.000037 
[Epoch 972/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000039, l1: 0.000041, l2: 0.000084, l3: 0.000154, l4: 0.000398, l5: 0.001060, l6: 0.001293
[epoch: 973/1000, batch:    16/ 1052, ite: 255640] train loss: 0.003845, tar: 0.000037 
l0: 0.000023, l1: 0.000022, l2: 0.000050, l3: 0.000105, l4: 0.000478, l5: 0.000664, l6: 0.001346
[epoch: 973/1000, batch:    96/ 1052, ite: 255660] train loss: 0.003844, tar: 0.000037 
l0: 0.000015, l1: 0.000014, l2: 0.000049, l3: 0.000254, l4: 0.000412, l5: 0.001183, l6: 0.002763
[epoch: 973/1000, batch:   176/ 1052, ite: 255680] train loss: 0.003843, tar: 0.000037 
l0: 0.000031, l1: 0.000028, l2: 0.000071, l3: 0.000143, l4: 0.000407, l5: 0.000841, l6: 0.002059
[epoch: 973/1000, batch:   256/ 1052, ite: 255700] train loss: 0.003846, tar: 0.000038 
l0: 0.000002, l1: 0.000002, l2: 0.000013, l3: 0.000093, l4: 0.000315, l5: 0.000669, l6: 0.001667
[epoch: 973/1000, batch:   336/ 1052, ite: 255720] train loss: 0.003845, tar: 0.000038 
l0: 0.000035, l1: 0.000033, l2: 0.000038, l3: 0.000125, l4: 0.000255, l5: 0.000804, l6: 0.001274
[epoch: 973/1000, batch:   416/ 1052, ite: 255740] train loss: 0.003843, tar: 0.000038 
l0: 0.000013, l1: 0.000014, l2: 0.000033, l3: 0.000123, l4: 0.000350, l5: 0.001342, l6: 0.002184
[epoch: 973/1000, batch:   496/ 1052, ite: 255760] train loss: 0.003847, tar: 0.000038 
l0: 0.000029, l1: 0.000029, l2: 0.000069, l3: 0.000165, l4: 0.000487, l5: 0.000978, l6: 0.001778
[epoch: 973/1000, batch:   576/ 1052, ite: 255780] train loss: 0.003847, tar: 0.000038 
l0: 0.000149, l1: 0.000154, l2: 0.000175, l3: 0.000304, l4: 0.000512, l5: 0.000989, l6: 0.001645
[epoch: 973/1000, batch:   656/ 1052, ite: 255800] train loss: 0.003849, tar: 0.000038 
l0: 0.000125, l1: 0.000126, l2: 0.000133, l3: 0.000251, l4: 0.000462, l5: 0.000641, l6: 0.001518
[epoch: 973/1000, batch:   736/ 1052, ite: 255820] train loss: 0.003845, tar: 0.000038 
l0: 0.000008, l1: 0.000007, l2: 0.000024, l3: 0.000100, l4: 0.000303, l5: 0.000591, l6: 0.000714
[epoch: 973/1000, batch:   816/ 1052, ite: 255840] train loss: 0.003846, tar: 0.000038 
l0: 0.000066, l1: 0.000068, l2: 0.000108, l3: 0.000256, l4: 0.000724, l5: 0.002170, l6: 0.003536
[epoch: 973/1000, batch:   896/ 1052, ite: 255860] train loss: 0.003845, tar: 0.000038 
l0: 0.000080, l1: 0.000079, l2: 0.000114, l3: 0.000235, l4: 0.000711, l5: 0.001307, l6: 0.001702
[epoch: 973/1000, batch:   976/ 1052, ite: 255880] train loss: 0.003845, tar: 0.000038 
[Epoch 973/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000087, l1: 0.000087, l2: 0.000197, l3: 0.000337, l4: 0.000554, l5: 0.001617, l6: 0.004079
[epoch: 974/1000, batch:     4/ 1052, ite: 255900] train loss: 0.003846, tar: 0.000038 
l0: 0.000006, l1: 0.000006, l2: 0.000013, l3: 0.000080, l4: 0.000260, l5: 0.000632, l6: 0.001639
[epoch: 974/1000, batch:    84/ 1052, ite: 255920] train loss: 0.003846, tar: 0.000038 
l0: 0.000028, l1: 0.000029, l2: 0.000057, l3: 0.000167, l4: 0.000438, l5: 0.000836, l6: 0.001306
[epoch: 974/1000, batch:   164/ 1052, ite: 255940] train loss: 0.003847, tar: 0.000038 
l0: 0.000027, l1: 0.000027, l2: 0.000045, l3: 0.000180, l4: 0.000390, l5: 0.000787, l6: 0.002255
[epoch: 974/1000, batch:   244/ 1052, ite: 255960] train loss: 0.003847, tar: 0.000038 
l0: 0.000039, l1: 0.000039, l2: 0.000076, l3: 0.000141, l4: 0.000509, l5: 0.001888, l6: 0.002590
[epoch: 974/1000, batch:   324/ 1052, ite: 255980] train loss: 0.003851, tar: 0.000038 
l0: 0.000000, l1: 0.000000, l2: 0.000008, l3: 0.000015, l4: 0.000099, l5: 0.000425, l6: 0.000875
[epoch: 974/1000, batch:   404/ 1052, ite: 256000] train loss: 0.003848, tar: 0.000038 
l0: 0.000010, l1: 0.000010, l2: 0.000038, l3: 0.000141, l4: 0.000321, l5: 0.000787, l6: 0.001613
[epoch: 974/1000, batch:   484/ 1052, ite: 256020] train loss: 0.003846, tar: 0.000038 
l0: 0.000009, l1: 0.000009, l2: 0.000022, l3: 0.000111, l4: 0.000329, l5: 0.000973, l6: 0.001603
[epoch: 974/1000, batch:   564/ 1052, ite: 256040] train loss: 0.003847, tar: 0.000038 
l0: 0.000057, l1: 0.000056, l2: 0.000088, l3: 0.000268, l4: 0.000538, l5: 0.001078, l6: 0.001837
[epoch: 974/1000, batch:   644/ 1052, ite: 256060] train loss: 0.003847, tar: 0.000038 
l0: 0.000008, l1: 0.000009, l2: 0.000015, l3: 0.000098, l4: 0.000636, l5: 0.000955, l6: 0.001130
[epoch: 974/1000, batch:   724/ 1052, ite: 256080] train loss: 0.003846, tar: 0.000038 
l0: 0.000017, l1: 0.000018, l2: 0.000048, l3: 0.000114, l4: 0.000287, l5: 0.001340, l6: 0.001308
[epoch: 974/1000, batch:   804/ 1052, ite: 256100] train loss: 0.003845, tar: 0.000038 
l0: 0.000023, l1: 0.000025, l2: 0.000047, l3: 0.000126, l4: 0.000388, l5: 0.000803, l6: 0.001705
[epoch: 974/1000, batch:   884/ 1052, ite: 256120] train loss: 0.003846, tar: 0.000038 
l0: 0.000040, l1: 0.000039, l2: 0.000057, l3: 0.000159, l4: 0.000506, l5: 0.000475, l6: 0.001005
[epoch: 974/1000, batch:   964/ 1052, ite: 256140] train loss: 0.003846, tar: 0.000038 
l0: 0.000012, l1: 0.000011, l2: 0.000035, l3: 0.000165, l4: 0.000482, l5: 0.000863, l6: 0.001660
[epoch: 974/1000, batch:  1044/ 1052, ite: 256160] train loss: 0.003846, tar: 0.000038 
[Epoch 974/1000] Test loss: 0.014, Test target loss: 0.002
l0: 0.000035, l1: 0.000035, l2: 0.000056, l3: 0.000124, l4: 0.000324, l5: 0.000892, l6: 0.001299
[epoch: 975/1000, batch:    72/ 1052, ite: 256180] train loss: 0.003846, tar: 0.000038 
l0: 0.000052, l1: 0.000054, l2: 0.000076, l3: 0.000239, l4: 0.000553, l5: 0.001291, l6: 0.002375
[epoch: 975/1000, batch:   152/ 1052, ite: 256200] train loss: 0.003846, tar: 0.000038 
l0: 0.000049, l1: 0.000050, l2: 0.000062, l3: 0.000134, l4: 0.000432, l5: 0.000894, l6: 0.001107
[epoch: 975/1000, batch:   232/ 1052, ite: 256220] train loss: 0.003848, tar: 0.000038 
l0: 0.000005, l1: 0.000005, l2: 0.000004, l3: 0.000057, l4: 0.000267, l5: 0.000475, l6: 0.001027
[epoch: 975/1000, batch:   312/ 1052, ite: 256240] train loss: 0.003848, tar: 0.000038 
l0: 0.000040, l1: 0.000039, l2: 0.000067, l3: 0.000173, l4: 0.000330, l5: 0.000728, l6: 0.001809
[epoch: 975/1000, batch:   392/ 1052, ite: 256260] train loss: 0.003850, tar: 0.000038 
l0: 0.000001, l1: 0.000001, l2: 0.000007, l3: 0.000036, l4: 0.000224, l5: 0.000598, l6: 0.001096
[epoch: 975/1000, batch:   472/ 1052, ite: 256280] train loss: 0.003849, tar: 0.000038 
l0: 0.000040, l1: 0.000039, l2: 0.000055, l3: 0.000127, l4: 0.000268, l5: 0.000721, l6: 0.000788
[epoch: 975/1000, batch:   552/ 1052, ite: 256300] train loss: 0.003848, tar: 0.000038 
l0: 0.000056, l1: 0.000056, l2: 0.000084, l3: 0.000238, l4: 0.000787, l5: 0.001205, l6: 0.002510
[epoch: 975/1000, batch:   632/ 1052, ite: 256320] train loss: 0.003850, tar: 0.000038 
l0: 0.000011, l1: 0.000011, l2: 0.000019, l3: 0.000092, l4: 0.000437, l5: 0.001126, l6: 0.001946
[epoch: 975/1000, batch:   712/ 1052, ite: 256340] train loss: 0.003850, tar: 0.000038 
l0: 0.000005, l1: 0.000005, l2: 0.000019, l3: 0.000176, l4: 0.000641, l5: 0.000936, l6: 0.001953
[epoch: 975/1000, batch:   792/ 1052, ite: 256360] train loss: 0.003850, tar: 0.000038 
l0: 0.000061, l1: 0.000062, l2: 0.000139, l3: 0.000370, l4: 0.000788, l5: 0.001234, l6: 0.005222
[epoch: 975/1000, batch:   872/ 1052, ite: 256380] train loss: 0.003852, tar: 0.000038 
l0: 0.000193, l1: 0.000193, l2: 0.000260, l3: 0.000327, l4: 0.000605, l5: 0.001174, l6: 0.002461
[epoch: 975/1000, batch:   952/ 1052, ite: 256400] train loss: 0.003854, tar: 0.000038 
l0: 0.000019, l1: 0.000018, l2: 0.000029, l3: 0.000107, l4: 0.000206, l5: 0.000814, l6: 0.001662
[epoch: 975/1000, batch:  1032/ 1052, ite: 256420] train loss: 0.003854, tar: 0.000038 
[Epoch 975/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000004, l1: 0.000004, l2: 0.000040, l3: 0.000159, l4: 0.000491, l5: 0.000863, l6: 0.001957
[epoch: 976/1000, batch:    60/ 1052, ite: 256440] train loss: 0.003853, tar: 0.000038 
l0: 0.000006, l1: 0.000005, l2: 0.000024, l3: 0.000262, l4: 0.000452, l5: 0.000702, l6: 0.002101
[epoch: 976/1000, batch:   140/ 1052, ite: 256460] train loss: 0.003854, tar: 0.000038 
l0: 0.000023, l1: 0.000024, l2: 0.000056, l3: 0.000172, l4: 0.000517, l5: 0.000876, l6: 0.001097
[epoch: 976/1000, batch:   220/ 1052, ite: 256480] train loss: 0.003852, tar: 0.000038 
l0: 0.000001, l1: 0.000001, l2: 0.000007, l3: 0.000060, l4: 0.000390, l5: 0.000892, l6: 0.001091
[epoch: 976/1000, batch:   300/ 1052, ite: 256500] train loss: 0.003850, tar: 0.000038 
l0: 0.000049, l1: 0.000050, l2: 0.000079, l3: 0.000234, l4: 0.000403, l5: 0.001027, l6: 0.001221
[epoch: 976/1000, batch:   380/ 1052, ite: 256520] train loss: 0.003849, tar: 0.000038 
l0: 0.000020, l1: 0.000020, l2: 0.000041, l3: 0.000176, l4: 0.000399, l5: 0.000897, l6: 0.001606
[epoch: 976/1000, batch:   460/ 1052, ite: 256540] train loss: 0.003849, tar: 0.000038 
l0: 0.000004, l1: 0.000005, l2: 0.000021, l3: 0.000096, l4: 0.000310, l5: 0.000922, l6: 0.001393
[epoch: 976/1000, batch:   540/ 1052, ite: 256560] train loss: 0.003849, tar: 0.000038 
l0: 0.000023, l1: 0.000021, l2: 0.000061, l3: 0.000236, l4: 0.000557, l5: 0.001314, l6: 0.003068
[epoch: 976/1000, batch:   620/ 1052, ite: 256580] train loss: 0.003853, tar: 0.000038 
l0: 0.000033, l1: 0.000032, l2: 0.000086, l3: 0.000368, l4: 0.001080, l5: 0.002944, l6: 0.004356
[epoch: 976/1000, batch:   700/ 1052, ite: 256600] train loss: 0.003852, tar: 0.000038 
l0: 0.000079, l1: 0.000081, l2: 0.000116, l3: 0.000204, l4: 0.000326, l5: 0.000838, l6: 0.001305
[epoch: 976/1000, batch:   780/ 1052, ite: 256620] train loss: 0.003854, tar: 0.000038 
l0: 0.000026, l1: 0.000025, l2: 0.000053, l3: 0.000117, l4: 0.000237, l5: 0.000988, l6: 0.001672
[epoch: 976/1000, batch:   860/ 1052, ite: 256640] train loss: 0.003854, tar: 0.000038 
l0: 0.000004, l1: 0.000006, l2: 0.000037, l3: 0.000079, l4: 0.000362, l5: 0.000620, l6: 0.001778
[epoch: 976/1000, batch:   940/ 1052, ite: 256660] train loss: 0.003853, tar: 0.000038 
l0: 0.000022, l1: 0.000022, l2: 0.000034, l3: 0.000125, l4: 0.000211, l5: 0.000727, l6: 0.000910
[epoch: 976/1000, batch:  1020/ 1052, ite: 256680] train loss: 0.003855, tar: 0.000038 
[Epoch 976/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000029, l1: 0.000031, l2: 0.000045, l3: 0.000167, l4: 0.000499, l5: 0.000821, l6: 0.002121
[epoch: 977/1000, batch:    48/ 1052, ite: 256700] train loss: 0.003854, tar: 0.000038 
l0: 0.000064, l1: 0.000064, l2: 0.000075, l3: 0.000210, l4: 0.000457, l5: 0.000976, l6: 0.001998
[epoch: 977/1000, batch:   128/ 1052, ite: 256720] train loss: 0.003852, tar: 0.000038 
l0: 0.000015, l1: 0.000015, l2: 0.000055, l3: 0.000148, l4: 0.000355, l5: 0.001128, l6: 0.002898
[epoch: 977/1000, batch:   208/ 1052, ite: 256740] train loss: 0.003853, tar: 0.000038 
l0: 0.000091, l1: 0.000090, l2: 0.000107, l3: 0.000157, l4: 0.000266, l5: 0.000897, l6: 0.001288
[epoch: 977/1000, batch:   288/ 1052, ite: 256760] train loss: 0.003855, tar: 0.000039 
l0: 0.000020, l1: 0.000020, l2: 0.000032, l3: 0.000112, l4: 0.000344, l5: 0.000724, l6: 0.001203
[epoch: 977/1000, batch:   368/ 1052, ite: 256780] train loss: 0.003857, tar: 0.000039 
l0: 0.000012, l1: 0.000015, l2: 0.000008, l3: 0.000048, l4: 0.000131, l5: 0.000635, l6: 0.001133
[epoch: 977/1000, batch:   448/ 1052, ite: 256800] train loss: 0.003857, tar: 0.000039 
l0: 0.000045, l1: 0.000045, l2: 0.000067, l3: 0.000143, l4: 0.000412, l5: 0.000551, l6: 0.001541
[epoch: 977/1000, batch:   528/ 1052, ite: 256820] train loss: 0.003855, tar: 0.000039 
l0: 0.000056, l1: 0.000057, l2: 0.000079, l3: 0.000121, l4: 0.000342, l5: 0.001090, l6: 0.002110
[epoch: 977/1000, batch:   608/ 1052, ite: 256840] train loss: 0.003858, tar: 0.000039 
l0: 0.000022, l1: 0.000023, l2: 0.000032, l3: 0.000108, l4: 0.000372, l5: 0.000737, l6: 0.001906
[epoch: 977/1000, batch:   688/ 1052, ite: 256860] train loss: 0.003858, tar: 0.000039 
l0: 0.000091, l1: 0.000092, l2: 0.000124, l3: 0.000379, l4: 0.000945, l5: 0.002432, l6: 0.003011
[epoch: 977/1000, batch:   768/ 1052, ite: 256880] train loss: 0.003861, tar: 0.000039 
l0: 0.000021, l1: 0.000020, l2: 0.000031, l3: 0.000086, l4: 0.000260, l5: 0.001141, l6: 0.001574
[epoch: 977/1000, batch:   848/ 1052, ite: 256900] train loss: 0.003860, tar: 0.000039 
l0: 0.000024, l1: 0.000026, l2: 0.000054, l3: 0.000092, l4: 0.000320, l5: 0.001010, l6: 0.001548
[epoch: 977/1000, batch:   928/ 1052, ite: 256920] train loss: 0.003860, tar: 0.000039 
l0: 0.000030, l1: 0.000030, l2: 0.000041, l3: 0.000066, l4: 0.000190, l5: 0.000552, l6: 0.000826
[epoch: 977/1000, batch:  1008/ 1052, ite: 256940] train loss: 0.003859, tar: 0.000039 
[Epoch 977/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000246, l1: 0.000249, l2: 0.000321, l3: 0.000530, l4: 0.001079, l5: 0.002384, l6: 0.004381
[epoch: 978/1000, batch:    36/ 1052, ite: 256960] train loss: 0.003859, tar: 0.000039 
l0: 0.000002, l1: 0.000001, l2: 0.000013, l3: 0.000131, l4: 0.000386, l5: 0.000862, l6: 0.001472
[epoch: 978/1000, batch:   116/ 1052, ite: 256980] train loss: 0.003860, tar: 0.000039 
l0: 0.000020, l1: 0.000021, l2: 0.000025, l3: 0.000089, l4: 0.000323, l5: 0.000665, l6: 0.000702
[epoch: 978/1000, batch:   196/ 1052, ite: 257000] train loss: 0.003858, tar: 0.000039 
l0: 0.000047, l1: 0.000046, l2: 0.000065, l3: 0.000179, l4: 0.000468, l5: 0.000973, l6: 0.001839
[epoch: 978/1000, batch:   276/ 1052, ite: 257020] train loss: 0.003857, tar: 0.000039 
l0: 0.000016, l1: 0.000016, l2: 0.000041, l3: 0.000157, l4: 0.000317, l5: 0.000780, l6: 0.001527
[epoch: 978/1000, batch:   356/ 1052, ite: 257040] train loss: 0.003858, tar: 0.000039 
l0: 0.000001, l1: 0.000001, l2: 0.000019, l3: 0.000064, l4: 0.000357, l5: 0.000963, l6: 0.001518
[epoch: 978/1000, batch:   436/ 1052, ite: 257060] train loss: 0.003858, tar: 0.000039 
l0: 0.000027, l1: 0.000028, l2: 0.000044, l3: 0.000109, l4: 0.000316, l5: 0.000465, l6: 0.001486
[epoch: 978/1000, batch:   516/ 1052, ite: 257080] train loss: 0.003856, tar: 0.000039 
l0: 0.000006, l1: 0.000006, l2: 0.000020, l3: 0.000085, l4: 0.000335, l5: 0.000780, l6: 0.000668
[epoch: 978/1000, batch:   596/ 1052, ite: 257100] train loss: 0.003855, tar: 0.000039 
l0: 0.000113, l1: 0.000114, l2: 0.000142, l3: 0.000270, l4: 0.000657, l5: 0.001508, l6: 0.002799
[epoch: 978/1000, batch:   676/ 1052, ite: 257120] train loss: 0.003857, tar: 0.000039 
l0: 0.000037, l1: 0.000039, l2: 0.000040, l3: 0.000128, l4: 0.000459, l5: 0.000817, l6: 0.001520
[epoch: 978/1000, batch:   756/ 1052, ite: 257140] train loss: 0.003855, tar: 0.000039 
l0: 0.000015, l1: 0.000015, l2: 0.000027, l3: 0.000102, l4: 0.000249, l5: 0.000555, l6: 0.000975
[epoch: 978/1000, batch:   836/ 1052, ite: 257160] train loss: 0.003857, tar: 0.000039 
l0: 0.000011, l1: 0.000012, l2: 0.000036, l3: 0.000136, l4: 0.000611, l5: 0.001188, l6: 0.002722
[epoch: 978/1000, batch:   916/ 1052, ite: 257180] train loss: 0.003857, tar: 0.000039 
l0: 0.000013, l1: 0.000014, l2: 0.000048, l3: 0.000127, l4: 0.000406, l5: 0.001065, l6: 0.002002
[epoch: 978/1000, batch:   996/ 1052, ite: 257200] train loss: 0.003858, tar: 0.000039 
[Epoch 978/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000024, l1: 0.000024, l2: 0.000042, l3: 0.000145, l4: 0.000401, l5: 0.000495, l6: 0.001404
[epoch: 979/1000, batch:    24/ 1052, ite: 257220] train loss: 0.003857, tar: 0.000039 
l0: 0.000001, l1: 0.000001, l2: 0.000004, l3: 0.000042, l4: 0.000127, l5: 0.000685, l6: 0.001116
[epoch: 979/1000, batch:   104/ 1052, ite: 257240] train loss: 0.003856, tar: 0.000039 
l0: 0.000033, l1: 0.000032, l2: 0.000061, l3: 0.000123, l4: 0.000409, l5: 0.001000, l6: 0.002319
[epoch: 979/1000, batch:   184/ 1052, ite: 257260] train loss: 0.003854, tar: 0.000039 
l0: 0.000087, l1: 0.000088, l2: 0.000108, l3: 0.000165, l4: 0.000440, l5: 0.001169, l6: 0.002906
[epoch: 979/1000, batch:   264/ 1052, ite: 257280] train loss: 0.003854, tar: 0.000039 
l0: 0.000007, l1: 0.000007, l2: 0.000026, l3: 0.000108, l4: 0.000280, l5: 0.001091, l6: 0.002680
[epoch: 979/1000, batch:   344/ 1052, ite: 257300] train loss: 0.003856, tar: 0.000039 
l0: 0.000002, l1: 0.000002, l2: 0.000021, l3: 0.000149, l4: 0.000443, l5: 0.001011, l6: 0.002060
[epoch: 979/1000, batch:   424/ 1052, ite: 257320] train loss: 0.003856, tar: 0.000039 
l0: 0.000008, l1: 0.000007, l2: 0.000028, l3: 0.000103, l4: 0.000465, l5: 0.000878, l6: 0.001831
[epoch: 979/1000, batch:   504/ 1052, ite: 257340] train loss: 0.003856, tar: 0.000039 
l0: 0.000006, l1: 0.000008, l2: 0.000011, l3: 0.000164, l4: 0.000495, l5: 0.001077, l6: 0.001776
[epoch: 979/1000, batch:   584/ 1052, ite: 257360] train loss: 0.003855, tar: 0.000039 
l0: 0.000002, l1: 0.000002, l2: 0.000011, l3: 0.000084, l4: 0.000296, l5: 0.001104, l6: 0.001418
[epoch: 979/1000, batch:   664/ 1052, ite: 257380] train loss: 0.003855, tar: 0.000039 
l0: 0.000018, l1: 0.000019, l2: 0.000057, l3: 0.000153, l4: 0.000324, l5: 0.000669, l6: 0.001465
[epoch: 979/1000, batch:   744/ 1052, ite: 257400] train loss: 0.003853, tar: 0.000039 
l0: 0.000062, l1: 0.000064, l2: 0.000075, l3: 0.000282, l4: 0.000850, l5: 0.001440, l6: 0.003495
[epoch: 979/1000, batch:   824/ 1052, ite: 257420] train loss: 0.003855, tar: 0.000039 
l0: 0.000030, l1: 0.000031, l2: 0.000033, l3: 0.000104, l4: 0.000323, l5: 0.001031, l6: 0.001191
[epoch: 979/1000, batch:   904/ 1052, ite: 257440] train loss: 0.003854, tar: 0.000039 
l0: 0.000028, l1: 0.000028, l2: 0.000078, l3: 0.000183, l4: 0.000514, l5: 0.001205, l6: 0.001985
[epoch: 979/1000, batch:   984/ 1052, ite: 257460] train loss: 0.003853, tar: 0.000039 
[Epoch 979/1000] Test loss: 0.016, Test target loss: 0.002
l0: 0.000083, l1: 0.000084, l2: 0.000210, l3: 0.000683, l4: 0.001524, l5: 0.002130, l6: 0.005297
[epoch: 980/1000, batch:    12/ 1052, ite: 257480] train loss: 0.003856, tar: 0.000039 
l0: 0.000102, l1: 0.000104, l2: 0.000110, l3: 0.000194, l4: 0.000396, l5: 0.000745, l6: 0.002492
[epoch: 980/1000, batch:    92/ 1052, ite: 257500] train loss: 0.003855, tar: 0.000039 
l0: 0.000002, l1: 0.000002, l2: 0.000013, l3: 0.000142, l4: 0.000553, l5: 0.001470, l6: 0.001802
[epoch: 980/1000, batch:   172/ 1052, ite: 257520] train loss: 0.003853, tar: 0.000039 
l0: 0.000002, l1: 0.000001, l2: 0.000029, l3: 0.000162, l4: 0.000696, l5: 0.001291, l6: 0.002186
[epoch: 980/1000, batch:   252/ 1052, ite: 257540] train loss: 0.003855, tar: 0.000039 
l0: 0.000106, l1: 0.000108, l2: 0.000124, l3: 0.000125, l4: 0.000373, l5: 0.000631, l6: 0.001037
[epoch: 980/1000, batch:   332/ 1052, ite: 257560] train loss: 0.003856, tar: 0.000039 
l0: 0.000003, l1: 0.000003, l2: 0.000022, l3: 0.000169, l4: 0.000340, l5: 0.000704, l6: 0.001431
[epoch: 980/1000, batch:   412/ 1052, ite: 257580] train loss: 0.003856, tar: 0.000039 
l0: 0.000044, l1: 0.000043, l2: 0.000067, l3: 0.000094, l4: 0.000268, l5: 0.001364, l6: 0.001563
[epoch: 980/1000, batch:   492/ 1052, ite: 257600] train loss: 0.003856, tar: 0.000039 
l0: 0.000027, l1: 0.000027, l2: 0.000045, l3: 0.000141, l4: 0.000504, l5: 0.001374, l6: 0.002333
[epoch: 980/1000, batch:   572/ 1052, ite: 257620] train loss: 0.003855, tar: 0.000039 
l0: 0.000025, l1: 0.000023, l2: 0.000055, l3: 0.000183, l4: 0.000447, l5: 0.001151, l6: 0.001317
[epoch: 980/1000, batch:   652/ 1052, ite: 257640] train loss: 0.003855, tar: 0.000039 
l0: 0.000093, l1: 0.000098, l2: 0.000109, l3: 0.000240, l4: 0.000525, l5: 0.000849, l6: 0.001867
[epoch: 980/1000, batch:   732/ 1052, ite: 257660] train loss: 0.003854, tar: 0.000039 
l0: 0.000001, l1: 0.000001, l2: 0.000003, l3: 0.000066, l4: 0.000268, l5: 0.000809, l6: 0.001127
[epoch: 980/1000, batch:   812/ 1052, ite: 257680] train loss: 0.003854, tar: 0.000039 
l0: 0.000028, l1: 0.000029, l2: 0.000043, l3: 0.000152, l4: 0.000472, l5: 0.000976, l6: 0.001292
[epoch: 980/1000, batch:   892/ 1052, ite: 257700] train loss: 0.003854, tar: 0.000039 
l0: 0.000005, l1: 0.000005, l2: 0.000020, l3: 0.000104, l4: 0.000277, l5: 0.000806, l6: 0.001874
[epoch: 980/1000, batch:   972/ 1052, ite: 257720] train loss: 0.003854, tar: 0.000039 
l0: 0.000031, l1: 0.000033, l2: 0.000057, l3: 0.000211, l4: 0.000566, l5: 0.001437, l6: 0.003576
[epoch: 980/1000, batch:  1052/ 1052, ite: 257740] train loss: 0.003855, tar: 0.000039 
[Epoch 980/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000026, l1: 0.000026, l2: 0.000040, l3: 0.000138, l4: 0.000458, l5: 0.001180, l6: 0.001091
[epoch: 981/1000, batch:    80/ 1052, ite: 257760] train loss: 0.003856, tar: 0.000039 
l0: 0.000052, l1: 0.000053, l2: 0.000086, l3: 0.000259, l4: 0.000661, l5: 0.001270, l6: 0.002564
[epoch: 981/1000, batch:   160/ 1052, ite: 257780] train loss: 0.003857, tar: 0.000039 
l0: 0.000021, l1: 0.000021, l2: 0.000033, l3: 0.000161, l4: 0.000258, l5: 0.000653, l6: 0.000981
[epoch: 981/1000, batch:   240/ 1052, ite: 257800] train loss: 0.003856, tar: 0.000039 
l0: 0.000001, l1: 0.000001, l2: 0.000003, l3: 0.000036, l4: 0.000102, l5: 0.000731, l6: 0.001198
[epoch: 981/1000, batch:   320/ 1052, ite: 257820] train loss: 0.003855, tar: 0.000039 
l0: 0.000106, l1: 0.000108, l2: 0.000118, l3: 0.000247, l4: 0.000392, l5: 0.001043, l6: 0.001680
[epoch: 981/1000, batch:   400/ 1052, ite: 257840] train loss: 0.003856, tar: 0.000039 
l0: 0.000019, l1: 0.000019, l2: 0.000034, l3: 0.000091, l4: 0.000310, l5: 0.000360, l6: 0.001063
[epoch: 981/1000, batch:   480/ 1052, ite: 257860] train loss: 0.003855, tar: 0.000039 
l0: 0.000025, l1: 0.000025, l2: 0.000032, l3: 0.000105, l4: 0.000405, l5: 0.000565, l6: 0.001362
[epoch: 981/1000, batch:   560/ 1052, ite: 257880] train loss: 0.003853, tar: 0.000039 
l0: 0.000002, l1: 0.000002, l2: 0.000017, l3: 0.000077, l4: 0.000494, l5: 0.001021, l6: 0.001387
[epoch: 981/1000, batch:   640/ 1052, ite: 257900] train loss: 0.003851, tar: 0.000039 
l0: 0.000002, l1: 0.000002, l2: 0.000051, l3: 0.000143, l4: 0.000437, l5: 0.000994, l6: 0.001711
[epoch: 981/1000, batch:   720/ 1052, ite: 257920] train loss: 0.003852, tar: 0.000039 
l0: 0.000101, l1: 0.000102, l2: 0.000110, l3: 0.000188, l4: 0.000587, l5: 0.001047, l6: 0.002153
[epoch: 981/1000, batch:   800/ 1052, ite: 257940] train loss: 0.003851, tar: 0.000039 
l0: 0.000020, l1: 0.000020, l2: 0.000036, l3: 0.000067, l4: 0.000199, l5: 0.000655, l6: 0.000977
[epoch: 981/1000, batch:   880/ 1052, ite: 257960] train loss: 0.003851, tar: 0.000039 
l0: 0.000025, l1: 0.000024, l2: 0.000077, l3: 0.000147, l4: 0.000335, l5: 0.000975, l6: 0.001598
[epoch: 981/1000, batch:   960/ 1052, ite: 257980] train loss: 0.003851, tar: 0.000038 
l0: 0.000153, l1: 0.000156, l2: 0.000164, l3: 0.000235, l4: 0.000398, l5: 0.001442, l6: 0.004752
[epoch: 981/1000, batch:  1040/ 1052, ite: 258000] train loss: 0.003852, tar: 0.000039 
[Epoch 981/1000] Test loss: 0.016, Test target loss: 0.002
l0: 0.000003, l1: 0.000004, l2: 0.000010, l3: 0.000033, l4: 0.000238, l5: 0.000579, l6: 0.000950
[epoch: 982/1000, batch:    68/ 1052, ite: 258020] train loss: 0.003852, tar: 0.000039 
l0: 0.000033, l1: 0.000034, l2: 0.000037, l3: 0.000114, l4: 0.000333, l5: 0.000700, l6: 0.001520
[epoch: 982/1000, batch:   148/ 1052, ite: 258040] train loss: 0.003852, tar: 0.000039 
l0: 0.000004, l1: 0.000004, l2: 0.000020, l3: 0.000200, l4: 0.000801, l5: 0.001384, l6: 0.002467
[epoch: 982/1000, batch:   228/ 1052, ite: 258060] train loss: 0.003851, tar: 0.000039 
l0: 0.000029, l1: 0.000030, l2: 0.000036, l3: 0.000093, l4: 0.000324, l5: 0.000708, l6: 0.001342
[epoch: 982/1000, batch:   308/ 1052, ite: 258080] train loss: 0.003850, tar: 0.000039 
l0: 0.000042, l1: 0.000041, l2: 0.000078, l3: 0.000183, l4: 0.000415, l5: 0.000791, l6: 0.001255
[epoch: 982/1000, batch:   388/ 1052, ite: 258100] train loss: 0.003851, tar: 0.000039 
l0: 0.000022, l1: 0.000022, l2: 0.000050, l3: 0.000098, l4: 0.000382, l5: 0.000469, l6: 0.001607
[epoch: 982/1000, batch:   468/ 1052, ite: 258120] train loss: 0.003852, tar: 0.000039 
l0: 0.000007, l1: 0.000007, l2: 0.000054, l3: 0.000137, l4: 0.000479, l5: 0.001473, l6: 0.002650
[epoch: 982/1000, batch:   548/ 1052, ite: 258140] train loss: 0.003854, tar: 0.000039 
l0: 0.000003, l1: 0.000004, l2: 0.000021, l3: 0.000082, l4: 0.000367, l5: 0.000740, l6: 0.001216
[epoch: 982/1000, batch:   628/ 1052, ite: 258160] train loss: 0.003851, tar: 0.000039 
l0: 0.000171, l1: 0.000173, l2: 0.000205, l3: 0.000284, l4: 0.000530, l5: 0.001476, l6: 0.002448
[epoch: 982/1000, batch:   708/ 1052, ite: 258180] train loss: 0.003853, tar: 0.000039 
l0: 0.000016, l1: 0.000021, l2: 0.000033, l3: 0.000114, l4: 0.000255, l5: 0.001025, l6: 0.001945
[epoch: 982/1000, batch:   788/ 1052, ite: 258200] train loss: 0.003852, tar: 0.000039 
l0: 0.000053, l1: 0.000053, l2: 0.000077, l3: 0.000160, l4: 0.000433, l5: 0.000853, l6: 0.001914
[epoch: 982/1000, batch:   868/ 1052, ite: 258220] train loss: 0.003853, tar: 0.000039 
l0: 0.000015, l1: 0.000016, l2: 0.000017, l3: 0.000082, l4: 0.000199, l5: 0.000654, l6: 0.000735
[epoch: 982/1000, batch:   948/ 1052, ite: 258240] train loss: 0.003853, tar: 0.000039 
l0: 0.000001, l1: 0.000001, l2: 0.000008, l3: 0.000088, l4: 0.000258, l5: 0.000344, l6: 0.001022
[epoch: 982/1000, batch:  1028/ 1052, ite: 258260] train loss: 0.003854, tar: 0.000039 
[Epoch 982/1000] Test loss: 0.016, Test target loss: 0.003
l0: 0.000029, l1: 0.000031, l2: 0.000042, l3: 0.000191, l4: 0.000474, l5: 0.001095, l6: 0.003120
[epoch: 983/1000, batch:    56/ 1052, ite: 258280] train loss: 0.003854, tar: 0.000039 
l0: 0.000014, l1: 0.000014, l2: 0.000068, l3: 0.000386, l4: 0.000853, l5: 0.001744, l6: 0.003464
[epoch: 983/1000, batch:   136/ 1052, ite: 258300] train loss: 0.003855, tar: 0.000039 
l0: 0.000078, l1: 0.000080, l2: 0.000127, l3: 0.000205, l4: 0.000474, l5: 0.000768, l6: 0.001519
[epoch: 983/1000, batch:   216/ 1052, ite: 258320] train loss: 0.003854, tar: 0.000039 
l0: 0.000052, l1: 0.000052, l2: 0.000082, l3: 0.000160, l4: 0.000684, l5: 0.001127, l6: 0.001488
[epoch: 983/1000, batch:   296/ 1052, ite: 258340] train loss: 0.003853, tar: 0.000039 
l0: 0.000123, l1: 0.000125, l2: 0.000180, l3: 0.000389, l4: 0.001198, l5: 0.001927, l6: 0.004564
[epoch: 983/1000, batch:   376/ 1052, ite: 258360] train loss: 0.003854, tar: 0.000039 
l0: 0.000030, l1: 0.000029, l2: 0.000070, l3: 0.000184, l4: 0.000296, l5: 0.000635, l6: 0.001630
[epoch: 983/1000, batch:   456/ 1052, ite: 258380] train loss: 0.003854, tar: 0.000039 
l0: 0.000035, l1: 0.000036, l2: 0.000065, l3: 0.000142, l4: 0.000408, l5: 0.001090, l6: 0.002199
[epoch: 983/1000, batch:   536/ 1052, ite: 258400] train loss: 0.003854, tar: 0.000039 
l0: 0.000021, l1: 0.000022, l2: 0.000040, l3: 0.000159, l4: 0.000738, l5: 0.001451, l6: 0.002313
[epoch: 983/1000, batch:   616/ 1052, ite: 258420] train loss: 0.003854, tar: 0.000039 
l0: 0.000023, l1: 0.000021, l2: 0.000098, l3: 0.000302, l4: 0.000645, l5: 0.001394, l6: 0.001990
[epoch: 983/1000, batch:   696/ 1052, ite: 258440] train loss: 0.003853, tar: 0.000039 
l0: 0.000004, l1: 0.000005, l2: 0.000021, l3: 0.000158, l4: 0.000314, l5: 0.000715, l6: 0.001779
[epoch: 983/1000, batch:   776/ 1052, ite: 258460] train loss: 0.003852, tar: 0.000039 
l0: 0.000049, l1: 0.000048, l2: 0.000069, l3: 0.000134, l4: 0.000435, l5: 0.001173, l6: 0.002327
[epoch: 983/1000, batch:   856/ 1052, ite: 258480] train loss: 0.003852, tar: 0.000039 
l0: 0.000018, l1: 0.000017, l2: 0.000025, l3: 0.000078, l4: 0.000202, l5: 0.000533, l6: 0.001007
[epoch: 983/1000, batch:   936/ 1052, ite: 258500] train loss: 0.003853, tar: 0.000039 
l0: 0.000055, l1: 0.000053, l2: 0.000078, l3: 0.000298, l4: 0.000803, l5: 0.001997, l6: 0.002682
[epoch: 983/1000, batch:  1016/ 1052, ite: 258520] train loss: 0.003854, tar: 0.000039 
[Epoch 983/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000000, l1: 0.000001, l2: 0.000002, l3: 0.000022, l4: 0.000346, l5: 0.000594, l6: 0.001296
[epoch: 984/1000, batch:    44/ 1052, ite: 258540] train loss: 0.003853, tar: 0.000039 
l0: 0.000002, l1: 0.000002, l2: 0.000034, l3: 0.000176, l4: 0.000822, l5: 0.000743, l6: 0.002631
[epoch: 984/1000, batch:   124/ 1052, ite: 258560] train loss: 0.003852, tar: 0.000039 
l0: 0.000129, l1: 0.000131, l2: 0.000181, l3: 0.000369, l4: 0.000764, l5: 0.001175, l6: 0.004620
[epoch: 984/1000, batch:   204/ 1052, ite: 258580] train loss: 0.003853, tar: 0.000039 
l0: 0.000008, l1: 0.000006, l2: 0.000047, l3: 0.000231, l4: 0.000523, l5: 0.001127, l6: 0.001901
[epoch: 984/1000, batch:   284/ 1052, ite: 258600] train loss: 0.003854, tar: 0.000039 
l0: 0.000054, l1: 0.000055, l2: 0.000066, l3: 0.000199, l4: 0.000465, l5: 0.001029, l6: 0.002190
[epoch: 984/1000, batch:   364/ 1052, ite: 258620] train loss: 0.003852, tar: 0.000039 
l0: 0.000015, l1: 0.000015, l2: 0.000023, l3: 0.000060, l4: 0.000444, l5: 0.000702, l6: 0.001282
[epoch: 984/1000, batch:   444/ 1052, ite: 258640] train loss: 0.003850, tar: 0.000039 
l0: 0.000026, l1: 0.000024, l2: 0.000068, l3: 0.000297, l4: 0.000629, l5: 0.002114, l6: 0.004978
[epoch: 984/1000, batch:   524/ 1052, ite: 258660] train loss: 0.003851, tar: 0.000039 
l0: 0.000036, l1: 0.000036, l2: 0.000054, l3: 0.000093, l4: 0.000391, l5: 0.001198, l6: 0.001680
[epoch: 984/1000, batch:   604/ 1052, ite: 258680] train loss: 0.003852, tar: 0.000039 
l0: 0.000047, l1: 0.000047, l2: 0.000108, l3: 0.000296, l4: 0.000502, l5: 0.001048, l6: 0.001405
[epoch: 984/1000, batch:   684/ 1052, ite: 258700] train loss: 0.003852, tar: 0.000039 
l0: 0.000005, l1: 0.000004, l2: 0.000038, l3: 0.000186, l4: 0.000425, l5: 0.001313, l6: 0.002533
[epoch: 984/1000, batch:   764/ 1052, ite: 258720] train loss: 0.003853, tar: 0.000039 
l0: 0.000096, l1: 0.000094, l2: 0.000148, l3: 0.000274, l4: 0.000728, l5: 0.001694, l6: 0.003353
[epoch: 984/1000, batch:   844/ 1052, ite: 258740] train loss: 0.003854, tar: 0.000039 
l0: 0.000017, l1: 0.000016, l2: 0.000046, l3: 0.000211, l4: 0.000545, l5: 0.001627, l6: 0.001620
[epoch: 984/1000, batch:   924/ 1052, ite: 258760] train loss: 0.003855, tar: 0.000039 
l0: 0.000020, l1: 0.000019, l2: 0.000052, l3: 0.000179, l4: 0.000457, l5: 0.000692, l6: 0.001582
[epoch: 984/1000, batch:  1004/ 1052, ite: 258780] train loss: 0.003854, tar: 0.000039 
[Epoch 984/1000] Test loss: 0.016, Test target loss: 0.003
l0: 0.000106, l1: 0.000105, l2: 0.000153, l3: 0.000347, l4: 0.000702, l5: 0.002084, l6: 0.003090
[epoch: 985/1000, batch:    32/ 1052, ite: 258800] train loss: 0.003853, tar: 0.000039 
l0: 0.000143, l1: 0.000148, l2: 0.000171, l3: 0.000275, l4: 0.000717, l5: 0.001519, l6: 0.003067
[epoch: 985/1000, batch:   112/ 1052, ite: 258820] train loss: 0.003854, tar: 0.000039 
l0: 0.000017, l1: 0.000015, l2: 0.000086, l3: 0.000177, l4: 0.000620, l5: 0.001733, l6: 0.002776
[epoch: 985/1000, batch:   192/ 1052, ite: 258840] train loss: 0.003854, tar: 0.000039 
l0: 0.000011, l1: 0.000010, l2: 0.000035, l3: 0.000121, l4: 0.000378, l5: 0.000690, l6: 0.001334
[epoch: 985/1000, batch:   272/ 1052, ite: 258860] train loss: 0.003854, tar: 0.000039 
l0: 0.000010, l1: 0.000009, l2: 0.000024, l3: 0.000106, l4: 0.000271, l5: 0.000602, l6: 0.001279
[epoch: 985/1000, batch:   352/ 1052, ite: 258880] train loss: 0.003855, tar: 0.000039 
l0: 0.000000, l1: 0.000000, l2: 0.000000, l3: 0.000004, l4: 0.000116, l5: 0.000575, l6: 0.000961
[epoch: 985/1000, batch:   432/ 1052, ite: 258900] train loss: 0.003853, tar: 0.000039 
l0: 0.000022, l1: 0.000022, l2: 0.000036, l3: 0.000149, l4: 0.000461, l5: 0.000957, l6: 0.001571
[epoch: 985/1000, batch:   512/ 1052, ite: 258920] train loss: 0.003854, tar: 0.000039 
l0: 0.000003, l1: 0.000004, l2: 0.000015, l3: 0.000083, l4: 0.000360, l5: 0.001202, l6: 0.002385
[epoch: 985/1000, batch:   592/ 1052, ite: 258940] train loss: 0.003854, tar: 0.000039 
l0: 0.000010, l1: 0.000010, l2: 0.000023, l3: 0.000148, l4: 0.000187, l5: 0.000740, l6: 0.001067
[epoch: 985/1000, batch:   672/ 1052, ite: 258960] train loss: 0.003855, tar: 0.000039 
l0: 0.000021, l1: 0.000021, l2: 0.000038, l3: 0.000155, l4: 0.000377, l5: 0.000972, l6: 0.001170
[epoch: 985/1000, batch:   752/ 1052, ite: 258980] train loss: 0.003856, tar: 0.000039 
l0: 0.000005, l1: 0.000005, l2: 0.000030, l3: 0.000174, l4: 0.000470, l5: 0.000921, l6: 0.002141
[epoch: 985/1000, batch:   832/ 1052, ite: 259000] train loss: 0.003855, tar: 0.000039 
l0: 0.000098, l1: 0.000098, l2: 0.000143, l3: 0.000283, l4: 0.000505, l5: 0.001390, l6: 0.001787
[epoch: 985/1000, batch:   912/ 1052, ite: 259020] train loss: 0.003853, tar: 0.000039 
l0: 0.000016, l1: 0.000018, l2: 0.000049, l3: 0.000159, l4: 0.000614, l5: 0.001610, l6: 0.002207
[epoch: 985/1000, batch:   992/ 1052, ite: 259040] train loss: 0.003854, tar: 0.000038 
[Epoch 985/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000001, l1: 0.000001, l2: 0.000007, l3: 0.000079, l4: 0.000331, l5: 0.000891, l6: 0.002791
[epoch: 986/1000, batch:    20/ 1052, ite: 259060] train loss: 0.003853, tar: 0.000038 
l0: 0.000046, l1: 0.000045, l2: 0.000072, l3: 0.000157, l4: 0.000348, l5: 0.001236, l6: 0.002271
[epoch: 986/1000, batch:   100/ 1052, ite: 259080] train loss: 0.003854, tar: 0.000038 
l0: 0.000049, l1: 0.000050, l2: 0.000061, l3: 0.000138, l4: 0.000356, l5: 0.000775, l6: 0.001700
[epoch: 986/1000, batch:   180/ 1052, ite: 259100] train loss: 0.003853, tar: 0.000038 
l0: 0.000005, l1: 0.000005, l2: 0.000050, l3: 0.000390, l4: 0.000966, l5: 0.002476, l6: 0.004630
[epoch: 986/1000, batch:   260/ 1052, ite: 259120] train loss: 0.003854, tar: 0.000038 
l0: 0.000010, l1: 0.000009, l2: 0.000041, l3: 0.000105, l4: 0.000382, l5: 0.001323, l6: 0.001480
[epoch: 986/1000, batch:   340/ 1052, ite: 259140] train loss: 0.003853, tar: 0.000038 
l0: 0.000005, l1: 0.000006, l2: 0.000019, l3: 0.000078, l4: 0.000320, l5: 0.000938, l6: 0.001960
[epoch: 986/1000, batch:   420/ 1052, ite: 259160] train loss: 0.003853, tar: 0.000038 
l0: 0.000043, l1: 0.000042, l2: 0.000055, l3: 0.000120, l4: 0.000402, l5: 0.000708, l6: 0.001525
[epoch: 986/1000, batch:   500/ 1052, ite: 259180] train loss: 0.003853, tar: 0.000038 
l0: 0.000017, l1: 0.000015, l2: 0.000043, l3: 0.000144, l4: 0.000247, l5: 0.001212, l6: 0.001560
[epoch: 986/1000, batch:   580/ 1052, ite: 259200] train loss: 0.003853, tar: 0.000038 
l0: 0.000004, l1: 0.000005, l2: 0.000027, l3: 0.000147, l4: 0.000416, l5: 0.001172, l6: 0.002246
[epoch: 986/1000, batch:   660/ 1052, ite: 259220] train loss: 0.003853, tar: 0.000038 
l0: 0.000003, l1: 0.000003, l2: 0.000012, l3: 0.000122, l4: 0.000333, l5: 0.001219, l6: 0.001088
[epoch: 986/1000, batch:   740/ 1052, ite: 259240] train loss: 0.003853, tar: 0.000038 
l0: 0.000059, l1: 0.000061, l2: 0.000064, l3: 0.000116, l4: 0.000259, l5: 0.000950, l6: 0.001063
[epoch: 986/1000, batch:   820/ 1052, ite: 259260] train loss: 0.003854, tar: 0.000039 
l0: 0.000045, l1: 0.000044, l2: 0.000109, l3: 0.000325, l4: 0.000750, l5: 0.001179, l6: 0.002955
[epoch: 986/1000, batch:   900/ 1052, ite: 259280] train loss: 0.003856, tar: 0.000039 
l0: 0.000039, l1: 0.000040, l2: 0.000078, l3: 0.000112, l4: 0.000323, l5: 0.000734, l6: 0.001865
[epoch: 986/1000, batch:   980/ 1052, ite: 259300] train loss: 0.003857, tar: 0.000039 
[Epoch 986/1000] Test loss: 0.016, Test target loss: 0.003
l0: 0.000023, l1: 0.000021, l2: 0.000055, l3: 0.000250, l4: 0.000654, l5: 0.001777, l6: 0.003439
[epoch: 987/1000, batch:     8/ 1052, ite: 259320] train loss: 0.003855, tar: 0.000039 
l0: 0.000008, l1: 0.000007, l2: 0.000036, l3: 0.000125, l4: 0.000486, l5: 0.000961, l6: 0.001730
[epoch: 987/1000, batch:    88/ 1052, ite: 259340] train loss: 0.003856, tar: 0.000038 
l0: 0.000137, l1: 0.000139, l2: 0.000153, l3: 0.000347, l4: 0.000914, l5: 0.001894, l6: 0.002201
[epoch: 987/1000, batch:   168/ 1052, ite: 259360] train loss: 0.003857, tar: 0.000039 
l0: 0.000096, l1: 0.000098, l2: 0.000118, l3: 0.000205, l4: 0.000501, l5: 0.001339, l6: 0.002057
[epoch: 987/1000, batch:   248/ 1052, ite: 259380] train loss: 0.003857, tar: 0.000039 
l0: 0.000029, l1: 0.000029, l2: 0.000051, l3: 0.000168, l4: 0.000666, l5: 0.001986, l6: 0.002734
[epoch: 987/1000, batch:   328/ 1052, ite: 259400] train loss: 0.003857, tar: 0.000039 
l0: 0.000137, l1: 0.000141, l2: 0.000165, l3: 0.000287, l4: 0.000545, l5: 0.001451, l6: 0.003065
[epoch: 987/1000, batch:   408/ 1052, ite: 259420] train loss: 0.003857, tar: 0.000039 
l0: 0.000024, l1: 0.000026, l2: 0.000075, l3: 0.000181, l4: 0.000378, l5: 0.001273, l6: 0.004585
[epoch: 987/1000, batch:   488/ 1052, ite: 259440] train loss: 0.003858, tar: 0.000039 
l0: 0.000005, l1: 0.000004, l2: 0.000026, l3: 0.000146, l4: 0.000516, l5: 0.001259, l6: 0.002580
[epoch: 987/1000, batch:   568/ 1052, ite: 259460] train loss: 0.003856, tar: 0.000039 
l0: 0.000022, l1: 0.000022, l2: 0.000055, l3: 0.000149, l4: 0.000327, l5: 0.000918, l6: 0.001242
[epoch: 987/1000, batch:   648/ 1052, ite: 259480] train loss: 0.003855, tar: 0.000039 
l0: 0.000005, l1: 0.000005, l2: 0.000041, l3: 0.000113, l4: 0.000331, l5: 0.001125, l6: 0.002355
[epoch: 987/1000, batch:   728/ 1052, ite: 259500] train loss: 0.003856, tar: 0.000039 
l0: 0.000001, l1: 0.000001, l2: 0.000007, l3: 0.000026, l4: 0.000208, l5: 0.000604, l6: 0.000858
[epoch: 987/1000, batch:   808/ 1052, ite: 259520] train loss: 0.003855, tar: 0.000039 
l0: 0.000066, l1: 0.000065, l2: 0.000089, l3: 0.000317, l4: 0.000683, l5: 0.001072, l6: 0.002808
[epoch: 987/1000, batch:   888/ 1052, ite: 259540] train loss: 0.003855, tar: 0.000039 
l0: 0.000006, l1: 0.000005, l2: 0.000035, l3: 0.000334, l4: 0.000924, l5: 0.002237, l6: 0.002831
[epoch: 987/1000, batch:   968/ 1052, ite: 259560] train loss: 0.003855, tar: 0.000039 
l0: 0.000025, l1: 0.000025, l2: 0.000029, l3: 0.000127, l4: 0.000303, l5: 0.000692, l6: 0.001152
[epoch: 987/1000, batch:  1048/ 1052, ite: 259580] train loss: 0.003855, tar: 0.000039 
[Epoch 987/1000] Test loss: 0.016, Test target loss: 0.002
l0: 0.000024, l1: 0.000024, l2: 0.000043, l3: 0.000083, l4: 0.000324, l5: 0.000888, l6: 0.001244
[epoch: 988/1000, batch:    76/ 1052, ite: 259600] train loss: 0.003856, tar: 0.000039 
l0: 0.000025, l1: 0.000025, l2: 0.000052, l3: 0.000177, l4: 0.000530, l5: 0.001495, l6: 0.002338
[epoch: 988/1000, batch:   156/ 1052, ite: 259620] train loss: 0.003855, tar: 0.000039 
l0: 0.000002, l1: 0.000003, l2: 0.000007, l3: 0.000078, l4: 0.000273, l5: 0.000787, l6: 0.001633
[epoch: 988/1000, batch:   236/ 1052, ite: 259640] train loss: 0.003855, tar: 0.000039 
l0: 0.000000, l1: 0.000000, l2: 0.000002, l3: 0.000026, l4: 0.000163, l5: 0.000348, l6: 0.000764
[epoch: 988/1000, batch:   316/ 1052, ite: 259660] train loss: 0.003853, tar: 0.000039 
l0: 0.000169, l1: 0.000170, l2: 0.000225, l3: 0.000434, l4: 0.000797, l5: 0.001426, l6: 0.002882
[epoch: 988/1000, batch:   396/ 1052, ite: 259680] train loss: 0.003853, tar: 0.000039 
l0: 0.000057, l1: 0.000056, l2: 0.000103, l3: 0.000230, l4: 0.000415, l5: 0.000921, l6: 0.003040
[epoch: 988/1000, batch:   476/ 1052, ite: 259700] train loss: 0.003853, tar: 0.000039 
l0: 0.000017, l1: 0.000018, l2: 0.000026, l3: 0.000086, l4: 0.000340, l5: 0.000865, l6: 0.001364
[epoch: 988/1000, batch:   556/ 1052, ite: 259720] train loss: 0.003855, tar: 0.000039 
l0: 0.000036, l1: 0.000037, l2: 0.000064, l3: 0.000169, l4: 0.000379, l5: 0.000888, l6: 0.002244
[epoch: 988/1000, batch:   636/ 1052, ite: 259740] train loss: 0.003854, tar: 0.000039 
l0: 0.000005, l1: 0.000005, l2: 0.000008, l3: 0.000089, l4: 0.000312, l5: 0.001068, l6: 0.001583
[epoch: 988/1000, batch:   716/ 1052, ite: 259760] train loss: 0.003855, tar: 0.000039 
l0: 0.000027, l1: 0.000028, l2: 0.000032, l3: 0.000075, l4: 0.000203, l5: 0.000433, l6: 0.000978
[epoch: 988/1000, batch:   796/ 1052, ite: 259780] train loss: 0.003856, tar: 0.000039 
l0: 0.000041, l1: 0.000040, l2: 0.000064, l3: 0.000253, l4: 0.000668, l5: 0.001328, l6: 0.002115
[epoch: 988/1000, batch:   876/ 1052, ite: 259800] train loss: 0.003856, tar: 0.000039 
l0: 0.000050, l1: 0.000050, l2: 0.000089, l3: 0.000216, l4: 0.000626, l5: 0.001370, l6: 0.001867
[epoch: 988/1000, batch:   956/ 1052, ite: 259820] train loss: 0.003856, tar: 0.000039 
l0: 0.000036, l1: 0.000036, l2: 0.000058, l3: 0.000164, l4: 0.000476, l5: 0.001106, l6: 0.001672
[epoch: 988/1000, batch:  1036/ 1052, ite: 259840] train loss: 0.003856, tar: 0.000039 
[Epoch 988/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000069, l1: 0.000070, l2: 0.000094, l3: 0.000217, l4: 0.000826, l5: 0.001847, l6: 0.001956
[epoch: 989/1000, batch:    64/ 1052, ite: 259860] train loss: 0.003855, tar: 0.000039 
l0: 0.000013, l1: 0.000017, l2: 0.000103, l3: 0.000155, l4: 0.000340, l5: 0.000846, l6: 0.001830
[epoch: 989/1000, batch:   144/ 1052, ite: 259880] train loss: 0.003855, tar: 0.000039 
l0: 0.000005, l1: 0.000004, l2: 0.000011, l3: 0.000059, l4: 0.000283, l5: 0.000686, l6: 0.001359
[epoch: 989/1000, batch:   224/ 1052, ite: 259900] train loss: 0.003855, tar: 0.000039 
l0: 0.000004, l1: 0.000004, l2: 0.000010, l3: 0.000068, l4: 0.000179, l5: 0.000940, l6: 0.001958
[epoch: 989/1000, batch:   304/ 1052, ite: 259920] train loss: 0.003857, tar: 0.000039 
l0: 0.000029, l1: 0.000030, l2: 0.000049, l3: 0.000138, l4: 0.000329, l5: 0.001168, l6: 0.002395
[epoch: 989/1000, batch:   384/ 1052, ite: 259940] train loss: 0.003858, tar: 0.000039 
l0: 0.000009, l1: 0.000013, l2: 0.000028, l3: 0.000228, l4: 0.001130, l5: 0.001272, l6: 0.002327
[epoch: 989/1000, batch:   464/ 1052, ite: 259960] train loss: 0.003859, tar: 0.000039 
l0: 0.000016, l1: 0.000017, l2: 0.000019, l3: 0.000104, l4: 0.000463, l5: 0.001127, l6: 0.002666
[epoch: 989/1000, batch:   544/ 1052, ite: 259980] train loss: 0.003858, tar: 0.000039 
l0: 0.000005, l1: 0.000004, l2: 0.000033, l3: 0.000303, l4: 0.000970, l5: 0.002722, l6: 0.002832
[epoch: 989/1000, batch:   624/ 1052, ite: 260000] train loss: 0.003859, tar: 0.000039 
l0: 0.000000, l1: 0.000000, l2: 0.000003, l3: 0.000028, l4: 0.000350, l5: 0.000682, l6: 0.001143
[epoch: 989/1000, batch:   704/ 1052, ite: 260020] train loss: 0.003858, tar: 0.000039 
l0: 0.000019, l1: 0.000019, l2: 0.000036, l3: 0.000088, l4: 0.000399, l5: 0.000757, l6: 0.001418
[epoch: 989/1000, batch:   784/ 1052, ite: 260040] train loss: 0.003858, tar: 0.000039 
l0: 0.000109, l1: 0.000109, l2: 0.000132, l3: 0.000227, l4: 0.000536, l5: 0.001535, l6: 0.002462
[epoch: 989/1000, batch:   864/ 1052, ite: 260060] train loss: 0.003857, tar: 0.000039 
l0: 0.000077, l1: 0.000077, l2: 0.000110, l3: 0.000241, l4: 0.000525, l5: 0.000752, l6: 0.002284
[epoch: 989/1000, batch:   944/ 1052, ite: 260080] train loss: 0.003857, tar: 0.000039 
l0: 0.000054, l1: 0.000055, l2: 0.000070, l3: 0.000141, l4: 0.000486, l5: 0.000846, l6: 0.001501
[epoch: 989/1000, batch:  1024/ 1052, ite: 260100] train loss: 0.003857, tar: 0.000039 
[Epoch 989/1000] Test loss: 0.016, Test target loss: 0.002
l0: 0.000078, l1: 0.000077, l2: 0.000106, l3: 0.000211, l4: 0.000580, l5: 0.001097, l6: 0.002627
[epoch: 990/1000, batch:    52/ 1052, ite: 260120] train loss: 0.003856, tar: 0.000039 
l0: 0.000005, l1: 0.000006, l2: 0.000014, l3: 0.000100, l4: 0.000318, l5: 0.000542, l6: 0.001927
[epoch: 990/1000, batch:   132/ 1052, ite: 260140] train loss: 0.003856, tar: 0.000039 
l0: 0.000024, l1: 0.000024, l2: 0.000027, l3: 0.000074, l4: 0.000488, l5: 0.000901, l6: 0.001946
[epoch: 990/1000, batch:   212/ 1052, ite: 260160] train loss: 0.003855, tar: 0.000039 
l0: 0.000044, l1: 0.000043, l2: 0.000064, l3: 0.000148, l4: 0.000452, l5: 0.000865, l6: 0.001582
[epoch: 990/1000, batch:   292/ 1052, ite: 260180] train loss: 0.003855, tar: 0.000039 
l0: 0.000091, l1: 0.000093, l2: 0.000114, l3: 0.000235, l4: 0.000739, l5: 0.001635, l6: 0.004220
[epoch: 990/1000, batch:   372/ 1052, ite: 260200] train loss: 0.003856, tar: 0.000039 
l0: 0.000004, l1: 0.000004, l2: 0.000015, l3: 0.000060, l4: 0.000450, l5: 0.001077, l6: 0.001689
[epoch: 990/1000, batch:   452/ 1052, ite: 260220] train loss: 0.003856, tar: 0.000039 
l0: 0.000002, l1: 0.000002, l2: 0.000018, l3: 0.000149, l4: 0.000342, l5: 0.001091, l6: 0.001790
[epoch: 990/1000, batch:   532/ 1052, ite: 260240] train loss: 0.003857, tar: 0.000039 
l0: 0.000011, l1: 0.000012, l2: 0.000036, l3: 0.000187, l4: 0.000406, l5: 0.001083, l6: 0.001979
[epoch: 990/1000, batch:   612/ 1052, ite: 260260] train loss: 0.003858, tar: 0.000039 
l0: 0.000009, l1: 0.000009, l2: 0.000018, l3: 0.000061, l4: 0.000319, l5: 0.001110, l6: 0.000578
[epoch: 990/1000, batch:   692/ 1052, ite: 260280] train loss: 0.003858, tar: 0.000039 
l0: 0.000021, l1: 0.000021, l2: 0.000055, l3: 0.000174, l4: 0.000314, l5: 0.001356, l6: 0.002737
[epoch: 990/1000, batch:   772/ 1052, ite: 260300] train loss: 0.003856, tar: 0.000039 
l0: 0.000119, l1: 0.000119, l2: 0.000208, l3: 0.000443, l4: 0.000998, l5: 0.002129, l6: 0.003151
[epoch: 990/1000, batch:   852/ 1052, ite: 260320] train loss: 0.003857, tar: 0.000039 
l0: 0.000088, l1: 0.000087, l2: 0.000117, l3: 0.000216, l4: 0.000387, l5: 0.000998, l6: 0.001794
[epoch: 990/1000, batch:   932/ 1052, ite: 260340] train loss: 0.003857, tar: 0.000039 
l0: 0.000110, l1: 0.000111, l2: 0.000203, l3: 0.000413, l4: 0.000835, l5: 0.002929, l6: 0.006510
[epoch: 990/1000, batch:  1012/ 1052, ite: 260360] train loss: 0.003857, tar: 0.000039 
[Epoch 990/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000049, l1: 0.000048, l2: 0.000072, l3: 0.000143, l4: 0.000466, l5: 0.001052, l6: 0.001508
[epoch: 991/1000, batch:    40/ 1052, ite: 260380] train loss: 0.003857, tar: 0.000039 
l0: 0.000003, l1: 0.000003, l2: 0.000016, l3: 0.000143, l4: 0.000362, l5: 0.000980, l6: 0.001909
[epoch: 991/1000, batch:   120/ 1052, ite: 260400] train loss: 0.003857, tar: 0.000039 
l0: 0.000108, l1: 0.000108, l2: 0.000133, l3: 0.000197, l4: 0.000507, l5: 0.000892, l6: 0.002647
[epoch: 991/1000, batch:   200/ 1052, ite: 260420] train loss: 0.003857, tar: 0.000039 
l0: 0.000069, l1: 0.000068, l2: 0.000094, l3: 0.000192, l4: 0.000606, l5: 0.002006, l6: 0.001554
[epoch: 991/1000, batch:   280/ 1052, ite: 260440] train loss: 0.003857, tar: 0.000039 
l0: 0.000020, l1: 0.000021, l2: 0.000029, l3: 0.000129, l4: 0.000442, l5: 0.001002, l6: 0.001569
[epoch: 991/1000, batch:   360/ 1052, ite: 260460] train loss: 0.003856, tar: 0.000039 
l0: 0.000007, l1: 0.000006, l2: 0.000022, l3: 0.000125, l4: 0.000407, l5: 0.000949, l6: 0.001562
[epoch: 991/1000, batch:   440/ 1052, ite: 260480] train loss: 0.003856, tar: 0.000039 
l0: 0.000005, l1: 0.000005, l2: 0.000010, l3: 0.000062, l4: 0.000254, l5: 0.000485, l6: 0.001544
[epoch: 991/1000, batch:   520/ 1052, ite: 260500] train loss: 0.003855, tar: 0.000039 
l0: 0.000048, l1: 0.000045, l2: 0.000071, l3: 0.000194, l4: 0.000383, l5: 0.001000, l6: 0.001611
[epoch: 991/1000, batch:   600/ 1052, ite: 260520] train loss: 0.003855, tar: 0.000039 
l0: 0.000014, l1: 0.000016, l2: 0.000014, l3: 0.000133, l4: 0.000397, l5: 0.000975, l6: 0.001451
[epoch: 991/1000, batch:   680/ 1052, ite: 260540] train loss: 0.003856, tar: 0.000039 
l0: 0.000003, l1: 0.000003, l2: 0.000012, l3: 0.000104, l4: 0.000310, l5: 0.000840, l6: 0.001430
[epoch: 991/1000, batch:   760/ 1052, ite: 260560] train loss: 0.003855, tar: 0.000039 
l0: 0.000031, l1: 0.000030, l2: 0.000045, l3: 0.000113, l4: 0.000282, l5: 0.000595, l6: 0.001774
[epoch: 991/1000, batch:   840/ 1052, ite: 260580] train loss: 0.003857, tar: 0.000039 
l0: 0.000002, l1: 0.000001, l2: 0.000006, l3: 0.000112, l4: 0.000429, l5: 0.000770, l6: 0.001135
[epoch: 991/1000, batch:   920/ 1052, ite: 260600] train loss: 0.003857, tar: 0.000039 
l0: 0.000011, l1: 0.000011, l2: 0.000029, l3: 0.000124, l4: 0.000363, l5: 0.000877, l6: 0.001228
[epoch: 991/1000, batch:  1000/ 1052, ite: 260620] train loss: 0.003858, tar: 0.000039 
[Epoch 991/1000] Test loss: 0.016, Test target loss: 0.002
l0: 0.000034, l1: 0.000035, l2: 0.000053, l3: 0.000195, l4: 0.000442, l5: 0.001034, l6: 0.000914
[epoch: 992/1000, batch:    28/ 1052, ite: 260640] train loss: 0.003857, tar: 0.000039 
l0: 0.000014, l1: 0.000013, l2: 0.000062, l3: 0.000183, l4: 0.000479, l5: 0.001442, l6: 0.002595
[epoch: 992/1000, batch:   108/ 1052, ite: 260660] train loss: 0.003858, tar: 0.000039 
l0: 0.000061, l1: 0.000060, l2: 0.000078, l3: 0.000182, l4: 0.000382, l5: 0.001008, l6: 0.002355
[epoch: 992/1000, batch:   188/ 1052, ite: 260680] train loss: 0.003857, tar: 0.000038 
l0: 0.000010, l1: 0.000010, l2: 0.000057, l3: 0.000145, l4: 0.000364, l5: 0.001403, l6: 0.002001
[epoch: 992/1000, batch:   268/ 1052, ite: 260700] train loss: 0.003858, tar: 0.000039 
l0: 0.000001, l1: 0.000001, l2: 0.000004, l3: 0.000058, l4: 0.000390, l5: 0.000789, l6: 0.001910
[epoch: 992/1000, batch:   348/ 1052, ite: 260720] train loss: 0.003857, tar: 0.000039 
l0: 0.000062, l1: 0.000063, l2: 0.000071, l3: 0.000152, l4: 0.000551, l5: 0.001180, l6: 0.001801
[epoch: 992/1000, batch:   428/ 1052, ite: 260740] train loss: 0.003858, tar: 0.000039 
l0: 0.000022, l1: 0.000022, l2: 0.000091, l3: 0.000197, l4: 0.000706, l5: 0.001417, l6: 0.002329
[epoch: 992/1000, batch:   508/ 1052, ite: 260760] train loss: 0.003860, tar: 0.000039 
l0: 0.000002, l1: 0.000001, l2: 0.000008, l3: 0.000056, l4: 0.000366, l5: 0.000613, l6: 0.001317
[epoch: 992/1000, batch:   588/ 1052, ite: 260780] train loss: 0.003860, tar: 0.000039 
l0: 0.000005, l1: 0.000005, l2: 0.000049, l3: 0.000277, l4: 0.000726, l5: 0.001306, l6: 0.001852
[epoch: 992/1000, batch:   668/ 1052, ite: 260800] train loss: 0.003859, tar: 0.000039 
l0: 0.000041, l1: 0.000041, l2: 0.000057, l3: 0.000108, l4: 0.000265, l5: 0.000322, l6: 0.001360
[epoch: 992/1000, batch:   748/ 1052, ite: 260820] train loss: 0.003859, tar: 0.000039 
l0: 0.000004, l1: 0.000004, l2: 0.000039, l3: 0.000104, l4: 0.000306, l5: 0.000918, l6: 0.001504
[epoch: 992/1000, batch:   828/ 1052, ite: 260840] train loss: 0.003858, tar: 0.000039 
l0: 0.000054, l1: 0.000053, l2: 0.000098, l3: 0.000236, l4: 0.000458, l5: 0.001069, l6: 0.001938
[epoch: 992/1000, batch:   908/ 1052, ite: 260860] train loss: 0.003857, tar: 0.000039 
l0: 0.000014, l1: 0.000016, l2: 0.000025, l3: 0.000086, l4: 0.000294, l5: 0.000559, l6: 0.001676
[epoch: 992/1000, batch:   988/ 1052, ite: 260880] train loss: 0.003857, tar: 0.000039 
[Epoch 992/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000104, l1: 0.000105, l2: 0.000173, l3: 0.000366, l4: 0.000867, l5: 0.001756, l6: 0.002668
[epoch: 993/1000, batch:    16/ 1052, ite: 260900] train loss: 0.003856, tar: 0.000039 
l0: 0.000022, l1: 0.000020, l2: 0.000144, l3: 0.000378, l4: 0.000701, l5: 0.001662, l6: 0.003818
[epoch: 993/1000, batch:    96/ 1052, ite: 260920] train loss: 0.003856, tar: 0.000039 
l0: 0.000183, l1: 0.000188, l2: 0.000207, l3: 0.000319, l4: 0.000689, l5: 0.000971, l6: 0.001676
[epoch: 993/1000, batch:   176/ 1052, ite: 260940] train loss: 0.003856, tar: 0.000039 
l0: 0.000075, l1: 0.000074, l2: 0.000107, l3: 0.000251, l4: 0.000471, l5: 0.000755, l6: 0.001460
[epoch: 993/1000, batch:   256/ 1052, ite: 260960] train loss: 0.003857, tar: 0.000039 
l0: 0.000020, l1: 0.000020, l2: 0.000045, l3: 0.000185, l4: 0.000658, l5: 0.001389, l6: 0.002634
[epoch: 993/1000, batch:   336/ 1052, ite: 260980] train loss: 0.003856, tar: 0.000039 
l0: 0.000061, l1: 0.000063, l2: 0.000097, l3: 0.000197, l4: 0.000346, l5: 0.000924, l6: 0.001303
[epoch: 993/1000, batch:   416/ 1052, ite: 261000] train loss: 0.003857, tar: 0.000039 
l0: 0.000001, l1: 0.000001, l2: 0.000009, l3: 0.000066, l4: 0.000211, l5: 0.000457, l6: 0.000886
[epoch: 993/1000, batch:   496/ 1052, ite: 261020] train loss: 0.003857, tar: 0.000039 
l0: 0.000030, l1: 0.000030, l2: 0.000071, l3: 0.000267, l4: 0.000795, l5: 0.001747, l6: 0.003400
[epoch: 993/1000, batch:   576/ 1052, ite: 261040] train loss: 0.003857, tar: 0.000039 
l0: 0.000018, l1: 0.000017, l2: 0.000066, l3: 0.000177, l4: 0.000345, l5: 0.001005, l6: 0.002177
[epoch: 993/1000, batch:   656/ 1052, ite: 261060] train loss: 0.003855, tar: 0.000039 
l0: 0.000037, l1: 0.000037, l2: 0.000052, l3: 0.000116, l4: 0.000286, l5: 0.000769, l6: 0.001096
[epoch: 993/1000, batch:   736/ 1052, ite: 261080] train loss: 0.003854, tar: 0.000038 
l0: 0.000005, l1: 0.000005, l2: 0.000022, l3: 0.000126, l4: 0.000365, l5: 0.001048, l6: 0.001582
[epoch: 993/1000, batch:   816/ 1052, ite: 261100] train loss: 0.003854, tar: 0.000038 
l0: 0.000044, l1: 0.000043, l2: 0.000071, l3: 0.000127, l4: 0.000257, l5: 0.000663, l6: 0.001243
[epoch: 993/1000, batch:   896/ 1052, ite: 261120] train loss: 0.003854, tar: 0.000038 
l0: 0.000075, l1: 0.000077, l2: 0.000094, l3: 0.000229, l4: 0.000562, l5: 0.001553, l6: 0.003252
[epoch: 993/1000, batch:   976/ 1052, ite: 261140] train loss: 0.003855, tar: 0.000038 
[Epoch 993/1000] Test loss: 0.016, Test target loss: 0.002
l0: 0.000014, l1: 0.000014, l2: 0.000045, l3: 0.000260, l4: 0.000621, l5: 0.001849, l6: 0.003368
[epoch: 994/1000, batch:     4/ 1052, ite: 261160] train loss: 0.003856, tar: 0.000038 
l0: 0.000003, l1: 0.000003, l2: 0.000007, l3: 0.000064, l4: 0.000226, l5: 0.000737, l6: 0.000907
[epoch: 994/1000, batch:    84/ 1052, ite: 261180] train loss: 0.003855, tar: 0.000038 
l0: 0.000068, l1: 0.000070, l2: 0.000090, l3: 0.000208, l4: 0.000585, l5: 0.001292, l6: 0.001950
[epoch: 994/1000, batch:   164/ 1052, ite: 261200] train loss: 0.003855, tar: 0.000039 
l0: 0.000162, l1: 0.000168, l2: 0.000160, l3: 0.000208, l4: 0.000419, l5: 0.001060, l6: 0.003053
[epoch: 994/1000, batch:   244/ 1052, ite: 261220] train loss: 0.003855, tar: 0.000039 
l0: 0.000018, l1: 0.000018, l2: 0.000048, l3: 0.000165, l4: 0.000331, l5: 0.001206, l6: 0.001657
[epoch: 994/1000, batch:   324/ 1052, ite: 261240] train loss: 0.003855, tar: 0.000039 
l0: 0.000130, l1: 0.000134, l2: 0.000202, l3: 0.000358, l4: 0.000740, l5: 0.001232, l6: 0.002422
[epoch: 994/1000, batch:   404/ 1052, ite: 261260] train loss: 0.003855, tar: 0.000039 
l0: 0.000002, l1: 0.000002, l2: 0.000020, l3: 0.000097, l4: 0.000181, l5: 0.000684, l6: 0.001309
[epoch: 994/1000, batch:   484/ 1052, ite: 261280] train loss: 0.003855, tar: 0.000039 
l0: 0.000058, l1: 0.000058, l2: 0.000092, l3: 0.000251, l4: 0.000714, l5: 0.001059, l6: 0.001760
[epoch: 994/1000, batch:   564/ 1052, ite: 261300] train loss: 0.003855, tar: 0.000039 
l0: 0.000080, l1: 0.000081, l2: 0.000133, l3: 0.000432, l4: 0.001129, l5: 0.002447, l6: 0.004610
[epoch: 994/1000, batch:   644/ 1052, ite: 261320] train loss: 0.003855, tar: 0.000039 
l0: 0.000051, l1: 0.000049, l2: 0.000088, l3: 0.000200, l4: 0.000595, l5: 0.000934, l6: 0.002108
[epoch: 994/1000, batch:   724/ 1052, ite: 261340] train loss: 0.003855, tar: 0.000039 
l0: 0.000103, l1: 0.000103, l2: 0.000123, l3: 0.000288, l4: 0.000534, l5: 0.001812, l6: 0.001832
[epoch: 994/1000, batch:   804/ 1052, ite: 261360] train loss: 0.003855, tar: 0.000039 
l0: 0.000025, l1: 0.000028, l2: 0.000050, l3: 0.000132, l4: 0.000413, l5: 0.001092, l6: 0.002153
[epoch: 994/1000, batch:   884/ 1052, ite: 261380] train loss: 0.003854, tar: 0.000039 
l0: 0.000000, l1: 0.000000, l2: 0.000016, l3: 0.000055, l4: 0.000298, l5: 0.000753, l6: 0.001405
[epoch: 994/1000, batch:   964/ 1052, ite: 261400] train loss: 0.003855, tar: 0.000039 
l0: 0.000014, l1: 0.000016, l2: 0.000019, l3: 0.000166, l4: 0.000448, l5: 0.001875, l6: 0.001814
[epoch: 994/1000, batch:  1044/ 1052, ite: 261420] train loss: 0.003856, tar: 0.000039 
[Epoch 994/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000114, l1: 0.000116, l2: 0.000147, l3: 0.000225, l4: 0.000467, l5: 0.001258, l6: 0.002637
[epoch: 995/1000, batch:    72/ 1052, ite: 261440] train loss: 0.003856, tar: 0.000039 
l0: 0.000015, l1: 0.000017, l2: 0.000017, l3: 0.000087, l4: 0.000214, l5: 0.000757, l6: 0.002396
[epoch: 995/1000, batch:   152/ 1052, ite: 261460] train loss: 0.003858, tar: 0.000039 
l0: 0.000090, l1: 0.000088, l2: 0.000141, l3: 0.000321, l4: 0.001016, l5: 0.002176, l6: 0.003229
[epoch: 995/1000, batch:   232/ 1052, ite: 261480] train loss: 0.003858, tar: 0.000039 
l0: 0.000002, l1: 0.000002, l2: 0.000009, l3: 0.000112, l4: 0.000373, l5: 0.000661, l6: 0.001194
[epoch: 995/1000, batch:   312/ 1052, ite: 261500] train loss: 0.003858, tar: 0.000039 
l0: 0.000010, l1: 0.000010, l2: 0.000017, l3: 0.000124, l4: 0.000304, l5: 0.001103, l6: 0.001631
[epoch: 995/1000, batch:   392/ 1052, ite: 261520] train loss: 0.003858, tar: 0.000039 
l0: 0.000039, l1: 0.000038, l2: 0.000082, l3: 0.000243, l4: 0.000934, l5: 0.001589, l6: 0.002248
[epoch: 995/1000, batch:   472/ 1052, ite: 261540] train loss: 0.003859, tar: 0.000039 
l0: 0.000077, l1: 0.000079, l2: 0.000087, l3: 0.000180, l4: 0.000335, l5: 0.001009, l6: 0.001690
[epoch: 995/1000, batch:   552/ 1052, ite: 261560] train loss: 0.003859, tar: 0.000039 
l0: 0.000033, l1: 0.000032, l2: 0.000049, l3: 0.000143, l4: 0.000348, l5: 0.000681, l6: 0.001451
[epoch: 995/1000, batch:   632/ 1052, ite: 261580] train loss: 0.003858, tar: 0.000039 
l0: 0.000020, l1: 0.000020, l2: 0.000052, l3: 0.000143, l4: 0.000470, l5: 0.001419, l6: 0.002362
[epoch: 995/1000, batch:   712/ 1052, ite: 261600] train loss: 0.003858, tar: 0.000039 
l0: 0.000002, l1: 0.000001, l2: 0.000019, l3: 0.000079, l4: 0.000287, l5: 0.000911, l6: 0.001839
[epoch: 995/1000, batch:   792/ 1052, ite: 261620] train loss: 0.003858, tar: 0.000039 
l0: 0.000000, l1: 0.000000, l2: 0.000000, l3: 0.000003, l4: 0.000131, l5: 0.000267, l6: 0.000228
[epoch: 995/1000, batch:   872/ 1052, ite: 261640] train loss: 0.003857, tar: 0.000039 
l0: 0.000005, l1: 0.000005, l2: 0.000027, l3: 0.000109, l4: 0.000500, l5: 0.000892, l6: 0.002616
[epoch: 995/1000, batch:   952/ 1052, ite: 261660] train loss: 0.003856, tar: 0.000039 
l0: 0.000016, l1: 0.000018, l2: 0.000025, l3: 0.000168, l4: 0.000411, l5: 0.001868, l6: 0.002747
[epoch: 995/1000, batch:  1032/ 1052, ite: 261680] train loss: 0.003855, tar: 0.000039 
[Epoch 995/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000056, l1: 0.000056, l2: 0.000076, l3: 0.000185, l4: 0.000570, l5: 0.001187, l6: 0.001667
[epoch: 996/1000, batch:    60/ 1052, ite: 261700] train loss: 0.003855, tar: 0.000039 
l0: 0.000024, l1: 0.000024, l2: 0.000047, l3: 0.000097, l4: 0.000350, l5: 0.000535, l6: 0.001377
[epoch: 996/1000, batch:   140/ 1052, ite: 261720] train loss: 0.003855, tar: 0.000039 
l0: 0.000006, l1: 0.000006, l2: 0.000066, l3: 0.000234, l4: 0.000507, l5: 0.001063, l6: 0.002124
[epoch: 996/1000, batch:   220/ 1052, ite: 261740] train loss: 0.003856, tar: 0.000039 
l0: 0.000030, l1: 0.000031, l2: 0.000045, l3: 0.000136, l4: 0.000460, l5: 0.000617, l6: 0.002293
[epoch: 996/1000, batch:   300/ 1052, ite: 261760] train loss: 0.003856, tar: 0.000039 
l0: 0.000045, l1: 0.000045, l2: 0.000066, l3: 0.000168, l4: 0.000360, l5: 0.000675, l6: 0.001329
[epoch: 996/1000, batch:   380/ 1052, ite: 261780] train loss: 0.003855, tar: 0.000039 
l0: 0.000004, l1: 0.000004, l2: 0.000042, l3: 0.000336, l4: 0.001170, l5: 0.001568, l6: 0.002661
[epoch: 996/1000, batch:   460/ 1052, ite: 261800] train loss: 0.003855, tar: 0.000039 
l0: 0.000071, l1: 0.000072, l2: 0.000108, l3: 0.000356, l4: 0.000957, l5: 0.001878, l6: 0.004392
[epoch: 996/1000, batch:   540/ 1052, ite: 261820] train loss: 0.003855, tar: 0.000039 
l0: 0.000099, l1: 0.000100, l2: 0.000118, l3: 0.000189, l4: 0.000411, l5: 0.000892, l6: 0.001555
[epoch: 996/1000, batch:   620/ 1052, ite: 261840] train loss: 0.003854, tar: 0.000039 
l0: 0.000009, l1: 0.000009, l2: 0.000040, l3: 0.000183, l4: 0.000612, l5: 0.001284, l6: 0.002284
[epoch: 996/1000, batch:   700/ 1052, ite: 261860] train loss: 0.003855, tar: 0.000039 
l0: 0.000006, l1: 0.000006, l2: 0.000061, l3: 0.000291, l4: 0.000526, l5: 0.001740, l6: 0.003054
[epoch: 996/1000, batch:   780/ 1052, ite: 261880] train loss: 0.003855, tar: 0.000039 
l0: 0.000019, l1: 0.000019, l2: 0.000036, l3: 0.000138, l4: 0.000301, l5: 0.000788, l6: 0.001128
[epoch: 996/1000, batch:   860/ 1052, ite: 261900] train loss: 0.003856, tar: 0.000039 
l0: 0.000047, l1: 0.000044, l2: 0.000087, l3: 0.000193, l4: 0.000328, l5: 0.001346, l6: 0.001893
[epoch: 996/1000, batch:   940/ 1052, ite: 261920] train loss: 0.003856, tar: 0.000039 
l0: 0.000004, l1: 0.000003, l2: 0.000012, l3: 0.000105, l4: 0.000300, l5: 0.000975, l6: 0.001515
[epoch: 996/1000, batch:  1020/ 1052, ite: 261940] train loss: 0.003855, tar: 0.000039 
[Epoch 996/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000063, l1: 0.000063, l2: 0.000096, l3: 0.000141, l4: 0.000272, l5: 0.000932, l6: 0.001176
[epoch: 997/1000, batch:    48/ 1052, ite: 261960] train loss: 0.003855, tar: 0.000039 
l0: 0.000010, l1: 0.000010, l2: 0.000036, l3: 0.000125, l4: 0.000269, l5: 0.001255, l6: 0.001906
[epoch: 997/1000, batch:   128/ 1052, ite: 261980] train loss: 0.003855, tar: 0.000039 
l0: 0.000106, l1: 0.000105, l2: 0.000167, l3: 0.000287, l4: 0.000725, l5: 0.001239, l6: 0.001968
[epoch: 997/1000, batch:   208/ 1052, ite: 262000] train loss: 0.003854, tar: 0.000038 
l0: 0.000005, l1: 0.000005, l2: 0.000025, l3: 0.000160, l4: 0.000601, l5: 0.001120, l6: 0.002115
[epoch: 997/1000, batch:   288/ 1052, ite: 262020] train loss: 0.003854, tar: 0.000038 
l0: 0.000046, l1: 0.000045, l2: 0.000098, l3: 0.000264, l4: 0.000610, l5: 0.001084, l6: 0.002836
[epoch: 997/1000, batch:   368/ 1052, ite: 262040] train loss: 0.003854, tar: 0.000038 
l0: 0.000036, l1: 0.000037, l2: 0.000080, l3: 0.000206, l4: 0.000690, l5: 0.001876, l6: 0.002842
[epoch: 997/1000, batch:   448/ 1052, ite: 262060] train loss: 0.003855, tar: 0.000038 
l0: 0.000008, l1: 0.000009, l2: 0.000057, l3: 0.000139, l4: 0.000390, l5: 0.000754, l6: 0.001944
[epoch: 997/1000, batch:   528/ 1052, ite: 262080] train loss: 0.003856, tar: 0.000038 
l0: 0.000022, l1: 0.000023, l2: 0.000027, l3: 0.000115, l4: 0.000323, l5: 0.000994, l6: 0.001682
[epoch: 997/1000, batch:   608/ 1052, ite: 262100] train loss: 0.003856, tar: 0.000038 
l0: 0.000027, l1: 0.000027, l2: 0.000055, l3: 0.000119, l4: 0.000549, l5: 0.002032, l6: 0.002297
[epoch: 997/1000, batch:   688/ 1052, ite: 262120] train loss: 0.003855, tar: 0.000038 
l0: 0.000081, l1: 0.000082, l2: 0.000099, l3: 0.000240, l4: 0.000381, l5: 0.000905, l6: 0.001014
[epoch: 997/1000, batch:   768/ 1052, ite: 262140] train loss: 0.003855, tar: 0.000038 
l0: 0.000013, l1: 0.000014, l2: 0.000033, l3: 0.000058, l4: 0.000159, l5: 0.000407, l6: 0.001328
[epoch: 997/1000, batch:   848/ 1052, ite: 262160] train loss: 0.003855, tar: 0.000038 
l0: 0.000058, l1: 0.000060, l2: 0.000076, l3: 0.000175, l4: 0.000517, l5: 0.001830, l6: 0.002967
[epoch: 997/1000, batch:   928/ 1052, ite: 262180] train loss: 0.003856, tar: 0.000038 
l0: 0.000004, l1: 0.000003, l2: 0.000015, l3: 0.000043, l4: 0.000263, l5: 0.000854, l6: 0.001768
[epoch: 997/1000, batch:  1008/ 1052, ite: 262200] train loss: 0.003855, tar: 0.000038 
[Epoch 997/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000069, l1: 0.000069, l2: 0.000106, l3: 0.000284, l4: 0.000614, l5: 0.001988, l6: 0.003034
[epoch: 998/1000, batch:    36/ 1052, ite: 262220] train loss: 0.003855, tar: 0.000038 
l0: 0.000005, l1: 0.000006, l2: 0.000026, l3: 0.000138, l4: 0.000258, l5: 0.000847, l6: 0.001406
[epoch: 998/1000, batch:   116/ 1052, ite: 262240] train loss: 0.003854, tar: 0.000038 
l0: 0.000068, l1: 0.000069, l2: 0.000093, l3: 0.000183, l4: 0.000495, l5: 0.001079, l6: 0.001521
[epoch: 998/1000, batch:   196/ 1052, ite: 262260] train loss: 0.003853, tar: 0.000038 
l0: 0.000003, l1: 0.000003, l2: 0.000032, l3: 0.000138, l4: 0.000632, l5: 0.001101, l6: 0.002101
[epoch: 998/1000, batch:   276/ 1052, ite: 262280] train loss: 0.003854, tar: 0.000038 
l0: 0.000008, l1: 0.000007, l2: 0.000028, l3: 0.000103, l4: 0.000306, l5: 0.000707, l6: 0.001935
[epoch: 998/1000, batch:   356/ 1052, ite: 262300] train loss: 0.003853, tar: 0.000038 
l0: 0.000094, l1: 0.000094, l2: 0.000111, l3: 0.000158, l4: 0.000438, l5: 0.001097, l6: 0.002254
[epoch: 998/1000, batch:   436/ 1052, ite: 262320] train loss: 0.003855, tar: 0.000038 
l0: 0.000002, l1: 0.000002, l2: 0.000008, l3: 0.000100, l4: 0.000140, l5: 0.000871, l6: 0.001254
[epoch: 998/1000, batch:   516/ 1052, ite: 262340] train loss: 0.003855, tar: 0.000038 
l0: 0.000037, l1: 0.000036, l2: 0.000074, l3: 0.000170, l4: 0.000476, l5: 0.001295, l6: 0.002439
[epoch: 998/1000, batch:   596/ 1052, ite: 262360] train loss: 0.003855, tar: 0.000038 
l0: 0.000001, l1: 0.000001, l2: 0.000018, l3: 0.000252, l4: 0.000315, l5: 0.001018, l6: 0.002147
[epoch: 998/1000, batch:   676/ 1052, ite: 262380] train loss: 0.003855, tar: 0.000038 
l0: 0.000049, l1: 0.000049, l2: 0.000070, l3: 0.000196, l4: 0.000549, l5: 0.001146, l6: 0.002192
[epoch: 998/1000, batch:   756/ 1052, ite: 262400] train loss: 0.003854, tar: 0.000038 
l0: 0.000017, l1: 0.000017, l2: 0.000034, l3: 0.000065, l4: 0.000175, l5: 0.000720, l6: 0.000861
[epoch: 998/1000, batch:   836/ 1052, ite: 262420] train loss: 0.003854, tar: 0.000038 
l0: 0.000006, l1: 0.000004, l2: 0.000032, l3: 0.000290, l4: 0.000453, l5: 0.001199, l6: 0.002308
[epoch: 998/1000, batch:   916/ 1052, ite: 262440] train loss: 0.003855, tar: 0.000038 
l0: 0.000034, l1: 0.000034, l2: 0.000074, l3: 0.000141, l4: 0.000361, l5: 0.001244, l6: 0.001981
[epoch: 998/1000, batch:   996/ 1052, ite: 262460] train loss: 0.003855, tar: 0.000038 
[Epoch 998/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000028, l1: 0.000028, l2: 0.000041, l3: 0.000129, l4: 0.000380, l5: 0.001103, l6: 0.002071
[epoch: 999/1000, batch:    24/ 1052, ite: 262480] train loss: 0.003855, tar: 0.000038 
l0: 0.000095, l1: 0.000096, l2: 0.000122, l3: 0.000221, l4: 0.000511, l5: 0.001706, l6: 0.002471
[epoch: 999/1000, batch:   104/ 1052, ite: 262500] train loss: 0.003854, tar: 0.000038 
l0: 0.000012, l1: 0.000011, l2: 0.000059, l3: 0.000119, l4: 0.000512, l5: 0.001205, l6: 0.001522
[epoch: 999/1000, batch:   184/ 1052, ite: 262520] train loss: 0.003855, tar: 0.000038 
l0: 0.000009, l1: 0.000009, l2: 0.000011, l3: 0.000088, l4: 0.000274, l5: 0.000898, l6: 0.001312
[epoch: 999/1000, batch:   264/ 1052, ite: 262540] train loss: 0.003855, tar: 0.000038 
l0: 0.000017, l1: 0.000015, l2: 0.000099, l3: 0.000289, l4: 0.000776, l5: 0.001733, l6: 0.002654
[epoch: 999/1000, batch:   344/ 1052, ite: 262560] train loss: 0.003855, tar: 0.000038 
l0: 0.000053, l1: 0.000054, l2: 0.000100, l3: 0.000255, l4: 0.000702, l5: 0.003366, l6: 0.003571
[epoch: 999/1000, batch:   424/ 1052, ite: 262580] train loss: 0.003855, tar: 0.000038 
l0: 0.000144, l1: 0.000144, l2: 0.000187, l3: 0.000415, l4: 0.000610, l5: 0.001233, l6: 0.002806
[epoch: 999/1000, batch:   504/ 1052, ite: 262600] train loss: 0.003856, tar: 0.000038 
l0: 0.000001, l1: 0.000001, l2: 0.000011, l3: 0.000122, l4: 0.000388, l5: 0.000896, l6: 0.002292
[epoch: 999/1000, batch:   584/ 1052, ite: 262620] train loss: 0.003855, tar: 0.000038 
l0: 0.000008, l1: 0.000007, l2: 0.000040, l3: 0.000155, l4: 0.000385, l5: 0.001033, l6: 0.002125
[epoch: 999/1000, batch:   664/ 1052, ite: 262640] train loss: 0.003855, tar: 0.000038 
l0: 0.000030, l1: 0.000033, l2: 0.000041, l3: 0.000127, l4: 0.000307, l5: 0.001001, l6: 0.003254
[epoch: 999/1000, batch:   744/ 1052, ite: 262660] train loss: 0.003855, tar: 0.000038 
l0: 0.000033, l1: 0.000037, l2: 0.000040, l3: 0.000135, l4: 0.000345, l5: 0.000770, l6: 0.001568
[epoch: 999/1000, batch:   824/ 1052, ite: 262680] train loss: 0.003854, tar: 0.000038 
l0: 0.000019, l1: 0.000019, l2: 0.000044, l3: 0.000082, l4: 0.000309, l5: 0.000665, l6: 0.000797
[epoch: 999/1000, batch:   904/ 1052, ite: 262700] train loss: 0.003854, tar: 0.000038 
l0: 0.000026, l1: 0.000027, l2: 0.000065, l3: 0.000149, l4: 0.000412, l5: 0.001017, l6: 0.002060
[epoch: 999/1000, batch:   984/ 1052, ite: 262720] train loss: 0.003855, tar: 0.000038 
[Epoch 999/1000] Test loss: 0.015, Test target loss: 0.002
l0: 0.000067, l1: 0.000066, l2: 0.000121, l3: 0.000306, l4: 0.000531, l5: 0.001127, l6: 0.002351
[epoch: 1000/1000, batch:    12/ 1052, ite: 262740] train loss: 0.003855, tar: 0.000038 
l0: 0.000039, l1: 0.000040, l2: 0.000065, l3: 0.000299, l4: 0.000675, l5: 0.001276, l6: 0.002588
[epoch: 1000/1000, batch:    92/ 1052, ite: 262760] train loss: 0.003855, tar: 0.000038 
l0: 0.000002, l1: 0.000002, l2: 0.000004, l3: 0.000053, l4: 0.000297, l5: 0.000900, l6: 0.002007
[epoch: 1000/1000, batch:   172/ 1052, ite: 262780] train loss: 0.003854, tar: 0.000038 
l0: 0.000002, l1: 0.000001, l2: 0.000044, l3: 0.000148, l4: 0.000295, l5: 0.000936, l6: 0.001697
[epoch: 1000/1000, batch:   252/ 1052, ite: 262800] train loss: 0.003854, tar: 0.000038 
l0: 0.000032, l1: 0.000034, l2: 0.000054, l3: 0.000138, l4: 0.000490, l5: 0.001072, l6: 0.002223
[epoch: 1000/1000, batch:   332/ 1052, ite: 262820] train loss: 0.003854, tar: 0.000038 
l0: 0.000005, l1: 0.000004, l2: 0.000017, l3: 0.000098, l4: 0.000369, l5: 0.000863, l6: 0.001127
[epoch: 1000/1000, batch:   412/ 1052, ite: 262840] train loss: 0.003853, tar: 0.000038 
l0: 0.000108, l1: 0.000109, l2: 0.000121, l3: 0.000181, l4: 0.000415, l5: 0.001118, l6: 0.002313
[epoch: 1000/1000, batch:   492/ 1052, ite: 262860] train loss: 0.003854, tar: 0.000038 
l0: 0.000004, l1: 0.000003, l2: 0.000021, l3: 0.000100, l4: 0.000365, l5: 0.000786, l6: 0.001637
[epoch: 1000/1000, batch:   572/ 1052, ite: 262880] train loss: 0.003853, tar: 0.000038 
l0: 0.000073, l1: 0.000073, l2: 0.000087, l3: 0.000260, l4: 0.000436, l5: 0.000928, l6: 0.002686
[epoch: 1000/1000, batch:   652/ 1052, ite: 262900] train loss: 0.003854, tar: 0.000038 
l0: 0.000157, l1: 0.000162, l2: 0.000169, l3: 0.000236, l4: 0.000367, l5: 0.001198, l6: 0.002343
[epoch: 1000/1000, batch:   732/ 1052, ite: 262920] train loss: 0.003855, tar: 0.000038 
l0: 0.000001, l1: 0.000001, l2: 0.000011, l3: 0.000079, l4: 0.000451, l5: 0.000490, l6: 0.001707
[epoch: 1000/1000, batch:   812/ 1052, ite: 262940] train loss: 0.003855, tar: 0.000038 
l0: 0.000004, l1: 0.000004, l2: 0.000009, l3: 0.000092, l4: 0.000280, l5: 0.001040, l6: 0.001559
[epoch: 1000/1000, batch:   892/ 1052, ite: 262960] train loss: 0.003856, tar: 0.000038 
l0: 0.000032, l1: 0.000032, l2: 0.000046, l3: 0.000128, l4: 0.000333, l5: 0.001127, l6: 0.001422
[epoch: 1000/1000, batch:   972/ 1052, ite: 262980] train loss: 0.003855, tar: 0.000038 
l0: 0.000077, l1: 0.000079, l2: 0.000133, l3: 0.000294, l4: 0.001183, l5: 0.002698, l6: 0.004230
[epoch: 1000/1000, batch:  1052/ 1052, ite: 263000] train loss: 0.003856, tar: 0.000038 
模型已保存至: /root/uiu/saved_models/uiunet/uiunet_iter_263000.pkl
[Epoch 1000/1000] Test loss: 0.015, Test target loss: 0.002
训练完成!
Loss curve saved at: /root/uiu/saved_models/uiunet/uiunet_loss_curve.png
Loss data saved at: /root/uiu/saved_models/uiunet/uiunet_loss_data.csv
